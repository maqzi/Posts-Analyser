AcceptedAnswerId,AnswerCount,Body,ClosedDate,CommentCount,CreationDate,FavoriteCount,Id,LastActivityDate,LastEditDate,LastEditorDisplayName,LastEditorUserId,OwnerDisplayName,OwnerUserId,ParentId,PostTypeId,Score,Tags,Title,ViewCount,Flesch_Reading_Ease_Value,Coleman_Liau_Index_Value,Dale_Chall_Readability_Score,Code_Count,Latex_Count,Punc_Count,Clean_Text
3.0,3.0,"<p>What does ""backprop"" mean? I've Googled it, but it's showing backpropagation.</p>

<p>Is the ""backprop"" term basically the same as ""backpropagation"" or does it have a different meaning?</p>
",,3,2016-08-02T15:39:14.947,,1,2016-08-18T13:20:02.773,2016-08-18T13:20:02.773,,42.0,,8.0,,1,5,<neural-networks><definitions>,"What is ""backprop""?",169.0,62.34,13.37,8.76,0.0,0.0,12.0,What does backprop mean Ive Googled it but its showing backpropagation Is the backprop term basically the same as backpropagation or does it have a different meaning
9.0,2.0,"<p>Does increasing the noise in data help to improve the learning ability of a network? Does it make any difference or does it depend on the problem being solved? How is it affect the generalization process overall?</p>
",,0,2016-08-02T15:40:20.623,,2,2016-08-02T18:42:34.193,2016-08-02T18:42:34.193,,128.0,,8.0,,1,7,<generalization>,How does noise affect generalization?,57.0,58.99,9.44,10.65,0.0,0.0,3.0,Does increasing the noise in data help to improve the learning ability of a network Does it make any difference or does it depend on the problem being solved How is it affect the generalization process overall
,,"<p>""Backprop"" is the same as ""backpropagation"": it's just a shorter way to say it. It is sometimes abbreviated as ""BP"".</p>
",,0,2016-08-02T15:40:24.820,,3,2016-08-02T15:40:24.820,,,,,4.0,1.0,2,10,,,,78.25,9.37,8.08,0.0,0.0,10.0,Backprop is the same as backpropagation its just a shorter way to say it It is sometimes abbreviated as BP
12.0,4.0,"<p>When you're writing your algorithm, how do you know how many neurons you need per single layer? Are there any methods for finding the optimal number of them, or is it a rule of thumb?</p>
",,0,2016-08-02T15:41:22.020,3.0,4,2016-10-11T01:02:00.430,2016-08-11T12:22:08.003,,145.0,,8.0,,1,16,<deep-network><neurons>,How to find the optimal number of neurons per layer?,456.0,79.09,7.13,9.47,0.0,0.0,5.0,When youre writing your algorithm how do you know how many neurons you need per single layer Are there any methods for finding the optimal number of them or is it a rule of thumb
14.0,2.0,"<p>I have a LEGO Mindstorms EV3 and I'm wondering if there's any way I could start coding the bot in Python rather than the default drag-and-drop system. Is a Mindstorm considered AI?</p>

<p>Is this possible?</p>

<hr>

<p>My goal is to write a basic walking program in Python. The bot is the EV3RSTORM. I searched and found <a href=""http://bitsandbricks.no/2014/01/19/getting-started-with-python-on-ev3/"" rel=""nofollow"">this</a>, but don't understand it. </p>
",2016-08-02T16:27:32.070,4,2016-08-02T15:42:08.177,,5,2016-11-13T21:04:39.963,2016-11-13T21:04:39.963,,8.0,,5.0,,1,0,<mindstorms>,How to program AI in Mindstorms,202.0,86.71,7.51,9.66,0.0,0.0,12.0,I have a LEGO Mindstorms EV3 and Im wondering if theres any way I could start coding the bot in Python rather than the default draganddrop system Is a Mindstorm considered AI Is this possible My goal is to write a basic walking program in Python The bot is the EV3RSTORM I searched and found this but dont understand it
20.0,2.0,"<p>The intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge. (paraphrased from <a href=""http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#Intelligent_agent_definition"" rel=""nofollow"">Wikipedia</a>)</p>

<p>Does this mean that humans are not intelligent? I think we all make mistakes that imply that we are not maximizing the expected value of a performance measure.</p>
",,0,2016-08-02T15:43:35.460,,6,2016-08-05T23:21:31.713,,,,,29.0,,1,3,<intelligent-agent><philosophy>,On the intelligent agent definition of intelligence,55.0,33.54,12.82,9.76,0.0,0.0,5.0,The intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge paraphrased from Wikipedia Does this mean that humans are not intelligent I think we all make mistakes that imply that we are not maximizing the expected value of a performance measure
,6.0,"<p>This quote by Stephen Hawking has been in headlines for quite some time:</p>

<blockquote>
  <p>Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants.</p>
</blockquote>

<p>Why does he say this? To put it simply in layman terms: what are the possible threats from AI? If we know that AI is so dangerous why are we still promoting it? Why is it not banned?</p>

<p>What are the adverse consequences of the so called <a href=""https://en.wikipedia.org/wiki/Technological_singularity"" rel=""nofollow"">Technological Singularity</a>? </p>
",2016-08-04T01:36:40.283,6,2016-08-02T15:45:09.070,1.0,7,2016-08-04T14:09:43.583,2016-08-04T14:09:43.583,,95.0,,26.0,,1,7,<intelligent-agent>,"Why does Stephen Hawking say ""Artificial Intelligence will kill us all""?",201.0,75.0,7.99,8.09,0.0,0.0,8.0,This quote by Stephen Hawking has been in headlines for quite some time Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants Why does he say this To put it simply in layman terms what are the possible threats from AI If we know that AI is so dangerous why are we still promoting it Why is it not banned What are the adverse consequences of the so called Technological Singularity
,,"<p>You can use <a href=""https://github.com/topikachu/python-ev3"" rel=""nofollow"">python-ev3</a> which can be used to program Lego Mindstorms EV3 using Python on ev3dev.</p>

<p>See: <a href=""http://www.ev3dev.org/docs/tutorials/setting-up-python-pycharm/"" rel=""nofollow"">Setting Up a Python Development Environment with PyCharm</a></p>
",,0,2016-08-02T15:45:48.597,,8,2016-08-02T15:45:48.597,,,,,8.0,5.0,2,2,,,,58.28,11.3,11.57,0.0,0.0,3.0,You can use pythonev3 which can be used to program Lego Mindstorms EV3 using Python on ev3dev See Setting Up a Python Development Environment with PyCharm
,,"<p>Noise in the data, to a reasonable amount, may help the network to generalize better. Sometime, it has the opposite effect. It partly depends on the kind of noise (""true"" vs. artificial).</p>

<p>The <a href=""ftp://ftp.sas.com/pub/neural/FAQ3.html#A_noise"">AI FAQ on ANN</a> gives a good overview. Except:</p>

<blockquote>
  <p>Noise in the actual data is never a good thing, since it limits the accuracy of generalization that can be achieved no matter how extensive the training set is. On the other hand, injecting artificial noise (jitter) into the inputs during training is one of several ways to improve generalization for smooth functions when you have a small training set.</p>
</blockquote>

<p>In some field, such as computer vision, it's common to increase the size of the training set by copying some samples and adding some noises or other transformation.</p>
",,0,2016-08-02T15:47:02.993,,9,2016-08-02T15:47:02.993,,,,,4.0,2.0,2,6,,,,61.06,10.68,9.9,0.0,0.0,23.0,Noise in the data to a reasonable amount may help the network to generalize better Sometime it has the opposite effect It partly depends on the kind of noise true vs artificial The AI FAQ on ANN gives a good overview Except Noise in the actual data is never a good thing since it limits the accuracy of generalization that can be achieved no matter how extensive the training set is On the other hand injecting artificial noise jitter into the inputs during training is one of several ways to improve generalization for smooth functions when you have a small training set In some field such as computer vision its common to increase the size of the training set by copying some samples and adding some noises or other transformation
32.0,3.0,"<p>I'm new to A.I. and I'd like to know in simple words, what is the fuzzy logic concept? How does it help, and when is it used?</p>
",,0,2016-08-02T15:47:56.593,6.0,10,2016-08-18T11:35:55.793,2016-08-18T11:35:55.793,,145.0,,8.0,,1,15,<deep-network><fuzzy-logic>,What is fuzzy logic?,283.0,104.64,2.23,7.01,0.0,0.0,8.0,Im new to AI and Id like to know in simple words what is the fuzzy logic concept How does it help and when is it used
,,"<p>We typically think of machine learning models as modeling two different parts of the training data--the underlying generalizable truth (the signal), and the randomness specific to that dataset (the noise).</p>

<p>Fitting both of those parts increases training set accuracy, but fitting the signal also increases test set accuracy (and real-world performance) while fitting the noise decreases both. So we use things like regularization and dropout and similar techniques in order to make it harder to fit the noise, and so more likely to fit the signal.</p>

<p>Just increasing the amount of noise in the training data is one such approach, but seems unlikely to be as useful. Compare random jitter to adversarial boosting, for example; the first will slowly and indirectly improve robustness whereas the latter will dramatically and directly improve it.</p>
",,0,2016-08-02T15:48:56.970,,11,2016-08-02T15:48:56.970,,,,,10.0,2.0,2,6,,,,36.22,14.1,11.29,0.0,0.0,20.0,We typically think of machine learning models as modeling two different parts of the training datathe underlying generalizable truth the signal and the randomness specific to that dataset the noise Fitting both of those parts increases training set accuracy but fitting the signal also increases test set accuracy and realworld performance while fitting the noise decreases both So we use things like regularization and dropout and similar techniques in order to make it harder to fit the noise and so more likely to fit the signal Just increasing the amount of noise in the training data is one such approach but seems unlikely to be as useful Compare random jitter to adversarial boosting for example the first will slowly and indirectly improve robustness whereas the latter will dramatically and directly improve it
,,"<p>There is no direct way to find the optimal number of them: people empirically try and see (e.g., using cross-validation). The most common search techniques are random, manual, and grid searches. </p>

<p>There exist more advanced techniques such as Gaussian processes, e.g. <em><a href=""http://arxiv.org/abs/1609.08703"" rel=""nofollow"">Optimizing Neural Network Hyperparameters with Gaussian Processes for Dialog Act Classification</a>, IEEE SLT 2016</em>.</p>
",,0,2016-08-02T15:50:27.867,,12,2016-09-29T00:24:06.177,2016-09-29T00:24:06.177,,4.0,,4.0,4.0,2,9,,,,49.01,16.06,10.92,0.0,0.0,16.0,There is no direct way to find the optimal number of them people empirically try and see eg using crossvalidation The most common search techniques are random manual and grid searches There exist more advanced techniques such as Gaussian processes eg Optimizing Neural Network Hyperparameters with Gaussian Processes for Dialog Act Classification IEEE SLT 2016
,1.0,"<p>In particular, an embedded computer (limited resources) analyzes live video stream from a traffic camera, trying to pick good frames that contain license plate numbers of passing cars. Once a plate is located, the frame is handed over to an OCR library to extract the registration and use it further.</p>

<p>In my country two types of license plates are in common use - rectangular (the typical) and square - actually, somewhat rectangular but ""higher than wider"", with the registration split over two rows.</p>

<p>(there are some more types, but let us disregard them; they are a small percent and usually belong to vehicles that lie outside our interest.)</p>

<p>Due to the limited resources and need for rapid, realtime processing, the maximum size of the network (number of cells and connections) the system can handle is fixed.</p>

<p>Would it be better to split this into two smaller networks, each recognizing one type of registration plates, or will the larger single network handle the two types better?</p>
",,1,2016-08-02T15:52:19.413,,13,2016-08-05T10:57:19.847,,,,,38.0,,1,3,<neural-networks><image-recognition>,"Can a single neural network handle recognizing two types of objects, or should it be split into two smaller networks?",37.0,43.87,12.19,10.8,0.0,0.0,29.0,In particular an embedded computer limited resources analyzes live video stream from a traffic camera trying to pick good frames that contain license plate numbers of passing cars Once a plate is located the frame is handed over to an OCR library to extract the registration and use it further In my country two types of license plates are in common use rectangular the typical and square actually somewhat rectangular but higher than wider with the registration split over two rows there are some more types but let us disregard them they are a small percent and usually belong to vehicles that lie outside our interest Due to the limited resources and need for rapid realtime processing the maximum size of the network number of cells and connections the system can handle is fixed Would it be better to split this into two smaller networks each recognizing one type of registration plates or will the larger single network handle the two types better
,,"<blockquote>
  <p>Is a Mindstorm considered AI?</p>
</blockquote>

<p>This depends on what type of software you write in it... The algorithms you write could be seen as AI. </p>

<p>You can absolutely use Python to progam it (or java or other languages). Check <a href=""http://bitsandbricks.no/2014/01/19/getting-started-with-python-on-ev3/"" rel=""nofollow"">this link</a> for a tutorial. </p>
",,0,2016-08-02T15:52:24.380,,14,2016-08-02T15:52:24.380,,,,,52.0,5.0,2,3,,,,79.46,6.52,8.38,0.0,0.0,9.0,Is a Mindstorm considered AI This depends on what type of software you write in it The algorithms you write could be seen as AI You can absolutely use Python to progam it or java or other languages Check this link for a tutorial
,4.0,"<p>The <a href=""https://en.wikipedia.org/wiki/Turing_test"">Turing Test</a> was the first test of artificial intelligence and is now a bit outdated. The <a href=""https://en.wikipedia.org/wiki/Turing_test#Total_Turing_test"">Total Turing Test</a> aims to be a more modern test which requires a much more sophisticated system. What techniques can we use to identify an artificial intelligence (weak AI) and an <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"">artificial general intelligence</a> (strong AI)?</p>
",,2,2016-08-02T15:52:50.827,3.0,15,2017-01-24T01:06:35.170,2016-08-04T14:10:10.990,,95.0,,9.0,,1,18,<turing-test><strong-ai><intelligent-agent><weak-ai>,"Is the Turing Test, or any of its variants, a reliable test of artificial intelligence?",342.0,53.51,11.42,8.09,0.0,0.0,7.0,The Turing Test was the first test of artificial intelligence and is now a bit outdated The Total Turing Test aims to be a more modern test which requires a much more sophisticated system What techniques can we use to identify an artificial intelligence weak AI and an artificial general intelligence strong AI
142.0,1.0,"<p>What is the ""early stopping"" and what are the advantages using this method? How does it help exactly.</p>
",2016-08-11T11:08:37.137,0,2016-08-02T15:53:00.447,,16,2016-08-03T11:54:30.310,,,,,8.0,,1,4,<generalization><definitions>,What is early stopping?,62.0,79.26,8.03,9.35,0.0,0.0,4.0,What is the early stopping and what are the advantages using this method How does it help exactly
45.0,3.0,"<p>I've heard the idea of the technological singularity, what is it and how does it relate to Artificial Intelligence?  Is this the theoretical point where Artificial Intelligence machines have progressed to the point where they grow and learn on their own beyond what humans can do and their growth takes off?  How would we know when we reach this point?</p>
",,2,2016-08-02T15:53:38.273,3.0,17,2016-08-11T10:34:47.403,2016-08-04T16:26:03.963,,55.0,,55.0,,1,13,<self-learning><singularity>,What is the concept of the technological singularity?,193.0,59.64,10.85,7.79,0.0,0.0,5.0,Ive heard the idea of the technological singularity what is it and how does it relate to Artificial Intelligence Is this the theoretical point where Artificial Intelligence machines have progressed to the point where they grow and learn on their own beyond what humans can do and their growth takes off How would we know when we reach this point
,,"<blockquote>
  <p>To put it simply in layman terms, what are the possible threats from AI? </p>
</blockquote>

<p>Currently, there are no threat. </p>

<p>The threat comes if humans create a so-called ultraintelligent machine, a machine that can surpass all intellectual activities by any human. This would be the last invention man would need to do, since this machine is better in inventing machines than humans are (since that is an intellectual activity).  However, this could cause the machine to invent machines that can destruct humans, and we can't stop them because they are so much smarter than we are.</p>

<p>This is all hypothetical, no one has even a clue of what an ultraintelligent machine looks like. </p>

<blockquote>
  <p>If we know that AI is so dangerous why are we still promoting it? Why is it not banned?</p>
</blockquote>

<p>As I said before, the existence of a ultraintelligent machine is hypothetical. Artificial Intelligence has lots of useful applications (more than this answer can contain), and if we develop it, we get even more useful applications. We just have to be careful that the machines won't overtake us. </p>
",,0,2016-08-02T15:54:26.937,,18,2016-08-02T15:54:26.937,,,,,29.0,7.0,2,2,,,,54.93,10.44,8.33,0.0,0.0,28.0,To put it simply in layman terms what are the possible threats from AI Currently there are no threat The threat comes if humans create a socalled ultraintelligent machine a machine that can surpass all intellectual activities by any human This would be the last invention man would need to do since this machine is better in inventing machines than humans are since that is an intellectual activity However this could cause the machine to invent machines that can destruct humans and we cant stop them because they are so much smarter than we are This is all hypothetical no one has even a clue of what an ultraintelligent machine looks like If we know that AI is so dangerous why are we still promoting it Why is it not banned As I said before the existence of a ultraintelligent machine is hypothetical Artificial Intelligence has lots of useful applications more than this answer can contain and if we develop it we get even more useful applications We just have to be careful that the machines wont overtake us
,,"<p>Because he did not yet know how far away current AI is... Working in an media AI lab, I get this question a lot. But really... we are still a long way from this. The robots still do everything we detailledly describe them to do. Instead of seeing the robot as intelligent, I would look to the human programmer for where the creativity really happens.</p>
",,0,2016-08-02T15:54:29.263,,19,2016-08-02T15:54:29.263,,,,,52.0,7.0,2,2,,,,83.66,7.35,8.9,0.0,0.0,12.0,Because he did not yet know how far away current AI is Working in an media AI lab I get this question a lot But really we are still a long way from this The robots still do everything we detailledly describe them to do Instead of seeing the robot as intelligent I would look to the human programmer for where the creativity really happens
,,"<p>It rather depends on how one defines several of the terms used. For example:</p>

<ul>
<li>Whether the term `expected' is interpreted in a formal (i.e.
statistical) sense.  </li>
<li>Whether it's assumed that humans have any kind of utilitarian
`performance measure'.</li>
</ul>

<p>The motivation for this description of `agent' arose from a desire to have a quantitative model - it's not clear that such a model is a good fit for human cognition.</p>

<p>However, there are alternative definitions of agents, for example the <a href=""https://en.wikipedia.org/wiki/Belief%E2%80%93desire%E2%80%93intention_software_model"" rel=""nofollow"">BDI model</a> which are rather more open-ended and hence more obviously applicable to humans.</p>
",,0,2016-08-02T15:54:45.237,,20,2016-08-03T10:00:57.600,2016-08-03T10:00:57.600,,42.0,,42.0,6.0,2,2,,,,44.34,12.71,10.56,0.0,0.0,22.0,It rather depends on how one defines several of the terms used For example Whether the term expected is interpreted in a formal ie statistical sense Whether its assumed that humans have any kind of utilitarian performance measure The motivation for this description of agent arose from a desire to have a quantitative model its not clear that such a model is a good fit for human cognition However there are alternative definitions of agents for example the BDI model which are rather more openended and hence more obviously applicable to humans
,0.0,"<p>I'm worry that my network become too complex. I don't want to end up with half of the network does nothing, but just take space and resources.</p>

<p>So, what are the techniques of detecting and preventing overfitting to avoid such problem?</p>
",,0,2016-08-02T15:55:15.957,,21,2016-08-02T15:55:15.957,,,,,8.0,,1,0,<deep-network><overfitting><optimization>,What are the methods of optimizing overfitted models?,9.0,74.49,9.45,8.94,0.0,0.0,7.0,Im worry that my network become too complex I dont want to end up with half of the network does nothing but just take space and resources So what are the techniques of detecting and preventing overfitting to avoid such problem
,,"<p>It's not just Hawking, you hear variations on this refrain from a lot of people.  And given that they're mostly very smart, well educated, well informed people (Elon Musk is another, for example), it probably shouldn't be dismissed out of hand.</p>

<p>Anyway, the basic idea seems to be this: If we create ""real"" artificial intelligence, at some point, it will be able to improve itself, which improves it's ability to improve itself, which means it can improve it's ability to improve itself even more, and so on... a runaway cascade leading to ""superhuman intelligence"".  That is to say, leading to something that more intelligent than we area.</p>

<p>So what happens if there is an entity on this planet which is literally more intelligent than us (humans)? Would it be a threat to us?  Well, it certainly seems reasonable to speculate that it <em>could</em> be so.   OTOH, we have no particular reason, right now, to think that it <em>will</em> be so. </p>

<p>So it seems that Hawking, Musk, etc. are just coming down on the more cautious / fearful side of things.  Since we don't <em>know</em> if a superhuman AI will be dangerous or not, and given that it could be unstoppable if it were to become malicious (remember, it's smarter than we are!), it's a reasonable thing to take under consideration.</p>

<p>Eliezer Yudkowsky has also written quite a bit on this subject, including come up with the famous ""AI Box"" experiment.  I think anybody interested in this topic should read some of his material.</p>

<p><a href=""http://www.yudkowsky.net/singularity/aibox/"" rel=""nofollow"">http://www.yudkowsky.net/singularity/aibox/</a></p>
",,2,2016-08-02T15:56:10.167,,22,2016-08-02T15:56:10.167,,,,,33.0,7.0,2,4,,,,62.78,10.73,8.91,0.0,0.0,69.0,Its not just Hawking you hear variations on this refrain from a lot of people And given that theyre mostly very smart well educated well informed people Elon Musk is another for example it probably shouldnt be dismissed out of hand Anyway the basic idea seems to be this If we create real artificial intelligence at some point it will be able to improve itself which improves its ability to improve itself which means it can improve its ability to improve itself even more and so on a runaway cascade leading to superhuman intelligence That is to say leading to something that more intelligent than we area So what happens if there is an entity on this planet which is literally more intelligent than us humans Would it be a threat to us Well it certainly seems reasonable to speculate that it could be so OTOH we have no particular reason right now to think that it will be so So it seems that Hawking Musk etc are just coming down on the more cautious fearful side of things Since we dont know if a superhuman AI will be dangerous or not and given that it could be unstoppable if it were to become malicious remember its smarter than we are its a reasonable thing to take under consideration Eliezer Yudkowsky has also written quite a bit on this subject including come up with the famous AI Box experiment I think anybody interested in this topic should read some of his material httpwwwyudkowskynetsingularityaibox
,,"<p>As Andrew Ng <a href=""http://www.theregister.co.uk/2015/03/19/andrew_ng_baidu_ai/"" rel=""nofollow noreferrer"">said</a>, worrying about such threat from AI is like worrying about of overpopulation on Mars. It is science fiction. </p>

<p><a href=""https://i.stack.imgur.com/m6jnl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m6jnl.png"" alt=""enter image description here""></a></p>

<p>That being said, given the rise of (much weaker) robots and other (semi-)autonomous agents, the fields of the law and ethics are increasingly incorporating them, e.g. see <a href=""https://en.wikipedia.org/wiki/Roboethics"" rel=""nofollow noreferrer"">Roboethics</a>.</p>
",,0,2016-08-02T15:57:19.303,,23,2016-08-02T15:57:19.303,,,,,4.0,7.0,2,3,,,,62.98,12.58,9.2,0.0,0.0,14.0,As Andrew Ng said worrying about such threat from AI is like worrying about of overpopulation on Mars It is science fiction That being said given the rise of much weaker robots and other semiautonomous agents the fields of the law and ethics are increasingly incorporating them eg see Roboethics
,,"<p>He says this because it can happen. If something becomes smarter than us, why would it continue to serve us? The worst case scenario is that it takes over all manufacturing processes and consumes all matter to convert it into material capable of computation, extending outward infinitely until all matter is consumed.</p>

<p>We know that AI is dangerous but it doesn't matter because most people don't believe in it. It goes against every comfort religion has to offer. Man is the end-all-be-all of the universe and if that fact is disputed, people will feel out of place and purposeless.</p>

<p>The fact is most people just don't acknowledge it's possible, or that it will happen in our lifetimes, even though many reputable AI experts put the occurrence of the singularity within two decades. If people truly acknowledged that AI that was smarter than them was possible, wouldn't they be living differently? Wouldn't they be looking to do things that they enjoy, knowing that whatever it is they do that they dread will be automated? Wouldn't everyone be calling for a universal basic income?</p>

<p>The other reason we don't ban it is because its promise is so great. One researcher could be augmented by 1,000 digital research assistants. All manual labor could be automated. For the first time, technology offers us real freedom to do whatever we please.</p>

<p>But even in this best case scenario where it doesn't overtake us, humans still have to adapt and alter their economic system to one where labor isn't necessary. Otherwise, those who aren't technically-trained will starve and revolt.</p>
",,2,2016-08-02T15:57:48.363,,24,2016-08-02T16:46:21.237,2016-08-02T16:46:21.237,,56.0,,56.0,7.0,2,2,,,,54.83,11.31,9.61,0.0,0.0,42.0,He says this because it can happen If something becomes smarter than us why would it continue to serve us The worst case scenario is that it takes over all manufacturing processes and consumes all matter to convert it into material capable of computation extending outward infinitely until all matter is consumed We know that AI is dangerous but it doesnt matter because most people dont believe in it It goes against every comfort religion has to offer Man is the endallbeall of the universe and if that fact is disputed people will feel out of place and purposeless The fact is most people just dont acknowledge its possible or that it will happen in our lifetimes even though many reputable AI experts put the occurrence of the singularity within two decades If people truly acknowledged that AI that was smarter than them was possible wouldnt they be living differently Wouldnt they be looking to do things that they enjoy knowing that whatever it is they do that they dread will be automated Wouldnt everyone be calling for a universal basic income The other reason we dont ban it is because its promise is so great One researcher could be augmented by 1000 digital research assistants All manual labor could be automated For the first time technology offers us real freedom to do whatever we please But even in this best case scenario where it doesnt overtake us humans still have to adapt and alter their economic system to one where labor isnt necessary Otherwise those who arent technicallytrained will starve and revolt
,,"<p>There are a number of long resources to answer this sort of question: consider Stuart Armstrong's book <a href=""http://rads.stackoverflow.com/amzn/click/B00IB4N4KU"" rel=""nofollow"">Smarter Than Us</a>, Nick Bostrom's book <a href=""http://rads.stackoverflow.com/amzn/click/B00LOOCGB2"" rel=""nofollow"">Superintelligence</a>, which grew out of this <a href=""http://www.nickbostrom.com/views/superintelligence.pdf"" rel=""nofollow"">edge.org answer</a>, <a href=""http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html"" rel=""nofollow"">Tim Urban's explanation</a>, or <a href=""https://aisafety.wordpress.com/"" rel=""nofollow"">Michael Cohen's explanation</a>.</p>

<p>But here's my (somewhat shorter) answer: intelligence is all about decision-making, and we don't have any reason to believe that humans are anywhere near close to being the best possible at decision-making. Once we are able to build an AI AI researcher (that is, a computer that knows how to make computers better at thinking), the economic and military relevance of humans will rapidly disappear as any decision that could be made by a human could be made better by a computer. (Why have human generals instead of robot generals, human engineers instead of robot engineers, and so on.)</p>

<p>This isn't necessarily a catastrophe. If the Vulcans showed up tomorrow and brought better decision-making to Earth, we could avoid a lot of misery. The hard part is making sure that what we get are Vulcans who want us around and happy, instead of something that doesn't share our values.</p>
",,0,2016-08-02T15:58:13.970,,25,2016-08-02T15:58:13.970,,,,,10.0,7.0,2,3,,,,47.72,12.31,10.12,0.0,0.0,38.0,There are a number of long resources to answer this sort of question consider Stuart Armstrongs book Smarter Than Us Nick Bostroms book Superintelligence which grew out of this edgeorg answer Tim Urbans explanation or Michael Cohens explanation But heres my somewhat shorter answer intelligence is all about decisionmaking and we dont have any reason to believe that humans are anywhere near close to being the best possible at decisionmaking Once we are able to build an AI AI researcher that is a computer that knows how to make computers better at thinking the economic and military relevance of humans will rapidly disappear as any decision that could be made by a human could be made better by a computer Why have human generals instead of robot generals human engineers instead of robot engineers and so on This isnt necessarily a catastrophe If the Vulcans showed up tomorrow and brought better decisionmaking to Earth we could avoid a lot of misery The hard part is making sure that what we get are Vulcans who want us around and happy instead of something that doesnt share our values
189.0,5.0,"<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  </p>

<ol>
<li><p>What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  </p></li>
<li><p>Are there examples where this is already happening to a degree today?  </p></li>
<li><p>Wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  </p>

<p>Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p></li>
</ol>
",,1,2016-08-02T15:58:31.413,3.0,26,2016-12-23T05:52:15.560,2016-12-23T05:52:15.560,,2444.0,,55.0,,1,13,<turing-test><emotional-intelligence>,How could emotional intelligence be implemented?,312.0,26.61,16.3,11.0,0.0,0.0,12.0,Ive seen emotional intelligence defined as the capacity to be aware of control and express ones emotions and to handle interpersonal relationships judiciously and empathetically What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers Are there examples where this is already happening to a degree today Wouldnt a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer Perhaps that is why early programs that pass the test represented young people who presumably have lower emotional intelligence
,,"<p>The problem of the Turing Test is that it tests the machines ability to resemble humans. Not necessarily every form of AI has to resemble humans. This makes the Turing Test less reliable. However, it is still useful since it is an actual test. It is also noteworthy that there is a prize for passing or coming closest to passing the Turing Test, the <a href=""https://en.wikipedia.org/wiki/Loebner_Prize"">Loebner Prize</a>.</p>

<p>The intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge. (paraphrased from <a href=""http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#Intelligent_agent_definition"">Wikipedia</a>). This definition is used more often and does not depend on the ability to resemble humans. However, it is harder to test this. </p>
",,0,2016-08-02T16:01:59.740,,27,2016-08-02T16:01:59.740,,,,,29.0,15.0,2,5,,,,57.57,10.08,9.07,0.0,0.0,14.0,The problem of the Turing Test is that it tests the machines ability to resemble humans Not necessarily every form of AI has to resemble humans This makes the Turing Test less reliable However it is still useful since it is an actual test It is also noteworthy that there is a prize for passing or coming closest to passing the Turing Test the Loebner Prize The intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge paraphrased from Wikipedia This definition is used more often and does not depend on the ability to resemble humans However it is harder to test this
143.0,5.0,"<p>Since human intelligence presumably is a function of a natural genetic algorithm in nature, is using a genetic algorithm in a computer an example of artificial intelligence?  If not, how do they differ?  Or perhaps some are and some are not expressing artificial intelligence depending upon the scale of the algorithm and what it evolves into?</p>
",,2,2016-08-02T16:02:44.553,1.0,28,2016-11-03T11:54:41.460,,,,,55.0,,1,6,<self-learning><genetic-algorithms>,Is a genetic algorithm an example of artificial intelligence?,237.0,35.57,12.36,10.49,0.0,0.0,5.0,Since human intelligence presumably is a function of a natural genetic algorithm in nature is using a genetic algorithm in a computer an example of artificial intelligence If not how do they differ Or perhaps some are and some are not expressing artificial intelligence depending upon the scale of the algorithm and what it evolves into
,,,,0,2016-08-02T16:03:16.133,,29,2016-08-04T14:45:26.583,2016-08-04T14:45:26.583,,5.0,,5.0,,5,0,,,,,,,,,,
,,,,0,2016-08-02T16:03:16.133,,30,2016-08-02T16:03:16.133,2016-08-02T16:03:16.133,,-1.0,,-1.0,,4,0,,,,,,,,,,
,,"<p>It's analogous to analogue versus digital, or the many shades of gray in between black and white: when evaluating the truthiness of a result, in binary boolean it's either true or false (0 or 1), but when utilizing fuzzy logic, it's an estimated probability between 0 and 1 (such as 0.75 being mostly probably true). It's useful for making calculated decisions when all information needed isn't necessarily available.</p>

<p><a href=""https://en.wikipedia.org/wiki/Fuzzy_logic"" rel=""nofollow"">Wikipedia has a fantastic page for this</a>.</p>
",,0,2016-08-02T16:04:09.333,,31,2016-08-02T16:04:09.333,,,,,62.0,10.0,2,4,,,,52.39,12.42,10.88,0.0,0.0,18.0,Its analogous to analogue versus digital or the many shades of gray in between black and white when evaluating the truthiness of a result in binary boolean its either true or false 0 or 1 but when utilizing fuzzy logic its an estimated probability between 0 and 1 such as 075 being mostly probably true Its useful for making calculated decisions when all information needed isnt necessarily available Wikipedia has a fantastic page for this
,,"<p><em>As complexity rises, precise statements lose meaning and meaningful statements lose precision.</em> (Albert Einstein).</p>

<p>Fuzzy logic deals with reasoning that is approximate rather than fixed and exact. This may make the reasoning more meaningful for a human:</p>

<p><a href=""https://i.stack.imgur.com/xdHPJ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xdHPJ.png"" alt=""enter image description here""></a></p>

<hr>

<p>Fuzzy logic is an extension of Boolean logic by Lotfi Zadeh in 1965 based on the
mathematical theory of fuzzy sets, which is a generalization of the classical set theory.
By introducing the notion of <em>degree in the verification</em> of a condition, thus enabling a
condition to be in a state other than true or false, fuzzy logic provides a very valuable
flexibility for reasoning, which makes it possible to take into account inaccuracies and
uncertainties.</p>

<p>One advantage of fuzzy logic in order to formalize human reasoning is that the rules
are set in natural language. For example, here are some rules of conduct that a driver
follows, assuming that he does not want to lose his driver’s licence:</p>

<p><a href=""https://i.stack.imgur.com/TM2UE.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/TM2UE.png"" alt=""enter image description here""></a></p>

<p>Intuitively, it thus seems that the input variables like in this example are approximately
appreciated by the brain, such as the degree of verification of a condition in fuzzy
logic.</p>

<hr>

<p>I've written a short <a href=""https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=kz2aIc8AAAAJ&amp;citation_for_view=kz2aIc8AAAAJ:eQOLeE2rZwMC"" rel=""noreferrer"">introduction to fuzzy logic</a> that goes into a bit more details but should be very accessible.</p>
",,1,2016-08-02T16:04:39.867,,32,2016-08-02T16:04:39.867,,,,,4.0,10.0,2,19,,,,33.17,12.54,10.54,0.0,0.0,22.0,As complexity rises precise statements lose meaning and meaningful statements lose precision Albert Einstein Fuzzy logic deals with reasoning that is approximate rather than fixed and exact This may make the reasoning more meaningful for a human Fuzzy logic is an extension of Boolean logic by Lotfi Zadeh in 1965 based on the mathematical theory of fuzzy sets which is a generalization of the classical set theory By introducing the notion of degree in the verification of a condition thus enabling a condition to be in a state other than true or false fuzzy logic provides a very valuable flexibility for reasoning which makes it possible to take into account inaccuracies and uncertainties One advantage of fuzzy logic in order to formalize human reasoning is that the rules are set in natural language For example here are some rules of conduct that a driver follows assuming that he does not want to lose his driver’s licence Intuitively it thus seems that the input variables like in this example are approximately appreciated by the brain such as the degree of verification of a condition in fuzzy logic Ive written a short introduction to fuzzy logic that goes into a bit more details but should be very accessible
,,"<p>The concept of ""the singularity"" is when machines outsmart the humans. Although Stephen Hawking opinion is that this situation is inevitable, but I think it'll be very difficult to reach that point, because every A.I. algorithm needs to be programmed by humans, therefore it would be always more limited than its creator.</p>

<p>We would probably know that point, when humanity will lost control over Artificial Intelligence where super-smart AI would be in competition with humans and maybe creating more sophisticated intelligent beings, but currently it's more like science fiction (aka <a href=""https://en.wikipedia.org/wiki/Skynet_(Terminator)"" rel=""nofollow"">Terminator's Skynet</a>).</p>

<p>The risk could involve killing people (like self-flying war <em>drones</em> making their own decision), destroying countries or even the whole planet (like A.I. connected to the nuclear weapons (aka <a href=""https://en.wikipedia.org/wiki/WarGames"" rel=""nofollow"">WarGames</a> movie), but it doesn't prove the point that the machines would be smarter than humans.</p>
",,2,2016-08-02T16:04:57.997,,33,2016-08-02T16:04:57.997,,,,,8.0,17.0,2,2,,,,48.33,14.92,11.22,0.0,0.0,30.0,The concept of the singularity is when machines outsmart the humans Although Stephen Hawking opinion is that this situation is inevitable but I think itll be very difficult to reach that point because every AI algorithm needs to be programmed by humans therefore it would be always more limited than its creator We would probably know that point when humanity will lost control over Artificial Intelligence where supersmart AI would be in competition with humans and maybe creating more sophisticated intelligent beings but currently its more like science fiction aka Terminators Skynet The risk could involve killing people like selfflying war drones making their own decision destroying countries or even the whole planet like AI connected to the nuclear weapons aka WarGames movie but it doesnt prove the point that the machines would be smarter than humans
,6.0,"<p>These two terms seem to be related, especially in their application in computer science and software engineering.  Is one a subset of another?  Is one a tool used to build a system for the other?  What are their differences and why are they significant?</p>
",,0,2016-08-02T16:05:26.390,6.0,35,2016-10-30T19:12:31.607,2016-08-02T16:07:31.773,,29.0,,69.0,,1,23,<machine-learning><terminology>,What is the difference between artificial intelligence and machine learning?,1450.0,60.31,8.8,8.85,0.0,0.0,5.0,These two terms seem to be related especially in their application in computer science and software engineering Is one a subset of another Is one a tool used to build a system for the other What are their differences and why are they significant
114.0,3.0,"<p>What aspects of quantum computers, if any, can help to further develop Artificial Intelligence?</p>
",,0,2016-08-02T16:06:25.853,3.0,36,2016-08-11T10:27:02.020,2016-08-02T19:13:49.690,,29.0,,29.0,,1,16,<quantum-computing>,To what extent can quantum computers help to develop Artificial Intelligence?,337.0,31.89,16.12,13.35,0.0,0.0,3.0,What aspects of quantum computers if any can help to further develop Artificial Intelligence
,2.0,"<p>I believe a Markov chain is a sequence of events where each subsequent event depends probabilistically on the current event.  What are examples of the application of a Markov chain and can it be used to create artificial intelligence?  Would a genetic algorithm be an example of a Markov chain since each generation depends upon the state of the prior generation?</p>
",,0,2016-08-02T16:07:29.317,2.0,37,2016-08-03T06:37:01.983,,,,,55.0,,1,4,<genetic-algorithms><markov-chain><probabilistic>,What is a Markov chain and how can it be used in creating artificial intelligence?,81.0,42.41,11.31,10.08,0.0,0.0,3.0,I believe a Markov chain is a sequence of events where each subsequent event depends probabilistically on the current event What are examples of the application of a Markov chain and can it be used to create artificial intelligence Would a genetic algorithm be an example of a Markov chain since each generation depends upon the state of the prior generation
,,"<p>This is kind of an opinion question, and it's probably more a question of philosophy than anything.  But in terms of how things are commonly defined, I'll say ""yes, genetic algorithms are part of AI"".  That is, if you pick up a comprehensive book on artificial intelligence, there will probably be a chapter on genetic algorithms"" (or more broadly, ""evolutionary algorithms""). </p>

<p>One area that has been extensively studied in the past, is the idea of using genetic algorithms to train neural networks.  I don't know if people are still actively researching this topic or not, but it at least illustrates that GA's are part of the overall rubric of AI in one regard.</p>
",,1,2016-08-02T16:08:06.920,,38,2016-08-02T16:08:06.920,,,,,33.0,28.0,2,2,,,,48.54,11.09,9.37,0.0,0.0,24.0,This is kind of an opinion question and its probably more a question of philosophy than anything But in terms of how things are commonly defined Ill say yes genetic algorithms are part of AI That is if you pick up a comprehensive book on artificial intelligence there will probably be a chapter on genetic algorithms or more broadly evolutionary algorithms One area that has been extensively studied in the past is the idea of using genetic algorithms to train neural networks I dont know if people are still actively researching this topic or not but it at least illustrates that GAs are part of the overall rubric of AI in one regard
,,"<p>The rhetorical point of the Turing Test is that it places the 'test' for 'humanity' in <em>observable outcomes</em>, instead of in <em>internal components</em>. If you would behave the same in interacting with an AI as you would with a person, how could <em>you</em> know the difference between them?</p>

<p>But that doesn't mean it's reliable, because intelligence has many different components and there are many sorts of intellectual tasks. The Turing Test, in some respects, is about the reaction of people to behavior, which is not at all reliable--remember that many people thought <a href=""https://en.wikipedia.org/wiki/ELIZA"">ELIZA</a>, a very simple chatbot, was an excellent listener and got deeply emotionally involved very quickly. It calls to mind the <a href=""https://www.youtube.com/watch?v=dBqhIVyfsRg"">Ikea commercial about throwing out a lamp</a>, where the emotional attachment comes <em>from the human viewer</em> (and the music), rather than from the lamp.</p>

<p>Turing tests for specific economic activities are much more practically interesting--if one can write an AI that replaces an Uber driver, for example, what that will imply is much clearer than if someone can create a conversational chatbot.</p>
",,0,2016-08-02T16:08:09.103,,39,2016-08-02T16:08:09.103,,,,,10.0,15.0,2,7,,,,41.84,12.89,10.59,0.0,0.0,30.0,The rhetorical point of the Turing Test is that it places the test for humanity in observable outcomes instead of in internal components If you would behave the same in interacting with an AI as you would with a person how could you know the difference between them But that doesnt mean its reliable because intelligence has many different components and there are many sorts of intellectual tasks The Turing Test in some respects is about the reaction of people to behavior which is not at all reliableremember that many people thought ELIZA a very simple chatbot was an excellent listener and got deeply emotionally involved very quickly It calls to mind the Ikea commercial about throwing out a lamp where the emotional attachment comes from the human viewer and the music rather than from the lamp Turing tests for specific economic activities are much more practically interestingif one can write an AI that replaces an Uber driver for example what that will imply is much clearer than if someone can create a conversational chatbot
44.0,2.0,"<p>What purpose does the ""dropout"" method serve and how does it improve the overall performance of the neural network?</p>
",,0,2016-08-02T16:08:23.377,2.0,40,2017-01-25T00:59:49.040,2016-08-02T16:10:22.307,,10.0,,8.0,,1,5,<deep-network><overfitting><performance>,"What is the ""dropout"" technique?",103.0,52.19,12.36,11.23,0.0,0.0,3.0,What purpose does the dropout method serve and how does it improve the overall performance of the neural network
65.0,3.0,"<p>Can an AI program have an IQ?</p>

<p>In other words, can the IQ of an AI program be measured?</p>

<p>Like how humans can do an IQ test.</p>
",,2,2016-08-02T16:08:34.350,1.0,41,2017-01-29T23:59:30.203,2016-08-03T08:04:16.350,,29.0,,72.0,,1,4,<intelligence-testing>,Can the IQ of an AI program be measured?,225.0,96.18,1.36,6.42,0.0,0.0,4.0,Can an AI program have an IQ In other words can the IQ of an AI program be measured Like how humans can do an IQ test
51.0,2.0,"<p>Why anybody would want to use the ""hidden layers""? How they enhance the learning ability of the network in comparison to the network which doesn't have them (linear models)?</p>
",,0,2016-08-02T16:09:25.427,1.0,42,2016-08-02T16:16:59.330,,,,,8.0,,1,4,<hidden-layers>,What is the purpose of the hidden layers?,37.0,65.22,11.13,9.26,0.0,0.0,7.0,Why anybody would want to use the hidden layers How they enhance the learning ability of the network in comparison to the network which doesnt have them linear models
,,"<p>Fuzzy logic is based on regular boolean logic. Boolean logic means you are working with truth values of either true or false (or 1 or 0 if you prefer). Fuzzy logic is the same apart from you can have truth values which are in-between true and false, that is to say you are working with any number between 0 (inclusive) and 1 (inclusive). The fact that you can have a 'partially true and partially false' truth value is where the word ""fuzzy"" comes from. Natural languages often use fuzzy logic like ""that balloon is red"" meaning that balloon could be any colour which is similar enough to red, or ""the shower is warm"". Here is a rough diagram for how ""the temperature of the shower is warm"" could be represented in terms of fuzzy logic (the y axis being the truth value and the x axis being the temperature):</p>

<p><a href=""https://i.stack.imgur.com/G7szY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G7szY.png"" alt=""y-axis=truth value of statement about temperature, x-axis=temperature""></a></p>

<p>Fuzzy logic can be applied to boolean operations such as <strong>and</strong>, <strong>or</strong>, and <strong>not</strong>. These would work like:</p>

<pre><code>A and B = A x B = min(A,B)
A or B  = 1-(1-A)x(1-B) = max(A,B)
not A   = 1-A
(where A and B are real values from 0 (inclusive) to 1 (inclusive) and
min and max return the smallest and the largest value of the two arguments given)
</code></pre>

<p>You can then use the three ""basic fuzzy logic operations"" to build all other ""fuzzy logic operations"", just like you can use the three ""basic boolean operations"" to build all other ""boolean logic operations"".</p>

<p>Sources:
<a href=""https://en.wikipedia.org/wiki/Fuzzy_logic"" rel=""nofollow noreferrer"">Fuzzy logic wikipedia</a>, 
<a href=""https://en.wikipedia.org/wiki/Boolean_algebra"" rel=""nofollow noreferrer"">Boolean algebra wikipedia</a>,
<a href=""https://www.youtube.com/watch?v=r804UF8Ia4c"" rel=""nofollow noreferrer"">Explanation of fuzzy logic on Youtube</a></p>

<p>Note: if anyone could suggest some more reliable sources in the comments, I will happily add them to the list (I understand that the current aren't too reliable).</p>
",,0,2016-08-02T16:10:01.630,,43,2016-08-18T09:47:26.303,2016-08-18T09:47:26.303,,47.0,,47.0,10.0,2,10,,,,49.28,10.51,9.11,229.0,0.0,50.0,Fuzzy logic is based on regular boolean logic Boolean logic means you are working with truth values of either true or false or 1 or 0 if you prefer Fuzzy logic is the same apart from you can have truth values which are inbetween true and false that is to say you are working with any number between 0 inclusive and 1 inclusive The fact that you can have a partially true and partially false truth value is where the word fuzzy comes from Natural languages often use fuzzy logic like that balloon is red meaning that balloon could be any colour which is similar enough to red or the shower is warm Here is a rough diagram for how the temperature of the shower is warm could be represented in terms of fuzzy logic the y axis being the truth value and the x axis being the temperature Fuzzy logic can be applied to boolean operations such as and or and not These would work like You can then use the three basic fuzzy logic operations to build all other fuzzy logic operations just like you can use the three basic boolean operations to build all other boolean logic operations Sources Fuzzy logic wikipedia Boolean algebra wikipedia Explanation of fuzzy logic on Youtube Note if anyone could suggest some more reliable sources in the comments I will happily add them to the list I understand that the current arent too reliable
,,"<p>Dropout means that every individual data point is only used to fit a random subset of the neurons. This is done to make the neural network more like an ensemble model.</p>

<p>That is, just as a random forest is averaging together the results of many individual decision trees, you can see a network network trained using dropout as averaging together the results of many individual neural networks (with 'results' understood to mean activations at every layer, rather than just the output layer).</p>
",,0,2016-08-02T16:12:08.767,,44,2016-08-02T16:12:08.767,,,,,10.0,40.0,2,5,,,,43.77,11.9,10.38,0.0,0.0,10.0,Dropout means that every individual data point is only used to fit a random subset of the neurons This is done to make the neural network more like an ensemble model That is just as a random forest is averaging together the results of many individual decision trees you can see a network network trained using dropout as averaging together the results of many individual neural networks with results understood to mean activations at every layer rather than just the output layer
,,"<p>The <a href=""https://en.wikipedia.org/wiki/Technological_singularity"" rel=""nofollow"">technological singularity</a> is a theoretical point in time at which a self-improving <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"" rel=""nofollow"">artificial general intelligence</a> becomes able to understand and manipulate concepts outside of the human brain's range, that is, the moment when it can understand things humans, by biological design, can't.</p>

<p>The fuzziness about the singularity comes from the fact that, from the singularity onwards, history is effectively unpredictable. Humankind would be unable to predict any future events, or explain any present events, as science itself becomes incapable of describing machine-triggered events. Essentially, machines would think of us the same way we think of ants. Thus, we can make no predictions past the singularity. Furthermore, as a logical consequence, we'd be unable to define the point at which the singularity may occur at all, or even recognize it when it happens.</p>

<p>However, in order for the singularity to take place, AGI needs to be developed, and <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence#Feasibility"" rel=""nofollow"">whether that is possible is quite a hot debate</a> right now. Moreover, an algorithm that creates superhuman intelligence out of bits and bytes would have to be designed. By definition, a human programmer wouldn't be able to do such a thing, as his/her brain would need to be able to comprehend concepts beyond its range. There is also the argument that an intelligence explosion (the mechanism by which a technological singularity would theoretically be formed) would be impossible due to the difficulty of the design challenge of making itself more intelligent, getting larger proportionally to its intelligence, and that the difficulty of the design itself may overtake the intelligence required to solve said challenge (last point credit to <a href=""http://ai.stackexchange.com/users/47/god-of-llamas"">god of llamas</a> in the comments).</p>

<p>Also, there are related theories involving machines taking over humankind and all of that sci-fi narrative. However, that's unlikely to happen, if <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics"" rel=""nofollow"">Asimov's laws</a> are followed appropriately. Even if Asimov's laws were not enough, a series of constraints would still be necessary in order to avoid the misuse of AGI by misintentioned individuals, and Asimov's laws are the nearest we have to that.</p>
",,3,2016-08-02T16:12:49.850,,45,2016-08-11T10:34:47.403,2016-08-11T10:34:47.403,,29.0,,71.0,17.0,2,9,,,,36.83,13.93,10.2,0.0,0.0,55.0,The technological singularity is a theoretical point in time at which a selfimproving artificial general intelligence becomes able to understand and manipulate concepts outside of the human brains range that is the moment when it can understand things humans by biological design cant The fuzziness about the singularity comes from the fact that from the singularity onwards history is effectively unpredictable Humankind would be unable to predict any future events or explain any present events as science itself becomes incapable of describing machinetriggered events Essentially machines would think of us the same way we think of ants Thus we can make no predictions past the singularity Furthermore as a logical consequence wed be unable to define the point at which the singularity may occur at all or even recognize it when it happens However in order for the singularity to take place AGI needs to be developed and whether that is possible is quite a hot debate right now Moreover an algorithm that creates superhuman intelligence out of bits and bytes would have to be designed By definition a human programmer wouldnt be able to do such a thing as hisher brain would need to be able to comprehend concepts beyond its range There is also the argument that an intelligence explosion the mechanism by which a technological singularity would theoretically be formed would be impossible due to the difficulty of the design challenge of making itself more intelligent getting larger proportionally to its intelligence and that the difficulty of the design itself may overtake the intelligence required to solve said challenge last point credit to god of llamas in the comments Also there are related theories involving machines taking over humankind and all of that scifi narrative However thats unlikely to happen if Asimovs laws are followed appropriately Even if Asimovs laws were not enough a series of constraints would still be necessary in order to avoid the misuse of AGI by misintentioned individuals and Asimovs laws are the nearest we have to that
,2.0,"<p>When did research into Artificial Intelligence first begin?  Was it called Artificial Intelligence then or was there another name?</p>
",,0,2016-08-02T16:14:26.350,1.0,46,2016-08-04T17:46:27.953,2016-08-02T16:26:16.990,,55.0,,55.0,,1,6,<history>,When did Artificial Intelligence research first start?,104.0,36.45,14.82,9.93,0.0,0.0,2.0,When did research into Artificial Intelligence first begin Was it called Artificial Intelligence then or was there another name
,,"<p>The notion of genetics used in Genetic Algorithms (GAs) is a <em>very</em> stripped down version relative to genetics in nature, essentially consisting of a population of 'genes' (representing solutions to some predefined problem) subject to `survival of the fittest' during iterated application of recombination and mutation.</p>

<p>Nowadays, the term 'Computational Intelligence' (CI) tends to be used to describe computational techniques intended to produce `the appearance of intelligence by <em>any</em> computational means', rather than specifically attempting to mimic the mechanisms that are believed to give rise to human (or animal) intelligence.</p>

<p>That said, the distinction between CI and AI is not so hard and fast, and arguably arose during the `AI Winter' when the term AI was out of fashion.</p>
",,0,2016-08-02T16:14:29.337,,47,2016-08-02T16:14:29.337,,,,,42.0,28.0,2,2,,,,14.26,15.56,12.51,0.0,0.0,26.0,The notion of genetics used in Genetic Algorithms GAs is a very stripped down version relative to genetics in nature essentially consisting of a population of genes representing solutions to some predefined problem subject to survival of the fittest during iterated application of recombination and mutation Nowadays the term Computational Intelligence CI tends to be used to describe computational techniques intended to produce the appearance of intelligence by any computational means rather than specifically attempting to mimic the mechanisms that are believed to give rise to human or animal intelligence That said the distinction between CI and AI is not so hard and fast and arguably arose during the AI Winter when the term AI was out of fashion
,,"<p>Hidden layers by themselves aren't useful. If you had hidden layers that were linear, the end result would still be a linear function of the inputs, and so you could collapse an arbitrary number of linear layers down to a single layer.</p>

<p>This is why we use nonlinear <a href=""https://en.wikipedia.org/wiki/Activation_function"" rel=""nofollow"">activation functions</a>, like RELU. This allows us to add a level of nonlinear complexity with each hidden layer, and with arbitrarily many hidden layers we can construct arbitrarily complicated nonlinear functions.</p>

<p>Because we can (at least in theory) capture any degree of complexity, we think of neural networks as ""universal learners,"" in that a large enough network could mimic any function.</p>
",,0,2016-08-02T16:15:49.970,,48,2016-08-02T16:15:49.970,,,,,10.0,42.0,2,2,,,,57.81,11.95,10.37,0.0,0.0,16.0,Hidden layers by themselves arent useful If you had hidden layers that were linear the end result would still be a linear function of the inputs and so you could collapse an arbitrary number of linear layers down to a single layer This is why we use nonlinear activation functions like RELU This allows us to add a level of nonlinear complexity with each hidden layer and with arbitrarily many hidden layers we can construct arbitrarily complicated nonlinear functions Because we can at least in theory capture any degree of complexity we think of neural networks as universal learners in that a large enough network could mimic any function
,,"<p>Machine learning is a subset of artificial intelligence. Roughly speaking, it corresponds to its learning side. There is no ""official"" definitions, boundaries are a bit fuzzy.</p>
",,4,2016-08-02T16:16:25.863,,49,2016-08-02T16:16:25.863,,,,,4.0,35.0,2,5,,,,45.72,14.11,11.36,0.0,0.0,7.0,Machine learning is a subset of artificial intelligence Roughly speaking it corresponds to its learning side There is no official definitions boundaries are a bit fuzzy
138.0,1.0,"<p>How would you estimate the generalisation error? What are the methods of achieving this?</p>
",,0,2016-08-02T16:16:46.797,,50,2016-08-02T20:32:12.480,,,,,8.0,,1,0,<deep-network><generalization>,How can generalization error be estimated?,23.0,55.91,11.14,9.62,0.0,0.0,2.0,How would you estimate the generalisation error What are the methods of achieving this
,,"<p>""Hidden"" layers really aren't all that special... a hidden layer is really no more than any layer that isn't input or output.  So even a very simple 3 layer NN has 1 hidden layer.  So I think the question isn't really ""how do hidden layers help?"" as much as ""why are deeper networks better?"".  </p>

<p>And the answer to that latter question is an area of active research.  Even top experts like Geoffrey Hinton and Andrew Ng will freely admit that we don't really understand why deep neural networks work.  That is, we don't understand them in complete detail anyway.</p>

<p>That said, the theory, as I understand it goes something like this...  successive layers of the network learn successively more sophisticated features, which build on the features from preceding layers.  So, for example, an NN used for facial recognition might work like this: the first layer detects edges and nothing else.  The next layer up recognizes geometric shapes (boxes, circles, etc).  The next layer up recognizes primitive features of a face, like eyes, noses, jaw, etc.   The next layer up then recognizes composites based on combinations of ""eye"" features, ""nose"" features, and so on.  </p>

<p>So, in theory, deeper networks (more hidden layers) are better in that they develop a more granular / detailed representation of ""thing"" being recognized.  </p>
",,0,2016-08-02T16:16:59.330,,51,2016-08-02T16:16:59.330,,,,,33.0,42.0,2,3,,,,65.32,11.3,9.03,0.0,0.0,59.0,Hidden layers really arent all that special a hidden layer is really no more than any layer that isnt input or output So even a very simple 3 layer NN has 1 hidden layer So I think the question isnt really how do hidden layers help as much as why are deeper networks better And the answer to that latter question is an area of active research Even top experts like Geoffrey Hinton and Andrew Ng will freely admit that we dont really understand why deep neural networks work That is we dont understand them in complete detail anyway That said the theory as I understand it goes something like this successive layers of the network learn successively more sophisticated features which build on the features from preceding layers So for example an NN used for facial recognition might work like this the first layer detects edges and nothing else The next layer up recognizes geometric shapes boxes circles etc The next layer up recognizes primitive features of a face like eyes noses jaw etc The next layer up then recognizes composites based on combinations of eye features nose features and so on So in theory deeper networks more hidden layers are better in that they develop a more granular detailed representation of thing being recognized
1437.0,1.0,"<p>I've implemented <a href=""https://en.wikipedia.org/wiki/Reinforcement_learning"" rel=""nofollow"">the reinforcement learning alogrithm</a> for an agent to play <a href=""https://github.com/admonkey/snappybird"" rel=""nofollow"">snappy bird</a> (a shameless cheap ripoff of flappy bird) utilizing a q-table for storing the history for future lookups. It works and eventually achieves perfect convergence after enough training.</p>

<p>Is it possible to implement a neural network to do function approximation in order to accomplish the purpose of the q-table? Obviously storage is a concern with the q-table, but it doesn't seem to ever train with the neural net alone. Perhaps training the nnet on an existing q-table would work, but I would like to not use a q-table at all if possible.</p>
",,2,2016-08-02T16:19:30.337,,52,2016-08-08T08:34:38.777,,,,,62.0,,1,2,<neural-networks>,Is it possible to implement reinforcement learning using a neural network?,98.0,58.82,11.66,11.04,0.0,0.0,16.0,Ive implemented the reinforcement learning alogrithm for an agent to play snappy bird a shameless cheap ripoff of flappy bird utilizing a qtable for storing the history for future lookups It works and eventually achieves perfect convergence after enough training Is it possible to implement a neural network to do function approximation in order to accomplish the purpose of the qtable Obviously storage is a concern with the qtable but it doesnt seem to ever train with the neural net alone Perhaps training the nnet on an existing qtable would work but I would like to not use a qtable at all if possible
,,"<p>Machine learning has been defined by many people in different ways. One definition says, that machine learning (ML) is the field of study that gives computers the <em>ability to learn</em> without being explicitly programmed.</p>

<p>Given the above definition, we might say that machine learning is geared towards problems, for which we have (lots of) data (experience), from which a program can learn and can get better at a task.</p>

<p>Artificial intelligence has many more aspects, where machines do not get better at tasks by learning from data, but may exhibit <em>intelligence</em> through rules (e.g. expert systems like <a href=""https://en.wikipedia.org/wiki/Mycin"">Mycin</a>), <a href=""http://rads.stackoverflow.com/amzn/click/0201403757"">logic</a> or algorithms, e.g. <a href=""https://en.wikipedia.org/wiki/Pathfinding"">finding paths</a>.</p>

<p>The TOC of <a href=""http://aima.cs.berkeley.edu/""><em>Artificial Intelligence: A Modern Approach</em></a> shows more research fields of AI, like <em>Constraint Satisfaction Problems</em>, <em>Probabilistic Reasoning</em> or <em>Philosophical Foundations</em>.</p>
",,0,2016-08-02T16:20:03.773,,53,2016-08-02T16:26:34.370,2016-08-02T16:26:34.370,,28.0,,28.0,35.0,2,16,,,,41.5,14.68,10.78,0.0,0.0,28.0,Machine learning has been defined by many people in different ways One definition says that machine learning ML is the field of study that gives computers the ability to learn without being explicitly programmed Given the above definition we might say that machine learning is geared towards problems for which we have lots of data experience from which a program can learn and can get better at a task Artificial intelligence has many more aspects where machines do not get better at tasks by learning from data but may exhibit intelligence through rules eg expert systems like Mycin logic or algorithms eg finding paths The TOC of Artificial Intelligence A Modern Approach shows more research fields of AI like Constraint Satisfaction Problems Probabilistic Reasoning or Philosophical Foundations
,3.0,"<p>I read that in the spring of 2016 a computer <a href=""https://en.wikipedia.org/wiki/Computer_Go"" rel=""nofollow"">Go program</a> was finally able to beat a professional human for the first time.  Now that this milestone has been reached, does that represent a significant advance in artificial intelligence techniques or was it just a matter of even more processing power being applied to the problem?  What are some of the methods used to program the successful Go playing program, and are those methods considered to be artificial intelligence?</p>
",,0,2016-08-02T16:20:40.520,1.0,54,2016-08-17T15:54:14.257,2016-08-17T15:54:14.257,,145.0,,55.0,,1,3,<game-theory>,Does the recent advent of a Go playing computer represent Artificial Intelligence?,69.0,44.37,11.44,10.09,0.0,0.0,5.0,I read that in the spring of 2016 a computer Go program was finally able to beat a professional human for the first time Now that this milestone has been reached does that represent a significant advance in artificial intelligence techniques or was it just a matter of even more processing power being applied to the problem What are some of the methods used to program the successful Go playing program and are those methods considered to be artificial intelligence
,,"<blockquote>
  <p>Machine learning is a science that involves development of
  self-learning algorithms. These algorithms are more generic in nature
  that it can be applied to various domain related problems.</p>
  
  <p>Artificial Intelligence is a science to develop a system or software
  to mimic human to respond and behave in a circumference. As field with
  extremely broad scope, AI has defined its goal into multiple chunks.
  Later each chuck has become a separate field of study to solve its
  problem.</p>
</blockquote>

<p><em><a href=""http://shakthydoss.com/what-is-the-difference-between-artificial-intelligence-machine-learning-statistics-and-data-mining/"" rel=""nofollow"">Sakthi Dasan Sekar</a></em></p>
",,0,2016-08-02T16:20:43.177,,55,2016-08-04T13:57:00.633,2016-08-04T13:57:00.633,,30.0,,5.0,35.0,2,2,,,,49.52,12.0,11.4,0.0,0.0,7.0,Machine learning is a science that involves development of selflearning algorithms These algorithms are more generic in nature that it can be applied to various domain related problems Artificial Intelligence is a science to develop a system or software to mimic human to respond and behave in a circumference As field with extremely broad scope AI has defined its goal into multiple chunks Later each chuck has become a separate field of study to solve its problem Sakthi Dasan Sekar
,,"<p>Many terms have 'mostly' the same meanings, and so the differences are just in emphasis, perspective, or historical descent. People disagree as to which label refers to the superset or the subset; there are people who will call AI a branch of ML and people who will call ML a branch of AI.</p>

<p>I typically hear Machine Learning used as a form of 'applied statistics' where we specify a learning problem in enough detail that we can just feed training data into it and get a useful model out the other side.</p>

<p>I typically hear Artificial Intelligence as a catch-all term to refer to any sort of intelligence embedded in the environment or in code. This is a very expansive definition, and others use narrower ones (such as focusing on artificial <em>general</em> intelligence, which is not domain-specific). (Taken to an extreme, my version includes thermostats.)</p>

<p>This is also a good time to point out other StackExchange sites, <a href=""http://stats.stackexchange.com/"">Cross Validated</a> and <a href=""http://datascience.stackexchange.com/"">Data Science</a>, which have quite a bit of overlap with this sit.</p>
",,0,2016-08-02T16:20:52.200,,56,2016-08-02T16:20:52.200,,,,,10.0,35.0,2,6,,,,46.51,10.8,10.55,0.0,0.0,26.0,Many terms have mostly the same meanings and so the differences are just in emphasis perspective or historical descent People disagree as to which label refers to the superset or the subset there are people who will call AI a branch of ML and people who will call ML a branch of AI I typically hear Machine Learning used as a form of applied statistics where we specify a learning problem in enough detail that we can just feed training data into it and get a useful model out the other side I typically hear Artificial Intelligence as a catchall term to refer to any sort of intelligence embedded in the environment or in code This is a very expansive definition and others use narrower ones such as focusing on artificial general intelligence which is not domainspecific Taken to an extreme my version includes thermostats This is also a good time to point out other StackExchange sites Cross Validated and Data Science which have quite a bit of overlap with this sit
,,"<p>The ""singularity,"" viewed narrowly, refers to a point at which economic growth is so fast that we can't make useful predictions about what the future past that point will look like.</p>

<p>It's often used interchangeably with ""intelligence explosion,"" which is when we get so-called Strong AI, which is AI that is intelligent enough to understand and improve itself. It seems reasonable to expect that the intelligence explosion would immediately lead to an economic singularity, but the reverse is not necessarily true.</p>
",,0,2016-08-02T16:23:13.273,,57,2016-08-02T16:23:13.273,,,,,10.0,17.0,2,2,,,,35.61,14.05,10.43,0.0,0.0,15.0,The singularity viewed narrowly refers to a point at which economic growth is so fast that we cant make useful predictions about what the future past that point will look like Its often used interchangeably with intelligence explosion which is when we get socalled Strong AI which is AI that is intelligent enough to understand and improve itself It seems reasonable to expect that the intelligence explosion would immediately lead to an economic singularity but the reverse is not necessarily true
,1.0,"<p>Who first coined the term Artificial Intelligence, is there a published research paper which is the first to use that term?</p>
",,0,2016-08-02T16:25:20.223,0.0,58,2016-08-02T16:38:45.147,,,,,55.0,,1,5,<history>,Who first coined the term Artificial Intelligence?,225.0,58.62,11.14,9.19,0.0,0.0,2.0,Who first coined the term Artificial Intelligence is there a published research paper which is the first to use that term
,,"<blockquote>
  <p>The creator of Artificial Intelligence studies begins with the work of
  pioneer computer scientist Alan Turing (1912-1954) who in the
  1930's evolved a concept of a ""Turing Machine.""</p>
</blockquote>

<p><em><a href=""http://www.enotes.com/homework-help/when-did-artifical-intelligence-research-start-158283"" rel=""nofollow"">eNotes</a></em>  </p>

<p>In layman's terms, it began in the 1930s, and they were most likely called ""Machines"" not AI.</p>
",,2,2016-08-02T16:26:17.990,,59,2016-08-04T17:46:27.953,2016-08-04T17:46:27.953,,42.0,,5.0,46.0,2,0,,,,56.59,12.77,11.64,0.0,0.0,13.0,The creator of Artificial Intelligence studies begins with the work of pioneer computer scientist Alan Turing 19121954 who in the 1930s evolved a concept of a Turing Machine eNotes In laymans terms it began in the 1930s and they were most likely called Machines not AI
,3.0,"<p>I have a background in Computer Engineering and have been working on developing better algorithms to mimic human thought. (One of my favorites is Analogical Modeling as applied to language processing and decision making.) However, the more I research, the more I realize just <em>how</em> complicated AI is.</p>

<p>I have tried to tackle many problems in this field, but sometimes I find that I am reinventing the wheel or am trying to solve a problem that has already been proven to be unsolvable (ie. the halting problem). So, to help in furthering AI, I want to better understand the current obstacles that are hindering our progress in this field.</p>

<p>For example, time and space complexity of some machine learning algorithms is super-polynomial which means that even with fast computers, it can take a while for the program to complete. Even still, some algorithms may be fast on a desktop or other computer while dealing with a small data set, but when increasing the size of the data, the algorithm becomes intractable.</p>

<p>What are other issues currently facing AI development?</p>
",,0,2016-08-02T16:27:49.533,,60,2016-08-08T19:11:47.130,,,,,77.0,,1,5,<machine-learning>,What are the main problems hindering current AI development?,111.0,51.28,11.37,10.18,0.0,0.0,24.0,I have a background in Computer Engineering and have been working on developing better algorithms to mimic human thought One of my favorites is Analogical Modeling as applied to language processing and decision making However the more I research the more I realize just how complicated AI is I have tried to tackle many problems in this field but sometimes I find that I am reinventing the wheel or am trying to solve a problem that has already been proven to be unsolvable ie the halting problem So to help in furthering AI I want to better understand the current obstacles that are hindering our progress in this field For example time and space complexity of some machine learning algorithms is superpolynomial which means that even with fast computers it can take a while for the program to complete Even still some algorithms may be fast on a desktop or other computer while dealing with a small data set but when increasing the size of the data the algorithm becomes intractable What are other issues currently facing AI development
,,"<p>Quantum computers can help further develop A.I. algorithms and solve the problems to the extent of our creativity and ability to define the problem. For example breaking cryptography can take seconds, where it can takes thousands of years for standard computers. The same with artificial intelligence, it can predict all the combinations for the given problem defined by algorithm. This is due to superposition of multiple states of quantum bits.</p>

<p>Currently, quantum computers are still in the early stages of development and can perform complex calculation. There are already technologies like <a href=""https://en.wikipedia.org/wiki/D-Wave_Systems"" rel=""nofollow"">D-Wave</a> systems which are used by Google and NASA for complex data analysis, using Multi-Qubit type quantum computers for <a href=""https://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations"" rel=""nofollow"">solving NSE fluid dynamics problems</a> of interest or global surveillance for military purposes, and many more which we're not aware.</p>

<p>Currently there are only a few quantum computers available to the public, like <a href=""http://www.research.ibm.com/quantum/"" rel=""nofollow"">IBM Quantum Experience</a> (the world’s first quantum computing platform delivered via the IBM Cloud), but it's programming on quantum logic gates levels, so we're many years behind creating artificial intelligence available to public. There are some <a href=""https://en.wikipedia.org/wiki/Quantum_programming"" rel=""nofollow"">quantum computing languages</a> such as QCL, Q or Quipper, but I'm not aware any libraries which can provide artificial intelligence frameworks. It doesn't mean it's not there, and I'm sure huge companies and governments organisations are using it for their agenda to outcome the competition (like financial market analysis, etc.).</p>
",,0,2016-08-02T16:28:29.363,,61,2016-08-04T21:06:48.983,2016-08-04T21:06:48.983,,8.0,,8.0,36.0,2,2,,,,31.31,15.21,10.84,0.0,0.0,37.0,Quantum computers can help further develop AI algorithms and solve the problems to the extent of our creativity and ability to define the problem For example breaking cryptography can take seconds where it can takes thousands of years for standard computers The same with artificial intelligence it can predict all the combinations for the given problem defined by algorithm This is due to superposition of multiple states of quantum bits Currently quantum computers are still in the early stages of development and can perform complex calculation There are already technologies like DWave systems which are used by Google and NASA for complex data analysis using MultiQubit type quantum computers for solving NSE fluid dynamics problems of interest or global surveillance for military purposes and many more which were not aware Currently there are only a few quantum computers available to the public like IBM Quantum Experience the world’s first quantum computing platform delivered via the IBM Cloud but its programming on quantum logic gates levels so were many years behind creating artificial intelligence available to public There are some quantum computing languages such as QCL Q or Quipper but Im not aware any libraries which can provide artificial intelligence frameworks It doesnt mean its not there and Im sure huge companies and governments organisations are using it for their agenda to outcome the competition like financial market analysis etc
,,"<p>Human intelligence is <strong>not</strong> an example of natural genetic algorithms.</p>

<p>Genetic algorithms have collections of solutions that are collided with each other to make new solutions, eventually returning the best solution. Human intelligence is a network of neurons doing information processing, and almost all of it doesn't behave the same way.</p>

<p>But that something doesn't behave in the same way that human intelligence does doesn't mean that it's not an AI algorithm; I would include 'genetic algorithms' as a numerical optimization technique, and since optimization and intelligence are deeply linked any numerical optimization technique could be seen as an AI technique.</p>
",,0,2016-08-02T16:29:11.850,,62,2016-08-02T16:29:11.850,,,,,10.0,28.0,2,2,,,,28.98,15.38,10.51,0.0,0.0,14.0,Human intelligence is not an example of natural genetic algorithms Genetic algorithms have collections of solutions that are collided with each other to make new solutions eventually returning the best solution Human intelligence is a network of neurons doing information processing and almost all of it doesnt behave the same way But that something doesnt behave in the same way that human intelligence does doesnt mean that its not an AI algorithm I would include genetic algorithms as a numerical optimization technique and since optimization and intelligence are deeply linked any numerical optimization technique could be seen as an AI technique
208.0,1.0,"<p>I've read that the most of the problems can be solved with 1-2 hidden layers.</p>

<p>How do you know you need more than 2? For what kind of problems you would need them (as example)?</p>
",,0,2016-08-02T16:29:24.803,1.0,63,2016-08-03T10:26:17.493,,,,,8.0,,1,6,<deep-network><hidden-layers>,What kind of problems require more than 2 hidden layers?,49.0,101.9,4.91,6.02,0.0,0.0,7.0,Ive read that the most of the problems can be solved with 12 hidden layers How do you know you need more than 2 For what kind of problems you would need them as example
,1.0,"<p>What were the first areas of research into Artificial Intelligence and what were some early successes?  More recently we've had:</p>

<ol>
<li>Beating a human at the game of chess</li>
<li>Convincing a human that a person was conversing with them (passing the Turing test)</li>
<li>Beating a human at Jeopardy game show</li>
<li>Beating a human at the game of go.</li>
</ol>

<p>Were there milestones that were considered major in the field before the 1990s?</p>
",,0,2016-08-02T16:29:29.207,,64,2016-08-04T17:34:17.233,,,,,55.0,,1,2,<history>,What were the first areas of research and what were some early successes?,17.0,56.29,10.51,9.3,0.0,0.0,7.0,What were the first areas of research into Artificial Intelligence and what were some early successes More recently weve had Beating a human at the game of chess Convincing a human that a person was conversing with them passing the Turing test Beating a human at Jeopardy game show Beating a human at the game of go Were there milestones that were considered major in the field before the 1990s
,,"<p>Short answer: No.</p>

<p>Longer answer: It depends on what IQ exactly is, and when the question is asked compared to ongoing development. The topic you're referring to is actually more commonly described as AGI, or Artificial General Intelligence, as opposed to AI, which could be any narrow problem solving capability represented in software/hardware.</p>

<p><a href=""https://en.wikipedia.org/wiki/Intelligence_quotient"">Intelligence quotient</a> is a rough estimate of how well humans are able to generally answer questions they have not previously encountered, but as a predictor it is somewhat flawed, and has many criticisms and detractors.</p>

<p>Currently (2016), no known programs have the ability to generalize, or apply learning from one domain to solving problems in an arbitrarily different domain through an abstract understanding. (However there are programs which can effectively analyze, or break down some information domains into simpler representations.) This seems likely to change as time goes on and both hardware and software techniques are developed toward this goal. Experts widely disagree as to the likely timing and approach of these developments, as well as to the most probable outcomes.</p>

<p>It's also worth noting that there seems to be a large deficit of understanding as to what exactly consciousness is, and disagreement over whether there is ever likely to be anything in the field of artificial intelligence that compares to it.</p>
",,0,2016-08-02T16:30:28.737,,65,2016-08-02T17:08:34.127,2016-08-02T17:08:34.127,,46.0,,46.0,41.0,2,7,,,,38.76,14.28,11.36,0.0,0.0,29.0,Short answer No Longer answer It depends on what IQ exactly is and when the question is asked compared to ongoing development The topic youre referring to is actually more commonly described as AGI or Artificial General Intelligence as opposed to AI which could be any narrow problem solving capability represented in softwarehardware Intelligence quotient is a rough estimate of how well humans are able to generally answer questions they have not previously encountered but as a predictor it is somewhat flawed and has many criticisms and detractors Currently 2016 no known programs have the ability to generalize or apply learning from one domain to solving problems in an arbitrarily different domain through an abstract understanding However there are programs which can effectively analyze or break down some information domains into simpler representations This seems likely to change as time goes on and both hardware and software techniques are developed toward this goal Experts widely disagree as to the likely timing and approach of these developments as well as to the most probable outcomes Its also worth noting that there seems to be a large deficit of understanding as to what exactly consciousness is and disagreement over whether there is ever likely to be anything in the field of artificial intelligence that compares to it
,,"<p><strong><a href=""https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)"" rel=""nofollow"">John McCarthy</a></strong> (1927 - 2011) was an American computer scientist. A pioneer in the foundations of artificial intelligence research, <strong>he coined the term ""artificial intelligence""</strong>. He was one of the creators of the (original) Lisp programming language, which was quite involved in early AI research in the 1960's and 1970's.</p>

<p>He coined the term in 1955, and organized the first Artificial Intelligence conference in 1956, while working as a math teacher at Dartmouth. He founded the AI labs at MIT and Stanford.</p>

<p>He's responsible for developing several other important concepts in today's mainstream computer science. Namely, he developed <a href=""https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)"" rel=""nofollow"">garbage collection</a> (used by a Lisp interpreter) and designed the first <a href=""https://en.wikipedia.org/wiki/Time-sharing"" rel=""nofollow"">time-sharing systems</a>.</p>

<p>On a side note, McCarthy predicted that creating a truly intelligent machine would require ""1.8 Einsteins and one-tenth the resources of the Manhattan Project.""</p>
",,0,2016-08-02T16:31:24.350,,66,2016-08-02T16:38:45.147,2016-08-02T16:38:45.147,,71.0,,71.0,58.0,2,5,,,,56.35,14.61,11.09,0.0,0.0,32.0,John McCarthy 1927 2011 was an American computer scientist A pioneer in the foundations of artificial intelligence research he coined the term artificial intelligence He was one of the creators of the original Lisp programming language which was quite involved in early AI research in the 1960s and 1970s He coined the term in 1955 and organized the first Artificial Intelligence conference in 1956 while working as a math teacher at Dartmouth He founded the AI labs at MIT and Stanford Hes responsible for developing several other important concepts in todays mainstream computer science Namely he developed garbage collection used by a Lisp interpreter and designed the first timesharing systems On a side note McCarthy predicted that creating a truly intelligent machine would require 18 Einsteins and onetenth the resources of the Manhattan Project
72.0,1.0,"<p>Why somebody would use SAT solvers (<a href=""https://en.wikipedia.org/wiki/Boolean_satisfiability_problem"" rel=""nofollow"">Boolean satisfiability problem</a>) to solve their real world problems?</p>

<p>Are there any examples of the real uses of this model?</p>
",,0,2016-08-02T16:31:51.380,,67,2016-08-11T14:54:21.720,,,,,8.0,,1,1,<models>,What are the real world uses for solvers?,34.0,58.28,11.53,10.35,0.0,0.0,4.0,Why somebody would use SAT solvers Boolean satisfiability problem to solve their real world problems Are there any examples of the real uses of this model
,2.0,"<p>What designs for genetic algorithms are there, if they are classified differently and/or have different names, that leverage models for epigenetics in evolution? What are the pros/cons of the designs? Are there vast insufficiencies or wide-open questions about their usefulness? </p>
",,2,2016-08-02T16:33:52.707,0.0,68,2016-08-04T16:42:49.973,2016-08-04T14:50:15.963,,46.0,,46.0,,1,4,<genetic-algorithms>,What genetic algorithm designs are there that includes models of epigenetics?,56.0,32.6,16.75,11.8,0.0,0.0,8.0,What designs for genetic algorithms are there if they are classified differently andor have different names that leverage models for epigenetics in evolution What are the proscons of the designs Are there vast insufficiencies or wideopen questions about their usefulness
,,"<p>It doesn't make much sense to have a single threshold with ""unintelligent"" below it and ""intelligent"" above it.</p>

<p>I think it makes more sense to have a gradation of intelligence by cognitive task. Inverting a matrix is a 'cognitive task,' and one where working memory pays off immensely; computers have been much better at that cognitive task than humans for a long time.</p>

<p>What the AlphaGo victory represents has several components. One is that we have algorithms that are competitive with the best board-game playing humans at doing tactical and strategic thinking in the well-described world of Go. Another is that the deeper structure of the human visual system seems to have been duplicated, and so we have algorithms that can recognize patterns as well as humans--with <em>very</em> limited resolution. (AlphaGo is seeing one pixel per stone, whereas we have very, very high-resolution eyes and the visual cortex to match.)</p>

<p>Different people have different intuitions, but it seems to me that visual intelligence is a huge component of human intelligence in general. If we know most of the secrets of human visual intelligence, that means there might be many tasks that computers could now perform as well as humans (if provided the correct training data).</p>
",,0,2016-08-02T16:34:26.163,,69,2016-08-02T16:34:26.163,,,,,10.0,54.0,2,5,,,,48.33,12.54,9.7,0.0,0.0,32.0,It doesnt make much sense to have a single threshold with unintelligent below it and intelligent above it I think it makes more sense to have a gradation of intelligence by cognitive task Inverting a matrix is a cognitive task and one where working memory pays off immensely computers have been much better at that cognitive task than humans for a long time What the AlphaGo victory represents has several components One is that we have algorithms that are competitive with the best boardgame playing humans at doing tactical and strategic thinking in the welldescribed world of Go Another is that the deeper structure of the human visual system seems to have been duplicated and so we have algorithms that can recognize patterns as well as humanswith very limited resolution AlphaGo is seeing one pixel per stone whereas we have very very highresolution eyes and the visual cortex to match Different people have different intuitions but it seems to me that visual intelligence is a huge component of human intelligence in general If we know most of the secrets of human visual intelligence that means there might be many tasks that computers could now perform as well as humans if provided the correct training data
,4.0,"<p>Can a Convolutional Neural Network be used for pattern recognition in a problem domain where there are no pre-existing images, say by representing abstract data graphically? Would that always be less efficient?</p>

<p><a href=""https://youtu.be/py5byOOHZM8?t=815"">This developer</a> says current development could go further but not if there's a limit outside image recognition. </p>
",,4,2016-08-02T16:38:55.800,1.0,70,2016-09-07T14:56:38.193,2016-08-04T15:17:46.150,,46.0,,46.0,,1,10,<deep-network><neural-networks><image-recognition><conv-neural-network>,Is the pattern recognition capability of CNNs limited to image processing?,114.0,38.01,14.73,12.5,0.0,0.0,6.0,Can a Convolutional Neural Network be used for pattern recognition in a problem domain where there are no preexisting images say by representing abstract data graphically Would that always be less efficient This developer says current development could go further but not if theres a limit outside image recognition
,,"<p>The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 30s, 40s and early 50s (e.g. <a href=""https://en.wikipedia.org/wiki/Logic"">formal logic</a>, automata, <a href=""https://en.wikipedia.org/wiki/Robot#Remote-controlled_systems"">robots</a>). Although the <a href=""https://en.wikipedia.org/wiki/Turing_test"">Turing test</a> was proposed in 1950s by <a href=""https://en.wikipedia.org/wiki/Alan_Turing"">Alan Turing</a>, the work culminated back in the 1940s in the invention of the programmable digital computers, an abstract essence of mathematical reasoning. These ideas were inspired by a handful of scientists from a variety of fields who began seriously considering the possibility of building an electronic brain. The field of artificial intelligence research was founded as an academic discipline in 1956.</p>

<p>However the concept of artificial beings is not new and it's as old as Greek myths of Hephaestus and Pygmalion which incorporated the idea of intelligent robots (such as <em>Talos</em>) and artificial beings (such as <em>Galatea</em> and <em>Pandora</em>).</p>

<p>See the following articles at Wikipedia for further details:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Artificial_intelligence#History"">Artificial intelligence (AI)</a> </li>
<li><a href=""https://en.wikipedia.org/wiki/History_of_artificial_intelligence"">History of artificial intelligence</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence"">Timeline of artificial intelligence</a></li>
</ul>
",,0,2016-08-02T16:39:00.200,,71,2016-08-02T16:57:20.110,2016-08-02T16:57:20.110,,8.0,,8.0,46.0,2,6,,,,40.08,14.86,10.95,0.0,0.0,22.0,The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 30s 40s and early 50s eg formal logic automata robots Although the Turing test was proposed in 1950s by Alan Turing the work culminated back in the 1940s in the invention of the programmable digital computers an abstract essence of mathematical reasoning These ideas were inspired by a handful of scientists from a variety of fields who began seriously considering the possibility of building an electronic brain The field of artificial intelligence research was founded as an academic discipline in 1956 However the concept of artificial beings is not new and its as old as Greek myths of Hephaestus and Pygmalion which incorporated the idea of intelligent robots such as Talos and artificial beings such as Galatea and Pandora See the following articles at Wikipedia for further details Artificial intelligence AI History of artificial intelligence Timeline of artificial intelligence
,,"<p>Instead of talking about just SAT solvers, let me talk about optimization in general. Many economic problems can be cast as optimization problems: for example, FedEx may have a list of packages and the destinations for those packages, and must decide which packages to put on which trucks, and what order to deliver those packages in.</p>

<p>If you write out a mathematical description of this problem, there are a truly stunning number of possible solutions, and a well-defined way to evaluate which of two solutions is better. A solver is an algorithm that will evaluate a solution, come up with another solution, and then evaluate that one, and so on.</p>

<p>In small cases and simple problems, the solver can also terminate with a proof that it is actually the best solution possible. But typically instead the solver just reports ""this is the best solution that I've seen,"" and that's used. An improvement in the solver means you can reliably get lower-cost solutions than you were seeing before.</p>

<p>For the SAT problem specifically, the Wikipedia page on <a href=""https://en.wikipedia.org/wiki/Boolean_satisfiability_problem"" rel=""nofollow"">SAT</a> gives some examples:</p>

<blockquote>
  <p>Since the SAT problem is NP-complete, only algorithms with exponential worst-case complexity are known for it. In spite of this, efficient and scalable algorithms for SAT were developed over the last decade[when?] and have contributed to dramatic advances in our ability to automatically solve problem instances involving tens of thousands of variables and millions of constraints (i.e. clauses).[1] Examples of such problems in electronic design automation (EDA) include formal equivalence checking, model checking, formal verification of pipelined microprocessors,[12] automatic test pattern generation, routing of FPGAs,[14] planning, and scheduling problems, and so on. A SAT-solving engine is now considered to be an essential component in the EDA toolbox.</p>
</blockquote>
",,0,2016-08-02T16:39:12.377,,72,2016-08-11T14:54:21.720,2016-08-11T14:54:21.720,,42.0,,10.0,67.0,2,2,,,,38.76,14.22,10.54,0.0,0.0,58.0,Instead of talking about just SAT solvers let me talk about optimization in general Many economic problems can be cast as optimization problems for example FedEx may have a list of packages and the destinations for those packages and must decide which packages to put on which trucks and what order to deliver those packages in If you write out a mathematical description of this problem there are a truly stunning number of possible solutions and a welldefined way to evaluate which of two solutions is better A solver is an algorithm that will evaluate a solution come up with another solution and then evaluate that one and so on In small cases and simple problems the solver can also terminate with a proof that it is actually the best solution possible But typically instead the solver just reports this is the best solution that Ive seen and thats used An improvement in the solver means you can reliably get lowercost solutions than you were seeing before For the SAT problem specifically the Wikipedia page on SAT gives some examples Since the SAT problem is NPcomplete only algorithms with exponential worstcase complexity are known for it In spite of this efficient and scalable algorithms for SAT were developed over the last decadewhen and have contributed to dramatic advances in our ability to automatically solve problem instances involving tens of thousands of variables and millions of constraints ie clauses1 Examples of such problems in electronic design automation EDA include formal equivalence checking model checking formal verification of pipelined microprocessors12 automatic test pattern generation routing of FPGAs14 planning and scheduling problems and so on A SATsolving engine is now considered to be an essential component in the EDA toolbox
,,"<p>A Markov model includes the probability of transitioning to each state considering the current state. ""Each state"" may be just one point - whether it rained on specific day, for instance - or it might look like multiple things - like a pair of words. You've probably seen automatically generated weird text that <em>almost</em> makes sense, like <a href=""https://blog.codinghorror.com/markov-and-you/"">Garkov</a> (the output of a Markov model based on the Garfield comic strips). That Coding Horror article also mentions the applications of Markov techniques to Google's PageRank.</p>

<p>Markov models are really only powerful when they have a lot of input to work with. If a machine looked through a lot of English text, it would get a pretty good idea of what words generally come after other words. Or after looking through someone's location history, it could figure out where that person is likely to go next from a certain place. Constantly updating the ""input corpus"" as more data is received would let the machine tune the probabilities of all the state transitions.</p>

<p>Genetic algorithms are fairly different things. They create functions by shuffling around parts of functions and seeing how good each function is at a certain task. A child algorithm will depend on its parents, but Markov models are interested mostly in predicting what thing will come next in a sequence, not creating a new chunk of code. You might be able to use a Markov model to spit out a candidate function, though, depending on how simple the ""alphabet"" is. You could even then give more weight to the transitions in successful algorithms.</p>
",,0,2016-08-02T16:40:20.240,,73,2016-08-02T16:40:20.240,,,,,75.0,37.0,2,7,,,,59.64,11.31,9.85,0.0,0.0,35.0,A Markov model includes the probability of transitioning to each state considering the current state Each state may be just one point whether it rained on specific day for instance or it might look like multiple things like a pair of words Youve probably seen automatically generated weird text that almost makes sense like Garkov the output of a Markov model based on the Garfield comic strips That Coding Horror article also mentions the applications of Markov techniques to Googles PageRank Markov models are really only powerful when they have a lot of input to work with If a machine looked through a lot of English text it would get a pretty good idea of what words generally come after other words Or after looking through someones location history it could figure out where that person is likely to go next from a certain place Constantly updating the input corpus as more data is received would let the machine tune the probabilities of all the state transitions Genetic algorithms are fairly different things They create functions by shuffling around parts of functions and seeing how good each function is at a certain task A child algorithm will depend on its parents but Markov models are interested mostly in predicting what thing will come next in a sequence not creating a new chunk of code You might be able to use a Markov model to spit out a candidate function though depending on how simple the alphabet is You could even then give more weight to the transitions in successful algorithms
141.0,3.0,"<p>I've heard the terms strong-AI and weak-AI used.  Are these well defined terms or subjective ones?  How are they generally defined?</p>
",,0,2016-08-02T16:42:35.817,4.0,74,2016-09-19T16:08:55.187,2016-08-03T18:24:46.713,,29.0,,55.0,,1,21,<strong-ai><weak-ai><terminology>,What is the difference between strong-AI and weak-AI?,1123.0,55.91,10.16,10.0,0.0,0.0,6.0,Ive heard the terms strongAI and weakAI used Are these well defined terms or subjective ones How are they generally defined
,1.0,"<p>As AI gains capabilities, and becomes more prevalent in society, our legal system will encounter questions it has not encountered before.  For example, if a self-driving car is involved in an accident while being controlled by the AI, who is at fault?  The ""driver"" (who's really just a passenger), the programmer(s) who made the AI, or the AI itself?</p>

<p>So, what's on the cutting edge in terms of these kinds of issues at the intersection of law and artificial intelligence?</p>
",2016-08-02T20:12:26.130,0,2016-08-02T16:42:53.110,,75,2016-08-08T17:45:06.713,2016-08-02T18:00:37.377,,33.0,,33.0,,1,-3,<legal>,What's the state of the art w.r.t research on the legal aspects of Artificial Intelligence?,37.0,59.64,11.08,9.76,0.0,0.0,20.0,As AI gains capabilities and becomes more prevalent in society our legal system will encounter questions it has not encountered before For example if a selfdriving car is involved in an accident while being controlled by the AI who is at fault The driver whos really just a passenger the programmers who made the AI or the AI itself So whats on the cutting edge in terms of these kinds of issues at the intersection of law and artificial intelligence
,,"<p>There are at least two questions in your question: </p>

<blockquote>
  <p>What are some of the methods used to program the successful go playing program?</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>Are those methods considered to be artificial intelligence?</p>
</blockquote>

<p>The first question is deep and technical, the second broad and philosophical.</p>

<p>The methods have been described in: <a href=""https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf"">Mastering the Game of Go with Deep Neural Networks and Tree Search</a>.</p>

<p>The problem of Go or perfect information games in general is that:</p>

<blockquote>
  <p>exhaustive search is infeasible.</p>
</blockquote>

<p>So the methods will concentrate on shrinking the search space in an efficient way.</p>

<p>Methods and structures described in the paper include:</p>

<ul>
<li>learning from expert human players in a supervised fashion</li>
<li>learning by playing against itself (reinforcement learning)</li>
<li>Monte-Carlo tree search (MCTS) combined with policy and value networks</li>
</ul>

<p>The second question has no definite answer, as you will have at least two angles on AI: <a href=""https://en.wikipedia.org/wiki/Chinese_room#Strong_AI"">strong</a> and <a href=""https://en.wikipedia.org/wiki/Weak_AI"">weak</a>.</p>

<blockquote>
  <p>All real-world systems labeled ""artificial intelligence"" of any sort are <strong>weak AI at most</strong>.</p>
</blockquote>

<p>So yes, it is artificial intelligence, but it is non-sentient.</p>
",,0,2016-08-02T16:43:14.880,,76,2016-08-02T16:43:14.880,,,,,28.0,54.0,2,7,,,,52.29,13.17,9.78,0.0,0.0,27.0,There are at least two questions in your question What are some of the methods used to program the successful go playing program and Are those methods considered to be artificial intelligence The first question is deep and technical the second broad and philosophical The methods have been described in Mastering the Game of Go with Deep Neural Networks and Tree Search The problem of Go or perfect information games in general is that exhaustive search is infeasible So the methods will concentrate on shrinking the search space in an efficient way Methods and structures described in the paper include learning from expert human players in a supervised fashion learning by playing against itself reinforcement learning MonteCarlo tree search MCTS combined with policy and value networks The second question has no definite answer as you will have at least two angles on AI strong and weak All realworld systems labeled artificial intelligence of any sort are weak AI at most So yes it is artificial intelligence but it is nonsentient
131.0,4.0,"<p>I know that language of <strong><code>Lisp</code></strong> was used early on when working on artificial intelligence problems.  Is it still being used today for significant work?  If not, is there a new language that has taken its place as the most common one being used for work in AI today?</p>
",,0,2016-08-02T16:45:01.487,3.0,77,2016-08-04T00:21:27.440,2016-08-02T17:44:07.997,,102.0,,55.0,,1,12,<history><programming-languages><lisp>,Is Lisp still being used to tackle AI problems?,239.0,72.16,7.54,8.38,4.0,0.0,4.0,I know that language of was used early on when working on artificial intelligence problems Is it still being used today for significant work If not is there a new language that has taken its place as the most common one being used for work in AI today
,,"<p>One person working in this space is Dr. Woody Barfield.  He just wrote a book titled ""<a href=""http://www.springer.com/us/book/9783319250489"" rel=""nofollow"">Cyberhumans: Our Future With Machines</a>"" that focuses largely on the legal/policy issues around AI (and related topics).  In addition to the book, he is continuing with other research in this area.</p>
",,0,2016-08-02T16:45:51.657,,79,2016-08-08T17:45:06.713,2016-08-08T17:45:06.713,,72.0,,33.0,75.0,2,1,,,,55.54,11.19,11.13,0.0,0.0,11.0,One person working in this space is Dr Woody Barfield He just wrote a book titled Cyberhumans Our Future With Machines that focuses largely on the legalpolicy issues around AI and related topics In addition to the book he is continuing with other research in this area
85.0,2.0,"<p>What are the specific requirements of the Turing Test?</p>

<ul>
<li>What requirements if any must the evaluator fulfill in order to be qualified to give the test?</li>
<li>Must there always be two participants to the conversation (one human and one computer) or can there be more</li>
<li>Are placebo tests (where there is not actually a computer involbed) allowed or encouraged?</li>
<li>Can there be multiple evaluators? If so does the decision need to be unanimous among all evaluators in order for the machine to have passed the test?</li>
</ul>
",,0,2016-08-02T16:46:07.253,,80,2016-08-02T17:42:05.387,,,,,96.0,,1,5,<turing-test>,Specific requirements of the Turing Test,38.0,54.02,10.26,9.45,0.0,0.0,9.0,What are the specific requirements of the Turing Test What requirements if any must the evaluator fulfill in order to be qualified to give the test Must there always be two participants to the conversation one human and one computer or can there be more Are placebo tests where there is not actually a computer involbed allowed or encouraged Can there be multiple evaluators If so does the decision need to be unanimous among all evaluators in order for the machine to have passed the test
,2.0,"<p>I believe that statistical AI uses inductive thought processes.  For example, deducing a trend from a pattern.  What are some examples of successfully applying statistical AI to real world problems.</p>
",,0,2016-08-02T16:49:40.830,,81,2016-08-03T12:31:34.307,,,,,55.0,,1,2,<statistical-ai>,What are some examples of statistical AI?,36.0,52.87,13.55,10.45,0.0,0.0,4.0,I believe that statistical AI uses inductive thought processes For example deducing a trend from a pattern What are some examples of successfully applying statistical AI to real world problems
,0.0,"<p>How do the basic components <a href=""https://en.wikipedia.org/wiki/Optimality_theory"" rel=""nofollow"">optimality theory</a> apply to artificial intelligence?</p>

<p>How is optimality theory related to neural network research?</p>
",,1,2016-08-02T16:50:15.330,,82,2016-08-02T16:54:10.720,2016-08-02T16:54:10.720,,28.0,,96.0,,1,1,<neural-networks>,Optimality theory and WI,16.0,19.03,16.33,12.82,0.0,0.0,2.0,How do the basic components optimality theory apply to artificial intelligence How is optimality theory related to neural network research
,,"<p>Yes, as Franck has rightly put, ""backprop"" means backpropogation, which is frequently used in the domain of neural networks for error optimization.</p>

<p>For a detailed explanation, I would point out <a href=""http://neuralnetworksanddeeplearning.com/chap2.html"" rel=""nofollow"">this tutorial</a> on the concept of backpropogation by a very good book of Michael Nielsen. </p>
",,0,2016-08-02T16:54:40.380,,83,2016-08-02T16:54:40.380,,,,,101.0,1.0,2,1,,,,57.1,13.58,11.07,0.0,0.0,8.0,Yes as Franck has rightly put backprop means backpropogation which is frequently used in the domain of neural networks for error optimization For a detailed explanation I would point out this tutorial on the concept of backpropogation by a very good book of Michael Nielsen
,2.0,"<p>Some programs do exhaustive searches for a solution while others do heuristic searches.  For example, in chess, the search for the best next move tends to be more exhaustive in nature whereas in go, the search for the best next move tends to be more heuristic in nature due to the much larger search space.</p>

<p>Is the technique of brute force exhaustive searching for a good answer considered to be AI or is it generally required that heuristic algorithms be used before being deemed AI?  If so, is the chess playing computer beating a human professional seen as a meaningful milestone?</p>
",,0,2016-08-02T16:55:37.050,,84,2016-08-09T18:57:04.447,2016-08-09T18:57:04.447,,8.0,,55.0,,1,4,<gaming><search><chess><heuristics>,Are methods of exhaustive search considered to be AI?,42.0,54.36,10.57,9.26,0.0,0.0,8.0,Some programs do exhaustive searches for a solution while others do heuristic searches For example in chess the search for the best next move tends to be more exhaustive in nature whereas in go the search for the best next move tends to be more heuristic in nature due to the much larger search space Is the technique of brute force exhaustive searching for a good answer considered to be AI or is it generally required that heuristic algorithms be used before being deemed AI If so is the chess playing computer beating a human professional seen as a meaningful milestone
,,"<p>The ""Turing Test"" is generally taken to mean an updated version of the Imitation Game Alan Turing proposed in his 1951 paper of the same name. An early version had a human (male or female) and a computer, and a judge had to decide which is which, and what gender they were if human. If they were correct less than 50% then the computer was considered ""intelligent.""</p>

<p>The current generally accepted version requires only one contestant, and a judge to decide whether it is human or machine. So yes, sometimes this will be a placebo, effectively, if we consider a human to be a placebo.</p>

<p>Your first and fourth questions are related - and there are no strict guidelines. If the computer can fool a greater number of judges then it will of course be considered a better AI.</p>

<p>The University of Toronto has a validity section in <a href=""http://www.psych.utoronto.ca/users/reingold/courses/ai/turing.html"">this paper on Turing</a>, which includes a link to <a href=""http://ciips.ee.uwa.edu.au/Papers/Technical_Reports/1997/05/Index.html"">Jason Hutchens' commentary</a> on why the Turing test may not be relevant (humans may also fail it) and the <a href=""http://www.loebner.net/Prizef/loebner-prize.html"">Loebner Prize</a>, a formal instantiation of a Turing Test .</p>
",,0,2016-08-02T16:59:11.467,,85,2016-08-02T17:07:15.277,2016-08-02T17:07:15.277,,97.0,,97.0,80.0,2,7,,,,56.59,9.58,9.58,0.0,0.0,27.0,The Turing Test is generally taken to mean an updated version of the Imitation Game Alan Turing proposed in his 1951 paper of the same name An early version had a human male or female and a computer and a judge had to decide which is which and what gender they were if human If they were correct less than 50 then the computer was considered intelligent The current generally accepted version requires only one contestant and a judge to decide whether it is human or machine So yes sometimes this will be a placebo effectively if we consider a human to be a placebo Your first and fourth questions are related and there are no strict guidelines If the computer can fool a greater number of judges then it will of course be considered a better AI The University of Toronto has a validity section in this paper on Turing which includes a link to Jason Hutchens commentary on why the Turing test may not be relevant humans may also fail it and the Loebner Prize a formal instantiation of a Turing Test
93.0,2.0,"<p>How is a neural network having the ""deep"" adjective actually distinguished from other similar networks?</p>
",,0,2016-08-02T16:59:30.683,4.0,86,2016-08-27T19:13:51.923,2016-08-27T19:13:51.923,,145.0,,8.0,,1,11,<neural-networks><deep-network><comparison>,How is deep learning different from other neural networks?,329.0,39.33,16.52,13.85,0.0,0.0,3.0,How is a neural network having the deep adjective actually distinguished from other similar networks
,,"<p>There are several examples. For example, one instance of using Statistical AI from my workplace is:</p>

<ol>
<li>Analyzing the behaviour of the customer and their food-ordering trends, and then trying to upsell by reccommending them the dishes which they might like to order. This can be done through the apriori and FP-growth algorithms. We then, automated the algorithm, and then the algorithm improves itself through a <code>Ordered/Not-Ordered</code> metric.</li>
<li>Self-driving cars. They use reinforcement and supervised learning algorithms for learning the route and the gradient/texture of the surface.</li>
</ol>
",,0,2016-08-02T16:59:50.293,,87,2016-08-02T16:59:50.293,,,,,101.0,81.0,2,6,,,,54.22,14.67,10.05,19.0,0.0,15.0,There are several examples For example one instance of using Statistical AI from my workplace is Analyzing the behaviour of the customer and their foodordering trends and then trying to upsell by reccommending them the dishes which they might like to order This can be done through the apriori and FPgrowth algorithms We then automated the algorithm and then the algorithm improves itself through a metric Selfdriving cars They use reinforcement and supervised learning algorithms for learning the route and the gradienttexture of the surface
2573.0,1.0,"<p>What is the effectiveness of pre-training of unsupervised deep learning?</p>

<p>Does unsupervised deep learning actually work?</p>
",,0,2016-08-02T17:01:18.317,,88,2016-12-30T07:14:36.183,2016-12-29T21:05:13.147,,4446.0,,8.0,,1,3,<deep-learning><unsupervised-learning>,Why does unsupervised pre-training help in deep learning?,62.0,37.98,18.35,9.95,0.0,0.0,3.0,What is the effectiveness of pretraining of unsupervised deep learning Does unsupervised deep learning actually work
,,"<p>If a computer is just brute-forcing the solution, it's not learning anything or using any kind of intelligence at all, and therefore it shouldn't be called ""artificial intelligence."" It has to make decisions based on what's happened before in similar instances. For something to be intelligent, it needs a way to keep track of what it's learned. A chess program might have a really awesome measurement algorithm to use on every possible board state, but if it's always trying each state and never storing what it learns about different approaches, it's not intelligent.</p>
",,2,2016-08-02T17:02:05.890,,89,2016-08-02T17:33:59.340,2016-08-02T17:33:59.340,,75.0,,75.0,84.0,2,3,,,,47.93,12.77,10.73,0.0,0.0,18.0,If a computer is just bruteforcing the solution its not learning anything or using any kind of intelligence at all and therefore it shouldnt be called artificial intelligence It has to make decisions based on whats happened before in similar instances For something to be intelligent it needs a way to keep track of what its learned A chess program might have a really awesome measurement algorithm to use on every possible board state but if its always trying each state and never storing what it learns about different approaches its not intelligent
97.0,1.0,"<p>Are search engines considered AI because of the way they analyze what you search for and remember it? Or how they send you ads of what you've searched for recently? Is this considered AI or just smart?</p>
",,1,2016-08-02T17:04:35.297,2.0,91,2016-08-11T10:47:36.777,2016-08-11T10:47:36.777,,29.0,,5.0,,1,9,<search>,Are search engines considered AI?,86.0,75.91,7.7,7.23,0.0,0.0,4.0,Are search engines considered AI because of the way they analyze what you search for and remember it Or how they send you ads of what youve searched for recently Is this considered AI or just smart
250.0,5.0,"<p>The following <a href=""http://www.evolvingai.org/fooling"" rel=""nofollow noreferrer"">page</a>/<a href=""http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf"" rel=""nofollow noreferrer"">study</a> demonstrates that the deep neural networks are easily fooled by giving high confidence predictions for unrecognisable images, e.g.</p>

<p><a href=""https://i.stack.imgur.com/7pgrH.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7pgrH.jpg"" alt=""Evolved images that are unrecognisable to humans, but that state-of-the-art DNNs trained on ImageNet believe with &gt;= 99.6% certainty to be a familiar object. This result highlights differences between how DNNs and humans recognise objects. Directly and indirectly encoded images""></a></p>

<p><a href=""https://i.stack.imgur.com/pBm48.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pBm48.png"" alt=""Evolving images to match DNN classes produces a tremendous diversity of images. The mean DNN confidence scores for these images is 99.12% for the listed class, meaning that the DNN believes with near-certainty that the image is that type of thing. Shown are images selected to showcase diversity from 5 independent evolutionary runs. The images shed light on what the DNN network cares about, and what it does not, when classifying an image. For example, a school bus is alternating yellow and black lines, but does not need to have a windshield or wheels""></a></p>

<p>How this is possible? Can you please explain ideally in plain English?</p>
",,1,2016-08-02T17:05:27.590,7.0,92,2016-10-14T17:31:31.923,2016-08-04T12:03:04.183,,135.0,,8.0,,1,29,<deep-network><image-recognition>,How is it possible that deep neural networks are so easily fooled?,951.0,43.39,15.64,11.84,0.0,0.0,6.0,The following pagestudy demonstrates that the deep neural networks are easily fooled by giving high confidence predictions for unrecognisable images eg How this is possible Can you please explain ideally in plain English
,,"<p>Short answer: The difference is mostly in the number of layers. </p>

<p>For a long time, it was believed that ""1-2 hidden layers are enough for most tasks"" and it was impractical to use more than that, because training neural networks can be very computationally demanding.</p>

<p>Nowadays, computers are capable of much more, so people have started to use networks with more layers and found that they work very well for some tasks.</p>

<p>The word ""deep"" is there simply to distinguish these networks from the traditional, ""more shallow"" ones.</p>
",,0,2016-08-02T17:06:21.223,,93,2016-08-02T17:13:14.933,2016-08-02T17:13:14.933,,30.0,,30.0,86.0,2,15,,,,57.61,11.43,9.21,0.0,0.0,17.0,Short answer The difference is mostly in the number of layers For a long time it was believed that 12 hidden layers are enough for most tasks and it was impractical to use more than that because training neural networks can be very computationally demanding Nowadays computers are capable of much more so people have started to use networks with more layers and found that they work very well for some tasks The word deep is there simply to distinguish these networks from the traditional more shallow ones
132.0,1.0,"<p>In a feedforward neural network the inputs are fed directly to the outputs via a series of <strong>weights</strong>.</p>

<p>What purpose do the weights serve and how are they significant in this neural network?</p>
",,0,2016-08-02T17:06:46.317,,94,2016-08-02T21:44:47.760,2016-08-02T20:38:12.183,,151.0,,8.0,,1,3,<untagged>,What is the significance of weights in a feedforward neural network?,31.0,71.65,9.51,9.24,0.0,0.0,2.0,In a feedforward neural network the inputs are fed directly to the outputs via a series of weights What purpose do the weights serve and how are they significant in this neural network
,,"<p>Deep Learning is just (feed forward)neural networks with many layers.</p>

<p>However, deep belief networks, Deep Boltzman networks, etc are not considered(debatable) as Deep Learning, as their topology is different (they ave undirected networks in their topology).</p>

<p><a href=""http://stats.stackexchange.com/a/59854/84191"">Helpful Reference</a></p>
",,0,2016-08-02T17:08:48.340,,95,2016-08-02T17:08:48.340,,,,,101.0,86.0,2,7,,,,35.27,19.03,11.23,0.0,0.0,12.0,Deep Learning is just feed forwardneural networks with many layers However deep belief networks Deep Boltzman networks etc are not considereddebatable as Deep Learning as their topology is different they ave undirected networks in their topology Helpful Reference
98.0,2.0,"<p>I'm pretty sure this a noob-y question, but what is Deep Network? As of now it is the most popular tag on AI. Is there a reason for this? </p>

<hr>

<p>Please note, I am not asking how to distinguish a deep network from a neural network, I am simply asking for the definition of deep network.</p>
",,3,2016-08-02T17:12:58.533,,96,2016-08-03T15:20:07.903,2016-08-03T15:20:07.903,,5.0,,5.0,,1,4,<deep-network>,What is Deep Network?,75.0,82.85,5.73,8.34,0.0,0.0,9.0,Im pretty sure this a nooby question but what is Deep Network As of now it is the most popular tag on AI Is there a reason for this Please note I am not asking how to distinguish a deep network from a neural network I am simply asking for the definition of deep network
,,"<p>I believe it would be more correct to say that (some) search engines <em>use</em> AI.  Broadly saying ""search engines are AI"" is not really correct.  At core, most search engines are nothing more than an inverted text index using something like tf–idf scoring.  That's a very mechanical / simple thing that nobody would really call AI. </p>

<p>But more sophisticated search engines may <em>use</em> AI or AI techniques to do things like semantic analysis - so they can actually ""answer questions"" instead of just looking up words in an index.  </p>
",,0,2016-08-02T17:15:22.887,,97,2016-08-02T17:15:22.887,,,,,33.0,91.0,2,11,,,,62.27,10.79,9.22,0.0,0.0,15.0,I believe it would be more correct to say that some search engines use AI Broadly saying search engines are AI is not really correct At core most search engines are nothing more than an inverted text index using something like tf–idf scoring Thats a very mechanical simple thing that nobody would really call AI But more sophisticated search engines may use AI or AI techniques to do things like semantic analysis so they can actually answer questions instead of just looking up words in an index
,,"<p><strong>Deep Network is nothing but a neural network which has multiple layers.</strong> <code>Multiple</code> can be subjective.</p>

<p>However, any network which has >=6 or 7 layers are considered deep. So, the above would form a very basic definition of a deep network. </p>
",,0,2016-08-02T17:18:12.383,,98,2016-08-02T17:18:12.383,,,,,101.0,96.0,2,6,,,,78.25,8.33,9.66,8.0,0.0,8.0,Deep Network is nothing but a neural network which has multiple layers can be subjective However any network which has 6 or 7 layers are considered deep So the above would form a very basic definition of a deep network
,,"<p>Deep networks have two main differences with 'normal' networks.</p>

<p>The first is that computational power and training datasets have grown immensely, meaning that it's practical to run larger networks and statistically valid (that is, we have enough training examples that we won't just run into over-fitting problems with larger networks).</p>

<p>The second is that back propagation is limited the more layers you have; each layer represents a gradient of the error, and so by the time one is about six layers deep there isn't much error left to modify the neuron weights. But one might reasonably expect earlier neurons to be more important than later neurons, since they represent 'concepts' that are closer to the raw inputs.</p>

<p>New training techniques sidestep this problem, typically by doing unsupervised learning on the raw inputs, creating higher-level 'concepts' that are then useful as inputs for supervised learning.</p>

<p>(For example, consider the problem of determining whether or not an image contains a cat from the pixels. The early layers of the network should be doing things like detecting edges, which one could expect to be shared among all images and mostly independent of what one is trying to do with the output layers, thus also hard to train through 'cat-not cat' signals many layers up.</p>
",,0,2016-08-02T17:36:52.900,,100,2016-08-03T15:13:11.603,2016-08-03T15:13:11.603,,5.0,,10.0,96.0,2,4,,,,49.38,13.12,10.82,0.0,0.0,34.0,Deep networks have two main differences with normal networks The first is that computational power and training datasets have grown immensely meaning that its practical to run larger networks and statistically valid that is we have enough training examples that we wont just run into overfitting problems with larger networks The second is that back propagation is limited the more layers you have each layer represents a gradient of the error and so by the time one is about six layers deep there isnt much error left to modify the neuron weights But one might reasonably expect earlier neurons to be more important than later neurons since they represent concepts that are closer to the raw inputs New training techniques sidestep this problem typically by doing unsupervised learning on the raw inputs creating higherlevel concepts that are then useful as inputs for supervised learning For example consider the problem of determining whether or not an image contains a cat from the pixels The early layers of the network should be doing things like detecting edges which one could expect to be shared among all images and mostly independent of what one is trying to do with the output layers thus also hard to train through catnot cat signals many layers up
,,"<p>If one thinks of intelligence as a continuous measure of optimization power (that is, how much better are outcomes for any unit of cognitive effort expended), then exhaustive search has non-zero intelligence (in that it does actually give better outcomes as more effort is expended) but <em>very, very low</em> intelligence (as the outcomes are better mostly by luck, and the amount of effort expended can be impossibly large).</p>
",,0,2016-08-02T17:38:38.237,,101,2016-08-02T17:38:38.237,,,,,10.0,84.0,2,3,,,,-6.0,13.83,12.12,0.0,0.0,12.0,If one thinks of intelligence as a continuous measure of optimization power that is how much better are outcomes for any unit of cognitive effort expended then exhaustive search has nonzero intelligence in that it does actually give better outcomes as more effort is expended but very very low intelligence as the outcomes are better mostly by luck and the amount of effort expended can be impossibly large
,,"<p>There are really two questions here, that I can see.  One is ""what were the specific requirements of the original Turing test, as stated by Turing himself.""   The other is ""what should the specific requirements of a modern Turing test be?""  Things have advanced a lot since Turing's day, and I think it's reasonable for us to consider extending / modifying his test to reflect our current understanding.</p>

<p>The answer to the first question is easy enough to look up, so I think the interesting one is the second one.  What <em>should</em> a test to determine intelligence look like? With that in mind, I think the answer to all four questions posed by the OP is ""it depends"".   I don't think there's universal consensus on how to structure a perfect Turing test, so a given experimenter is really free to set things up however he/she wants.  </p>

<p>This is all, of course, based on the assumption that the Turing test, or a Turing-test-like test is actually of value.  That's not necessarily a given.  Consider that, to some extent, what we're talking about is designing an AI with an exceptional ability for deceit!  That is, assuming the questioner is allowed to simply ask ""are you human"", then we have to assume that the AI is supposed to lie if it wants to pass the test.  So one might rightly ask, is designing a system to be really good at telling lies, a valuable approach to AI?</p>
",,0,2016-08-02T17:42:05.387,,102,2016-08-02T17:42:05.387,,,,,33.0,80.0,2,1,,,,69.41,9.28,8.66,0.0,0.0,46.0,There are really two questions here that I can see One is what were the specific requirements of the original Turing test as stated by Turing himself The other is what should the specific requirements of a modern Turing test be Things have advanced a lot since Turings day and I think its reasonable for us to consider extending modifying his test to reflect our current understanding The answer to the first question is easy enough to look up so I think the interesting one is the second one What should a test to determine intelligence look like With that in mind I think the answer to all four questions posed by the OP is it depends I dont think theres universal consensus on how to structure a perfect Turing test so a given experimenter is really free to set things up however heshe wants This is all of course based on the assumption that the Turing test or a Turingtestlike test is actually of value Thats not necessarily a given Consider that to some extent what were talking about is designing an AI with an exceptional ability for deceit That is assuming the questioner is allowed to simply ask are you human then we have to assume that the AI is supposed to lie if it wants to pass the test So one might rightly ask is designing a system to be really good at telling lies a valuable approach to AI
,1.0,"<p>I believe that Classical AI uses deductive thought processes. For example, given as a set of constraints, deduce a conclusion.  What are some examples of successfully applying Classical AI to real world problems.</p>
",,0,2016-08-02T17:47:41.750,,103,2016-08-02T19:32:26.097,,,,,55.0,,1,1,<classical-ai>,What are some examples of Classical AI?,49.0,51.85,12.97,10.88,0.0,0.0,5.0,I believe that Classical AI uses deductive thought processes For example given as a set of constraints deduce a conclusion What are some examples of successfully applying Classical AI to real world problems
,2.0,"<p>In <a href=""https://youtu.be/oSdPmxRCWws?t=30"">this video</a> an expert says, ""One way of thinking about what intelligence is [specifically with regard to artificial intelligence], is as an optimization process.""</p>

<p>Can intelligence always be thought of as an optimization process, and can artificial intelligence always be modeled as an optimization problem? What about pattern recognition? Or is he mischaracterizing?</p>
",,0,2016-08-02T17:56:02.743,3.0,104,2016-08-06T19:03:40.640,2016-08-02T18:01:01.713,,46.0,,46.0,,1,8,<optimization><agi>,Can artificial intelligence be thought of as optimization?,70.0,32.39,15.94,9.57,0.0,0.0,11.0,In this video an expert says One way of thinking about what intelligence is specifically with regard to artificial intelligence is as an optimization process Can intelligence always be thought of as an optimization process and can artificial intelligence always be modeled as an optimization problem What about pattern recognition Or is he mischaracterizing
,,"<p>The term <em>classical AI</em> refers to the concept of intelligence that was broadly accepted after the <a href=""https://en.wikipedia.org/wiki/Dartmouth_Conferences"" rel=""nofollow"">Dartmouth Conference</a> and basically refers to a kind of intelligence that is strongly symbolic and oriented to logic and language processing. One basic point is the duality <strong>body</strong> vs. <strong>mind</strong>. It's in this period that the mind start to be compared with computer software.</p>

<h2>Two classical historic examples of this conception of intelligence</h2>

<ul>
<li><p><a href=""https://upload.wikimedia.org/wikipedia/commons/b/be/Deep_Blue.jpg"" rel=""nofollow"">Deep Blue</a>, whose aim in life was to be the master of chess, ruling over the (not-so) intelligent mankind</p></li>
<li><p><a href=""http://www.manifestation.com/neurotoys/eliza.php3"" rel=""nofollow"">Eliza</a>, a computer-based therapist that turned out to <a href=""https://en.wikipedia.org/wiki/Joseph_Weizenbaum"" rel=""nofollow"">trigger a critic</a> to the classical AI</p></li>
</ul>

<h2>Two technical examples of classical AI</h2>

<ul>
<li><p>Expert systems, which are computer programs that strongly rely on the type of constrains and conclusions that you refer to, *in order to accomplish feats of apparent intelligence](<a href=""https://www.britannica.com/technology/expert-system"" rel=""nofollow"">https://www.britannica.com/technology/expert-system</a>)</p></li>
<li><p>Fuzzy logic, <a href=""https://de.mathworks.com/help/fuzzy/what-is-fuzzy-logic.html?requestedDomain=www.mathworks.com"" rel=""nofollow""><em>which is an extension of multivalued logic</em></a>, but with continuous values instead of discrete ones</p></li>
</ul>

<p>Note that in all cases the <em>hardware</em> (once compared with the <em>body</em>) does not play any role: Intelligence is abstract and independent from the material world.</p>
",,0,2016-08-02T18:12:04.120,,105,2016-08-02T19:32:26.097,2016-08-02T19:32:26.097,,70.0,,70.0,103.0,2,5,,,,26.48,14.92,11.39,0.0,0.0,32.0,The term classical AI refers to the concept of intelligence that was broadly accepted after the Dartmouth Conference and basically refers to a kind of intelligence that is strongly symbolic and oriented to logic and language processing One basic point is the duality body vs mind Its in this period that the mind start to be compared with computer software Two classical historic examples of this conception of intelligence Deep Blue whose aim in life was to be the master of chess ruling over the notso intelligent mankind Eliza a computerbased therapist that turned out to trigger a critic to the classical AI Two technical examples of classical AI Expert systems which are computer programs that strongly rely on the type of constrains and conclusions that you refer to in order to accomplish feats of apparent intelligencehttpswwwbritannicacomtechnologyexpertsystem Fuzzy logic which is an extension of multivalued logic but with continuous values instead of discrete ones Note that in all cases the hardware once compared with the body does not play any role Intelligence is abstract and independent from the material world
,,"<blockquote>
  <p>Now that this milestone has been reached, does that represent a significant advance in artificial intelligence techniques or was it just a matter of ever more processing power being applied to the problem? </p>
</blockquote>

<p>Neither, really. It is a milestone and a significant advance in computers beating humans in games, but the techniques used are only relevant to that game, not for other purposes in AI. </p>

<p>The solution lies in humans analysing the game and implementing algorithms for finding a good move. This is the main reason that a computer can beat the humans, together with the fact that it can calculate much faster and that it doesn't make really bad moves by not seeing something.</p>

<p>Processing power helps, but the game-tree complexity for go is very large, estimated to be larger than 10<sup>200</sup>, whereas the game-tree complexity for chess is only 10<sup>120</sup> (known as the Shannon number), so chess is less hard. This means that for neither chess nor go a database can be created with all possible positions. </p>

<p>The fact that Deep Blue beat Kasparov in a six-game match in 1997 was quite a development, since this was one of the first ""hard"" games where a computer beat a top human. But it still isn't really Artificial Intelligence, more analysing the game. Implementing an opening and endgame book was a large part for chess, the middle game was done using analysis, I don't know the details. </p>
",,0,2016-08-02T18:14:37.117,,106,2016-08-02T18:41:35.577,2016-08-02T18:41:35.577,,29.0,,29.0,54.0,2,2,,,,53.24,10.91,9.4,0.0,0.0,33.0,Now that this milestone has been reached does that represent a significant advance in artificial intelligence techniques or was it just a matter of ever more processing power being applied to the problem Neither really It is a milestone and a significant advance in computers beating humans in games but the techniques used are only relevant to that game not for other purposes in AI The solution lies in humans analysing the game and implementing algorithms for finding a good move This is the main reason that a computer can beat the humans together with the fact that it can calculate much faster and that it doesnt make really bad moves by not seeing something Processing power helps but the gametree complexity for go is very large estimated to be larger than 10200 whereas the gametree complexity for chess is only 10120 known as the Shannon number so chess is less hard This means that for neither chess nor go a database can be created with all possible positions The fact that Deep Blue beat Kasparov in a sixgame match in 1997 was quite a development since this was one of the first hard games where a computer beat a top human But it still isnt really Artificial Intelligence more analysing the game Implementing an opening and endgame book was a large part for chess the middle game was done using analysis I dont know the details
,,"<p>There're many online services that uses statistical neural networks for recommendations. For example we have <a href=""http://imhonet.ru"" rel=""nofollow"">well known service</a> here in Russia that could give it's users recommendations for movies and shows to watch and books to read. It's recommendation core is based on many things known about user: what movies/books he or she loves and what not, analyses his or her friends likes and do on. While you have only few items rayed it will give you very strange recommendations but then it becomes more correct and really could give you some true gems.</p>
",,2,2016-08-02T18:16:43.960,,107,2016-08-03T12:31:34.307,2016-08-03T12:31:34.307,,112.0,,112.0,81.0,2,2,,,,56.08,11.26,9.84,0.0,0.0,10.0,Therere many online services that uses statistical neural networks for recommendations For example we have well known service here in Russia that could give its users recommendations for movies and shows to watch and books to read Its recommendation core is based on many things known about user what moviesbooks he or she loves and what not analyses his or her friends likes and do on While you have only few items rayed it will give you very strange recommendations but then it becomes more correct and really could give you some true gems
,2.0,"<p>What specific advantages of declarative languages make them more applicable to AI than imperative languages?  What can declarative languages do easily that other languages styles find difficult for this kind of problem?</p>
",,3,2016-08-02T18:17:44.297,1.0,108,2016-08-02T21:09:00.547,2016-08-02T21:09:00.547,,71.0,,69.0,,1,3,<declarative-programming>,What are the main advantages of using declarative programming languages for building AI?,86.0,29.86,16.3,10.35,0.0,0.0,2.0,What specific advantages of declarative languages make them more applicable to AI than imperative languages What can declarative languages do easily that other languages styles find difficult for this kind of problem
,0.0,"<p>In years past, GOFAI (Good Old Fashioned AI) was heavily based on ""rules"" and <a href=""https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence"" rel=""nofollow"">symbolic computation</a> based on rules.  Unfortunately, that approach ran into stumbling blocks, and the world moved heavily towards statistical / probabilistic approaches leading to the current wave of interest in ""machine learning"".</p>

<p>It seems though, that the symbolic / rule based approach probably still has application. So, could one ""learn"" rules using a probabilistic <a href=""https://en.wikipedia.org/wiki/Rule_induction"" rel=""nofollow"">rule induction</a> method, and then layer symbolic computation on top?  If so, how could the whole process be made truly two-way, so that something ""learned"" from processing rules, can be fed back into how the system learns rules? </p>
",,0,2016-08-02T18:29:19.443,,109,2016-08-02T18:29:19.443,,,,,33.0,,1,1,<gofai><symbolic-computing>,"Can rule induction be considered a way to ""hybridize"" probabilistic / statistical approaches and symbolic approaches?",18.0,50.36,14.39,10.59,0.0,0.0,27.0,In years past GOFAI Good Old Fashioned AI was heavily based on rules and symbolic computation based on rules Unfortunately that approach ran into stumbling blocks and the world moved heavily towards statistical probabilistic approaches leading to the current wave of interest in machine learning It seems though that the symbolic rule based approach probably still has application So could one learn rules using a probabilistic rule induction method and then layer symbolic computation on top If so how could the whole process be made truly twoway so that something learned from processing rules can be fed back into how the system learns rules
134.0,12.0,"<p>Obviously driverless cars aren't perfect, so imagine that the Google car (as an example) got into difficult situation.</p>

<p>Here are a few examples of unfortunate situations caused by set of events:</p>

<ul>
<li>the car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),</li>
<li>avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,</li>
<li>killing animal on the street in favour of human being,</li>
<li>changing lanes to crash into another car to avoid killing a dog,</li>
</ul>

<p>And here are few dilemmas:</p>

<ul>
<li>Does the algorithm recognize the difference between a human being and an animal?</li>
<li>Does the size of the human being or animal matter?</li>
<li>Does it count how many passengers it has vs. people in the front?</li>
<li>Does it ""know"" when babies/children are on board?</li>
<li>Does it take into the account the age (e.g. killing the older first)?</li>
</ul>

<p>How would an algorithm decide what should it do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)?</p>

<p>Related articles:</p>

<ul>
<li><a href=""https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/"">Why Self-Driving Cars Must Be Programmed to Kill</a></li>
<li><a href=""https://www.technologyreview.com/s/539731/how-to-help-self-driving-cars-make-ethical-decisions/"">How to Help Self-Driving Cars Make Ethical Decisions</a></li>
</ul>
",,4,2016-08-02T18:57:57.550,8.0,111,2017-03-07T15:55:25.200,2016-08-12T15:22:36.363,,75.0,,8.0,,1,37,<algorithm><self-driving><decision-theory><ethics>,How could self-driving cars make ethical decisions about who to kill?,2371.0,59.53,10.68,9.21,0.0,0.0,38.0,Obviously driverless cars arent perfect so imagine that the Google car as an example got into difficult situation Here are a few examples of unfortunate situations caused by set of events the car is heading toward a crowd of 10 people crossing the road so it cannot stop in time but it can avoid killing 10 people by hitting the wall killing the passengers avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car killing animal on the street in favour of human being changing lanes to crash into another car to avoid killing a dog And here are few dilemmas Does the algorithm recognize the difference between a human being and an animal Does the size of the human being or animal matter Does it count how many passengers it has vs people in the front Does it know when babieschildren are on board Does it take into the account the age eg killing the older first How would an algorithm decide what should it do from the technical perspective Is it being aware of above counting the probability of kills or not killing people just to avoid its own destruction Related articles Why SelfDriving Cars Must Be Programmed to Kill How to Help SelfDriving Cars Make Ethical Decisions
,2.0,"<p>Which deep neural network is used in <a href=""https://en.wikipedia.org/wiki/Google_self-driving_car"" rel=""nofollow"">Google's driverless cars</a> to analyse the surroundings? Is this information is open?</p>
",,0,2016-08-02T18:59:44.230,,112,2016-11-05T10:34:25.450,,,,,8.0,,1,3,<deep-network><algorithm><self-driving>,Which machine learning algorithm is used in self-driving cars?,153.0,61.83,11.8,11.59,0.0,0.0,3.0,Which deep neural network is used in Googles driverless cars to analyse the surroundings Is this information is open
,1.0,"<p>Two common activation functions used in deep learning are the hyperbolic tangent function and the sigmoid activation function. I understand that the hyperbolic tangent is just a rescaling and translation of the sigmoid function (i,e tanh(z) = 2*sigma(z) - 1). </p>

<p>Is there a significant difference between these two activation functions, and in particular, <strong>when is one preferable to the other</strong>?</p>

<p>I realize that in some cases (like when estimating probabilities) outputs in the range of [0,1] are more convenient than outputs that range from [-1,1]. I want to know if there are differences <strong>other than convenience</strong> which distinguish the two activation functions.</p>
",,0,2016-08-02T19:02:33.003,1.0,113,2016-08-02T19:28:00.640,,,,,127.0,,1,6,<deep-network><neural-networks><machine-learning><hidden-layers>,What's the difference between hyperbolic tangent and sigmoid neurons?,60.0,42.72,14.74,9.68,0.0,0.0,26.0,Two common activation functions used in deep learning are the hyperbolic tangent function and the sigmoid activation function I understand that the hyperbolic tangent is just a rescaling and translation of the sigmoid function ie tanhz 2sigmaz 1 Is there a significant difference between these two activation functions and in particular when is one preferable to the other I realize that in some cases like when estimating probabilities outputs in the range of 01 are more convenient than outputs that range from 11 I want to know if there are differences other than convenience which distinguish the two activation functions
,,"<p>Quantum computers are super awesome at matrix multiplication, <a href=""http://twistedoakstudios.com/blog/Post8887_what-quantum-computers-do-faster-with-caveats"">with some limitations</a>. Quantum superposition allows each bit to be in <em>a lot</em> more states than just zero or one, and quantum gates can fiddle those bits in many different ways. Because of that, a quantum computer can process a lot of information at once for certain applications.</p>

<p>One of those applications is the <a href=""http://algorithmicassertions.com/quantum/2014/03/07/Building-your-own-Quantum-Fourier-Transform.html"">Fourier transform</a>, which is useful in a lot of problems, like <a href=""http://dsp.stackexchange.com/q/69"">signal analysis</a> and array processing. There's also <a href=""http://twistedoakstudios.com/blog/Post2644_grovers-quantum-search-algorithm"">Grover's quantum search algorithm</a>, which finds the single value for which a given function returns something different. If an AI problem can be expressed in a mathematical form <a href=""http://algorithmicassertions.com/quantum/2014/04/27/The-Not-Quantum-Laplace-Transform.html"">amenable to quantum computing</a>, it can receive great speedups. Sufficient speedups could transform an AI idea from ""theoretically interesting but insanely slow"" to ""quite practical once we get a good handle on quantum computing.""</p>
",,1,2016-08-02T19:02:42.300,,114,2016-08-02T19:02:42.300,,,,,75.0,36.0,2,11,,,,42.41,13.34,11.09,0.0,0.0,20.0,Quantum computers are super awesome at matrix multiplication with some limitations Quantum superposition allows each bit to be in a lot more states than just zero or one and quantum gates can fiddle those bits in many different ways Because of that a quantum computer can process a lot of information at once for certain applications One of those applications is the Fourier transform which is useful in a lot of problems like signal analysis and array processing Theres also Grovers quantum search algorithm which finds the single value for which a given function returns something different If an AI problem can be expressed in a mathematical form amenable to quantum computing it can receive great speedups Sufficient speedups could transform an AI idea from theoretically interesting but insanely slow to quite practical once we get a good handle on quantum computing
,,"<p>In my opinion python and java have taken over from LISP. Many people use them, there is a large amount of libraries available. And more importantly, they are easy to integrate in web technologies. </p>
",,0,2016-08-02T19:14:12.347,,115,2016-08-02T19:14:12.347,,,,,52.0,77.0,2,2,,,,51.55,9.32,10.7,0.0,0.0,5.0,In my opinion python and java have taken over from LISP Many people use them there is a large amount of libraries available And more importantly they are easy to integrate in web technologies
,,"<p>The advantage of a declarative language like Prolog is that it can be used to express facts and inference rules separately from control flow.  </p>

<p>This allows the developer to focus on the data and inference rules (the knowledge model), and allows the developer to extend the knowledge model more easily.</p>

<p>I should add that in practice, this dichotomy between facts/rules on the one hand, and control flow on the other, is not strict. A knowledge engineer who writes a code base in Prolog does sometimes have to consider control flow. The ""!"" operator is used so that the developer can influence the evaluation of the rules.</p>
",,0,2016-08-02T19:16:46.910,,116,2016-08-02T19:16:46.910,,,,,66.0,108.0,2,3,,,,50.16,11.08,9.49,0.0,0.0,15.0,The advantage of a declarative language like Prolog is that it can be used to express facts and inference rules separately from control flow This allows the developer to focus on the data and inference rules the knowledge model and allows the developer to extend the knowledge model more easily I should add that in practice this dichotomy between factsrules on the one hand and control flow on the other is not strict A knowledge engineer who writes a code base in Prolog does sometimes have to consider control flow The operator is used so that the developer can influence the evaluation of the rules
,,"<p>A good answer to this question depends on what you want to use the labels for.</p>

<p>When I think about ""optimization,"" I think about a solution space and a cost function; that is, there are many possible answers that could be returned and we can know what the cost is of any particular answer.</p>

<p>In this view, the answer is ""yes""--pattern recognition is a case where each pattern is a possible answer, and the optimization method is trying to find the one where the cost is lowest (that is, where the answer matches what you want it to match).</p>

<p>But most interesting optimization problems are characterized by exponential solution spaces and clean cost functions, and so can be thought of more as 'search' problems, whereas most pattern recognition problems are characterized by simple solution spaces and complicated cost functions, and it might feel unnatural to put the two of them together.</p>

<p>(In general, I do think that optimization and intelligence are deeply linked enough that optimization power is a good measure of intelligence, and certainly a better measure of the <em>practical</em> use of intelligence than pattern recognition.)</p>
",,0,2016-08-02T19:18:06.670,,117,2016-08-02T19:18:06.670,,,,,10.0,104.0,2,7,,,,33.51,12.37,9.46,0.0,0.0,28.0,A good answer to this question depends on what you want to use the labels for When I think about optimization I think about a solution space and a cost function that is there are many possible answers that could be returned and we can know what the cost is of any particular answer In this view the answer is yespattern recognition is a case where each pattern is a possible answer and the optimization method is trying to find the one where the cost is lowest that is where the answer matches what you want it to match But most interesting optimization problems are characterized by exponential solution spaces and clean cost functions and so can be thought of more as search problems whereas most pattern recognition problems are characterized by simple solution spaces and complicated cost functions and it might feel unnatural to put the two of them together In general I do think that optimization and intelligence are deeply linked enough that optimization power is a good measure of intelligence and certainly a better measure of the practical use of intelligence than pattern recognition
122.0,2.0,"<p><a href=""http://ai.stackexchange.com/questions/10/what-is-fuzzy-logic"">Fuzzy logic</a> is the logic where every statement can have any real truth value between 0 and 1.</p>

<p>How can fuzzy logic be used in creating AI? Is it useful for certain decision problems involving multiple inputs? Can you give an example of an AI that uses it?</p>
",,0,2016-08-02T19:22:20.577,,118,2016-08-02T20:10:13.770,,,,,29.0,,1,3,<fuzzy-logic>,How can fuzzy logic be used in creating AI?,36.0,76.22,6.83,10.15,0.0,0.0,4.0,Fuzzy logic is the logic where every statement can have any real truth value between 0 and 1 How can fuzzy logic be used in creating AI Is it useful for certain decision problems involving multiple inputs Can you give an example of an AI that uses it
,,"<p>I don't think it makes sense to decide activation functions based on desired properties of the output; you can easily insert a calibration step that maps the 'neural network score' to whatever units you actually want to use (dollars, probability, etc.).</p>

<p>So I think preference between different activation functions mostly boils down to the <a href=""https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions"" rel=""nofollow"">different properties</a> of those activation functions (like whether or not they're continuously differentiable). Because there's just a linear transformation between the two, I think that means there isn't a meaningful difference between them.</p>
",,0,2016-08-02T19:28:00.640,,119,2016-08-02T19:28:00.640,,,,,10.0,113.0,2,3,,,,33.58,16.26,10.7,0.0,0.0,18.0,I dont think it makes sense to decide activation functions based on desired properties of the output you can easily insert a calibration step that maps the neural network score to whatever units you actually want to use dollars probability etc So I think preference between different activation functions mostly boils down to the different properties of those activation functions like whether or not theyre continuously differentiable Because theres just a linear transformation between the two I think that means there isnt a meaningful difference between them
125.0,2.0,"<p>In <a href=""http://users.ox.ac.uk/~jrlucas/Godel/mmg.html"" rel=""nofollow"">Minds, Machines and Gödel</a> (1959), J. R. Lucas shows that any human mathematician can not be represented by an algorithmic automaton (a Turing Machine, but any computer is equivalent to it by the Church-Turing thesis), using Gödel's incompleteness theorem. </p>

<p>As I understand it, he states that since the computer is an algorithm and hence a formal system, Gödel's incompleteness theorem applies. But a human mathematician also has to work in a formal axiom system to prove a theorem, so wouldn't it apply there as well? </p>
",,0,2016-08-02T19:31:01.370,1.0,120,2016-08-02T19:49:46.050,,,,,29.0,,1,4,<philosophy><incompleteness-theorems>,How does Lucas's argument work?,40.0,49.65,12.01,10.03,0.0,0.0,20.0,In Minds Machines and Gödel 1959 J R Lucas shows that any human mathematician can not be represented by an algorithmic automaton a Turing Machine but any computer is equivalent to it by the ChurchTuring thesis using Gödels incompleteness theorem As I understand it he states that since the computer is an algorithm and hence a formal system Gödels incompleteness theorem applies But a human mathematician also has to work in a formal axiom system to prove a theorem so wouldnt it apply there as well
,,"<p>My impression is that fuzzy logic has mostly declined in relevance and <a href=""https://en.wikipedia.org/wiki/Probabilistic_logic"" rel=""nofollow"">probabilistic logic</a> has taken over its niche. (See the <a href=""https://en.wikipedia.org/wiki/Fuzzy_logic#Comparison_to_probability"" rel=""nofollow"">comparison on Wikipedia</a>.) The two are somewhat deeply related, and so it's mostly a change in perspective and language.</p>

<p>That is, fuzzy logic mostly applies to <em>labels</em> which have <em>uncertain ranges</em>. An object that's cool but not too cool <em>could</em> be described as either cold or warm, and fuzzy logic handles this by assigning some fractional truth value to the 'cold' and 'warm' labels and no truth to the 'hot' label.</p>

<p>Probabilistic logic focuses more on the probability of some fact given some observations, and is deeply focused on the uncertainty of observations. When we look at an email, we track our belief that the email is ""spam"" and shouldn't be shown to the user with some number, and adjust that number as we see evidence for and against it being spam.</p>
",,0,2016-08-02T19:34:16.217,,121,2016-08-02T20:10:13.770,2016-08-02T20:10:13.770,,10.0,,10.0,118.0,2,2,,,,57.71,10.39,9.26,0.0,0.0,26.0,My impression is that fuzzy logic has mostly declined in relevance and probabilistic logic has taken over its niche See the comparison on Wikipedia The two are somewhat deeply related and so its mostly a change in perspective and language That is fuzzy logic mostly applies to labels which have uncertain ranges An object thats cool but not too cool could be described as either cold or warm and fuzzy logic handles this by assigning some fractional truth value to the cold and warm labels and no truth to the hot label Probabilistic logic focuses more on the probability of some fact given some observations and is deeply focused on the uncertainty of observations When we look at an email we track our belief that the email is spam and shouldnt be shown to the user with some number and adjust that number as we see evidence for and against it being spam
,,"<p>A classical example of fuzzy logic in an AI is the expert system Mycin.</p>

<p>Fuzzy logic can be used to deal with probabilities and uncertainties. </p>

<p>If one looks at, for example, predicate logic, then every statement is either true or false. In reality, we don't have this mathematical certainty. </p>

<p>For example, let's say a physician (or expert system) sees a symptom that can be attributed to a few different diseases (say A, B and C). The physician will now attribute a higher likelihood to the possibility of the patient having any of these three diseases. There is no definite true or false statement, but there is a change of weights. This can be reflected in fuzzy logic, but not so easily in symbolic logic.</p>
",,0,2016-08-02T19:34:26.923,,122,2016-08-02T19:34:26.923,,,,,66.0,118.0,2,6,,,,55.74,9.63,9.5,0.0,0.0,22.0,A classical example of fuzzy logic in an AI is the expert system Mycin Fuzzy logic can be used to deal with probabilities and uncertainties If one looks at for example predicate logic then every statement is either true or false In reality we dont have this mathematical certainty For example lets say a physician or expert system sees a symptom that can be attributed to a few different diseases say A B and C The physician will now attribute a higher likelihood to the possibility of the patient having any of these three diseases There is no definite true or false statement but there is a change of weights This can be reflected in fuzzy logic but not so easily in symbolic logic
,4.0,"<p>Back in college, I had a Complexity Theory teacher who stated that artificial intelligence was a contradiction in terms. If it could be calculated mechanically, he argued, it wasn't intelligence.</p>

<p>This seems to be a variant of the Chinese Room argument. This argument is a metaphor, where  a person is put in a room full of Chinese books. This person doesn't understand a word of Chinese, but is slipped messages in Chinese under the door. The person has to use the books, which contain transformation rules, to answer these messages. The person can apply the transformation rules, but does not understand what (s)he is communicating.</p>

<p>Does the chinese room argument hold? Can we argue that artificial intelligence is merely clever algorithmics?</p>
",,0,2016-08-02T19:42:07.160,1.0,123,2016-08-11T14:43:26.343,2016-08-11T14:43:26.343,,145.0,,66.0,,1,8,<philosophy>,Does the Chinese Room argument hold against AI?,122.0,57.87,11.94,8.74,0.0,0.0,21.0,Back in college I had a Complexity Theory teacher who stated that artificial intelligence was a contradiction in terms If it could be calculated mechanically he argued it wasnt intelligence This seems to be a variant of the Chinese Room argument This argument is a metaphor where a person is put in a room full of Chinese books This person doesnt understand a word of Chinese but is slipped messages in Chinese under the door The person has to use the books which contain transformation rules to answer these messages The person can apply the transformation rules but does not understand what she is communicating Does the chinese room argument hold Can we argue that artificial intelligence is merely clever algorithmics
,,"<p>After he lays out his argument, he deals with some counterarguments. The following looks like the weakest one to me:</p>

<blockquote>
  <p>We can use the same analogy also against those who, finding a formula their first machine cannot produce as being true, concede that that machine is indeed inadequate, but thereupon seek to construct a second, more adequate, machine, in which the formula can be produced as being true. This they can indeed do: but then the second machine will have a Gödelian formula all of its own, constructed by applying Gödel's procedure to the formal system which represents its (the second machine's) own, enlarged, scheme of operations. And this formula the second machine will not be able to produce as being true, while a mind will be able to see that it is true. And if now a third machine is constructed, able to do what the second machine was unable to do, exactly the same will happen: there will be yet a third formula, the Gödelian formula for the formal system corresponding to the third machine's scheme of operations, which the third machine is unable to produce as being true, while a mind will still be able to see that it is true. And so it will go on.</p>
</blockquote>

<p>In short, by making the system more complex, it can see the inadequacy of a less complex system, but a yet more complex system can see its inadequacy. But from whence comes the claim that a mind <em>could</em> see the inadequacy in the <em>n</em>th machine? If, say, the Gödelian formula had as many components to it as a human brain had neurons, it seems suspect to claim that the human <em>could</em> evaluate that formula and identify that it is in fact a Gödelian formula, rather than a similar but not quite identical sentence.  </p>
",,0,2016-08-02T19:46:58.277,,124,2016-08-02T19:46:58.277,,,,,10.0,120.0,2,2,,,,54.19,9.7,8.54,0.0,0.0,40.0,After he lays out his argument he deals with some counterarguments The following looks like the weakest one to me We can use the same analogy also against those who finding a formula their first machine cannot produce as being true concede that that machine is indeed inadequate but thereupon seek to construct a second more adequate machine in which the formula can be produced as being true This they can indeed do but then the second machine will have a Gödelian formula all of its own constructed by applying Gödels procedure to the formal system which represents its the second machines own enlarged scheme of operations And this formula the second machine will not be able to produce as being true while a mind will be able to see that it is true And if now a third machine is constructed able to do what the second machine was unable to do exactly the same will happen there will be yet a third formula the Gödelian formula for the formal system corresponding to the third machines scheme of operations which the third machine is unable to produce as being true while a mind will still be able to see that it is true And so it will go on In short by making the system more complex it can see the inadequacy of a less complex system but a yet more complex system can see its inadequacy But from whence comes the claim that a mind could see the inadequacy in the nth machine If say the Gödelian formula had as many components to it as a human brain had neurons it seems suspect to claim that the human could evaluate that formula and identify that it is in fact a Gödelian formula rather than a similar but not quite identical sentence
,,"<p>Yes, it applies. If a statement cannot be derived in a finite number of steps, then it doesn't matter if the person trying to prove it is a human or a computer.  </p>

<p>The mathematician has one advantage over a standard theorem proving algorithm: the mathematician can ""step out of the system"" (as Douglas Hofstadter called in <em>G&ouml;del, Escher, Bach</em>), and start thinking <em>about</em> the system. From this point of view, the mathematician may find that the derivation is impossible.</p>

<p>However, an AI for proving theorems could be programmed to recognize patterns in the derivation, just like our hypothetical mathematician, and start reasoning <em>about</em> the formal system to derive properties of the formal system itself.
Both the AI and the mathematician would still be bound by the laws of mathematics, and not be able to prove a theorem if it was mathematically improvable.</p>
",,0,2016-08-02T19:49:46.050,,125,2016-08-02T19:49:46.050,,,,,66.0,120.0,2,2,,,,47.42,12.31,9.93,0.0,0.0,24.0,Yes it applies If a statement cannot be derived in a finite number of steps then it doesnt matter if the person trying to prove it is a human or a computer The mathematician has one advantage over a standard theorem proving algorithm the mathematician can step out of the system as Douglas Hofstadter called in Goumldel Escher Bach and start thinking about the system From this point of view the mathematician may find that the derivation is impossible However an AI for proving theorems could be programmed to recognize patterns in the derivation just like our hypothetical mathematician and start reasoning about the formal system to derive properties of the formal system itself Both the AI and the mathematician would still be bound by the laws of mathematics and not be able to prove a theorem if it was mathematically improvable
,,"<p>Depends on who you ask! John Searle, who proposed this argument, would say ""yes"", but others would say it is irrelevant. The Turing Test does not stipulate that a machine must actually ""understand"" what it is doing, as long as it seems that way to a human. You could argue that our ""thinking"" is only a more sophisticated form of clever algorithmics.</p>
",,0,2016-08-02T19:51:31.490,,126,2016-08-11T10:38:18.247,2016-08-11T10:38:18.247,,29.0,,148.0,123.0,2,2,,,,72.66,9.45,8.73,0.0,0.0,14.0,Depends on who you ask John Searle who proposed this argument would say yes but others would say it is irrelevant The Turing Test does not stipulate that a machine must actually understand what it is doing as long as it seems that way to a human You could argue that our thinking is only a more sophisticated form of clever algorithmics
,,"<p>There are two broad types of responses to philosophical queries like this.</p>

<p>The first is to make analogies and refer to intuition; one could, for example, actually calculate the necessary size for such a Chinese room, and suggest that it exists outside the realm of intuition and thus any analogies using it are suspect.</p>

<p>The second is to try to define the terms more precisely. If by ""intelligence"" we mean not ""the magic thing that humans do"" but ""information processing,"" then we can say ""yes, obviously the Chinese Room involves successful information processing.""</p>

<p>I tend to prefer the second because it forces conversations towards <em>observable outcomes</em>, and puts the difficulty of defining a term like ""intelligence"" on the person who wants to make claims about it. If ""understanding"" is allowed to have an amorphous definition, then <em>any</em> system could be said to have or not have understanding. But if ""understand"" is itself understood in terms of observable behavior, then it becomes increasingly difficult to construct an example of a system that ""is not intelligent"" and yet shares all the observable consequences of intelligence.</p>
",,0,2016-08-02T19:52:48.540,,127,2016-08-02T19:52:48.540,,,,,10.0,123.0,2,4,,,,36.52,13.18,10.28,0.0,0.0,32.0,There are two broad types of responses to philosophical queries like this The first is to make analogies and refer to intuition one could for example actually calculate the necessary size for such a Chinese room and suggest that it exists outside the realm of intuition and thus any analogies using it are suspect The second is to try to define the terms more precisely If by intelligence we mean not the magic thing that humans do but information processing then we can say yes obviously the Chinese Room involves successful information processing I tend to prefer the second because it forces conversations towards observable outcomes and puts the difficulty of defining a term like intelligence on the person who wants to make claims about it If understanding is allowed to have an amorphous definition then any system could be said to have or not have understanding But if understand is itself understood in terms of observable behavior then it becomes increasingly difficult to construct an example of a system that is not intelligent and yet shares all the observable consequences of intelligence
,,"<p>It depends on the definition of (artificial) intelligence. </p>

<p>The position that Searle originally tried to refute with the Chinese room experiment was the so-called position of strong AI: An appropriately programmed computer would have a mind in the exact same sense as humans have minds. </p>

<p>Alan Turing tried to give an definition of artificial intelligence with the Turing Test, stating that a machine is intelligent if it can pass the test. The Turing Test is introduced <a href=""https://en.wikipedia.org/wiki/Turing_test"">here</a>. I won't explain it in detail because it is not really relevant to the answer. If you define (artificial) intelligence as Turing did, then the Chinese room experiment is not valid.</p>

<p>So the point of the Chinese room experiment is to show that an appropriately programmed computer is not the same as a human mind, and therefore that Turing's Test is not a good one. </p>
",,1,2016-08-02T19:53:45.877,,128,2016-08-02T19:53:45.877,,,,,29.0,123.0,2,5,,,,50.87,11.08,8.54,0.0,0.0,18.0,It depends on the definition of artificial intelligence The position that Searle originally tried to refute with the Chinese room experiment was the socalled position of strong AI An appropriately programmed computer would have a mind in the exact same sense as humans have minds Alan Turing tried to give an definition of artificial intelligence with the Turing Test stating that a machine is intelligent if it can pass the test The Turing Test is introduced here I wont explain it in detail because it is not really relevant to the answer If you define artificial intelligence as Turing did then the Chinese room experiment is not valid So the point of the Chinese room experiment is to show that an appropriately programmed computer is not the same as a human mind and therefore that Turings Test is not a good one
,,"<p>Genetic algorithms are an <em>analogy</em> to biology, not a copy of it. The core piece of the analogy is that the ""phenotype,"" or the observable portion of a solution, is constructed from the ""genotype,"" or the internal portion of a solution. </p>

<p>For example, a number (the phenotype) can be stored as a binary series of 0s and 1s (the genotype), and by changing individual bits we make potentially dramatic changes in the resulting number, and by combining two genotypes we can get a broad range of 'related' numbers.</p>

<p>Epigenetics are a wrinkle in the genotype -> phenotype mapping, making it a non-deterministic function, and so incorporating them would degrade the performance of a genetic algorithm by adding unnecessary noise.</p>
",,3,2016-08-02T19:57:51.270,,129,2016-08-02T19:57:51.270,,,,,10.0,68.0,2,4,,,,33.07,12.78,11.26,0.0,0.0,26.0,Genetic algorithms are an analogy to biology not a copy of it The core piece of the analogy is that the phenotype or the observable portion of a solution is constructed from the genotype or the internal portion of a solution For example a number the phenotype can be stored as a binary series of 0s and 1s the genotype and by changing individual bits we make potentially dramatic changes in the resulting number and by combining two genotypes we can get a broad range of related numbers Epigenetics are a wrinkle in the genotype phenotype mapping making it a nondeterministic function and so incorporating them would degrade the performance of a genetic algorithm by adding unnecessary noise
135.0,1.0,"<p>What are the main differences between <a href=""https://en.wikipedia.org/wiki/Boltzmann_machine"" rel=""nofollow"">Deep Boltzmann Machines</a> (DBM) recurrent neural network and <a href=""https://en.wikipedia.org/wiki/Deep_belief_network"" rel=""nofollow"">Deep Belief Network</a> (which is based on RBMs)?</p>
",,0,2016-08-02T19:58:28.117,,130,2016-08-02T20:12:35.360,,,,,8.0,,1,1,<boltzmann-machine>,What are the main differences between Deep Boltzmann Machines and Deep Belief Network?,12.0,66.07,14.62,11.9,0.0,0.0,5.0,What are the main differences between Deep Boltzmann Machines DBM recurrent neural network and Deep Belief Network which is based on RBMs
,,"<p>The following thread has many answers regarding why LISP used to be thought of as the AI language: <a href=""http://stackoverflow.com/questions/130475/why-is-lisp-used-for-ai"">Why is Lisp used for AI</a> and the following is an answer by Peter Norvig, who wrote a popular textbook on the subject and is currently Director of Research at Google: <a href=""https://www.quora.com/Is-it-true-that-Lisp-is-highly-used-programming-language-in-AI"" rel=""nofollow"">Is it true that Lisp is highly used programming language in AI?</a></p>

<p>I am not overly familiar with the history, but I think LISP was oversold to industry as ""the AI language"". It is a good language for humans to think in and pioneered many important ideas which have since been incorporated into many modern languages (see <a href=""https://en.wikipedia.org/wiki/Lisp_(programming_language)"" rel=""nofollow"">the Wikipedia page</a>), but it is no way the ""best"". It was likely also popular because it is very expressive: you can write short programs to represent complex ideas, a property it shares with other functional languages in use such as Scala. This also means that it is easy to write a program that is very hard to debug in LISP. Modern functional languages have been trying to do better in this regard through typing etc.</p>

<p>The paradigm for AI that currently receives most attention is Machine Learning, i.e. learning hypothesis from data, as opposed to previous approaches like Expert Systems where experts wrote rules for the AI to follow. Python is currently the most widely used language for prototyping machine learning algorithms and has many libraries and an active community. Another important detail about modern AI is the volume of data it uses. Big Data analysis is done using cluster computing systems like Hadoop (with code written in Java) and Spark (with code written in Python or Scala). Often, the core time-intensive subroutines are written in C, but this is often done in the form of third-party libraries.</p>

<p>Finally it must be said that the AI Winter of the 80s was not because we did not have the right language, but because we did not have the right algorithms and did not have enough computational power. This has changed as GPUs have gotten better. If you're trying to learn AI, spend your time studying algorithms and not languages.</p>
",,0,2016-08-02T20:01:05.593,,131,2016-08-04T00:21:27.440,2016-08-04T00:21:27.440,,8.0,,130.0,77.0,2,6,,,,55.88,10.45,9.79,0.0,0.0,42.0,The following thread has many answers regarding why LISP used to be thought of as the AI language Why is Lisp used for AI and the following is an answer by Peter Norvig who wrote a popular textbook on the subject and is currently Director of Research at Google Is it true that Lisp is highly used programming language in AI I am not overly familiar with the history but I think LISP was oversold to industry as the AI language It is a good language for humans to think in and pioneered many important ideas which have since been incorporated into many modern languages see the Wikipedia page but it is no way the best It was likely also popular because it is very expressive you can write short programs to represent complex ideas a property it shares with other functional languages in use such as Scala This also means that it is easy to write a program that is very hard to debug in LISP Modern functional languages have been trying to do better in this regard through typing etc The paradigm for AI that currently receives most attention is Machine Learning ie learning hypothesis from data as opposed to previous approaches like Expert Systems where experts wrote rules for the AI to follow Python is currently the most widely used language for prototyping machine learning algorithms and has many libraries and an active community Another important detail about modern AI is the volume of data it uses Big Data analysis is done using cluster computing systems like Hadoop with code written in Java and Spark with code written in Python or Scala Often the core timeintensive subroutines are written in C but this is often done in the form of thirdparty libraries Finally it must be said that the AI Winter of the 80s was not because we did not have the right language but because we did not have the right algorithms and did not have enough computational power This has changed as GPUs have gotten better If youre trying to learn AI spend your time studying algorithms and not languages
,,"<p>You described a single layer feed forward network. They can have multiple layers. The significance of the weights is that they make a linear transformation from the output of the previous layer and hand it to the node they are going to. To say it more simplistically, they specify how important (and in what way: negative or positive) is the activation of node they are coming from to activating the node they are going to.</p>

<p>In your example, since there is only one layer (a row of input nodes and a row of output nodes) it is easy to explain what each node represents. However in multilayer feed forward networks they can become abstract representations which makes it difficult to explain them and therefore explain what the weights that come to them or go out of them represent.</p>

<p>Another way of thinking about it is that they describe hyperplanes in the space of the output of the previous node layer. If each output from the previous layer represents a point in space, a hyperplane decides which part of the space should give a positive value to the plane's corresponding node in the next layer and which part should give a negative input to it. It actually cuts that space into two halves. If you consider the input space of a multi-layer feed forward network, the weights of the first layer parametrize hyperplanes, however in the next layers they can represent non-linear surfaces in the input space.</p>
",,0,2016-08-02T20:01:55.600,,132,2016-08-02T21:44:47.760,2016-08-02T21:44:47.760,,143.0,,143.0,94.0,2,4,,,,54.97,10.39,8.64,0.0,0.0,23.0,You described a single layer feed forward network They can have multiple layers The significance of the weights is that they make a linear transformation from the output of the previous layer and hand it to the node they are going to To say it more simplistically they specify how important and in what way negative or positive is the activation of node they are coming from to activating the node they are going to In your example since there is only one layer a row of input nodes and a row of output nodes it is easy to explain what each node represents However in multilayer feed forward networks they can become abstract representations which makes it difficult to explain them and therefore explain what the weights that come to them or go out of them represent Another way of thinking about it is that they describe hyperplanes in the space of the output of the previous node layer If each output from the previous layer represents a point in space a hyperplane decides which part of the space should give a positive value to the planes corresponding node in the next layer and which part should give a negative input to it It actually cuts that space into two halves If you consider the input space of a multilayer feed forward network the weights of the first layer parametrize hyperplanes however in the next layers they can represent nonlinear surfaces in the input space
,,"<p>The answer to a lot of those questions depends on how the device is programmed. A computer capable of driving around and recognizing where the road goes is likely to have the ability to visually distinguish a human from an animal, whether that be based on outline, image, or size. With sufficiently sharp image recognition, it might be able to count the number and kind of people in another vehicle. It could even use existing data on the likelihood of injury to people in different kinds of vehicles.</p>

<p>Ultimately, people disagree on the ethical choices involved. Perhaps there could be ""ethics settings"" for the user/owner to configure, like ""consider life count only"" vs. ""younger lives are more valuable."" I personally would think it's not terribly controversial that a machine should damage itself before harming a human, but people disagree on how important pet lives are. If explicit kill-this-first settings make people uneasy, the answers could be determined from a questionnaire given to the user.</p>
",,9,2016-08-02T20:10:47.850,,134,2016-10-14T09:49:20.523,2016-10-14T09:49:20.523,,2989.0,,75.0,111.0,2,24,,,,44.54,12.47,11.18,0.0,0.0,27.0,The answer to a lot of those questions depends on how the device is programmed A computer capable of driving around and recognizing where the road goes is likely to have the ability to visually distinguish a human from an animal whether that be based on outline image or size With sufficiently sharp image recognition it might be able to count the number and kind of people in another vehicle It could even use existing data on the likelihood of injury to people in different kinds of vehicles Ultimately people disagree on the ethical choices involved Perhaps there could be ethics settings for the userowner to configure like consider life count only vs younger lives are more valuable I personally would think its not terribly controversial that a machine should damage itself before harming a human but people disagree on how important pet lives are If explicit killthisfirst settings make people uneasy the answers could be determined from a questionnaire given to the user
,,"<p>The graph that represents a Deep Boltzmann Machine can be any weighted undirected graph. 
However, the graph that represents a Deep Belief Network must be a connection of graphs that represent Restricted Boltzmann Machines. Those graphs are biparite, so there are two groups of vertices in those graphs so that every edge connects two vertices from different groups. Those groups are usually the visible and hidden components of the machine. </p>

<p>Learning is hard and impractical and hard in a general Deep Boltzmann Machine, but easier and practical in a  Restricted Boltzmann Machine and hence in a  Deep Belief Network, which is a connection of some of these machines. </p>
",,0,2016-08-02T20:12:35.360,,135,2016-08-02T20:12:35.360,,,,,29.0,130.0,2,2,,,,49.55,12.59,9.24,0.0,0.0,9.0,The graph that represents a Deep Boltzmann Machine can be any weighted undirected graph However the graph that represents a Deep Belief Network must be a connection of graphs that represent Restricted Boltzmann Machines Those graphs are biparite so there are two groups of vertices in those graphs so that every edge connects two vertices from different groups Those groups are usually the visible and hidden components of the machine Learning is hard and impractical and hard in a general Deep Boltzmann Machine but easier and practical in a Restricted Boltzmann Machine and hence in a Deep Belief Network which is a connection of some of these machines
,0.0,"<p>I'd like to learn more about the differences between <a href=""https://en.wikipedia.org/wiki/Cellular_automaton#Related_automata"" rel=""nofollow"">related automata</a> which can be based on hexagonal cells instead of squares (rule 34/2), like in <a href=""https://en.wikipedia.org/wiki/CoDi"" rel=""nofollow"">CoDi model</a> which uses spiking neural network (SNN). </p>

<p>Is using a plane tiled with regular <a href=""https://en.wikipedia.org/wiki/Hexagonal_tiling"" rel=""nofollow"">hexagons</a> is more efficient and reliable than using square cells? What is the difference and how do I know which one I should use in which scenario?</p>

<hr>

<p>In other words, more efficient in terms of flexibility that it learn to solve much more difficult problems and can be used for more scenarios (for me hexagonal sounds like it has more possibilities, because it can send/share the signal with/to more tiles). Or maybe one is more modern than the other, or both they're on the same level? In general, I'd like to learn the differences between them to know when I should use one over another. </p>
",,3,2016-08-02T20:15:22.637,1.0,136,2016-08-06T01:32:48.703,2016-08-06T01:32:48.703,,8.0,,8.0,,1,0,<efficiency>,Which cellular automaton model is more efficient? Squares or hexagonal?,38.0,55.37,10.16,9.41,0.0,0.0,23.0,Id like to learn more about the differences between related automata which can be based on hexagonal cells instead of squares rule 342 like in CoDi model which uses spiking neural network SNN Is using a plane tiled with regular hexagons is more efficient and reliable than using square cells What is the difference and how do I know which one I should use in which scenario In other words more efficient in terms of flexibility that it learn to solve much more difficult problems and can be used for more scenarios for me hexagonal sounds like it has more possibilities because it can sendshare the signal withto more tiles Or maybe one is more modern than the other or both theyre on the same level In general Id like to learn the differences between them to know when I should use one over another
,,"<p>First of all, for a detailed view of the argument, check out the <a href=""http://plato.stanford.edu/entries/chinese-room/"" rel=""nofollow"">SEP entry on the Chinese Room</a>.</p>

<p>I consider the CRA as an indicator of you definition of intelligence. If the argument holds, yes, the person in the room understands Chinese. However, let's sum up the three replies discussed in the SEP entry:</p>

<ol>
<li><p>The <em>man</em> himself doesn't understand Chinese (he wouldn't be able to understand it when outside the room), but the <em>system</em> man+room understands it. Accepting that reply suggests that there can exist an intelligent system which parts aren't themselves intelligent (which can be argued of the human body itself).</p></li>
<li><p>The system doesn't understand Chinese, as it cannot interact with the world in the same way a robot or a human could (i.e. it cannot learn, is limited in the set of questions it can answer)</p></li>
<li><p>The system doesn't understand Chinese (depending on your definition of <em>understanding</em>), and you couldn't say a human performing the same feats as the Chinese room understands Chinese either.</p></li>
</ol>

<p>So whether the argument, or a variant of it holds, depends on your definitions of <em>intelligent</em>, <em>understanding</em>, on how you define the system, etc. The point being that the thought experiment is a nice way to differentiate between the definitions (and many, many debates have been held about them), in order to avoid talking past each other endlessly.</p>
",,0,2016-08-02T20:16:42.047,,137,2016-08-02T20:16:42.047,,,,,149.0,123.0,2,4,,,,54.56,11.84,9.37,0.0,0.0,45.0,First of all for a detailed view of the argument check out the SEP entry on the Chinese Room I consider the CRA as an indicator of you definition of intelligence If the argument holds yes the person in the room understands Chinese However lets sum up the three replies discussed in the SEP entry The man himself doesnt understand Chinese he wouldnt be able to understand it when outside the room but the system manroom understands it Accepting that reply suggests that there can exist an intelligent system which parts arent themselves intelligent which can be argued of the human body itself The system doesnt understand Chinese as it cannot interact with the world in the same way a robot or a human could ie it cannot learn is limited in the set of questions it can answer The system doesnt understand Chinese depending on your definition of understanding and you couldnt say a human performing the same feats as the Chinese room understands Chinese either So whether the argument or a variant of it holds depends on your definitions of intelligent understanding on how you define the system etc The point being that the thought experiment is a nice way to differentiate between the definitions and many many debates have been held about them in order to avoid talking past each other endlessly
,,"<p>Generalization error is the error obtained by applying a model to data it has not seem before. So, if you want to measure generalization error, you need to remove a subset from your data and don't train your model on it. After training, you verify your model accuracy (or other performance measure) on the subset you have removed, since your model hasn't seem it before. Hence, this subset is called a ""<a href=""https://en.wikipedia.org/wiki/Test_set"" rel=""nofollow"">test set</a>"". </p>

<p>Additionally, another subset can also be used for parameter selection, which we call a ""<a href=""https://en.wikipedia.org/wiki/Test_set#Validation_set"" rel=""nofollow"">validation set</a>"". We can't use the training set for parameter tuning, since it does not measure generalization error, but we can't use the test set too, since our parameter tuning would overfit test data. That's why we need a third subset.</p>

<p>Finally, in order to obtain more predictive performance measures, we can use many different train/test partitions and average the results. This is called ""<a href=""https://en.wikipedia.org/wiki/Cross-validation_(statistics)"" rel=""nofollow"">cross-validation</a>"".</p>
",,0,2016-08-02T20:32:12.480,,138,2016-08-02T20:32:12.480,,,,,144.0,50.0,2,3,,,,54.32,11.54,9.25,0.0,0.0,36.0,Generalization error is the error obtained by applying a model to data it has not seem before So if you want to measure generalization error you need to remove a subset from your data and dont train your model on it After training you verify your model accuracy or other performance measure on the subset you have removed since your model hasnt seem it before Hence this subset is called a test set Additionally another subset can also be used for parameter selection which we call a validation set We cant use the training set for parameter tuning since it does not measure generalization error but we cant use the test set too since our parameter tuning would overfit test data Thats why we need a third subset Finally in order to obtain more predictive performance measures we can use many different traintest partitions and average the results This is called crossvalidation
,,"<p>There's no objective reason to state that declarative languages are better suited for AI development. However, there's indeed a bias towards them in practice. Although most functional languages are impure (that is, they allow side effects), and such can't count as ""declarative"", a few languages are purely functional (that is, they don't allow side effects), most prominently <a href=""https://en.wikipedia.org/wiki/Declarative_programming#Functional_programming"" rel=""nofollow noreferrer"">Haskell</a>. Purity is key here. In Haskell, <a href=""http://stackoverflow.com/a/4066401/5249858"">even I/O is pure</a>.</p>

<p>The key difference between imperative languages and (purely) functional languages is in the way they describe the program. An imperative program describes <em>how</em> to do stuff, that is, algorithms. It specifies the specific instructions that the machine must carry on in order to perform the computation. OTOH, purely functional languages describe <em>what</em> is to be computed, that is, the relationship between the input and the output. In mathematics, ""function"" is just a fancy name for a relationship between an input and an output.</p>

<p>Again, in the mathematical sense, the only variability is that of the function's arguments. That is, the function's output depends solely on its input (arguments). This is known as <a href=""https://en.wikipedia.org/wiki/Referential_transparency"" rel=""nofollow noreferrer"">referential transparency</a>. Referential transparency states that:</p>

<p><a href=""https://i.stack.imgur.com/ssAGM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ssAGM.png"" alt=""enter image description here""></a></p>

<p>Where <code>ϕ</code> is the set of all functions, and <code>δ(f)</code> is <code>f</code>'s domain. For the typical imperative language's definition of ""function"", the above doesn't hold. For instance, C's <code>getchar()</code> does not always return the same value.</p>

<p>Let's say we want to calculate the set of the ten least prime numbers whose least significant digit is 3. First, in mathematical notation:</p>

<p><a href=""https://i.stack.imgur.com/F94i9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F94i9.png"" alt=""enter image description here""></a></p>

<p>Where <code>G(n, s)</code> is the set of the lesser <code>n</code>th elements from <code>s</code>. In mathematics, you don't worry about how is the set <code>S</code> supposed to be computed, but rather about <code>S</code>'s definition itself.</p>

<p>Now, in Python (in imperative style):</p>

<pre><code>def is_prime(n):
  for x in range(2, n):
    if n % x == 0:
      return False

  return True

def foo():
  s = set()
  n = 2
  while len(s) &lt; 10:
    if is_prime(n) and n % 10 == 3:
       s.append(n)

  return set(s)
</code></pre>

<p>In Python, we care about (and are responsible for) the algorithm being used to compute the set. We specify, pretty much in recipe-style, how to build the set from scratch. If there's an algorithm that may be better suited for checking whether a number is prime or odd, but we don't use it, it's our fault, not Python's.</p>

<p>Finally, Haskell steps in:</p>

<pre><code>import qualified Data.Set as Set

isPrime :: Integer -&gt; Bool
isPrime n = ( == 1 ) . length . filter ( == 0 ) . map ( n `mod` ) $ [ 2 .. n ]

s = Set.fromList . take 10 . filter ( ( == 3 ) . ( `mod` 10 ) ) . filter isPrime $ [1..]
</code></pre>

<p>Haskell's version is a lot more like the mathematical model than Python is. We define the <code>isPrime</code> function in terms of the constraints that a prime number must obey, not by describing a step-by-step algorithm to do such a check. Moreover, <code>s</code> (the set we have been defining so far) is defined in terms of the constraints its members must obey, rather than in terms of an algorithm to compute <code>s</code> itself. The compiler, more often than not <a href=""https://downloads.haskell.org/~ghc/7.8.3/docs/html/users_guide/"" rel=""nofollow noreferrer"">The Glorious Glasgow Haskell Compilation System</a> (a.k.a GHC), is the one responsible for generating an algorithm. GHC's optimizer is known to be one of the strongest in the world, not because its the best compiler of 'em all, but because Haskell's nature allows for this.</p>

<p>Haskell (and other functional languages), in summary, have several features that make it look, taste, and behave like pure math:</p>

<ul>
<li><p>Referential transparency and purity.</p></li>
<li><p><a href=""https://en.wikipedia.org/wiki/Lazy_evaluation"" rel=""nofollow noreferrer"">Haskell is lazy</a>.</p></li>
<li><p><a href=""http://programmers.stackexchange.com/questions/279316/what-exactly-makes-the-haskell-type-system-so-revered-vs-say-java"">A <strong>very</strong> strong type system</a>, with such exotic (but very useful!) stuff as <a href=""https://en.wikipedia.org/wiki/Recursive_data_type"" rel=""nofollow noreferrer"">recursive</a> (and <a href=""https://en.wikipedia.org/wiki/Algebraic_data_type"" rel=""nofollow noreferrer"">algebraic</a>) data types.</p></li>
</ul>

<p>So, the bottom line is that AI researchers often prefer functional languages over imperative languages (or, more assertively, pure over impure languages), because they are attempting to <em>define</em> artificial intelligence itself, by means of functions (relationship between an input and an output). At the end, we do this because we have no real algorithm for human-level intelligence to raise from a handful of transistors. Also, there's been a historical bias towards these kind of languages, starting with <a href=""https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)"" rel=""nofollow noreferrer"">John McCarthy</a>, Lisp's creator and a pioneer in early AI research.</p>
",,0,2016-08-02T20:37:36.760,,139,2016-08-02T20:37:36.760,,,,,71.0,108.0,2,3,,,,54.22,12.29,9.03,503.0,0.0,152.0,Theres no objective reason to state that declarative languages are better suited for AI development However theres indeed a bias towards them in practice Although most functional languages are impure that is they allow side effects and such cant count as declarative a few languages are purely functional that is they dont allow side effects most prominently Haskell Purity is key here In Haskell even IO is pure The key difference between imperative languages and purely functional languages is in the way they describe the program An imperative program describes how to do stuff that is algorithms It specifies the specific instructions that the machine must carry on in order to perform the computation OTOH purely functional languages describe what is to be computed that is the relationship between the input and the output In mathematics function is just a fancy name for a relationship between an input and an output Again in the mathematical sense the only variability is that of the functions arguments That is the functions output depends solely on its input arguments This is known as referential transparency Referential transparency states that Where is the set of all functions and is s domain For the typical imperative languages definition of function the above doesnt hold For instance Cs does not always return the same value Lets say we want to calculate the set of the ten least prime numbers whose least significant digit is 3 First in mathematical notation Where is the set of the lesser th elements from In mathematics you dont worry about how is the set supposed to be computed but rather about s definition itself Now in Python in imperative style In Python we care about and are responsible for the algorithm being used to compute the set We specify pretty much in recipestyle how to build the set from scratch If theres an algorithm that may be better suited for checking whether a number is prime or odd but we dont use it its our fault not Pythons Finally Haskell steps in Haskells version is a lot more like the mathematical model than Python is We define the function in terms of the constraints that a prime number must obey not by describing a stepbystep algorithm to do such a check Moreover the set we have been defining so far is defined in terms of the constraints its members must obey rather than in terms of an algorithm to compute itself The compiler more often than not The Glorious Glasgow Haskell Compilation System aka GHC is the one responsible for generating an algorithm GHCs optimizer is known to be one of the strongest in the world not because its the best compiler of em all but because Haskells nature allows for this Haskell and other functional languages in summary have several features that make it look taste and behave like pure math Referential transparency and purity Haskell is lazy A very strong type system with such exotic but very useful stuff as recursive and algebraic data types So the bottom line is that AI researchers often prefer functional languages over imperative languages or more assertively pure over impure languages because they are attempting to define artificial intelligence itself by means of functions relationship between an input and an output At the end we do this because we have no real algorithm for humanlevel intelligence to raise from a handful of transistors Also theres been a historical bias towards these kind of languages starting with John McCarthy Lisps creator and a pioneer in early AI research
144.0,3.0,"<p>An ultraintelligent machine is a machine that can surpass all intellectual activities by any human, and such machine is often used in science fiction as a machine that brings mankind to an end. </p>

<p>Any machine is executed using an algorithm. By the Church-Turing thesis, any algorithm that can be executed by a modern computer can be executed by a Turing Machine. However, a human can easily simulate a Turing Machine. Doesn't this mean that a machine can't surpass all intellectual activities, since we can also execute the algorithm?</p>

<p>This argument is most likely flawed, since my intuition tells me that  ultraintelligent machines are possible. However, it is not clear to me where the flaw is. Note that this is my own argument. </p>
",,2,2016-08-02T20:37:59.927,,140,2016-08-05T13:35:09.893,,,,,29.0,,1,6,<philosophy><ultraintelligent-machine>,Does this argument refuting the existence of ultraintelligent machines work?,89.0,47.59,10.9,9.18,0.0,0.0,17.0,An ultraintelligent machine is a machine that can surpass all intellectual activities by any human and such machine is often used in science fiction as a machine that brings mankind to an end Any machine is executed using an algorithm By the ChurchTuring thesis any algorithm that can be executed by a modern computer can be executed by a Turing Machine However a human can easily simulate a Turing Machine Doesnt this mean that a machine cant surpass all intellectual activities since we can also execute the algorithm This argument is most likely flawed since my intuition tells me that ultraintelligent machines are possible However it is not clear to me where the flaw is Note that this is my own argument
,,"<p>The terms <em>strong</em> and <em>weak</em> don't actually refer to processing, or optimization power, or any interpretation leading to ""strong AI"" being <em>stronger</em> than ""weak AI"". It holds conveniently in practice, but the terms come from elsewhere. In 1980, <a href=""https://en.wikipedia.org/wiki/John_Searle"">John Searle</a> coined the following statements:</p>

<ul>
<li>AI hypothesis, strong form: an AI system can <em>think</em> and have a <em>mind</em> (in the philosophical definition of the term);</li>
<li>AI hypothesis, weak form: an AI system can only <em>act</em> like it thinks and has a mind.</li>
</ul>

<p>So <em>strong AI</em> is a shortcut for an AI systems that verifies the <em>strong AI hypothesis</em>. Similarly, for the weak form. The terms have then evolved: strong AI refers to AI that performs as well as humans (who have minds), weak AI refers to AI that doesn't.</p>

<p>The problem with these definitions is that they're fuzzy. For example, <a href=""https://en.wikipedia.org/wiki/AlphaGo"">AlphaGo</a> is an example of weak AI, but is ""strong"" by Go-playing standards. A hypothetical AI replicating a human baby would be a strong AI, while being ""weak"" at most tasks.</p>

<p>Other terms exist: <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"">Artificial General Intelligence</a> (AGI), which has cross-domain capability (like humans), can learn from a wide range of experiences (like humans), among other features. Artificial Narrow Intelligence refers to systems bound to a certain range of tasks (where they may nevertheless have superhuman ability), lacking capacity to significantly improve themselves.</p>

<p>Beyond AGI, we find Artificial Superintelligence (ASI), based on the idea that a system with the capabilities of an AGI, without the physical limitations of humans would learn and improve far beyond human level.</p>
",,0,2016-08-02T20:48:24.037,,141,2016-08-02T20:48:24.037,,,,,149.0,74.0,2,19,,,,49.86,12.47,9.38,0.0,0.0,63.0,The terms strong and weak dont actually refer to processing or optimization power or any interpretation leading to strong AI being stronger than weak AI It holds conveniently in practice but the terms come from elsewhere In 1980 John Searle coined the following statements AI hypothesis strong form an AI system can think and have a mind in the philosophical definition of the term AI hypothesis weak form an AI system can only act like it thinks and has a mind So strong AI is a shortcut for an AI systems that verifies the strong AI hypothesis Similarly for the weak form The terms have then evolved strong AI refers to AI that performs as well as humans who have minds weak AI refers to AI that doesnt The problem with these definitions is that theyre fuzzy For example AlphaGo is an example of weak AI but is strong by Goplaying standards A hypothetical AI replicating a human baby would be a strong AI while being weak at most tasks Other terms exist Artificial General Intelligence AGI which has crossdomain capability like humans can learn from a wide range of experiences like humans among other features Artificial Narrow Intelligence refers to systems bound to a certain range of tasks where they may nevertheless have superhuman ability lacking capacity to significantly improve themselves Beyond AGI we find Artificial Superintelligence ASI based on the idea that a system with the capabilities of an AGI without the physical limitations of humans would learn and improve far beyond human level
,,"<p>In some iterative learning methods the more iterations you apply the more specific your model becomes about the training set. If there are too much iterations, your model will become too specifically trained for the training samples and will score less on other samples that are not seen during the training phase. This is call over-fitting, though over-fitting is not specific to iterative learning methods.</p>

<p>One solution to prevent over-fitting in these iterative learning algorithms is early stopping. Normally a control group of samples called validation samples (validation set) are used to validate the model and notify when it starts to over-fit. The validation set is not used by the training algorithm however its corresponding outputs are known and after each iteration its samples are employed to measure how good the model currently works. As soon as the performance on the validation set stops to grow and starts to drop we stop iterating the training algorithm. This is called early stopping which can help to maximize the generalization power of our learned model.</p>

<p>Note that if we use the training set itself for validation the performance will always increase because that is what the learning algorithm is designed to do. However the learning algorithm does not know how specifically it should learn the training set and that is why we need methods like early stopping.</p>
",,0,2016-08-02T20:57:51.603,,142,2016-08-03T11:54:30.310,2016-08-03T11:54:30.310,,143.0,,143.0,16.0,2,5,,,,48.64,12.94,8.82,0.0,0.0,18.0,In some iterative learning methods the more iterations you apply the more specific your model becomes about the training set If there are too much iterations your model will become too specifically trained for the training samples and will score less on other samples that are not seen during the training phase This is call overfitting though overfitting is not specific to iterative learning methods One solution to prevent overfitting in these iterative learning algorithms is early stopping Normally a control group of samples called validation samples validation set are used to validate the model and notify when it starts to overfit The validation set is not used by the training algorithm however its corresponding outputs are known and after each iteration its samples are employed to measure how good the model currently works As soon as the performance on the validation set stops to grow and starts to drop we stop iterating the training algorithm This is called early stopping which can help to maximize the generalization power of our learned model Note that if we use the training set itself for validation the performance will always increase because that is what the learning algorithm is designed to do However the learning algorithm does not know how specifically it should learn the training set and that is why we need methods like early stopping
,,"<ul>
<li>An ability that is commonly attributed to intelligence is <strong>problem solving</strong>. </li>
<li>Another one is <strong>learning</strong> (improving itself from experience).</li>
<li>Artificial intelligence can be defined as ""replicating intelligence, or parts of it, at least in appearance, inside a computer"" (dodging the definition of intelligence itself).</li>
<li>Genetic algorithms are computational <strong>problem solving</strong> tools that find and improve solutions (they <strong>learn</strong>).</li>
</ul>

<p>Thus, genetic algorithms are a kind of artificial intelligence.</p>

<p>Regarding scale, I don't see it as an important factor for defining G.A. as A.I or not. The same way we can simply classify different living forms as more or less intelligent instead of just saying intelligent or not intelligent.</p>

<p>Finally, let's just make an important distinction: our brains are the product of natural selection, but the brains themselves don't use the same principle in order to achieve intelligence.</p>
",,3,2016-08-02T21:01:10.770,,143,2016-08-02T21:01:10.770,,,,,144.0,28.0,2,3,,,,39.23,14.72,10.31,0.0,0.0,30.0,An ability that is commonly attributed to intelligence is problem solving Another one is learning improving itself from experience Artificial intelligence can be defined as replicating intelligence or parts of it at least in appearance inside a computer dodging the definition of intelligence itself Genetic algorithms are computational problem solving tools that find and improve solutions they learn Thus genetic algorithms are a kind of artificial intelligence Regarding scale I dont see it as an important factor for defining GA as AI or not The same way we can simply classify different living forms as more or less intelligent instead of just saying intelligent or not intelligent Finally lets just make an important distinction our brains are the product of natural selection but the brains themselves dont use the same principle in order to achieve intelligence
,,"<p>I believe this argument is based on the fact that intelligence is a single dimension when it really isn't. Are machines and humans really on the same level if a machine can solve a complex problem in a millionth of the time a human can? </p>

<p>It also assumes that the Turing machine is still the best computational model for the time period that you are in, which is not necessarily true for the future, it is just true until this point in time. </p>
",,0,2016-08-02T21:06:19.000,,144,2016-08-05T13:35:09.893,2016-08-05T13:35:09.893,,29.0,,152.0,140.0,2,2,,,,60.28,7.96,9.58,0.0,0.0,6.0,I believe this argument is based on the fact that intelligence is a single dimension when it really isnt Are machines and humans really on the same level if a machine can solve a complex problem in a millionth of the time a human can It also assumes that the Turing machine is still the best computational model for the time period that you are in which is not necessarily true for the future it is just true until this point in time
,2.0,"<p>From Wikipedia:</p>

<blockquote>
  <p>AIXI ['ai̯k͡siː] is a theoretical mathematical formalism for artificial general intelligence. It combines Solomonoff induction with sequential decision theory. AIXI was first proposed by Marcus Hutter in 2000[1] and the results below are proved in Hutter's 2005 book Universal Artificial Intelligence.[2]</p>
</blockquote>

<p>Albeit non-computable, approximations are possible, such as <em>AIXItl</em>. Finding approximations to AIXI could be an objective way for solving AI.</p>

<p>My question is: is <em>AIXI</em> really a big deal in artificial <em>general</em> intelligence research? Can it be thought as a central concept for the field? If so, why don't we have more publications on this subject (or maybe we have and I'm not aware of them)?</p>
",,0,2016-08-02T21:10:26.693,,145,2016-08-05T15:09:39.057,2016-08-05T15:09:39.057,,144.0,,144.0,,1,3,<models><agi>,What is the relevance of AIXI on current artificial intelligence research?,38.0,49.21,14.03,10.4,0.0,0.0,26.0,From Wikipedia AIXI ai̯k͡siː is a theoretical mathematical formalism for artificial general intelligence It combines Solomonoff induction with sequential decision theory AIXI was first proposed by Marcus Hutter in 20001 and the results below are proved in Hutters 2005 book Universal Artificial Intelligence2 Albeit noncomputable approximations are possible such as AIXItl Finding approximations to AIXI could be an objective way for solving AI My question is is AIXI really a big deal in artificial general intelligence research Can it be thought as a central concept for the field If so why dont we have more publications on this subject or maybe we have and Im not aware of them
,1.0,"<p>In what ways can connectionist artificial intelligence (neural networks) be integrated with <em>Good Old-Fashioned A.I.</em> (<em>GOFAI</em>)? For instance, how could deep neural networks be integrated with knowledge bases or logical inference? One such example seems to be the <a href=""http://wiki.opencog.org/w/DestinOpenCog"" rel=""nofollow"">OpenCog + Destin integration</a>.</p>
",,0,2016-08-02T21:14:36.133,,146,2016-08-03T08:22:36.737,,,,,144.0,,1,1,<neural-networks><classical-ai><gofai><symbolic-computing>,In what ways can connectionist A.I. be integrated with GOFAI?,27.0,40.65,17.22,11.63,0.0,0.0,12.0,In what ways can connectionist artificial intelligence neural networks be integrated with Good OldFashioned AI GOFAI For instance how could deep neural networks be integrated with knowledge bases or logical inference One such example seems to be the OpenCog Destin integration
,2.0,"<p>It is proved that a recurrent neural net with rational weights can be a super-Turing machine. Can we achieve this in practice ?</p>
",,1,2016-08-02T21:15:34.483,1.0,147,2016-09-03T17:38:06.577,,,,,159.0,,1,3,<neural-networks><hypercomputation><recurrent-neural-networks>,Can we ever achieve hypercomputation using recurrent neural networks?,132.0,68.77,9.2,9.92,0.0,0.0,3.0,It is proved that a recurrent neural net with rational weights can be a superTuring machine Can we achieve this in practice
170.0,2.0,"<p>Given the proven <a href=""https://en.wikipedia.org/wiki/Halting_problem"">halting problem</a> for <a href=""https://en.wikipedia.org/wiki/Turing_machine"">Turing machines</a>, can we infer limits on the ability of strong Artificial Intelligence?</p>
",,5,2016-08-02T21:16:44.013,1.0,148,2016-08-03T18:33:16.540,2016-08-03T18:33:16.540,,10.0,,55.0,,1,10,<halting-problem>,"What limits, if any, does the halting problem put on Artificial Intelligence?",150.0,35.27,14.79,13.72,0.0,0.0,2.0,Given the proven halting problem for Turing machines can we infer limits on the ability of strong Artificial Intelligence
,,"<p>A quantum computer has a huge amount of internal state that even the machine can't get at directly. (You can only sample the matrix state.) The amount of that state goes up exponentially with each quantum bit involved in the system. Some operations get insane speedups from quantum computing: you just put the quantum wire through a quantum gate and you've updated the entire matrix at once.</p>

<p>Simulating a quantum computer with a classical one would take exponentially longer for each qubit. With several dozen qubits, the machine's computing power for some tasks couldn't even be approached by a normal computer, much less a human mind.</p>

<p>Relevant: my answer on <a href=""http://ai.stackexchange.com/a/114/75"">To what extent can quantum computers help to develop Artificial Intelligence?</a></p>

<p>Note that with quantum computers, you've gone beyond the normal zeroes and ones. You then need a <a href=""https://en.wikipedia.org/wiki/Quantum_Turing_machine"" rel=""nofollow"">quantum Turing machine</a>, which is a generalization of the classical one.</p>
",,4,2016-08-02T21:18:16.763,,149,2016-08-02T21:18:16.763,,,,,75.0,140.0,2,1,,,,54.63,11.95,9.33,0.0,0.0,22.0,A quantum computer has a huge amount of internal state that even the machine cant get at directly You can only sample the matrix state The amount of that state goes up exponentially with each quantum bit involved in the system Some operations get insane speedups from quantum computing you just put the quantum wire through a quantum gate and youve updated the entire matrix at once Simulating a quantum computer with a classical one would take exponentially longer for each qubit With several dozen qubits the machines computing power for some tasks couldnt even be approached by a normal computer much less a human mind Relevant my answer on To what extent can quantum computers help to develop Artificial Intelligence Note that with quantum computers youve gone beyond the normal zeroes and ones You then need a quantum Turing machine which is a generalization of the classical one
,0.0,"<p>By default using <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow"">DeepDream</a> technique you can creating a dreamlike image out of two different images.</p>

<p>Is it possible to easily enhance this technique to generate one image out from three?</p>
",,1,2016-08-02T21:25:25.313,,151,2016-08-02T21:25:25.313,,,,,8.0,,1,1,<conv-neural-network><deepdream>,"Can DeepDream produce a ""dream"" from 3 images?",20.0,47.28,11.6,11.03,0.0,0.0,2.0,By default using DeepDream technique you can creating a dreamlike image out of two different images Is it possible to easily enhance this technique to generate one image out from three
1728.0,2.0,"<p>Consider these neural style algorithms which produce some art work:</p>

<ul>
<li><a href=""https://github.com/alexjc/neural-doodle"">Neural Doodle</a></li>
<li><a href=""https://github.com/jcjohnson/neural-style"">neural-style</a></li>
</ul>

<p>Why is generating such images so slow and why does it take huge amounts of memory? Isn't there any method of optimizing the algorithm?</p>

<p>What is the mechanism or technical limitation behind this? Why we can't have a realtime processing?</p>

<p>Here are few user comments (<a href=""https://www.reddit.com/r/deepdream/comments/3jwl76/how_anyone_can_create_deep_style_images/"">How ANYONE can create Deep Style images</a>):</p>

<ul>
<li><blockquote>
  <p>Anything above 640x480 and we're talking days of heavy crunching and an insane amount of ram.</p>
</blockquote></li>
<li><blockquote>
  <p>I tried doing a 1024pixel image and it still crashed with 14gigs memory, and 26gigs swap. So most of the VM space is just the swapfile. Plus it takes several hours potentially days cpu rendering this.</p>
</blockquote></li>
<li><blockquote>
  <p>I tried 1024x768 and with 16gig ram and 20+ gig swap it was still dying from lack of memory.</p>
</blockquote></li>
<li><blockquote>
  <p>Having a memory issue, though. I'm using the ""g2.8xlarge"" instance type.</p>
</blockquote></li>
</ul>
",,0,2016-08-02T21:34:32.107,,152,2016-10-07T18:22:43.637,2016-08-18T13:24:43.510,,145.0,,8.0,,1,6,<performance><neural-doodle><deepdreaming>,Why is the generation of deep style images so slow and resource-hungry?,120.0,67.55,10.02,9.4,0.0,0.0,26.0,Consider these neural style algorithms which produce some art work Neural Doodle neuralstyle Why is generating such images so slow and why does it take huge amounts of memory Isnt there any method of optimizing the algorithm What is the mechanism or technical limitation behind this Why we cant have a realtime processing Here are few user comments How ANYONE can create Deep Style images Anything above 640x480 and were talking days of heavy crunching and an insane amount of ram I tried doing a 1024pixel image and it still crashed with 14gigs memory and 26gigs swap So most of the VM space is just the swapfile Plus it takes several hours potentially days cpu rendering this I tried 1024x768 and with 16gig ram and 20 gig swap it was still dying from lack of memory Having a memory issue though Im using the g28xlarge instance type
,1.0,"<p>Can autoencoders be used for supervised learning <em>without adding an output layer</em>? Can we simply feed it with a concatenated input-output vector for training, and reconstruct the output part from the input part when doing inference? The output part would be treated as missing values during inference and some imputation would be applied.</p>
",,2,2016-08-02T21:36:28.053,,153,2017-03-01T14:35:54.190,2016-08-04T20:48:58.537,,144.0,,144.0,,1,7,<neural-networks><unsupervised-learning>,Can autoencoders be used for supervised learning?,179.0,53.51,13.51,11.07,0.0,0.0,5.0,Can autoencoders be used for supervised learning without adding an output layer Can we simply feed it with a concatenated inputoutput vector for training and reconstruct the output part from the input part when doing inference The output part would be treated as missing values during inference and some imputation would be applied
158.0,4.0,"<p>I'm aware that neural networks are probably not designed to do that, however asking hypothetically, is it possible to train the deep neural network (or similar) to solve math equations?</p>

<p>So given the 3 inputs: 1st number, operator sign represented by the number (1 - <code>+</code>, 2 - <code>-</code>, 3 - <code>/</code>, 4 - <code>*</code>, and so on), and the 2nd number, then after training the network should give me the valid results.</p>

<p>Example 1 (<code>2+2</code>):</p>

<ul>
<li>Input 1: <code>2</code>; Input 2: <code>1</code> (<code>+</code>); Input 3: <code>2</code>; Expected output: <code>4</code></li>
<li>Input 1: <code>10</code>; Input 2: <code>2</code> (<code>-</code>); Input 3: <code>10</code>; Expected output: <code>0</code></li>
<li>Input 1: <code>5</code>; Input 2: <code>4</code> (<code>*</code>); Input 3: <code>5</code>; Expected output: <code>25</code></li>
<li>and so</li>
</ul>

<p>The above can be extended to more sophisticated examples.</p>

<p>Is that possible? If so, what kind of network can learn/achieve that?</p>
",,0,2016-08-02T21:37:32.420,,154,2017-03-09T17:03:38.327,2017-03-09T17:03:38.327,,8.0,,8.0,,1,7,<neural-networks><math>,Is it possible to train the neural network to solve math equations?,231.0,65.25,11.38,9.2,44.0,0.0,56.0,Im aware that neural networks are probably not designed to do that however asking hypothetically is it possible to train the deep neural network or similar to solve math equations So given the 3 inputs 1st number operator sign represented by the number 1 2 3 4 and so on and the 2nd number then after training the network should give me the valid results Example 1 Input 1 Input 2 Input 3 Expected output Input 1 Input 2 Input 3 Expected output Input 1 Input 2 Input 3 Expected output and so The above can be extended to more sophisticated examples Is that possible If so what kind of network can learnachieve that
,,"<p>Not really.</p>

<p>Neural networks are good for determining non-linear relationships between inputs when there are hidden variables. In the examples above the relationships are linear, and there are no hidden variables. But even if they were non-linear, a traditional ANN design would not be well suited to accomplish this.</p>

<p>By carefully constructing the layers and tightly supervising the training, you could get a network to consistently produce the output 4.01, say, for the inputs: 2, 1 (+), and 2, but this is not only wrong, it's an inherently unreliable application of the technology.</p>
",,0,2016-08-02T21:57:56.160,,155,2016-08-02T21:57:56.160,,,,,46.0,154.0,2,2,,,,52.8,13.69,10.38,0.0,0.0,22.0,Not really Neural networks are good for determining nonlinear relationships between inputs when there are hidden variables In the examples above the relationships are linear and there are no hidden variables But even if they were nonlinear a traditional ANN design would not be well suited to accomplish this By carefully constructing the layers and tightly supervising the training you could get a network to consistently produce the output 401 say for the inputs 2 1 and 2 but this is not only wrong its an inherently unreliable application of the technology
,3.0,"<p>From Wikipedia:</p>

<blockquote>
  <p>A mirror neuron is a neuron that fires both when an animal acts and when the animal observes the same action performed by another.</p>
</blockquote>

<p>Mirror neurons are related to imitation learning, a very useful feature that is missing in current real-world A.I. implementations. Instead of learning from input-output examples (supervised learning) or from rewards (reinforcement learning), an agent with mirror neurons would be able to learn by simply observing other agents, translating their movements to its own coordinate system. What do we have on this subject regarding computational models?</p>
",,0,2016-08-02T21:59:01.093,5.0,156,2016-10-14T13:08:35.790,,,,,144.0,,1,12,<neural-networks><models>,Are there any computational models of mirror neurons?,225.0,39.87,15.32,12.4,0.0,0.0,16.0,From Wikipedia A mirror neuron is a neuron that fires both when an animal acts and when the animal observes the same action performed by another Mirror neurons are related to imitation learning a very useful feature that is missing in current realworld AI implementations Instead of learning from inputoutput examples supervised learning or from rewards reinforcement learning an agent with mirror neurons would be able to learn by simply observing other agents translating their movements to its own coordinate system What do we have on this subject regarding computational models
267.0,1.0,"<p>If I have a paragraph I want to summarize, for example:</p>

<blockquote>
  <p>Ponzo and Fila went to the mall during the day. They walked for a long while, stopping at shops. They went to many shops. At first, they didn't buy anything. After going to a number of shops, they eventually bought a shirt, and a pair of pants.</p>
</blockquote>

<p>Better summarized as:</p>

<blockquote>
  <p>They shopped at the mall today and bought some clothes.</p>
</blockquote>

<p>What is the best AI strategy to automate this process, if there is one? If there isn't, is it because it would be dependent on first having an external information resource that would inform any algorithm? Or is it because the problem is inherently contextual?</p>
",,0,2016-08-02T22:38:50.823,0.0,157,2016-08-03T22:48:30.993,2016-08-03T18:24:37.057,,72.0,,46.0,,1,5,<algorithm><natural-language><pattern-recognition>,What artificial intelligence strategies are useful for summarization?,43.0,75.3,8.05,8.9,0.0,0.0,20.0,If I have a paragraph I want to summarize for example Ponzo and Fila went to the mall during the day They walked for a long while stopping at shops They went to many shops At first they didnt buy anything After going to a number of shops they eventually bought a shirt and a pair of pants Better summarized as They shopped at the mall today and bought some clothes What is the best AI strategy to automate this process if there is one If there isnt is it because it would be dependent on first having an external information resource that would inform any algorithm Or is it because the problem is inherently contextual
,,"<p>Yes, it has been done!</p>

<p>However, the applications aren't to replace calculators or anything like that. The lab I'm associated with develops neural network models of equational reasoning to better understand how humans might solve these problems. This is a part of the field known as <a href=""http://psych.stanford.edu/~jlm/"">Mathematical Cognition</a>. Unfortunately, our website isn't terribly informative, but here's a <a href=""http://web.stanford.edu/~kmickey/pdf/MickeyMcClelland2014.pdf"">link</a> to an example of such work.</p>

<p>Apart from that, recent work on extending neural networks to include external memory stores (e.g. Neural Turing Machines) tend to use solving math problems as a good proof of concept. This is because many arithmetic problems involve long procedures with stored intermediate results. See the sections of <a href=""http://arxiv.org/pdf/1511.08228.pdf"">this paper</a> on long binary addition and multiplication.</p>
",,0,2016-08-02T22:50:52.560,,158,2016-08-02T22:50:52.560,,,,,109.0,154.0,2,6,,,,49.62,13.33,11.19,0.0,0.0,21.0,Yes it has been done However the applications arent to replace calculators or anything like that The lab Im associated with develops neural network models of equational reasoning to better understand how humans might solve these problems This is a part of the field known as Mathematical Cognition Unfortunately our website isnt terribly informative but heres a link to an example of such work Apart from that recent work on extending neural networks to include external memory stores eg Neural Turing Machines tend to use solving math problems as a good proof of concept This is because many arithmetic problems involve long procedures with stored intermediate results See the sections of this paper on long binary addition and multiplication
238.0,1.0,"<p>What happens if you apply the same <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow"">deep dream technique</a> which produces ""dream"" visuals, but to media streams such as audio files?</p>

<p>Does changing image functions into audio and enhancing the logic would work, or it won't work or doesn't make any sense?</p>

<p>My goal is to create ""dream"" like audio based on the two samples.</p>
",,0,2016-08-02T23:01:56.157,,159,2016-08-18T10:21:39.723,2016-08-18T10:21:39.723,,145.0,,8.0,,1,1,<conv-neural-network><deepdreaming>,Is it possible to apply deep dream technique for the audio streams?,59.0,77.87,9.75,9.36,0.0,0.0,11.0,What happens if you apply the same deep dream technique which produces dream visuals but to media streams such as audio files Does changing image functions into audio and enhancing the logic would work or it wont work or doesnt make any sense My goal is to create dream like audio based on the two samples
,0.0,"<p>With which neural network it is possible to scale the learning between the independent networks?</p>

<p>For example given the stream of images one network is trained to recognise cats, another dogs, and so on, all of them are talking to the main visual network which is responsible for making some decision and pass the analysis to the main ""brain"" network. Then another network of neural network are given the audio so each network can recognise specific pattern, then they talk to the common audio specific network which talks to the main ""brain"" network (mentioned before). In other words, something like a robot.</p>

<p>Which type of network would be the most suitable and scalable for such configuration, so you can easily extend it for additional separate network modules? Does it matter which type of deep network (or not deep) I should choose, or not?</p>
",,0,2016-08-02T23:09:22.910,,160,2016-08-03T11:52:46.510,2016-08-03T11:52:46.510,,72.0,,8.0,,1,0,<algorithm><deep-learning><computer-vision>,"Which deep networks are scalable in terms of ""talking"" to each other?",28.0,55.78,11.55,9.12,0.0,0.0,21.0,With which neural network it is possible to scale the learning between the independent networks For example given the stream of images one network is trained to recognise cats another dogs and so on all of them are talking to the main visual network which is responsible for making some decision and pass the analysis to the main brain network Then another network of neural network are given the audio so each network can recognise specific pattern then they talk to the common audio specific network which talks to the main brain network mentioned before In other words something like a robot Which type of network would be the most suitable and scalable for such configuration so you can easily extend it for additional separate network modules Does it matter which type of deep network or not deep I should choose or not
,,"<p>This is the well known <a href=""https://en.wikipedia.org/wiki/Trolley_problem""><em>Trolley Problem</em></a>. As <a href=""http://ai.stackexchange.com/a/134/8"">Ben N</a> said, people disagree on the right course of action for trolley problem scenarios, but it should be noted that with self-driving cars, reliability is so high that these scenarios are really unlikely. So, not much effort will be put into the problems you are describing, at least in the short term.</p>
",,2,2016-08-02T23:31:57.630,,161,2016-08-04T00:19:36.570,2016-08-04T00:19:36.570,,8.0,,130.0,111.0,2,10,,,,67.79,9.92,9.3,0.0,0.0,9.0,This is the well known Trolley Problem As Ben N said people disagree on the right course of action for trolley problem scenarios but it should be noted that with selfdriving cars reliability is so high that these scenarios are really unlikely So not much effort will be put into the problems you are describing at least in the short term
,,"<p>For a driverless car that is designed by a single entity, the best way for it to make decisions about whom to kill is by estimating and minimizing the probable liability.</p>

<p>It doesn't need to absolutely correctly identify all the potential victims in the area to have a defense for its decision, only to identify them as well as a human could be expected to.</p>

<p>It doesn't even need to know the age and physical condition of everyone in the car, as it can ask for that information and if refused, has the defense that the passengers chose not to provide it, and therefore took responsibility for depriving it of the ability to make a better decision.</p>

<p>It only has to have a viable model for minimizing exposure of the entity to lawsuits, which can then be improved over time to make it more profitable.</p>
",,0,2016-08-02T23:44:57.983,,162,2016-08-02T23:44:57.983,,,,,46.0,111.0,2,4,,,,34.73,9.41,10.11,0.0,0.0,12.0,For a driverless car that is designed by a single entity the best way for it to make decisions about whom to kill is by estimating and minimizing the probable liability It doesnt need to absolutely correctly identify all the potential victims in the area to have a defense for its decision only to identify them as well as a human could be expected to It doesnt even need to know the age and physical condition of everyone in the car as it can ask for that information and if refused has the defense that the passengers chose not to provide it and therefore took responsibility for depriving it of the ability to make a better decision It only has to have a viable model for minimizing exposure of the entity to lawsuits which can then be improved over time to make it more profitable
,,"<p>Well I do not know what type of features you are giving to your neural network. However in general I would go with a single neural network. It seems that you have no limitation in resources for training your network and the only problem is resources while you apply your network. </p>

<p>The thing is that probably the two problems have things in common (e.g. both types of plates are rectangular). This means that if you use two networks, each has to solve the same sub-problem (the common part) again. If you use only one network the common part of the problem takes fewer cells/weights to be solved and the remaining weights/cells can be employed for better recognition.</p>

<p>At the end if I was in your place I would try both of them. I think that is the only way to be really sure what is the best solution. When speaking theoretically it is possible that we do not include some factors.</p>
",,0,2016-08-02T23:56:02.343,,163,2016-08-05T10:57:19.847,2016-08-05T10:57:19.847,,143.0,,143.0,13.0,2,1,,,,72.05,8.35,8.36,0.0,0.0,19.0,Well I do not know what type of features you are giving to your neural network However in general I would go with a single neural network It seems that you have no limitation in resources for training your network and the only problem is resources while you apply your network The thing is that probably the two problems have things in common eg both types of plates are rectangular This means that if you use two networks each has to solve the same subproblem the common part again If you use only one network the common part of the problem takes fewer cellsweights to be solved and the remaining weightscells can be employed for better recognition At the end if I was in your place I would try both of them I think that is the only way to be really sure what is the best solution When speaking theoretically it is possible that we do not include some factors
,,"<p>LISP is still used significantly, but less and less. There is still momentum due to so many people using it in the past, who are still active in the industry or research (anecdote: the last VCR was produced by a Japanese maker in July 2016, yes). The language is however used (to my knowledge) for the kind of AI that does not leverage Machine Learning, typically as the reference books from Russell and Norvig. These applications are still very useful, but Machine Learning gets all the steam these days.</p>

<p>Another reason for the decline is that LISP practitioners have partially moved to Clojure and other recent languages.</p>

<p>If you are learning about AI technologies, LISP (or Scheme or Prolog) is good choice to understand what is going on with ""AI"" at large. But if you wish or have to be very pragmatic, Python or R are the community choices</p>

<p>Note: The above lacks concrete example and reference. I am aware of some work in universities, and some companies inspired by or directly using LISP.</p>

<hr>

<p>To add on @Harsh's answer, LISP (and Scheme, and Prolog) has qualities that made it look like it was better suited for creating intelligent mechanisms---making AI as perceived in the 60s.</p>

<p>One of the qualities was that the language design leads the developer to think in a quite elegant way, to decompose a big problem into small problems, etc. Quite ""clever"", or ""intelligent"" if you will. Compared to some other languages, there is almost no choice but to develop that way. LISP is a list processing language, and ""purely functional"".</p>

<p>One problem, though, can be seen in work related to LISP. A notable one in the AI domain is the work on the <a href=""https://en.wikipedia.org/wiki/Situation_calculus"" rel=""nofollow"">Situation Calculus</a>, where (in short) one describes objects and rules in a ""world"", and can let it evolve to compute <em>situations</em>---states of the world. So it is a model for reasoning on situations. The main problem is called the <a href=""https://en.wikipedia.org/wiki/Frame_problem"" rel=""nofollow"">frame problem</a>, meaning this calculus cannot tell what does <em>not</em> change---just what changes. Anything that is not defined in the world cannot be processed (note the difference here with ML). First implementations used LISPs, because that was the AI language then. And there were bound by the frame problem. But, as @Harsh mentioned, it is not LISP's fault: Any language would face the same framing issue (a conceptual problem of the Situation Calculus).</p>

<p>So the language really does not matter from the AI / AGI / ASI perspective. The concepts (algorithms, etc.) are really what matters.</p>

<p>Even in Machine Learning, the language is just a practical choice. Python and R are popular today, primarily due to their library ecosystem and the focus of key companies. But try to use Python or R to run a model for a RaspberryPI-based application, and you will face some severe limitations (but still possible, I am doing it :-)). So the language choice burns down to pragmatism.</p>
",,0,2016-08-03T00:16:14.550,,164,2016-08-03T00:16:14.550,,,,,169.0,77.0,2,3,,,,62.27,10.38,8.95,0.0,0.0,106.0,LISP is still used significantly but less and less There is still momentum due to so many people using it in the past who are still active in the industry or research anecdote the last VCR was produced by a Japanese maker in July 2016 yes The language is however used to my knowledge for the kind of AI that does not leverage Machine Learning typically as the reference books from Russell and Norvig These applications are still very useful but Machine Learning gets all the steam these days Another reason for the decline is that LISP practitioners have partially moved to Clojure and other recent languages If you are learning about AI technologies LISP or Scheme or Prolog is good choice to understand what is going on with AI at large But if you wish or have to be very pragmatic Python or R are the community choices Note The above lacks concrete example and reference I am aware of some work in universities and some companies inspired by or directly using LISP To add on Harshs answer LISP and Scheme and Prolog has qualities that made it look like it was better suited for creating intelligent mechanismsmaking AI as perceived in the 60s One of the qualities was that the language design leads the developer to think in a quite elegant way to decompose a big problem into small problems etc Quite clever or intelligent if you will Compared to some other languages there is almost no choice but to develop that way LISP is a list processing language and purely functional One problem though can be seen in work related to LISP A notable one in the AI domain is the work on the Situation Calculus where in short one describes objects and rules in a world and can let it evolve to compute situationsstates of the world So it is a model for reasoning on situations The main problem is called the frame problem meaning this calculus cannot tell what does not changejust what changes Anything that is not defined in the world cannot be processed note the difference here with ML First implementations used LISPs because that was the AI language then And there were bound by the frame problem But as Harsh mentioned it is not LISPs fault Any language would face the same framing issue a conceptual problem of the Situation Calculus So the language really does not matter from the AI AGI ASI perspective The concepts algorithms etc are really what matters Even in Machine Learning the language is just a practical choice Python and R are popular today primarily due to their library ecosystem and the focus of key companies But try to use Python or R to run a model for a RaspberryPIbased application and you will face some severe limitations but still possible I am doing it So the language choice burns down to pragmatism
,,"<p>The flaw in your argument is that ""surpass"" doesn't just mean that you should be able to run all algorithms, it includes a notion of complexity, i.e. how many time steps you will take to simulate an algorithm. </p>

<p>How do you simulate an algorithm with a Turing machine? A <a href=""https://en.wikipedia.org/wiki/Turing_machine"" rel=""nofollow"">Turing machine</a> consists of a finite state machine and an infinite tape. A TM does run an algorithm, determined by its initial state and the state transition matrix, but what I think you are talking about is Universal Turing Machines (UTM)  that can read ""code"" (which is usually a description of another Turing machine) written on a ""code segment"" of the tape and then simulate that machine on input data written on the ""data segment"" of the tape.</p>

<p>Turing machines can differ in the number of states in their finite state machines (and also in the alphabet they write on the tape but any finite alphabet is easily encoded in binary so this should not be the big reason for differences among Turing machines). So, you can have UTMs with bigger state machines and UTMs with smaller state machines. The bigger UTM could possibly surpass the smaller one if they use the same encoding for the ""code"" part of the tape.</p>

<p>You can also play around with the code used to describe the TM being simulated. This code could be C++ for example, or could be a Neural network with the synapse strength written down as a matrix. Which description is better for computation depends on the problem.</p>

<p>An example comparison among UTMs with different state machines: consider different compilers for the same language, say C++. Both of them will first compile C++ to assembly and then run another UTM which reads and executes assembly (your physical CPU). So, a better compiler will run the same code faster.</p>

<p>Back to humans vs computers, humans are neural networks that run algorithms like those you would write in C++. This involves a costly and inefficient conversion of the algorithm into hand movements. A computer uses a compiler to convert C++ to assembly that it can run natively, so its able to do a much more efficient implementation of C++ code. Alternately, humans have a ton of neurons, and the neural code, i.e. synapse strength, is hard to read, so current computers cannot run that code yet.</p>
",,3,2016-08-03T00:40:35.383,,165,2016-08-03T01:01:12.243,2016-08-03T01:01:12.243,,130.0,,130.0,140.0,2,1,,,,58.92,10.1,8.8,0.0,0.0,68.0,The flaw in your argument is that surpass doesnt just mean that you should be able to run all algorithms it includes a notion of complexity ie how many time steps you will take to simulate an algorithm How do you simulate an algorithm with a Turing machine A Turing machine consists of a finite state machine and an infinite tape A TM does run an algorithm determined by its initial state and the state transition matrix but what I think you are talking about is Universal Turing Machines UTM that can read code which is usually a description of another Turing machine written on a code segment of the tape and then simulate that machine on input data written on the data segment of the tape Turing machines can differ in the number of states in their finite state machines and also in the alphabet they write on the tape but any finite alphabet is easily encoded in binary so this should not be the big reason for differences among Turing machines So you can have UTMs with bigger state machines and UTMs with smaller state machines The bigger UTM could possibly surpass the smaller one if they use the same encoding for the code part of the tape You can also play around with the code used to describe the TM being simulated This code could be C for example or could be a Neural network with the synapse strength written down as a matrix Which description is better for computation depends on the problem An example comparison among UTMs with different state machines consider different compilers for the same language say C Both of them will first compile C to assembly and then run another UTM which reads and executes assembly your physical CPU So a better compiler will run the same code faster Back to humans vs computers humans are neural networks that run algorithms like those you would write in C This involves a costly and inefficient conversion of the algorithm into hand movements A computer uses a compiler to convert C to assembly that it can run natively so its able to do a much more efficient implementation of C code Alternately humans have a ton of neurons and the neural code ie synapse strength is hard to read so current computers cannot run that code yet
,,"<p>I definitely continue to often use Lisp when working on AI models.</p>

<p>You asked if it is being used for <em>substantial</em> work.  That's too subjective for me to answer regarding my own work, but I queried one my AI models whether or not it considered itself substantial, and it replied with an affirmative response.  Of course, it's response is naturally biased as well.</p>

<p>Overall, a significant amount of AI research and development is conducted in Lisp.  Furthermore, even for non-AI problems, Lisp is sometimes used.  To demonstrate the power of Lisp, I engineered the first neural network simulation system written entirely in Lisp over a quarter century ago.</p>
",,0,2016-08-03T01:08:53.837,,166,2016-08-03T01:08:53.837,,,,,156.0,77.0,2,4,,,,55.84,11.48,11.42,0.0,0.0,17.0,I definitely continue to often use Lisp when working on AI models You asked if it is being used for substantial work Thats too subjective for me to answer regarding my own work but I queried one my AI models whether or not it considered itself substantial and it replied with an affirmative response Of course its response is naturally biased as well Overall a significant amount of AI research and development is conducted in Lisp Furthermore even for nonAI problems Lisp is sometimes used To demonstrate the power of Lisp I engineered the first neural network simulation system written entirely in Lisp over a quarter century ago
174.0,1.0,"<p>In <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow"">DeepDream</a> wikipedia page it's suggested that a dreamlike images created by a convolutional neural network may be related to how visual cortex works in humans when they're tripping.</p>

<blockquote>
  <p>The imagery to LSD- and psilocybin-induced hallucinations is suggestive of a functional resemblance between artificial neural networks and particular layers of the visual cortex.</p>
</blockquote>

<p>How this is even possible?</p>

<p>How exactly convolutional neural networks have anything to do with human visual cortex?</p>
",,0,2016-08-03T01:55:30.377,,167,2016-08-18T11:34:48.770,2016-08-18T11:34:48.770,,145.0,,8.0,,1,4,<conv-neural-network><deepdream><computer-vision><deepdreaming>,Why would neural network dream scenes mirror the hallucinations people experience when they're tripping?,44.0,36.49,15.66,11.41,0.0,0.0,8.0,In DeepDream wikipedia page its suggested that a dreamlike images created by a convolutional neural network may be related to how visual cortex works in humans when theyre tripping The imagery to LSD and psilocybininduced hallucinations is suggestive of a functional resemblance between artificial neural networks and particular layers of the visual cortex How this is even possible How exactly convolutional neural networks have anything to do with human visual cortex
176.0,1.0,"<p>This 2014 <a href=""https://medium.com/the-physics-arxiv-blog/first-demonstration-of-artificial-intelligence-on-a-quantum-computer-17a6b9d1c5fb"" rel=""nofollow"">article</a> saying that a Chinese team of physicists have trained a quantum computer to recognise handwritten characters.</p>

<p><strong>Why did they have to use a quantum computer</strong> to do that?</p>

<p>Is it just for fun and demonstration, or is it that recognising the handwritten characters is so difficult that standard (non-quantum) computers or algorithms cannot do that?</p>

<p>If standard computers can achieve the same thing, what are the benefits of using quantum computers to do that then over standard methods?</p>
",,0,2016-08-03T02:12:37.943,1.0,169,2016-08-06T00:07:31.233,2016-08-06T00:07:31.233,,8.0,,8.0,,1,1,<quantum-computing><handwritten-characters><ocr>,What are the challenges for recognising the handwritten characters?,33.0,59.43,12.53,9.12,0.0,0.0,9.0,This 2014 article saying that a Chinese team of physicists have trained a quantum computer to recognise handwritten characters Why did they have to use a quantum computer to do that Is it just for fun and demonstration or is it that recognising the handwritten characters is so difficult that standard nonquantum computers or algorithms cannot do that If standard computers can achieve the same thing what are the benefits of using quantum computers to do that then over standard methods
,,"<p>Does the halting problem imply any limits on human cognition?</p>

<p>Yes, absolutely--that there are pieces of code a human could look at and not be sure whether or not it will halt in finite time. (Certainly there are pieces of code that a human can look at and say ""yes"" or ""no"" definitely, but we're talking about the ones that are actually quite difficult to analyze.)</p>

<p>The halting problem means that there are types of code analysis that no computer could do, because it's mathematically impossible. But the realm of <em>possibility</em> is still large enough to allow strong artificial intelligence (in the sense of code that can understand itself well enough to improve itself).</p>
",,0,2016-08-03T02:17:01.983,,170,2016-08-03T02:17:01.983,,,,,10.0,148.0,2,5,,,,48.33,11.26,9.48,0.0,0.0,20.0,Does the halting problem imply any limits on human cognition Yes absolutelythat there are pieces of code a human could look at and not be sure whether or not it will halt in finite time Certainly there are pieces of code that a human can look at and say yes or no definitely but were talking about the ones that are actually quite difficult to analyze The halting problem means that there are types of code analysis that no computer could do because its mathematically impossible But the realm of possibility is still large enough to allow strong artificial intelligence in the sense of code that can understand itself well enough to improve itself
,,"<p>In contrast to the <em>philosophical</em> definitions, which rely on terms like ""mind"" and ""think,"" there are also definitions that hinge on <em>observables</em>.</p>

<p>That is, a Strong AI is an AI that understands itself well enough to self-improve. Even if it is philosophically not equivalent to a human, or unable to perform <em>all</em> cognitive tasks that a human can, this AI can still generate a tremendous amount of optimization power / good decision-making, and its creation would be of historic importance (to put it lightly).</p>

<p>A Weak AI, in contrast, is an AI with no or limited ability to self-modify. A chessbot that runs on your laptop might have superhuman ability to play chess, but it can <em>only</em> play chess, and while it might tune its weights or its architecture and slowly improve, it cannot modify itself in a deep enough way to generalize to other tasks.</p>

<p>Another way to think about this is that a Strong AI is an AI researcher in its own right, and a Weak AI is what AI researchers produce.</p>
",,0,2016-08-03T02:23:28.267,,171,2016-08-03T02:23:28.267,,,,,10.0,74.0,2,5,,,,50.7,10.28,9.63,0.0,0.0,28.0,In contrast to the philosophical definitions which rely on terms like mind and think there are also definitions that hinge on observables That is a Strong AI is an AI that understands itself well enough to selfimprove Even if it is philosophically not equivalent to a human or unable to perform all cognitive tasks that a human can this AI can still generate a tremendous amount of optimization power good decisionmaking and its creation would be of historic importance to put it lightly A Weak AI in contrast is an AI with no or limited ability to selfmodify A chessbot that runs on your laptop might have superhuman ability to play chess but it can only play chess and while it might tune its weights or its architecture and slowly improve it cannot modify itself in a deep enough way to generalize to other tasks Another way to think about this is that a Strong AI is an AI researcher in its own right and a Weak AI is what AI researchers produce
,3.0,"<p>Is it possible that at some time in the future, AIs will be able to initiatively develop themselves, rather than passively being developed by humanity?</p>
",,0,2016-08-03T03:33:35.193,1.0,172,2016-08-03T20:03:22.657,,,,,104.0,,1,0,<neural-networks><machine-learning><unsupervised-learning><self-learning>,AIs' self-evaluation threshold,25.0,29.18,12.48,11.19,0.0,0.0,3.0,Is it possible that at some time in the future AIs will be able to initiatively develop themselves rather than passively being developed by humanity
,,"<p>The halting problem is an example of a general phenomenon known as <a href=""https://en.wikipedia.org/wiki/Undecidable_problem"" rel=""nofollow"">Undecidability</a>, which shows that there are problems no Turing machine can solve in finite time. Let's consider the generalization that it is undecidable whether a Turing Machine satisfies some non-trivial property P, called <a href=""https://en.wikipedia.org/wiki/Rice%27s_theorem"" rel=""nofollow"">Rice's theorem</a>.</p>

<p>First note that the halting problem applies only if the Turing machine takes arbitrarily long input. If the input is bounded, it is possible to enumerate all possible cases and the problem is no longer undecidable. It might still be inefficient to calculate it, but then we are turning to the complexity theory, which should be a separate question.</p>

<p>Rice's theorem implies that an intelligence (a human) cannot be able to determine whether another intelligence (such as an AGI) possesses a certain property, such as being <a href=""https://en.wikipedia.org/wiki/Friendly_artificial_intelligence"" rel=""nofollow"">friendly</a>. This does not mean that we cannot design a Friendly AGI, but it does mean that we cannot check whether an arbitrary AGI is friendly. So, while we can possibly create an AI which is guaranteed to be friendly, we also need to ensure that IT cannot create another AI which is unfriendly.</p>
",,1,2016-08-03T04:22:28.030,,173,2016-08-03T18:25:39.313,2016-08-03T18:25:39.313,,169.0,,130.0,148.0,2,6,,,,47.72,12.07,10.29,0.0,0.0,25.0,The halting problem is an example of a general phenomenon known as Undecidability which shows that there are problems no Turing machine can solve in finite time Lets consider the generalization that it is undecidable whether a Turing Machine satisfies some nontrivial property P called Rices theorem First note that the halting problem applies only if the Turing machine takes arbitrarily long input If the input is bounded it is possible to enumerate all possible cases and the problem is no longer undecidable It might still be inefficient to calculate it but then we are turning to the complexity theory which should be a separate question Rices theorem implies that an intelligence a human cannot be able to determine whether another intelligence such as an AGI possesses a certain property such as being friendly This does not mean that we cannot design a Friendly AGI but it does mean that we cannot check whether an arbitrary AGI is friendly So while we can possibly create an AI which is guaranteed to be friendly we also need to ensure that IT cannot create another AI which is unfriendly
,,"<p>The similarity of artificial neural networks and the human visual cortex goes very deep, and in many ways the human visual cortex was the inspiration for the techniques we use for the design and implementation of ANNs designed for image recognition. So in that direction, the similarity seems obvious to me.</p>

<p>The reverse direction, though, is a question about how the human mind works under the influence of LSD, which you'll probably get a better answer asking about in the <a href=""http://biology.stackexchange.com/"">biology</a> or <a href=""http://cogsci.stackexchange.com/"">cognitive science</a> stack exchange sites.</p>

<p>Some brief details to add to the answer, though: the human visual cortex is arranged in layers that correspond to increasing layers of abstraction. In the eyes themselves, photons are detected by light-sensitive cells and added together to make what are essentially the color elements of pixels. Those are then routed to another layer which does something like edge detection, and then the next layer does something like shape detection, and so on up to higher level concepts like ""a cat's face.' If LSD lowers the activation threshold for those neurons, or makes them more excitable, then more things will be interpreted as having the higher level concept (and so a patch of rough texture may have a face jump out of it, for example).</p>

<p>The way that CNN ""deep dreaming' works is that the base image is amplified. That is, to make a particular patch look more like a dog, the shapes are nudged to be more dog-like, and the shapes nudge the edges, and the edges nudge the pixels.</p>
",,0,2016-08-03T04:27:10.673,,174,2016-08-03T04:27:10.673,,,,,10.0,167.0,2,7,,,,50.8,11.27,9.53,0.0,0.0,36.0,The similarity of artificial neural networks and the human visual cortex goes very deep and in many ways the human visual cortex was the inspiration for the techniques we use for the design and implementation of ANNs designed for image recognition So in that direction the similarity seems obvious to me The reverse direction though is a question about how the human mind works under the influence of LSD which youll probably get a better answer asking about in the biology or cognitive science stack exchange sites Some brief details to add to the answer though the human visual cortex is arranged in layers that correspond to increasing layers of abstraction In the eyes themselves photons are detected by lightsensitive cells and added together to make what are essentially the color elements of pixels Those are then routed to another layer which does something like edge detection and then the next layer does something like shape detection and so on up to higher level concepts like a cats face If LSD lowers the activation threshold for those neurons or makes them more excitable then more things will be interpreted as having the higher level concept and so a patch of rough texture may have a face jump out of it for example The way that CNN deep dreaming works is that the base image is amplified That is to make a particular patch look more like a dog the shapes are nudged to be more doglike and the shapes nudge the edges and the edges nudge the pixels
,,"<p>Handwritten digit recognition is a standard benchmark in Machine Learning in the form of the <a href=""https://en.wikipedia.org/wiki/MNIST_database"">MNIST dataset</a>. For example, <a href=""http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html"">scikit-learn</a>, a python package for Machine Learning uses it as a tutorial example. </p>

<p>The paper you cite uses this standard task as a proof of concept, to show that their system works.</p>
",,0,2016-08-03T04:41:50.923,,176,2016-08-03T04:41:50.923,,,,,130.0,169.0,2,5,,,,62.68,11.08,10.05,0.0,0.0,7.0,Handwritten digit recognition is a standard benchmark in Machine Learning in the form of the MNIST dataset For example scikitlearn a python package for Machine Learning uses it as a tutorial example The paper you cite uses this standard task as a proof of concept to show that their system works
,,"<p>As for your comment about a computer program showing lower emotional intelligence, you may find Eliza (which you can try <a href=""http://manifestation.com/neurotoys/eliza.php3/"" rel=""nofollow"">here</a>) interesting. It is a classical in the history of AI, and pretends to mimic an analyst (psychology).</p>

<p>However, I think your question fits nowadays more in the field of <a href=""https://en.wikipedia.org/wiki/Human%E2%80%93robot_interaction"" rel=""nofollow"">Human-Robot Interaction</a>, which relies largely on <a href=""http://nordicapis.com/20-emotion-recognition-apis-that-will-leave-you-impressed-and-concerned/"" rel=""nofollow"">vision</a> for recognition of gestures and follow movements, as well as <em>soft, natural</em> movements as a response. Note that the movements of the face and hands belong to the most complex tasks, involving many muscles at a time. </p>

<p>I strongly recommend the film <a href=""http://www.plugandpray-film.de/en/trailer.html"" rel=""nofollow"">Plug&amp;Pray</a> to have an idea of what people are researching in this area.</p>

<p>On the <em>purely human</em> end of the scale, I sometimes wonder about our (my) emotional intelligence myself. Would I want to implement such an intelligence in an artificial agent at all?</p>

<hr>

<p>I remember why I thought of Eliza: not because of its emotional intelligence, but because it was <a href=""http://www.alicebot.org/articles/wallace/eliza.html"" rel=""nofollow"">apparently taken seriously</a> by a couple of humans. Could this be taken as a sort of (approved) Turing test? What does it say about the humans it met?</p>
",,2,2016-08-03T04:48:19.643,,177,2016-08-13T00:31:56.517,2016-08-13T00:31:56.517,,70.0,,70.0,26.0,2,4,,,,52.39,10.68,9.94,0.0,0.0,31.0,As for your comment about a computer program showing lower emotional intelligence you may find Eliza which you can try here interesting It is a classical in the history of AI and pretends to mimic an analyst psychology However I think your question fits nowadays more in the field of HumanRobot Interaction which relies largely on vision for recognition of gestures and follow movements as well as soft natural movements as a response Note that the movements of the face and hands belong to the most complex tasks involving many muscles at a time I strongly recommend the film PlugampPray to have an idea of what people are researching in this area On the purely human end of the scale I sometimes wonder about our my emotional intelligence myself Would I want to implement such an intelligence in an artificial agent at all I remember why I thought of Eliza not because of its emotional intelligence but because it was apparently taken seriously by a couple of humans Could this be taken as a sort of approved Turing test What does it say about the humans it met
,,"<p>This is known as the <a href=""https://en.wikipedia.org/wiki/Intelligence_explosion"" rel=""nofollow"">Intelligence Explosion</a> hypothesis or <a href=""https://wiki.lesswrong.com/wiki/Recursive_self-improvement"" rel=""nofollow"">Recursive self-improvement</a></p>
",,0,2016-08-03T04:52:30.067,,178,2016-08-03T04:52:30.067,,,,,130.0,172.0,2,3,,,,9.55,20.57,11.36,0.0,0.0,1.0,This is known as the Intelligence Explosion hypothesis or Recursive selfimprovement
,2.0,"<p>I have been wondering since a while ago about the <a href=""https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences"" rel=""nofollow"">multiple intelligences</a> and how they could fit in the field of Artificial Intelligence as a whole.</p>

<p>We hear from time to time about <a href=""https://www.theguardian.com/artanddesign/jonathanjonesblog/2016/feb/08/leonardo-da-vinci-mechanics-of-genius-science-museum-london"" rel=""nofollow"">Leonardo</a> being a genius or <a href=""https://www.youtube.com/watch?v=xUHQ2ybTejU"" rel=""nofollow"">Bach's musical intelligence</a>. These persons are commonly said to be (have been) <em>more intelligent</em>. But the multiple intelligences speak about cooking or dancing or chatting as well, i.e. <em>coping with everyday tasks</em> (at least that's my interpretation).</p>

<p><strong>Are there some approaches on incorporating multiple intelligences into AI?</strong></p>

<hr>

<p><a href=""http://ai.stackexchange.com/questions/26/how-could-emotional-intelligence-be-implemented"">Related question - How could emotional intelligence be implemented?</a></p>
",,0,2016-08-03T05:15:39.443,,179,2016-08-26T18:17:40.033,2016-08-26T18:17:40.033,,33.0,,70.0,,1,5,<emotional-intelligence><new-ai>,How can the multiple intelligences model be incorporated into AI?,73.0,41.06,13.33,9.39,0.0,0.0,16.0,I have been wondering since a while ago about the multiple intelligences and how they could fit in the field of Artificial Intelligence as a whole We hear from time to time about Leonardo being a genius or Bachs musical intelligence These persons are commonly said to be have been more intelligent But the multiple intelligences speak about cooking or dancing or chatting as well ie coping with everyday tasks at least thats my interpretation Are there some approaches on incorporating multiple intelligences into AI Related question How could emotional intelligence be implemented
,0.0,"<p>Which is the preferred algorithm to build word vector for a given language?</p>
",2016-11-10T22:25:54.347,1,2016-08-03T05:23:01.403,,180,2016-08-03T10:22:48.630,2016-08-03T10:22:48.630,,72.0,,202.0,,1,1,<algorithm><nlp><wordvector>,How can we build a word vector for a language?,35.0,66.74,9.96,10.35,0.0,0.0,1.0,Which is the preferred algorithm to build word vector for a given language
183.0,1.0,"<p>How to decide the optimum number of layers to be created while implementing a Neural Network (Feedforward, back propagation or RNN)?</p>
",,0,2016-08-03T05:31:30.353,,182,2016-08-16T18:52:36.853,,,,,202.0,,1,9,<neural-networks><hidden-layers>,Number of layers in a Neural network,67.0,50.16,13.63,12.2,0.0,0.0,4.0,How to decide the optimum number of layers to be created while implementing a Neural Network Feedforward back propagation or RNN
,,"<p>There is a technique called <code>Pruning</code> in neural networks, which is used just for this same purpose.</p>

<p>The pruning is done on the number of hidden layers. The process is very similar to the pruning process of decision trees. The pruning process is done as follows:</p>

<ul>
<li>Train a large, densely connected, network with a standard training
algorithm</li>
<li>Examine the trained network to assess the relative importance of the
weights</li>
<li>Remove the least important weight(s)</li>
<li>retrain the pruned network</li>
<li>Repeat steps 2-4 until satisfied</li>
</ul>

<p>However, there are <a href=""https://arxiv.org/abs/1510.00149"">several optimized methods</a> for pruning neural nets, and it is also a <a href=""http://www.idiap.ch/ftp/reports/1997/rr97-03.pdf"">very active area of research</a>.</p>
",,6,2016-08-03T05:35:48.807,,183,2016-08-03T05:35:48.807,,,,,101.0,182.0,2,5,,,,45.59,12.48,11.09,7.0,0.0,13.0,There is a technique called in neural networks which is used just for this same purpose The pruning is done on the number of hidden layers The process is very similar to the pruning process of decision trees The pruning process is done as follows Train a large densely connected network with a standard training algorithm Examine the trained network to assess the relative importance of the weights Remove the least important weights retrain the pruned network Repeat steps 24 until satisfied However there are several optimized methods for pruning neural nets and it is also a very active area of research
,1.0,"<p>I am interested in the <a href=""https://en.wikipedia.org/wiki/Emergence"" rel=""nofollow"">emergence</a> of properties in <a href=""https://en.wikipedia.org/wiki/Agent-based_model#Theory"" rel=""nofollow"">agents</a>, and, more generally in robotics.</p>

<p>I was wondering if there is work on the emergence of time-related concepts, on the low-level representation of notions like <em>before</em> and <em>after</em>. I know, for example, that there is work on the emergence of <a href=""http://www.scholarpedia.org/article/Kohonen_network"" rel=""nofollow"">spatial representation</a> (similar to <a href=""https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"" rel=""nofollow"">knn</a>), or even <a href=""https://infoscience.epfl.ch/record/129415/files/Mitrietal_1.pdf"" rel=""nofollow"">communication</a>* but time seems to be a tricky concept. </p>

<p>This has everything to do with the <em>platform</em>, i.e. the way that the representation would be coded in. We tend to favour ways that have some meaning or somehow mimic natural, well, yes, human structures, like the brain. I am not a neuroscientist and do not know that the sense of time <em>looks like</em> in humans, or if it is even present in other living beings.</p>

<p><strong>Is there some work on the (emergence of the) representation of <em>time</em> in artificial agents?</strong></p>

<hr>

<p>*I remember watching a really cool... Actually creepy video from these robots but cannot find it anymore. Does anyone have the link at hand?</p>
",,7,2016-08-03T06:03:28.903,1.0,184,2016-09-02T21:11:12.350,2016-09-02T21:11:12.350,,70.0,,70.0,,1,1,<knowledge-representation><time><embodied-cognition>,Are there emergent models of time in robots?,31.0,64.2,9.92,8.93,0.0,0.0,34.0,I am interested in the emergence of properties in agents and more generally in robotics I was wondering if there is work on the emergence of timerelated concepts on the lowlevel representation of notions like before and after I know for example that there is work on the emergence of spatial representation similar to knn or even communication but time seems to be a tricky concept This has everything to do with the platform ie the way that the representation would be coded in We tend to favour ways that have some meaning or somehow mimic natural well yes human structures like the brain I am not a neuroscientist and do not know that the sense of time looks like in humans or if it is even present in other living beings Is there some work on the emergence of the representation of time in artificial agents I remember watching a really cool Actually creepy video from these robots but cannot find it anymore Does anyone have the link at hand
,,"<p>Humans might create somewhere in the future a so-called ultraintelligent machine, a machine that can surpass all intellectual activities by any human. This would be the last invention man would need to do, since this machine is better in inventing machines than humans are (since that is an intellectual activity). Also, since humans can create machines as good as the ultraintelligent machine, this machine can create better machines, which in turn can create better machines, etcetera. This is known as the <a href=""https://en.wikipedia.org/wiki/Intelligence_explosion"" rel=""nofollow"">Intelligence explosion</a>, and it is also called recursive self-improvement (as has been pointed out by @Harsh).</p>

<p>The existence, let alone the development, if an ultraintelligent machine is still hypothetical. We are nowhere close to creating an ultraintelligent machine.</p>
",,0,2016-08-03T06:10:03.807,,185,2016-08-03T06:10:03.807,,,,,29.0,172.0,2,2,,,,34.46,14.79,9.26,0.0,0.0,22.0,Humans might create somewhere in the future a socalled ultraintelligent machine a machine that can surpass all intellectual activities by any human This would be the last invention man would need to do since this machine is better in inventing machines than humans are since that is an intellectual activity Also since humans can create machines as good as the ultraintelligent machine this machine can create better machines which in turn can create better machines etcetera This is known as the Intelligence explosion and it is also called recursive selfimprovement as has been pointed out by Harsh The existence let alone the development if an ultraintelligent machine is still hypothetical We are nowhere close to creating an ultraintelligent machine
,2.0,"<p>Have there been proposed extensions to go beyond a Turing machine that solve the halting problem and if so, would those proposed extensions have value to advance strong Artificial Intelligence?  For example, does quantum computing go beyond the definition of a Turing machine and resolve the halting problem, and does that help in creating strong AI?</p>
",2016-08-15T03:28:50.017,5,2016-08-03T06:20:12.393,,186,2016-08-07T23:34:29.837,,,,,55.0,,1,2,<quantum-computing><halting-problem><strong-ai>,Does a quantum computer resolve the halting problem and would that advance strong AI?,108.0,43.06,13.47,9.54,0.0,0.0,5.0,Have there been proposed extensions to go beyond a Turing machine that solve the halting problem and if so would those proposed extensions have value to advance strong Artificial Intelligence For example does quantum computing go beyond the definition of a Turing machine and resolve the halting problem and does that help in creating strong AI
,,"<blockquote>
  <p>(this was intended as a comment, but turned out long and longer)</p>
</blockquote>

<p>A couple of points to elaborate on <a href=""http://ai.stackexchange.com/a/73/70"">Ben's answer</a>:</p>

<ul>
<li>It is possible to generate different models (out of existing data!) and then look for the model that best fit new data (e.g. with <a href=""https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"" rel=""nofollow"">knn</a>). Example: 

<ul>
<li>States = {<em>sleep</em>, <em>eat</em>, <em>walk</em>, <em>work</em>}</li>
<li>Model 1: Most probable sequence on weekdays, say: sleep → sleep → eat → walk → work → work → eat → walk → sleep  → sleep</li>
<li>Model 2: Most probable sequence on weekends, some: sleep → sleep → eat → walk → eat → walk → sleep → sleep</li>
<li>New data arrives: Which sequence is more probable that it came from? Check model 1, check model 2. Which fits better? → Assign</li>
</ul></li>
<li>Note that the previous example is oversimplified. Also note that a <em>unit time</em> is needed there (other than letters / words, for instance).</li>
<li>You can <em>nest</em> Markov models. That means that you generate a model (a set of probabilities for all the states) in a ""lower scale"" and then use it in a more abstract model. For example, you can nest your day-scale model to a month or year (to include holidays, for instance).</li>
</ul>

<p>Also <a href=""http://blog.wolfram.com/2013/02/04/centennial-of-markov-chains/"" rel=""nofollow"">see this link for a nice introduction</a> and <a href=""https://stats.stackexchange.com/questions/tagged/mcmc?sort=votes&amp;pageSize=50"">some posts in crossvalidated</a>.</p>

<hr>

<p>As for the question if artificial intelligence can be created by using this kind of methods, my personal (easy) answer would be <strong>no</strong>, because they only relate data and probabilities and thus belong more to the statistics and machine learning branch. </p>

<p>A longer answer needs to take into account the <a href=""http://ai.stackexchange.com/questions/74/what-is-the-difference-between-strong-ai-and-weak-ai"">weak vs. strong AI question</a>.</p>
",,0,2016-08-03T06:37:01.983,,187,2016-08-03T06:37:01.983,,,,,70.0,37.0,2,1,,,,77.57,8.65,8.62,0.0,0.0,57.0,this was intended as a comment but turned out long and longer A couple of points to elaborate on Bens answer It is possible to generate different models out of existing data and then look for the model that best fit new data eg with knn Example States sleep eat walk work Model 1 Most probable sequence on weekdays say sleep → sleep → eat → walk → work → work → eat → walk → sleep → sleep Model 2 Most probable sequence on weekends some sleep → sleep → eat → walk → eat → walk → sleep → sleep New data arrives Which sequence is more probable that it came from Check model 1 check model 2 Which fits better → Assign Note that the previous example is oversimplified Also note that a unit time is needed there other than letters words for instance You can nest Markov models That means that you generate a model a set of probabilities for all the states in a lower scale and then use it in a more abstract model For example you can nest your dayscale model to a month or year to include holidays for instance Also see this link for a nice introduction and some posts in crossvalidated As for the question if artificial intelligence can be created by using this kind of methods my personal easy answer would be no because they only relate data and probabilities and thus belong more to the statistics and machine learning branch A longer answer needs to take into account the weak vs strong AI question
,,"<p>To my knowledge, this is very  much an open research issue.</p>

<p>Here is a paper by Prof Leslie Smith, an acknowledged expert on neuromorphic perceptual coding, which explains the importance of the notion of perceptual time for Artificial General Intelligence and sketches an architecture from which a notion of 'now' might emerge: <a href=""http://www.cs.stir.ac.uk/~lss/recentpapers/perctime.pdf"" rel=""nofollow"">Perceptual Time</a>.</p>
",,1,2016-08-03T07:00:16.047,,188,2016-08-03T07:00:16.047,,,,,42.0,184.0,2,1,,,,35.61,14.05,10.82,0.0,0.0,8.0,To my knowledge this is very much an open research issue Here is a paper by Prof Leslie Smith an acknowledged expert on neuromorphic perceptual coding which explains the importance of the notion of perceptual time for Artificial General Intelligence and sketches an architecture from which a notion of now might emerge Perceptual Time
,,"<p>Architectures for recognising and generating emotion are typically somewhat complex and don't generally have short descriptions, so it's probably better to reference the literature rather than give a misleading soundbite:</p>

<p>Some of the early work in `Affective Computing' was done by Rosamund Picard.
There is a <a href=""http://affect.media.mit.edu/"">research group at MIT</a> specializing in this area.</p>

<p>Some of the more developed architectural ideas are due to Marvin Minsky.
A pre-publication draft of his book, `The Emotion Machine' is available via <a href=""https://en.wikipedia.org/wiki/The_Emotion_Machine"">Wikipedia</a>.</p>

<p>Emotional intelligence would certainly seem to be a necessary component of passing the Turing test - indeed, in the original Turing test essay in <a href=""http://www.csee.umbc.edu/courses/471/papers/turing.pdf"">Computing Machinery and Intelligence</a> implied some degree of ""Theory of Mind"" about Mr Pickwick's preferences:</p>

<p><em>""Yet Christmas is a Winter’s day, and I do not think Mr Pickwick would mind the comparison.""</em></p>
",,0,2016-08-03T07:13:10.390,,189,2016-08-03T07:13:10.390,,,,,42.0,26.0,2,10,,,,27.35,14.74,11.56,0.0,0.0,24.0,Architectures for recognising and generating emotion are typically somewhat complex and dont generally have short descriptions so its probably better to reference the literature rather than give a misleading soundbite Some of the early work in Affective Computing was done by Rosamund Picard There is a research group at MIT specializing in this area Some of the more developed architectural ideas are due to Marvin Minsky A prepublication draft of his book The Emotion Machine is available via Wikipedia Emotional intelligence would certainly seem to be a necessary component of passing the Turing test indeed in the original Turing test essay in Computing Machinery and Intelligence implied some degree of Theory of Mind about Mr Pickwicks preferences Yet Christmas is a Winter’s day and I do not think Mr Pickwick would mind the comparison
,,"<p>It depends a bit on what you mean by 'quantum computer'. The 'conventional' notion is that quantum computation buys a (in some cases, exponential) speedup - it doesn't change <em>what</em> can be computed, just how quickly.</p>

<p>In contrast, advocates of <em>hypercomputation</em> claim that quantum effects may make it possible to do infinite computations in finite time. Note, however, that this is not a mainstream belief - the reknowned logician Martin Davis has written an article claiming that hypercomputation is <a href=""http://www1.maths.leeds.ac.uk/~pmt6sbc/docs/davis.myth.pdf"" rel=""nofollow"">a myth</a>.</p>

<p>Roger Penrose has also claimed that quantum vibrations in neural microtubules may be <a href=""https://en.wikipedia.org/wiki/Quantum_mind"" rel=""nofollow"">responsible for consciousness</a>.</p>
",,0,2016-08-03T07:36:08.923,,190,2016-08-03T07:36:08.923,,,,,42.0,186.0,2,3,,,,43.73,14.74,11.06,0.0,0.0,19.0,It depends a bit on what you mean by quantum computer The conventional notion is that quantum computation buys a in some cases exponential speedup it doesnt change what can be computed just how quickly In contrast advocates of hypercomputation claim that quantum effects may make it possible to do infinite computations in finite time Note however that this is not a mainstream belief the reknowned logician Martin Davis has written an article claiming that hypercomputation is a myth Roger Penrose has also claimed that quantum vibrations in neural microtubules may be responsible for consciousness
1526.0,2.0,"<p>What was the first AI that was able to carry on a conversation, with real responses, such as in the famous <a href=""https://www.youtube.com/watch?v=WnzlbyTZsQY"" rel=""nofollow"">'I am not a robot. I am a unicorn' case?</a></p>

<p>A 'real response' constitutes a sort-of personalized answer to a specific input by a user.</p>
",,2,2016-08-03T07:57:21.743,,191,2016-08-10T14:30:24.480,2016-08-10T14:30:24.480,,145.0,,145.0,,1,4,<history><turing-test><natural-language>,What was the first machine that was able to carry on a conversation?,62.0,72.87,6.49,9.54,0.0,0.0,10.0,What was the first AI that was able to carry on a conversation with real responses such as in the famous I am not a robot I am a unicorn case A real response constitutes a sortof personalized answer to a specific input by a user
,,"<p>A neural net with even a single hidden layer is capable of Universal function approximation - it can approximate any continuous function 'as closely as you like'.</p>

<p>Hence, one option would be to look for GOFAI applications that would benefit from this property - for example, in state-space search approaches where the utility of a state is not readily defined in advance, and could instead be learned.</p>
",,0,2016-08-03T08:22:36.737,,193,2016-08-03T08:22:36.737,,,,,42.0,146.0,2,1,,,,30.03,13.12,11.56,0.0,0.0,10.0,A neural net with even a single hidden layer is capable of Universal function approximation it can approximate any continuous function as closely as you like Hence one option would be to look for GOFAI applications that would benefit from this property for example in statespace search approaches where the utility of a state is not readily defined in advance and could instead be learned
,,,,0,2016-08-03T08:44:31.107,,194,2016-08-03T08:44:31.107,2016-08-03T08:44:31.107,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions regarding handwriting recognition,,0,2016-08-03T08:44:31.107,,195,2016-08-04T02:54:15.227,2016-08-04T02:54:15.227,,101.0,,101.0,,4,0,,,,7.18,28.16,16.52,0.0,0.0,0.0,For questions regarding handwriting recognition
,,"<p>In 1986, the first PC therapist program was written by Joseph Weintraub. This program won the first Loebner Prize in 1991, and then again in 1992, 1993 and 1995. </p>

<p>In 1981 or 1982, Jabberwacky was founded, which is the foundation of the current Cleverbot. Jabberwacky  appeared on the internet in 1997, reaching the third place for the Loebner Prize in 2003, the second place in 2004, and won in 2005 and 2006. In 2008, Cleverbot was launched as an variant of Jabberwacky. </p>

<p>I'm not sure these are really the earliest, but that also depends on what you want earliest (programming started, first conversation,  first decent conversation, etc.). Also, it depends on what you call a ""real response"".</p>
",,3,2016-08-03T08:49:27.637,,196,2016-08-03T08:49:27.637,,,,,29.0,191.0,2,5,,,,79.9,10.9,8.65,0.0,0.0,27.0,In 1986 the first PC therapist program was written by Joseph Weintraub This program won the first Loebner Prize in 1991 and then again in 1992 1993 and 1995 In 1981 or 1982 Jabberwacky was founded which is the foundation of the current Cleverbot Jabberwacky appeared on the internet in 1997 reaching the third place for the Loebner Prize in 2003 the second place in 2004 and won in 2005 and 2006 In 2008 Cleverbot was launched as an variant of Jabberwacky Im not sure these are really the earliest but that also depends on what you want earliest programming started first conversation first decent conversation etc Also it depends on what you call a real response
,2.0,"<p>This question stems from quite a few ""informal"" sources. Movies like <em>2001, A Space Odyssey</em> and <em>Ex Machina</em>; books like <em>Destination Void</em> (Frank Herbert), and others suggest that general intelligence <em>wants</em> to survive, and even learn the importance for it.</p>

<p>There may be several arguments for survival. What would be the most prominent?</p>
",,0,2016-08-03T08:56:53.430,,197,2016-08-03T15:58:13.493,,,,,169.0,,1,2,<agi>,Is there a strong argument that survival instinct is a prerequisite for creating an AGI?,29.0,58.08,12.34,9.95,0.0,0.0,12.0,This question stems from quite a few informal sources Movies like 2001 A Space Odyssey and Ex Machina books like Destination Void Frank Herbert and others suggest that general intelligence wants to survive and even learn the importance for it There may be several arguments for survival What would be the most prominent
1345.0,2.0,"<p>Identifying sarcasm is considered as one of the most difficult open-ended problems in the domain of ML and NLP.</p>

<p>So, was there any considerable research done in that front? If yes, then what is the accuracy like? Please also explain the NLP model briefly.</p>
",,2,2016-08-03T09:01:05.790,4.0,198,2016-08-05T04:52:42.947,,,,,101.0,,1,12,<natural-language><research>,"What research has been done in the domain of ""Identifying sarcasm in text""?",94.0,68.77,9.38,9.57,0.0,0.0,7.0,Identifying sarcasm is considered as one of the most difficult openended problems in the domain of ML and NLP So was there any considerable research done in that front If yes then what is the accuracy like Please also explain the NLP model briefly
,,"<p>It all depends of what your A.I. can do. Even humans cannot do everything.</p>

<p>If your AI program is so smart, ask it to take the general IQ tests for humans. Because the real IQ tests are made of several questions from different areas, so in that way you can measure IQ of your AI.</p>

<p>This is because the <strong>IQ</strong> means the tests which are <strong>designed</strong> to assess human intelligence.</p>

<blockquote>
  <p>An intelligence quotient (IQ) is a total score derived from one of several standardized tests designed to assess human intelligence.<sup><a href=""https://en.wikipedia.org/wiki/Intelligence_quotient"" rel=""nofollow"">wiki</a></sup></p>
</blockquote>

<p>So there is no any other way of measuring IQ without taking IQ test, otherwise it won't be IQ (very logical).</p>

<p>If your program is not so smart, you should look for specific tests related to the expertise or problem being solved. Ideally let it compete with humans who has the same expertise in that area, but it's important make the test on the same ground/level.</p>

<p>For example the intelligence of <a href=""https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)"" rel=""nofollow"">Deep Blue</a> project was measured by playing chess with Kasparov. Then if world champion cannot win the game, who will?</p>

<p>If you're writing program to play a game, make it play with compete with humans and measure the intelligence in terms of score.</p>

<hr>

<p>The equivalent of IQ for AI is a Turing Test (like <a href=""http://ai.stackexchange.com/q/1397/8"">MIST</a> and other), see:</p>

<ul>
<li><a href=""http://ai.stackexchange.com/q/15/8"">Is the Turing Test, or any of its variants, a reliable test of artificial intelligence?</a></li>
</ul>
",,1,2016-08-03T09:06:26.437,,199,2016-08-06T11:09:15.160,2016-08-06T11:09:15.160,,8.0,,8.0,41.0,2,2,,,,61.67,8.99,8.31,0.0,0.0,36.0,It all depends of what your AI can do Even humans cannot do everything If your AI program is so smart ask it to take the general IQ tests for humans Because the real IQ tests are made of several questions from different areas so in that way you can measure IQ of your AI This is because the IQ means the tests which are designed to assess human intelligence An intelligence quotient IQ is a total score derived from one of several standardized tests designed to assess human intelligencewiki So there is no any other way of measuring IQ without taking IQ test otherwise it wont be IQ very logical If your program is not so smart you should look for specific tests related to the expertise or problem being solved Ideally let it compete with humans who has the same expertise in that area but its important make the test on the same groundlevel For example the intelligence of Deep Blue project was measured by playing chess with Kasparov Then if world champion cannot win the game who will If youre writing program to play a game make it play with compete with humans and measure the intelligence in terms of score The equivalent of IQ for AI is a Turing Test like MIST and other see Is the Turing Test or any of its variants a reliable test of artificial intelligence
,,"<p>Personally, I think this might be an overhyped issue. Trolley problems only occur when the situation is optimized to prevent ""3rd options"".</p>

<p>A car has brakes, does it not? ""But what if the brakes don't work?"" Well, then <strong>the car is not allowed to drive at all.</strong> Even in regular traffic, human operators are taught that your speed should be limited as such that you can stop within the area you can see. Solutions like these will reduce the possibility of a trolley problem.</p>

<p>As for animals... if there is no explicit effort to deal with humans on the road I think animals will be treated the same. This sounds implausible - roadkill happens often and human ""roadkill"" is unwanted, but animals are a lot smaller and harder to see than humans, so I think detecting humans will be easier, preventing a lot of the accidents.</p>

<p>In other cases (bugs, faults while driving, multiple failures stacked onto each other), perhaps accidents will occur, they'll be analysed, and vehicles will be updated to avoid causing similar situations. </p>
",,1,2016-08-03T09:17:14.977,,200,2016-08-03T09:31:21.937,2016-08-03T09:31:21.937,,74.0,,74.0,111.0,2,20,,,,63.8,10.44,10.2,0.0,0.0,36.0,Personally I think this might be an overhyped issue Trolley problems only occur when the situation is optimized to prevent 3rd options A car has brakes does it not But what if the brakes dont work Well then the car is not allowed to drive at all Even in regular traffic human operators are taught that your speed should be limited as such that you can stop within the area you can see Solutions like these will reduce the possibility of a trolley problem As for animals if there is no explicit effort to deal with humans on the road I think animals will be treated the same This sounds implausible roadkill happens often and human roadkill is unwanted but animals are a lot smaller and harder to see than humans so I think detecting humans will be easier preventing a lot of the accidents In other cases bugs faults while driving multiple failures stacked onto each other perhaps accidents will occur theyll be analysed and vehicles will be updated to avoid causing similar situations
,,"<p>Convolutional Nets (CNN) rely on mathematical convolution (e.g. 2D or 3D convolutions), which is commonly used for signal processing. Images are a type of signal, and convolution can equally be used on sound, vibrations, etc. So, in principle, CNNs can find applications to any signal, and probably more.</p>

<p>In practice, there exists already work on NLP (as mentioned by Matthew Graves), where some people process text with CNNs rather than recursive networks. Some other works apply to sound processing (no reference here, but I have yet unpublished work ongoing).</p>

<hr>

<p><em>Original contents: In answer to the original title question, which has changed now. Perhaps need to delete this one</em>.</p>

<p>Research on adversarial networks (and related) show that even <a href=""http://arxiv.org/abs/1412.1897"" rel=""nofollow"">deep networks can easily be fooled</a>, leading them to see a dog (or whatever object) in what appears to be random noise when a human look at it (the article has clear examples).</p>

<p>Another issue is the generalization power of a neural network. Convolutional nets have amazed the world with their capability to generalize way better than other techniques. But if the network is only fed images of cats, it will recognize only cats (and probably see cats everywhere, as by adversarial network results). In other words, even CNs have a hard time generalizing too far <em>beyond</em> what they learned from.</p>

<p>The recognition limit is hard to define precisely. I would simply say that the diversity of the learning data pushes the limit (I assume further detail should lead to more appropriate venue for discussion).</p>
",,0,2016-08-03T09:18:24.867,,201,2016-08-07T09:03:34.373,2016-08-07T09:03:34.373,,169.0,,169.0,70.0,2,5,,,,54.42,12.06,10.11,0.0,0.0,50.0,Convolutional Nets CNN rely on mathematical convolution eg 2D or 3D convolutions which is commonly used for signal processing Images are a type of signal and convolution can equally be used on sound vibrations etc So in principle CNNs can find applications to any signal and probably more In practice there exists already work on NLP as mentioned by Matthew Graves where some people process text with CNNs rather than recursive networks Some other works apply to sound processing no reference here but I have yet unpublished work ongoing Original contents In answer to the original title question which has changed now Perhaps need to delete this one Research on adversarial networks and related show that even deep networks can easily be fooled leading them to see a dog or whatever object in what appears to be random noise when a human look at it the article has clear examples Another issue is the generalization power of a neural network Convolutional nets have amazed the world with their capability to generalize way better than other techniques But if the network is only fed images of cats it will recognize only cats and probably see cats everywhere as by adversarial network results In other words even CNs have a hard time generalizing too far beyond what they learned from The recognition limit is hard to define precisely I would simply say that the diversity of the learning data pushes the limit I assume further detail should lead to more appropriate venue for discussion
206.0,2.0,"<p>I'd like to know more about <a href=""http://ai.stackexchange.com/q/26/8"">implementing emotional intelligence</a>.</p>

<p>Given I'm implementing a chat bot and I'd like to introduce the levels of curiosity to measure whether user text input is interesting or not.</p>

<p>High level would mean bot is asking more questions and is following the topic, lower level of curiosity makes the bot not asking any questions and changing the topics.</p>

<p>Less interesting content could mean the bot doesn't see any opportunity to learn something new or it doesn't understand the topic or doesn't want to talk about it, because of its low quality. </p>

<p>How this possibly can be achieved? Are there any examples?</p>
",,0,2016-08-03T09:18:52.437,1.0,202,2016-08-30T21:04:22.253,,,,,8.0,,1,1,<emotional-intelligence><chat-bots>,How can you simulate level of curiosity for a chat bot?,72.0,53.51,10.9,9.43,0.0,0.0,14.0,Id like to know more about implementing emotional intelligence Given Im implementing a chat bot and Id like to introduce the levels of curiosity to measure whether user text input is interesting or not High level would mean bot is asking more questions and is following the topic lower level of curiosity makes the bot not asking any questions and changing the topics Less interesting content could mean the bot doesnt see any opportunity to learn something new or it doesnt understand the topic or doesnt want to talk about it because of its low quality How this possibly can be achieved Are there any examples
219.0,1.0,"<p>I would like to learn more whether it is possible and how to write a program which decompiles executable binary (an object file) to the C source. I'm not asking exactly 'how', but rather how this can be achieved.</p>

<p>Given the following <code>hello.c</code> file (as example):</p>

<pre><code>#include &lt;stdio.h&gt;
int main() {
  printf(""Hello World!"");
}
</code></pre>

<p>Then after compilation (<code>gcc hello.c</code>) I've got the binary file like:</p>

<pre><code>$ hexdump -C a.out | head
00000000  cf fa ed fe 07 00 00 01  03 00 00 80 02 00 00 00  |................|
00000010  0f 00 00 00 b0 04 00 00  85 00 20 00 00 00 00 00  |.......... .....|
00000020  19 00 00 00 48 00 00 00  5f 5f 50 41 47 45 5a 45  |....H...__PAGEZE|
00000030  52 4f 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |RO..............|
00000040  00 00 00 00 01 00 00 00  00 00 00 00 00 00 00 00  |................|
00000050  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|
00000060  00 00 00 00 00 00 00 00  19 00 00 00 d8 01 00 00  |................|
00000070  5f 5f 54 45 58 54 00 00  00 00 00 00 00 00 00 00  |__TEXT..........|
$ wc -c hello.c a.out 
  60 hello.c
8432 a.out
</code></pre>

<p>For the learning dataset I assume I'll have to have thousands of source code files along with its binary representation, so algorithm can learn about moving parts on certain changes.</p>

<p>My concerns are:</p>

<ul>
<li>do my algorithm needs to be aware about the header file, or it's ""smart"" enough to figure it out,</li>
<li>if it needs to know about the header, how do I tell my algorithm 'here is the header file',</li>
<li>what should be input/output mapping (whether some section to section or file to file),</li>
<li>do I need to divide my source code into some sections,</li>
<li>do I need to know exactly how decompilers work or AI can figure it out for me,</li>
<li>or should I've two networks, one for header, another for body it-self,</li>
<li>or more separate networks, each one for each logical component (e.g. byte->C tag, etc.)</li>
</ul>

<p>How would you tackle this?</p>
",,1,2016-08-03T10:03:58.587,,205,2016-08-03T14:19:39.400,2016-08-03T11:51:11.470,,8.0,,8.0,,1,2,<algorithm><deep-learning>,How to write C decompiler using AI?,182.0,46.44,9.88,9.74,792.0,0.0,49.0,I would like to learn more whether it is possible and how to write a program which decompiles executable binary an object file to the C source Im not asking exactly how but rather how this can be achieved Given the following file as example Then after compilation Ive got the binary file like For the learning dataset I assume Ill have to have thousands of source code files along with its binary representation so algorithm can learn about moving parts on certain changes My concerns are do my algorithm needs to be aware about the header file or its smart enough to figure it out if it needs to know about the header how do I tell my algorithm here is the header file what should be inputoutput mapping whether some section to section or file to file do I need to divide my source code into some sections do I need to know exactly how decompilers work or AI can figure it out for me or should Ive two networks one for header another for body itself or more separate networks each one for each logical component eg byteC tag etc How would you tackle this
,,"<p>It's possible to implement a form of curiosity-driven behavior without requiring full 'emotional intelligence'. One elementary strategy would be to define some form of similarity measure on inputs.</p>

<p>More generally, Jurgen Schmidhuber has pioneered work on 'Artificial Curiosity/Creativity' and 'Intrinsic Motivation' and has written a number of papers on the subject:</p>

<ul>
<li><a href=""http://people.idsia.ch/~juergen/curioussingapore/curioussingapore.html"">Artificial Curiosity</a> </li>
<li><a href=""http://people.idsia.ch/~juergen/ieeecreative.pdf"">Intrinsic Motivation</a></li>
</ul>

<p>Here is a <a href=""https://www.youtube.com/watch?v=Ipomu0MLFaI"">video</a> of a nice associated presentation.</p>
",,0,2016-08-03T10:12:58.320,,206,2016-08-03T10:12:58.320,,,,,42.0,202.0,2,6,,,,7.56,18.62,12.83,0.0,0.0,14.0,Its possible to implement a form of curiositydriven behavior without requiring full emotional intelligence One elementary strategy would be to define some form of similarity measure on inputs More generally Jurgen Schmidhuber has pioneered work on Artificial CuriosityCreativity and Intrinsic Motivation and has written a number of papers on the subject Artificial Curiosity Intrinsic Motivation Here is a video of a nice associated presentation
,1.0,"<p>Text summarization is a long-standing research problem that was <em>""ignited""</em> by Luhn in 1958. However, a half century later, we still came nowhere close  to solving this problem (abstractive summarization). The reason for this might be because researchers are resorting to statistical (and sometimes linguistic) methods to find &amp; extract the most salient parts of the text.</p>

<p>Is summarization problem solvable using AI (neural networks to be precise)? </p>
",,0,2016-08-03T10:14:49.743,,207,2016-08-04T06:13:13.763,2016-08-04T06:13:13.763,,220.0,,220.0,,1,3,<neural-networks><natural-language><text-summarization>,Can abstractive summarization be achieved using neural networks?,28.0,45.76,15.02,10.75,0.0,0.0,17.0,Text summarization is a longstanding research problem that was ignited by Luhn in 1958 However a half century later we still came nowhere close to solving this problem abstractive summarization The reason for this might be because researchers are resorting to statistical and sometimes linguistic methods to find amp extract the most salient parts of the text Is summarization problem solvable using AI neural networks to be precise
,,"<p>Formally, a single hidden layer is sufficient to approximate a continuous function to any desired degree of accuracy, so in that sense, you never need more than 1.</p>

<p>Finding the best topology for a given problem is an open research problem. As far as I know, there are few universal 'rules of thumb' for this.</p>

<p>For a given problem, one option is to apply a <em>neuroevolutionary</em> approach such as <a href=""https://www.cs.ucf.edu/~kstanley/neat.html"">NEAT</a>, which attempts to find a topology that works well for the problem at hand.</p>
",,0,2016-08-03T10:26:17.493,,208,2016-08-03T10:26:17.493,,,,,42.0,63.0,2,6,,,,58.62,9.4,9.57,0.0,0.0,12.0,Formally a single hidden layer is sufficient to approximate a continuous function to any desired degree of accuracy so in that sense you never need more than 1 Finding the best topology for a given problem is an open research problem As far as I know there are few universal rules of thumb for this For a given problem one option is to apply a neuroevolutionary approach such as NEAT which attempts to find a topology that works well for the problem at hand
215.0,2.0,"<p>I'd like to know which common file format is more efficient in terms of simplicity and storage space for storing the state of artificial neural network.</p>

<p>I'm not talking about memory storage, but file storage, so the data can be loaded later on.</p>

<p>My first guess would be XML, but having millions of connections and weights would generate huge amount of data. Another thing would be to dump object instances into binary file using some export/serialize functions, but the disadvantage is that the file isn't common and it's language specific.</p>

<p>Are there any common file format standards which can be used for exporting huge artificial neural network into the file to be loaded by another program? If so, which one.</p>
",2016-08-13T00:12:44.823,0,2016-08-03T12:02:55.280,,210,2016-08-03T13:02:40.337,2016-08-03T12:34:22.730,,8.0,,8.0,,1,0,<storage>,What's the most suitable format to store huge number of neurons states?,20.0,51.18,11.26,10.02,0.0,0.0,16.0,Id like to know which common file format is more efficient in terms of simplicity and storage space for storing the state of artificial neural network Im not talking about memory storage but file storage so the data can be loaded later on My first guess would be XML but having millions of connections and weights would generate huge amount of data Another thing would be to dump object instances into binary file using some exportserialize functions but the disadvantage is that the file isnt common and its language specific Are there any common file format standards which can be used for exporting huge artificial neural network into the file to be loaded by another program If so which one
1664.0,1.0,"<p>What AI techniques does IBM use for its Watson platform, specifically its natural language analysis?</p>
",2016-08-18T19:55:42.773,7,2016-08-03T12:27:56.120,,211,2016-08-17T14:48:12.957,2016-08-17T13:53:37.483,,75.0,,8.0,,1,1,<algorithm><natural-language><watson><lexical-recognition>,How does DeepQA analyze natural language?,58.0,30.87,15.36,11.75,0.0,0.0,2.0,What AI techniques does IBM use for its Watson platform specifically its natural language analysis
,2.0,"<p>I'm investigating the possibility of storing the semantic-lexical connections (such as the relationships to the other words such as phrases and other dependencies, its strength, part of speech, language, etc.) in order to provide analysis of the input text.</p>

<p>I assume this has been already done. If so, to avoid reinventing the wheel, is there any efficient method to store and manage such data in some common format which has been already researched and tested?</p>
",,0,2016-08-03T12:41:06.953,1.0,212,2016-08-26T23:10:09.577,,,,,8.0,,1,0,<algorithm><models><research><storage><lexical-recognition>,How to store datasets of lexical connections?,47.0,52.39,12.65,10.25,0.0,0.0,14.0,Im investigating the possibility of storing the semanticlexical connections such as the relationships to the other words such as phrases and other dependencies its strength part of speech language etc in order to provide analysis of the input text I assume this has been already done If so to avoid reinventing the wheel is there any efficient method to store and manage such data in some common format which has been already researched and tested
,,"<p>In general purpose for this kind of applications, One can use databases such as sqlite, mysql, mssql etc. It simplifies read / write operations, allows for a common language to interact with different databases from different vendors and platforms. </p>
",,0,2016-08-03T12:46:08.687,,213,2016-08-03T12:46:08.687,,,,,87.0,210.0,2,0,,,,43.73,14.79,9.98,0.0,0.0,7.0,In general purpose for this kind of applications One can use databases such as sqlite mysql mssql etc It simplifies read write operations allows for a common language to interact with different databases from different vendors and platforms
,0.0,"<p>Which objective and measurable tests have been developed to test the intelligence of AI? </p>

<p>The classical test is the Turing Test, which has objective criteria and is measurable since it can be measured what percentage of the jury is fooled by the AI.</p>

<p>I am looking for other, more modern tests. </p>
",,2,2016-08-03T13:01:20.740,,214,2016-08-03T13:01:20.740,,,,,29.0,,1,1,<intelligence-testing>,Which objective tests have been developed to test the intelligence of AI?,25.0,54.22,9.86,8.81,0.0,0.0,5.0,Which objective and measurable tests have been developed to test the intelligence of AI The classical test is the Turing Test which has objective criteria and is measurable since it can be measured what percentage of the jury is fooled by the AI I am looking for other more modern tests
,,"<p>One option is <a href=""https://www.neuroml.org/"" rel=""nofollow"">NeuroML</a>, one of the goals of which is:</p>

<blockquote>
  <p>To facilitate the exchange of complex neuronal models between researchers, allowing for greater transparency and accessibility of models</p>
</blockquote>

<p>In general, the matrices associated with large neural network models are likely to be sparse. Hence a 'homebrew' alternative to the above would be to use something like the <a href=""http://people.sc.fsu.edu/~jburkardt/data/mm/mm.html"" rel=""nofollow"">Matrix Market format</a>.</p>
",,0,2016-08-03T13:02:40.337,,215,2016-08-03T13:02:40.337,,,,,42.0,210.0,2,1,,,,23.6,14.11,11.88,0.0,0.0,8.0,One option is NeuroML one of the goals of which is To facilitate the exchange of complex neuronal models between researchers allowing for greater transparency and accessibility of models In general the matrices associated with large neural network models are likely to be sparse Hence a homebrew alternative to the above would be to use something like the Matrix Market format
,,"<p>Over the last few years, evolutionary computation research has shown increasing interest in including some aspect of epigenetics. For example:</p>

<ul>
<li>A 2008 paper by <a href=""http://www.sciencedirect.com/science/article/pii/S0020025508002880"" rel=""nofollow"">Tanev and Yuta</a></li>
<li>Work from <a href=""http://faculty.hampshire.edu/lspector/pubs/Epigenetics_2015_GECCO_final.pdf"" rel=""nofollow"">Lee Spector's genetic programming group</a></li>
<li>A recent paper by <a href=""http://link.springer.com/chapter/10.1007/978-3-319-30668-1_9"" rel=""nofollow"">Ricalde and Banzhaf</a></li>
</ul>
",,0,2016-08-03T13:35:29.993,,216,2016-08-04T16:42:49.973,2016-08-04T16:42:49.973,,42.0,,42.0,68.0,2,5,,,,42.21,13.98,11.97,0.0,0.0,4.0,Over the last few years evolutionary computation research has shown increasing interest in including some aspect of epigenetics For example A 2008 paper by Tanev and Yuta Work from Lee Spectors genetic programming group A recent paper by Ricalde and Banzhaf
,,"<p>The ability to re-frame summarization as a problem for ANN is rather dependent on what kind of output you're looking for: you mentioned 'salient parts of the text'.</p>

<p>One possibly is to use a deep learning approach that first chunks together words that belong in the same phrase as a single 'feature'.</p>

<p>Another possibility is to identify both key words and relations between them. Here is some previous work on using neural nets for <a href=""https://lirias.kuleuven.be/bitstream/123456789/131932/1/41238.pdf"" rel=""nofollow"">relational learning</a>.</p>
",,2,2016-08-03T14:03:23.977,,217,2016-08-03T14:03:23.977,,,,,42.0,207.0,2,1,,,,60.65,11.31,10.19,0.0,0.0,11.0,The ability to reframe summarization as a problem for ANN is rather dependent on what kind of output youre looking for you mentioned salient parts of the text One possibly is to use a deep learning approach that first chunks together words that belong in the same phrase as a single feature Another possibility is to identify both key words and relations between them Here is some previous work on using neural nets for relational learning
1745.0,2.0,"<p>I'm interested in implementing a program for natural language processing (aka <a href=""https://en.wikipedia.org/wiki/ELIZA"" rel=""nofollow"">ELIZA</a>).</p>

<p>Assuming that I'm already <a href=""http://ai.stackexchange.com/q/212/8"">storing semantic-lexical connections</a> between the words and its strength.</p>

<p>What are the methods of dealing with words which have very distinct meaning?</p>

<p>Few examples:</p>

<ul>
<li><p>'Are we on the same page?'</p>

<p>The 'page' in this context isn't a document page, but it's part of the phrase.</p></li>
<li><p>'I'm living in Reading.'</p>

<p>The 'Reading' is a city (noun), so it's not a verb. Otherwise it doesn't make any sense. Checking for the capital letter would work in that specific example, but it won't work for other (like 'make' can be either verb or noun).</p></li>
<li><p>'I've read something on the Facebook wall, do you want to know what?'</p>

<p>The 'Facebook wall' has nothing to do with wall at all.</p></li>
</ul>

<p>In general, how algorithm should distinguish the word meaning and recognise the word within the context?</p>

<p>For example:</p>

<ul>
<li>Detecting the word for different type of speech, so it should recognise whether it's a verb or noun.</li>
<li>Detecting whether the word is part of phrase.</li>
<li>Detecting word for multiple meaning.</li>
</ul>

<p>What are the possible approaches to solve that problem in order to  identify the correct sense of a word with the context?</p>
",,1,2016-08-03T14:17:17.257,2.0,218,2016-10-08T00:11:37.960,2016-08-26T16:24:42.217,,33.0,,8.0,,1,4,<nlp><lexical-recognition>,How to resolve lexical ambiguity in natural language processing?,118.0,75.61,10.08,8.8,0.0,0.0,55.0,Im interested in implementing a program for natural language processing aka ELIZA Assuming that Im already storing semanticlexical connections between the words and its strength What are the methods of dealing with words which have very distinct meaning Few examples Are we on the same page The page in this context isnt a document page but its part of the phrase Im living in Reading The Reading is a city noun so its not a verb Otherwise it doesnt make any sense Checking for the capital letter would work in that specific example but it wont work for other like make can be either verb or noun Ive read something on the Facebook wall do you want to know what The Facebook wall has nothing to do with wall at all In general how algorithm should distinguish the word meaning and recognise the word within the context For example Detecting the word for different type of speech so it should recognise whether its a verb or noun Detecting whether the word is part of phrase Detecting word for multiple meaning What are the possible approaches to solve that problem in order to identify the correct sense of a word with the context
,,"<p>In-between your input and desired output, there's obviously a huge space to search. The more relevant domain information you include as features, the higher chance that the Deep Learning (DL) algorithm can find the desired mapping.</p>

<p>At this early stage in DL research, there aren't so many rules of thumb to tell you what features to explicitly encode - not least because it depends on the size of your training corpus. My suggestion would be: obtain (or generate) a large corpus of C code, train on that with the most naive feature representation that you think might work, then repeatedly gather data and add more feature preprocessing as necessary.</p>

<p>This following paper describes a DL approach to what is almost the 'reverse problem' to yours - <a href=""http://arxiv.org/pdf/1510.07211.pdf"" rel=""nofollow"">generating the source code for a program described in natural language</a>.</p>

<p>I found the strength of the results reported in this paper surprising, but it does give me some hope that what you are asking might be possible.</p>
",,0,2016-08-03T14:19:39.400,,219,2016-08-03T14:19:39.400,,,,,42.0,205.0,2,2,,,,52.53,11.67,10.34,0.0,0.0,24.0,Inbetween your input and desired output theres obviously a huge space to search The more relevant domain information you include as features the higher chance that the Deep Learning DL algorithm can find the desired mapping At this early stage in DL research there arent so many rules of thumb to tell you what features to explicitly encode not least because it depends on the size of your training corpus My suggestion would be obtain or generate a large corpus of C code train on that with the most naive feature representation that you think might work then repeatedly gather data and add more feature preprocessing as necessary This following paper describes a DL approach to what is almost the reverse problem to yours generating the source code for a program described in natural language I found the strength of the results reported in this paper surprising but it does give me some hope that what you are asking might be possible
223.0,1.0,"<p>Unsupervised learning does not involve target values, so basically targets are most likely the same as the inputs (in other words, involves no target values).</p>

<p>So how does this model learn?</p>
",,3,2016-08-03T14:23:50.760,,220,2016-10-14T16:54:29.477,2016-08-03T14:43:33.990,,8.0,,8.0,,1,1,<models><unsupervised-learning>,How does unsupervised learning model learn?,51.0,55.74,12.0,11.03,0.0,0.0,6.0,Unsupervised learning does not involve target values so basically targets are most likely the same as the inputs in other words involves no target values So how does this model learn
,0.0,"<p>Currently, many different organizations do cutting-edge AI research, and some innovations are shared freely (at a time lag) while others are kept private. I'm referring to this state of affairs as 'multipolar,' where instead of there being one world leader that's far ahead of everyone else, there are many competitors who can be mentioned in the same breath. (There's not only one academic center of AI research worth mentioning, there might be particularly hot companies but there's not only one worth mentioning, and so on.)</p>

<p>But we could imagine instead there being one institution that mattered when it comes to AI (be it a company, a university, a research group, or a non-profit). This is what I'm referring to as ""monolithic."" Maybe they have access to tools and resources no one else has access to, maybe they attract the best and brightest in a way that gives them an unsurmountable competitive edge, maybe returns to research compound in a way that means early edges can't be overcome, maybe they have some sort of government coercion preventing competitors from popping up. (For other industries, network or first-mover effects might be other good examples of why you would expect that industry to be monolithic instead of multipolar.)</p>

<p>It seems like we should be able to use insights from social sciences like economics or organizational design or history of science in order to figure out, if not which path seems more likely, <em>how we would know</em> which path seems more likely.</p>

<p>(For example, we may be able to measure how much returns to research compound, in the sense of one organization coming up with an insight meaning that organization is likely to come up with the next relevant insight, and knowing this number makes it easier to figure out where the boundary between the two trajectories is located.)</p>
",,1,2016-08-03T14:32:39.333,,221,2016-08-03T14:32:39.333,,,,,10.0,,1,2,<research><ai-community>,"How would we know if AI development will continue to be multipolar, or will become monolithic?",51.0,37.07,12.08,10.18,0.0,0.0,50.0,Currently many different organizations do cuttingedge AI research and some innovations are shared freely at a time lag while others are kept private Im referring to this state of affairs as multipolar where instead of there being one world leader thats far ahead of everyone else there are many competitors who can be mentioned in the same breath Theres not only one academic center of AI research worth mentioning there might be particularly hot companies but theres not only one worth mentioning and so on But we could imagine instead there being one institution that mattered when it comes to AI be it a company a university a research group or a nonprofit This is what Im referring to as monolithic Maybe they have access to tools and resources no one else has access to maybe they attract the best and brightest in a way that gives them an unsurmountable competitive edge maybe returns to research compound in a way that means early edges cant be overcome maybe they have some sort of government coercion preventing competitors from popping up For other industries network or firstmover effects might be other good examples of why you would expect that industry to be monolithic instead of multipolar It seems like we should be able to use insights from social sciences like economics or organizational design or history of science in order to figure out if not which path seems more likely how we would know which path seems more likely For example we may be able to measure how much returns to research compound in the sense of one organization coming up with an insight meaning that organization is likely to come up with the next relevant insight and knowing this number makes it easier to figure out where the boundary between the two trajectories is located
,,"<p>'Backprop' is short for 'backpropagation of error' in order to avoid confusion when using <em>backpropagation</em> term.</p>

<p>Basically <em>backpropagation</em> refers to the method for computing the gradient of the case-wise error function with respect to the weights for a feedforward network<sup>Werbos</sup>. And <em>backprop</em> refers to a training method that uses backpropagation to compute the gradient.</p>

<p>So we can say that a <em>backprop</em> network is a feedforward network trained by <em>backpropagation</em>.</p>

<p>The 'standard backprop' term is a euphemism for the <em>generalized delta rule</em> which is most widely used supervised training method.</p>

<p>Source: <a href=""ftp://ftp.sas.com/pub/neural/FAQ2.html#A_backprop"" rel=""nofollow"">What is backprop?</a> at FAQ of Usenet newsgroup comp.ai.neural-nets</p>

<p>References:</p>

<ul>
<li>Werbos, P. J. (1974). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard University.</li>
<li>Werbos, P. J. (1994). The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting,Wiley Interscience.</li>
<li>Bertsekas, D. P. (1995), Nonlinear Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-14-0.</li>
<li>Bertsekas, D. P. and Tsitsiklis, J. N. (1996), Neuro-Dynamic Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-10-8.</li>
<li>Polyak, B.T. (1964), ""Some methods of speeding up the convergence of iteration methods,"" Z. Vycisl. Mat. i Mat. Fiz., 4, 1-17.</li>
<li>Polyak, B.T. (1987), Introduction to Optimization, NY: Optimization Software, Inc.</li>
<li>Reed, R.D., and Marks, R.J, II (1999), Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, Cambridge, MA: The MIT Press, ISBN 0-262-18190-8.</li>
<li>Rumelhart, D.E., Hinton, G.E., and Williams, R.J. (1986), ""Learning internal representations by error propagation"", in Rumelhart, D.E. and McClelland, J. L., eds. (1986), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1, 318-362, Cambridge, MA: The MIT Press.</li>
<li>Werbos, P.J. (1974/1994), The Roots of Backpropagation, NY: John Wiley &amp; Sons. Includes Werbos's 1974 Harvard Ph.D. thesis, Beyond Regression.</li>
</ul>
",,0,2016-08-03T14:39:02.827,,222,2016-08-03T14:39:02.827,,,,,8.0,1.0,2,1,,,,60.11,17.38,10.11,0.0,0.0,169.0,Backprop is short for backpropagation of error in order to avoid confusion when using backpropagation term Basically backpropagation refers to the method for computing the gradient of the casewise error function with respect to the weights for a feedforward networkWerbos And backprop refers to a training method that uses backpropagation to compute the gradient So we can say that a backprop network is a feedforward network trained by backpropagation The standard backprop term is a euphemism for the generalized delta rule which is most widely used supervised training method Source What is backprop at FAQ of Usenet newsgroup compaineuralnets References Werbos P J 1974 Beyond Regression New Tools for Prediction and Analysis in the Behavioral Sciences PhD thesis Harvard University Werbos P J 1994 The Roots of Backpropagation From Ordered Derivatives to Neural Networks and Political ForecastingWiley Interscience Bertsekas D P 1995 Nonlinear Programming Belmont MA Athena Scientific ISBN 1886529140 Bertsekas D P and Tsitsiklis J N 1996 NeuroDynamic Programming Belmont MA Athena Scientific ISBN 1886529108 Polyak BT 1964 Some methods of speeding up the convergence of iteration methods Z Vycisl Mat i Mat Fiz 4 117 Polyak BT 1987 Introduction to Optimization NY Optimization Software Inc Reed RD and Marks RJ II 1999 Neural Smithing Supervised Learning in Feedforward Artificial Neural Networks Cambridge MA The MIT Press ISBN 0262181908 Rumelhart DE Hinton GE and Williams RJ 1986 Learning internal representations by error propagation in Rumelhart DE and McClelland J L eds 1986 Parallel Distributed Processing Explorations in the Microstructure of Cognition Volume 1 318362 Cambridge MA The MIT Press Werbos PJ 19741994 The Roots of Backpropagation NY John Wiley amp Sons Includes Werboss 1974 Harvard PhD thesis Beyond Regression
,,"<p>Supervised learning is typically an attempt to learn a mathematical function, <em>f(X)=y</em>. For this, you need both the input vector <em>X</em> and the output vector <em>y</em>. The model outputs have whatever type / dimensionality / etc. that the target values have.</p>

<blockquote>
  <p>basically targets are most likely the same as the inputs.</p>
</blockquote>

<p>This doesn't seem right to me.</p>

<p>Unsupervised learning models instead learn a <em>structure</em> from the data. A clustering model, for example, is learning both how many clusters exist in the data (a number that's not the same type as the inputs) and where those clusters are located (which is also a different type from the inputs). The output of running this model on a new datapoint <em>x</em> is not the same type as <em>x</em>, but instead a classification label.</p>

<p>Similarly, time series models learn parameters that symbolize how vectors in the input relate to each other, rather than raw inputs themselves.</p>

<p>As for how they learn, the structures are mathematical objects whose fitness is determined by the input data. The simplest possible unstructured unsupervised learning problem is probably ""what's the mean of the data?"", and it should be clear how that's 'learned' through processing the input. More sophisticated models are just adding more pieces to that calculation.</p>
",,0,2016-08-03T14:41:02.937,,223,2016-10-14T16:54:29.477,2016-10-14T16:54:29.477,,10.0,,10.0,220.0,2,5,,,,56.55,11.59,9.65,0.0,0.0,40.0,Supervised learning is typically an attempt to learn a mathematical function fXy For this you need both the input vector X and the output vector y The model outputs have whatever type dimensionality etc that the target values have basically targets are most likely the same as the inputs This doesnt seem right to me Unsupervised learning models instead learn a structure from the data A clustering model for example is learning both how many clusters exist in the data a number thats not the same type as the inputs and where those clusters are located which is also a different type from the inputs The output of running this model on a new datapoint x is not the same type as x but instead a classification label Similarly time series models learn parameters that symbolize how vectors in the input relate to each other rather than raw inputs themselves As for how they learn the structures are mathematical objects whose fitness is determined by the input data The simplest possible unstructured unsupervised learning problem is probably whats the mean of the data and it should be clear how thats learned through processing the input More sophisticated models are just adding more pieces to that calculation
,1.0,"<p>One of the most compelling applications for AI would be in augmenting human biological intelligence. What are some of the currently proposed methods for doing this aside from vague notions such as ""nanobots swimming around our brains and bodies"" or ""electrodes connected to our skulls""?</p>
",,3,2016-08-03T14:58:03.663,,224,2016-08-03T16:12:18.483,,,,,148.0,,1,4,<cyborg>,How could AI be used to augment human biological intelligence?,44.0,48.64,14.22,10.72,0.0,0.0,6.0,One of the most compelling applications for AI would be in augmenting human biological intelligence What are some of the currently proposed methods for doing this aside from vague notions such as nanobots swimming around our brains and bodies or electrodes connected to our skulls
228.0,2.0,"<p>Given list of fixed numbers from a mathematical constant such as Pi, is it is possible to train AI to attempt to predict the next numbers?</p>

<p>Which AI or neural network would be more suitable for this task? </p>

<p>Especially the one which will work without memorizing the entire training set, but the one which will attempt to find some patterns or statistical association.</p>
",2016-08-03T18:21:18.760,4,2016-08-03T15:25:50.843,,225,2016-08-09T10:02:46.153,2016-08-09T10:02:46.153,,8.0,,8.0,,1,-6,<training><math><recurrent-neural-networks>,What are the approaches to predict sequence of π numbers?,74.0,58.62,10.44,9.69,0.0,0.0,5.0,Given list of fixed numbers from a mathematical constant such as Pi is it is possible to train AI to attempt to predict the next numbers Which AI or neural network would be more suitable for this task Especially the one which will work without memorizing the entire training set but the one which will attempt to find some patterns or statistical association
,,"<p>'Direct augmentation' of human intelligence, of the sort that you would see in science fiction, looks to be very hard. Most of our promising approaches deal with <em>avoiding damage</em> rather than <em>adding capabilities</em>--there's no drug that you can take now that will make you smarter to the degree that missing a night of sleep can make you dumber.</p>

<p>The most informative area of current practice is probably game-playing, where '<a href=""http://bloomreach.com/2014/12/centaur-chess-brings-best-humans-machines/"">centaurs</a>,' or humans working with computers, outcompete human players or computer players.</p>

<p>But a centaur player doesn't have a wire jutting out of their skull to jack into the computer; they're looking at a laptop screen. One of the reasons to be pessimistic about cyborg augmentation is because current I/O technology is already so good. Why install a new wire to put information into your visual cortex, when you come already equipped with two? </p>

<p>If you could think code directly onto the screen, how much better would that be than typing code through a keyboard? Probably some, but I find it difficult to imagine that it'll be more than twice as good. So most human-computer intelligence augmentation will look like people using software, and software using human inputs, rather than humans and computers evolving together.</p>

<hr>

<p>Transcranial Direct Current Stimulation (TDCS) and similar approaches cause temporary changes in mental abilities by raising or lowering the activation potentials of neurons in particular regions of the brain. (I've done it myself, a few years ago, and what weak effects I noticed were probably negative. Not too much surprise for a DIY setup!)</p>

<p>It looks like it has a number of useful implications. One article about TDCS that I found particularly striking was the journalist who tried it gushing about how their anxiety disappeared for a few days, presumably because the part of their brain behind the anxiety was dampened. One could imagine it being useful for the treatment of many different mental disorders.</p>

<p>That said, I'm pessimistic that it will translate into superior <em>peak</em> performance, and I think that's the sort of thing that's more relevant for discussions of augmentation. (Is there TDCS that we could do that would make Terrence Tao better at doing mathematics?)</p>

<p>Where improved AI methods will come into play is by improving our models of the brain, allowing us to better target interventions, much in the way that AI methods are improving our treatment of cancer (through superior diagnosis and targeting of radiotherapy, as two easy examples). These effects will all be indirect--for example, AI empowering an app or gadget that helps you sleep better won't <em>directly</em> augment your intelligence, but will cause population-level increases in effective intelligence through reducing sleep deprivation.</p>

<p>I haven't talked yet about nootropics, chemicals that increase intelligence, but it's reasonable to expect that AI will improve drug discovery there like it improves drug discovery for anything else. But the same caveats apply--the effect of nootropics seem to be negatively correlated with intelligence (that is, the smarter someone already is, the harder it is to increase their intelligence further).</p>
",,5,2016-08-03T15:27:15.343,,226,2016-08-03T16:12:18.483,2016-08-03T16:12:18.483,,10.0,,10.0,224.0,2,5,,,,47.12,13.64,9.71,0.0,0.0,81.0,Direct augmentation of human intelligence of the sort that you would see in science fiction looks to be very hard Most of our promising approaches deal with avoiding damage rather than adding capabilitiestheres no drug that you can take now that will make you smarter to the degree that missing a night of sleep can make you dumber The most informative area of current practice is probably gameplaying where centaurs or humans working with computers outcompete human players or computer players But a centaur player doesnt have a wire jutting out of their skull to jack into the computer theyre looking at a laptop screen One of the reasons to be pessimistic about cyborg augmentation is because current IO technology is already so good Why install a new wire to put information into your visual cortex when you come already equipped with two If you could think code directly onto the screen how much better would that be than typing code through a keyboard Probably some but I find it difficult to imagine that itll be more than twice as good So most humancomputer intelligence augmentation will look like people using software and software using human inputs rather than humans and computers evolving together Transcranial Direct Current Stimulation TDCS and similar approaches cause temporary changes in mental abilities by raising or lowering the activation potentials of neurons in particular regions of the brain Ive done it myself a few years ago and what weak effects I noticed were probably negative Not too much surprise for a DIY setup It looks like it has a number of useful implications One article about TDCS that I found particularly striking was the journalist who tried it gushing about how their anxiety disappeared for a few days presumably because the part of their brain behind the anxiety was dampened One could imagine it being useful for the treatment of many different mental disorders That said Im pessimistic that it will translate into superior peak performance and I think thats the sort of thing thats more relevant for discussions of augmentation Is there TDCS that we could do that would make Terrence Tao better at doing mathematics Where improved AI methods will come into play is by improving our models of the brain allowing us to better target interventions much in the way that AI methods are improving our treatment of cancer through superior diagnosis and targeting of radiotherapy as two easy examples These effects will all be indirectfor example AI empowering an app or gadget that helps you sleep better wont directly augment your intelligence but will cause populationlevel increases in effective intelligence through reducing sleep deprivation I havent talked yet about nootropics chemicals that increase intelligence but its reasonable to expect that AI will improve drug discovery there like it improves drug discovery for anything else But the same caveats applythe effect of nootropics seem to be negatively correlated with intelligence that is the smarter someone already is the harder it is to increase their intelligence further
1287.0,2.0,"<p>What are the main differences between two types of feedforward networks such as <em>multilayer perceptrons</em> (MLP) and <em>radial basis function</em> (RBF)?</p>

<p>What are the fundamental differences between these two types?</p>
",,3,2016-08-03T15:32:46.710,,227,2016-08-04T13:56:49.603,2016-08-03T20:20:16.283,,249.0,,8.0,,1,6,<comparison><mlp>,What is the difference between MLP and RBF?,126.0,56.25,15.77,10.7,0.0,0.0,6.0,What are the main differences between two types of feedforward networks such as multilayer perceptrons MLP and radial basis function RBF What are the fundamental differences between these two types
,,"<p>Pseudo-random number generators are specifically defined to defeat any form of prediction via 'black box' observation. Certainly, some (e.g. linear congruential) have weaknesses, but you are unlikely to have any success in general in predicting the output of a modern RNG. For devices based on chaotic physical systems (e.g. most national lotteries), there is no realistic possibility of prediction.</p>

<p>""Patterns or statistical association"" is a much weaker criterion than 'prediction'. Some very recent work has applied topological data analysis to visualize patterns within the infamous Randu RNG.</p>
",,0,2016-08-03T15:32:55.467,,228,2016-08-03T15:52:47.397,2016-08-03T15:52:47.397,,42.0,,42.0,225.0,2,3,,,,33.51,15.82,12.6,0.0,0.0,23.0,Pseudorandom number generators are specifically defined to defeat any form of prediction via black box observation Certainly some eg linear congruential have weaknesses but you are unlikely to have any success in general in predicting the output of a modern RNG For devices based on chaotic physical systems eg most national lotteries there is no realistic possibility of prediction Patterns or statistical association is a much weaker criterion than prediction Some very recent work has applied topological data analysis to visualize patterns within the infamous Randu RNG
,,"<blockquote>
  <p>Multilayer Perceptron networks (MLP) have been applied to distinct
  areas, performing tasks such as function fitting and pattern
  recognition problems, by using the supervised training with an
  algorithm known as “error back propagation”. </p>
  
  <p>Radial basis function (RBF) networks have the advantages of an easy
  design (just three layer architecture), good generalization, and high
  tolerance of input noises and ability of online learning. From the
  point of generalization, RBF networks can respond well well to
  patterns that were not used for training.</p>
</blockquote>

<p><a href=""http://www.aidic.it/cet/13/32/230.pdf"" rel=""nofollow""><em>1.1, 1.2 of 230.pdf</em></a></p>
",,5,2016-08-03T15:39:35.443,,229,2016-08-04T13:56:49.603,2016-08-04T13:56:49.603,,30.0,,5.0,227.0,2,-4,,,,49.96,15.55,11.19,0.0,0.0,18.0,Multilayer Perceptron networks MLP have been applied to distinct areas performing tasks such as function fitting and pattern recognition problems by using the supervised training with an algorithm known as “error back propagation” Radial basis function RBF networks have the advantages of an easy design just three layer architecture good generalization and high tolerance of input noises and ability of online learning From the point of generalization RBF networks can respond well well to patterns that were not used for training 11 12 of 230pdf
,,"<p>You would probably have to pack recursive structures into finite-dimensional real vectors and there have been such attempts. The finite precision limits goes as far as the recursion can go.</p>

<p>The limitation of <em>feedforward</em> neural networks is restricted to finite input and output spaces, so <em>recurrent</em> may be more suitable for this task as in theory can process arbitrarily long strings of numbers, but it has much more practical difficulties than feedforward network.</p>

<p>These kind of methods are open to debate.</p>

<p>Source: <a href=""ftp://ftp.sas.com/pub/neural/FAQ.html"" rel=""nofollow"">SAS FAQ</a></p>

<p>References:</p>

<ul>
<li>Blair, 1997; Pollack, 1990; Chalmers, 1990; Chrisman, 1991; Plate, 1994; Hammerton, 1998; Hadley, 1999</li>
</ul>
",,0,2016-08-03T15:42:13.340,,230,2016-08-03T15:50:40.390,2016-08-03T15:50:40.390,,8.0,,8.0,225.0,2,-1,,,,59.84,14.27,10.52,0.0,0.0,22.0,You would probably have to pack recursive structures into finitedimensional real vectors and there have been such attempts The finite precision limits goes as far as the recursion can go The limitation of feedforward neural networks is restricted to finite input and output spaces so recurrent may be more suitable for this task as in theory can process arbitrarily long strings of numbers but it has much more practical difficulties than feedforward network These kind of methods are open to debate Source SAS FAQ References Blair 1997 Pollack 1990 Chalmers 1990 Chrisman 1991 Plate 1994 Hammerton 1998 Hadley 1999
,,"<p>The concept of 'survival instinct' probably falls in the category of what Marvin Minsky would call a 'suitcase word', i.e. it packages together a number of related phenomena into what at first appears to be a singular notion. </p>

<p>So it's quite possible that we can construct mechanisms that have the appearance of some kind of 'hard-coded' survival instinct, without that ever featuring as an explicit rule(s) in the design.</p>

<p>See the beautiful little book <a href=""https://mitpress.mit.edu/books/vehicles"" rel=""nofollow"">'Vehicles'</a> by the neuroanatomist Valentino Braitenberg for a compelling narrative of how such 'top down' concepts as 'survival instinct' might evolve 'from the bottom up'.</p>

<p>Also, trying to ensure that intelligent artefacts place too high a priority on their survival might easily lead to a <a href=""https://xkcd.com/1613/"" rel=""nofollow"">Killbot Hellscape</a>.</p>
",,0,2016-08-03T15:45:41.770,,232,2016-08-03T15:45:41.770,,,,,42.0,197.0,2,2,,,,46.91,13.58,11.23,0.0,0.0,27.0,The concept of survival instinct probably falls in the category of what Marvin Minsky would call a suitcase word ie it packages together a number of related phenomena into what at first appears to be a singular notion So its quite possible that we can construct mechanisms that have the appearance of some kind of hardcoded survival instinct without that ever featuring as an explicit rules in the design See the beautiful little book Vehicles by the neuroanatomist Valentino Braitenberg for a compelling narrative of how such top down concepts as survival instinct might evolve from the bottom up Also trying to ensure that intelligent artefacts place too high a priority on their survival might easily lead to a Killbot Hellscape
236.0,3.0,"<p>According to my knowledge most of the current artificial intelligence study uses of some kind of neural network or its variants. A good example would be DeepMind's alphago which I believe is a deep neural network, for vision CNN, text, music and other ordered features RNN's, etc. But for machine learning application we have neural networks, support vector machines, random forest, regression methods, etc. available for applications. </p>

<p>So are neural networks and its variants the only way to reach ""true"" artificial intelligence? </p>
",,2,2016-08-03T15:56:18.480,1.0,233,2016-08-10T12:36:26.970,2016-08-10T12:36:26.970,,42.0,,39.0,,1,7,<neural-networks>,Are neural networks and its variants the only way to reach true artificial intelligence?,163.0,54.83,14.09,11.57,0.0,0.0,17.0,According to my knowledge most of the current artificial intelligence study uses of some kind of neural network or its variants A good example would be DeepMinds alphago which I believe is a deep neural network for vision CNN text music and other ordered features RNNs etc But for machine learning application we have neural networks support vector machines random forest regression methods etc available for applications So are neural networks and its variants the only way to reach true artificial intelligence
,,"<p>Steve Omohudro wrote a paper called <a href=""https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf"" rel=""nofollow"">Basic AI Drives</a> that steps through why we would expect an AI with narrow goals to find some basic, general concepts as instrumentally useful for their narrow goals. </p>

<p>For example, an AI designed to maximize stock market returns but whose design is silent on the importance of continuing to survive would realize that its continued survival is a key component of maximizing stock market returns, and thus take actions to keep itself operational.</p>

<p>In general, we should be skeptical of 'anthropomorphizing' AI and other code, but it seems like there <em>are</em> reasons to expect this beyond ""well, humans behave this way, so it must be how all intelligence behaves.""</p>
",,0,2016-08-03T15:58:13.493,,234,2016-08-03T15:58:13.493,,,,,10.0,197.0,2,3,,,,32.6,12.37,11.03,0.0,0.0,14.0,Steve Omohudro wrote a paper called Basic AI Drives that steps through why we would expect an AI with narrow goals to find some basic general concepts as instrumentally useful for their narrow goals For example an AI designed to maximize stock market returns but whose design is silent on the importance of continuing to survive would realize that its continued survival is a key component of maximizing stock market returns and thus take actions to keep itself operational In general we should be skeptical of anthropomorphizing AI and other code but it seems like there are reasons to expect this beyond well humans behave this way so it must be how all intelligence behaves
,,"<p>I presume the proof the OP is referring to can be found in <a href=""http://link.springer.com/book/10.1007%2F978-1-4612-0707-8"" rel=""nofollow"">this monograph</a> by Hava Siegelmann?</p>

<p>In his article <a href=""http://www1.maths.leeds.ac.uk/~pmt6sbc/docs/davis.myth.pdf"" rel=""nofollow"">'The Myth of Hypercomputation'</a>, the eminent computer scientist Martin Davis explains (p8-9) that there is nothing 'super Turing' about this formulation.</p>

<p>EDIT: It's looking like the claim about <em>rational</em> weights being super-Turing is made in <a href=""http://www.ncbi.nlm.nih.gov/pubmed/25354762"" rel=""nofollow"">this</a>  more recent paper by Siegelmann, which introduces an additional assumption of <em>plasticity</em>, i.e. that weights can be dynamically updated.</p>
",,0,2016-08-03T16:05:28.357,,235,2016-09-03T17:38:06.577,2016-09-03T17:38:06.577,,42.0,,42.0,147.0,2,3,,,,52.19,14.33,11.02,0.0,0.0,18.0,I presume the proof the OP is referring to can be found in this monograph by Hava Siegelmann In his article The Myth of Hypercomputation the eminent computer scientist Martin Davis explains p89 that there is nothing super Turing about this formulation EDIT Its looking like the claim about rational weights being superTuring is made in this more recent paper by Siegelmann which introduces an additional assumption of plasticity ie that weights can be dynamically updated
,,"<p>If by true AI, you mean 'like human beings', the answer is - no-one knows what the appropriate computational mechanisms (neural or otherwise) are or indeed whether we are capable of constructing them.</p>

<p>What Artificial Neural Nets (ANNs) do is essentially 'nonlinear regression' - perhaps this is not a sufficiently strong model to express humanlike behaviour. </p>

<p>Despite the 'Universal function approximation' property of ANNs, what if human intelligence depends on some as-yet-unguessed mechanism of the physical world?</p>

<p>With respect to your question about ""the only way"":
Even if (physical) neural mechanisms somehow actually were the <em>only</em> route to intelligence (e.g. via Penrose's quantum microtubules), how could that be proved? </p>

<p>Even in the formal world of mathematics, there's a saying that ""Proofs of non-existence are hard"". It scarcely seems conceivable that, in the physical world, it would be possible to demonstrate that intelligence could not arise by any other mechanism.</p>

<p>Moving back to computational systems, note that Stephen Wolfram made the interesting observation in his book <a href=""http://www.wolframscience.com/nksonline/toc.html"">'A New Kind of Science'</a> that many of the apparently distinct mechanisms he observed seem to be capable of 'Universal Computation', so in that sense there's nothing very particular about ANNs.</p>
",,0,2016-08-03T16:21:42.693,,236,2016-08-03T16:21:42.693,,,,,42.0,233.0,2,8,,,,38.45,15.44,10.62,0.0,0.0,50.0,If by true AI you mean like human beings the answer is noone knows what the appropriate computational mechanisms neural or otherwise are or indeed whether we are capable of constructing them What Artificial Neural Nets ANNs do is essentially nonlinear regression perhaps this is not a sufficiently strong model to express humanlike behaviour Despite the Universal function approximation property of ANNs what if human intelligence depends on some asyetunguessed mechanism of the physical world With respect to your question about the only way Even if physical neural mechanisms somehow actually were the only route to intelligence eg via Penroses quantum microtubules how could that be proved Even in the formal world of mathematics theres a saying that Proofs of nonexistence are hard It scarcely seems conceivable that in the physical world it would be possible to demonstrate that intelligence could not arise by any other mechanism Moving back to computational systems note that Stephen Wolfram made the interesting observation in his book A New Kind of Science that many of the apparently distinct mechanisms he observed seem to be capable of Universal Computation so in that sense theres nothing very particular about ANNs
253.0,1.0,"<p>I'm interested in hardware implementation of ANNs (artificial neural networks). Are there any popular existing technology implementations in form of microchips which are purpose designed to run artificial neural networks? For example, a chip which is optimised for an application like image recognition or something similar?</p>
",,4,2016-08-03T16:22:36.073,1.0,237,2016-08-06T01:22:37.030,2016-08-03T19:55:57.227,,33.0,,8.0,,1,3,<image-recognition><hardware>,Are there any microchips specifically designed to run ANNs?,49.0,22.11,17.33,12.29,0.0,0.0,7.0,Im interested in hardware implementation of ANNs artificial neural networks Are there any popular existing technology implementations in form of microchips which are purpose designed to run artificial neural networks For example a chip which is optimised for an application like image recognition or something similar
,,"<p>As far as I can see, there's no reason why you couldn't (for example) take the convolutional inputs to deepdream from adjacent sample points, rather than adjacent spatial positions, as is the case with image input.</p>

<p>Given the 'self similar' nature of deep dream images, listening to this <a href=""https://vimeo.com/13541969"" rel=""nofollow"">fractal granular synthesis</a> technique might be of interest/inspiration.</p>
",,0,2016-08-03T16:46:33.883,,238,2016-08-03T16:46:33.883,,,,,42.0,159.0,2,2,,,,43.06,14.28,11.51,0.0,0.0,13.0,As far as I can see theres no reason why you couldnt for example take the convolutional inputs to deepdream from adjacent sample points rather than adjacent spatial positions as is the case with image input Given the self similar nature of deep dream images listening to this fractal granular synthesis technique might be of interestinspiration
,,"<p>It depends on what you consider ""true artificial intelligence"". But this probably means to be able to think like a human - and perhaps, do so in a more rational manner, as in the human brain emotion comes before ratio.  </p>

<p>It would seem that a neural network, or a genetic algorithm that evolves neural networks, is the closest way - mimicking humans.  </p>

<p>However, the traditional counter-argument to this is that we tried to do the same with flight. We tried to copy nature, mimick the birds - trying to fly by flapping wings. But eventually we made airplanes that did not rely on flapping their wings.  </p>

<p>In AI, there are far more variables than in aerodynamics. So it is quite likely that a human-like intelligence can be attained by other methods than neural networks.</p>

<p>In the end, neural networks are one approach to machine learning. There are others, all governed by the rules for what can and cannot be learnt. (There is a field called Computational Learning Theory that covers this). </p>

<p>Although it is possible to extend learning systems beyond what can be learnt according to COLT, this means that such a learning system - neural network or otherwise - is essentially flawed, and will draw wrong conclusions at one point or another.</p>
",,0,2016-08-03T17:17:16.590,,239,2016-08-03T17:17:16.590,,,,,66.0,233.0,2,3,,,,62.38,10.73,9.66,0.0,0.0,34.0,It depends on what you consider true artificial intelligence But this probably means to be able to think like a human and perhaps do so in a more rational manner as in the human brain emotion comes before ratio It would seem that a neural network or a genetic algorithm that evolves neural networks is the closest way mimicking humans However the traditional counterargument to this is that we tried to do the same with flight We tried to copy nature mimick the birds trying to fly by flapping wings But eventually we made airplanes that did not rely on flapping their wings In AI there are far more variables than in aerodynamics So it is quite likely that a humanlike intelligence can be attained by other methods than neural networks In the end neural networks are one approach to machine learning There are others all governed by the rules for what can and cannot be learnt There is a field called Computational Learning Theory that covers this Although it is possible to extend learning systems beyond what can be learnt according to COLT this means that such a learning system neural network or otherwise is essentially flawed and will draw wrong conclusions at one point or another
243.0,5.0,"<p>I've noticed that a few questions on this site mention genetic algorithms and it made me realize that I don't really know much about those.</p>

<p>I have heard the term before, but it's not something I've ever used, so I don't have much idea about how they work and what they are good for. All I know is that they involve some sort of evolution and randomly changing values.</p>

<p>Can you give me a short explanation, preferably including some sort of practical example that illustrates the basic principles?</p>
",,9,2016-08-03T17:22:05.433,3.0,240,2016-08-04T18:01:36.313,,,,,30.0,,1,8,<genetic-algorithms>,What exactly are genetic algorithms and what sort of problems are they good for?,119.0,57.61,9.63,9.39,0.0,0.0,12.0,Ive noticed that a few questions on this site mention genetic algorithms and it made me realize that I dont really know much about those I have heard the term before but its not something Ive ever used so I dont have much idea about how they work and what they are good for All I know is that they involve some sort of evolution and randomly changing values Can you give me a short explanation preferably including some sort of practical example that illustrates the basic principles
,3.0,"<p>In detective novels, the point is often that the reader gets enough information to solve the crime themselves. This ""puzzle"" aspect of detective novels is part of the attraction.</p>

<p>Often the difficulty for humans is to keep track of all the variables - events, items, motivations.<br>
An AI would have an easier time keeping track of all the details, but would rely on real-world knowledge to prevent making crazy mistakes. For example, if it was stated that a character took the train, the AI would need to know that this is a method of transportation - that it changes the location property of an agent over time.</p>

<p>Has an AI ever been able to solve a detective mystery?</p>
",,1,2016-08-03T17:24:18.480,,241,2016-08-04T13:40:05.177,,,,,66.0,,1,3,<natural-language><problem-solving><world-knowledge>,Has an AI ever solved a detective mystery?,58.0,60.35,10.1,10.17,0.0,0.0,17.0,In detective novels the point is often that the reader gets enough information to solve the crime themselves This puzzle aspect of detective novels is part of the attraction Often the difficulty for humans is to keep track of all the variables events items motivations An AI would have an easier time keeping track of all the details but would rely on realworld knowledge to prevent making crazy mistakes For example if it was stated that a character took the train the AI would need to know that this is a method of transportation that it changes the location property of an agent over time Has an AI ever been able to solve a detective mystery
,,"<p>A genetic algorithm is an algorithm that randomly generates a number of attempted solutions for a problem. This set of attempted solutions is called the ""population"".  </p>

<p>It then tries to see how well these solutions solve the problem, using a given <em>fitness function</em>. The attempted solutions with the best <em>fitness</em> value are used to generate a new population. This can be done by making small changes to the attempted solutions (mutation) or by combining existing attempted solutions (crossover).</p>

<p>The idea is that, over time, an attempted solution emerges that has a high enough <em>fitness</em> value to solve the problem.</p>

<p>The inspiration for this came from the theory of evolution; the fittest solutions survive and procreate.</p>

<p><strong>Example 1</strong></p>

<p>Suppose you were looking for the most efficient way to cut a number of shapes out of a piece of wood. You want to waste as little wood as possible.</p>

<p>Your attempted solutions would be random arrangements of these shapes on your piece of wood. <em>Fitness</em> would be determined by how little wood would be left after cutting the shapes following this arrangement.<br>
The less wood is left, the better the attempted solution. </p>

<p><strong>Example 2</strong></p>

<p>Suppose you were trying to find a polynomial that passes through a number of points. Your attempted solutions would be random polynomials.<br>
To determine the <em>fitness</em> of these polynomials, you determine how well they fit the given points. (In this particular case, you would probably use the least squares method to determine how well the polynomial fit the points).
Over a number of trials, you would get polynomials that fit the points better, until you had a polynomial that fit the points closely enough.</p>
",,2,2016-08-03T17:27:28.260,,242,2016-08-03T17:57:10.623,2016-08-03T17:57:10.623,,66.0,,66.0,240.0,2,5,,,,63.49,11.37,8.27,0.0,0.0,34.0,A genetic algorithm is an algorithm that randomly generates a number of attempted solutions for a problem This set of attempted solutions is called the population It then tries to see how well these solutions solve the problem using a given fitness function The attempted solutions with the best fitness value are used to generate a new population This can be done by making small changes to the attempted solutions mutation or by combining existing attempted solutions crossover The idea is that over time an attempted solution emerges that has a high enough fitness value to solve the problem The inspiration for this came from the theory of evolution the fittest solutions survive and procreate Example 1 Suppose you were looking for the most efficient way to cut a number of shapes out of a piece of wood You want to waste as little wood as possible Your attempted solutions would be random arrangements of these shapes on your piece of wood Fitness would be determined by how little wood would be left after cutting the shapes following this arrangement The less wood is left the better the attempted solution Example 2 Suppose you were trying to find a polynomial that passes through a number of points Your attempted solutions would be random polynomials To determine the fitness of these polynomials you determine how well they fit the given points In this particular case you would probably use the least squares method to determine how well the polynomial fit the points Over a number of trials you would get polynomials that fit the points better until you had a polynomial that fit the points closely enough
,,"<p>Evolutionary algorithms are a family of optimization algorithms based on the principle of <strong>Darwinian natural selection</strong>. As part of natural selection, a given environment has a population of individuals that compete for survival and reproduction. The ability of each individual to achieve these goals determines their chance to have children, in other words to pass on their genes to the next generation of individuals, who for genetic reasons will have an increased chance of doing well, even better, in realizing these two objectives. </p>

<p>This principle of continuous improvement over the generations is taken by evolutionary algorithms to optimize solutions to a problem. In the <strong>initial generation</strong>, a population composed of different <strong>individuals</strong> is generated randomly or by other methods. An individual is a solution to the problem, more or less good: the quality of the individual in regards to the problem is called <strong>fitness</strong>, which reflects the adequacy of the solution to the problem to be solved. The higher the fitness of an individual, the higher it is likely to pass some or all of its genotype to the individuals of the next generation.</p>

<p>An individual is coded as a <strong>genotype</strong>, which can have any shape, such as a** bit vector (<strong>genetic algorithms</strong>) or a vector of real (evolution strategies). Each genotype is transformed into a <strong>phenotype</strong> when assessing the individual, i.e. when its fitness is calculated. In some cases, the phenotype is identical to the genotype: it is called <strong>direct</strong> <strong>coding</strong>. Otherwise, the coding is called indirect. For example, suppose you want to optimize the size of a rectangular parallelepiped defined by its length, height and width. To simplify the example, assume that these three quantities are integers between 0 and 15. We can then describe each of them using a 4-bit binary number. An example of a potential solution may be to genotype 0001 0111 01010. The corresponding phenotype is a parallelepiped of length 1, height 7 and width 10.</p>

<p>During the transition from the old to the new generation are called <strong>variation</strong> <strong>operators</strong>, whose purpose is to manipulate individuals. There are two distinct types of variation operators:</p>

<ul>
<li>the <strong>mutation</strong> <strong>operators</strong>, which are used to introduce variations within the same individual, as genetic mutations;</li>
<li>the <strong>crossover</strong> <strong>operators</strong>, which are used to cross at least two different genotypes, as genetic crosses from breeding.</li>
</ul>

<p>Evolutionary algorithms have proven themselves in various fields such as operations research, robotics, biology, nuance, or cryptography. In addition, they can optimize multiple objectives simultaneously and can be used as black boxes because they do not assume any properties in the mathematical model to optimize. Their only real limitation is the computational complexity.</p>

<p><a href=""https://i.stack.imgur.com/wweBO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wweBO.png"" alt=""enter image description here""></a></p>
",,0,2016-08-03T17:42:02.623,,243,2016-08-03T17:42:02.623,,,,,4.0,240.0,2,5,,,,34.36,13.58,9.86,0.0,0.0,62.0,Evolutionary algorithms are a family of optimization algorithms based on the principle of Darwinian natural selection As part of natural selection a given environment has a population of individuals that compete for survival and reproduction The ability of each individual to achieve these goals determines their chance to have children in other words to pass on their genes to the next generation of individuals who for genetic reasons will have an increased chance of doing well even better in realizing these two objectives This principle of continuous improvement over the generations is taken by evolutionary algorithms to optimize solutions to a problem In the initial generation a population composed of different individuals is generated randomly or by other methods An individual is a solution to the problem more or less good the quality of the individual in regards to the problem is called fitness which reflects the adequacy of the solution to the problem to be solved The higher the fitness of an individual the higher it is likely to pass some or all of its genotype to the individuals of the next generation An individual is coded as a genotype which can have any shape such as a bit vector genetic algorithms or a vector of real evolution strategies Each genotype is transformed into a phenotype when assessing the individual ie when its fitness is calculated In some cases the phenotype is identical to the genotype it is called direct coding Otherwise the coding is called indirect For example suppose you want to optimize the size of a rectangular parallelepiped defined by its length height and width To simplify the example assume that these three quantities are integers between 0 and 15 We can then describe each of them using a 4bit binary number An example of a potential solution may be to genotype 0001 0111 01010 The corresponding phenotype is a parallelepiped of length 1 height 7 and width 10 During the transition from the old to the new generation are called variation operators whose purpose is to manipulate individuals There are two distinct types of variation operators the mutation operators which are used to introduce variations within the same individual as genetic mutations the crossover operators which are used to cross at least two different genotypes as genetic crosses from breeding Evolutionary algorithms have proven themselves in various fields such as operations research robotics biology nuance or cryptography In addition they can optimize multiple objectives simultaneously and can be used as black boxes because they do not assume any properties in the mathematical model to optimize Their only real limitation is the computational complexity
,,"<p>As observed in another answer, all you need to apply Genetic Algorithms (GAs) is to represent a potential solution to your problem in a form that is subject to crossover and mutation. Ideally, the fitness function will provide some kind of smooth feedback about the quality of a solution, rather than simply being a 'Needle in a Haystack'.</p>

<p>Here are some characteristics of problems that Genetic Algorithms (and indeed <a href=""https://cs.gmu.edu/~sean/book/metaheuristics/"" rel=""nofollow"">Metaheuristics</a> in general) are good for:</p>

<ul>
<li>NP-complete - The number of possible solutions to the problem is
exponential, but checking the fitness of a solution is relatively
cheap (technically, with time polynomial in the input size). </li>
<li>Black box - GAs work reasonably well even if you don't have a particularly
informed model of the problem to be solved. This means that these
approaches are also useful as a 'rapid prototyping' approach to
solving problems.</li>
</ul>

<p>However, despite their widespread use for the purpose, note that GAs are actually <a href=""http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.161.5655"" rel=""nofollow""><em>not</em> function optimizers</a> - GA mechanisms tend not to explore 'outlying' regions of the search space in the hope of finding some distant high quality solution, but rather to cluster around more easily attainable peaks in the 'fitness landscape'.</p>

<p>More detail on the applicability of GAs is given in a famous early paper <a href=""http://download.springer.com/static/pdf/167/art%253A10.1007%252FBF00993046.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2FBF00993046&amp;token2=exp=1470247330~acl=%2Fstatic%2Fpdf%2F167%2Fart%25253A10.1007%25252FBF00993046.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1007%252FBF00993046*~hmac=5686f87439a520f9c56ff0bd34b38bacf7c442edb6ecaaf9b93a2c02d17815c2"" rel=""nofollow"">""What makes a problem hard for a Genetic Algorithm?""</a></p>
",,0,2016-08-03T17:43:29.303,,244,2016-08-03T17:43:29.303,,,,,42.0,240.0,2,2,,,,40.31,13.18,11.03,0.0,0.0,37.0,As observed in another answer all you need to apply Genetic Algorithms GAs is to represent a potential solution to your problem in a form that is subject to crossover and mutation Ideally the fitness function will provide some kind of smooth feedback about the quality of a solution rather than simply being a Needle in a Haystack Here are some characteristics of problems that Genetic Algorithms and indeed Metaheuristics in general are good for NPcomplete The number of possible solutions to the problem is exponential but checking the fitness of a solution is relatively cheap technically with time polynomial in the input size Black box GAs work reasonably well even if you dont have a particularly informed model of the problem to be solved This means that these approaches are also useful as a rapid prototyping approach to solving problems However despite their widespread use for the purpose note that GAs are actually not function optimizers GA mechanisms tend not to explore outlying regions of the search space in the hope of finding some distant high quality solution but rather to cluster around more easily attainable peaks in the fitness landscape More detail on the applicability of GAs is given in a famous early paper What makes a problem hard for a Genetic Algorithm
,,"<p>This answer requests a practical example of how one might be used, which I will attempt to provide in addition to the other answers. They seem to due a very good job of explaining what a genetic algorithm is. So, this will give an example.</p>

<p>Let's say you have a neural network (although they are not the only application of it), which, from some given inputs, will yield some outputs. A genetic algorithm can create a population of these, and by seeing which output is the best, breed and kill off members of the population. Eventually, this should optimise the neural network if it is complicated enough. </p>

<p>Here is a demonstration I've made, which despite being badly coded, might help you understand. <a href=""http://khrabanas.github.io/projects/evo/evo.html"" rel=""nofollow"">http://khrabanas.github.io/projects/evo/evo.html</a>
Hit the evolve button and mess around with the goals.</p>

<p>It uses a simple genetic algorithm to breed, mutate and decide between which of the population survive. Depending on how the input variables are set, the network will be able to get to some level of closeness to them.In this fashion, wthe population will likely eventually become a homogenenous group, whose outputs resemble the goals.</p>

<p>The genetic algorithm is trying to create a ""neural network"" of sorts, that by taking in RGB, will yield an output color. First it generates a random population. It then by taking 3 random members from the population, selecting the one with the lowest fitness and removing it from the population. The fitness is equal to the difference in the top goal squared + the difference in the bottom goal squared. It then breeds the two remaining ones together and adds the child to the same place in the population as the dead member.
When mating occurs, there is a chance a mutation will occur. This mutation will change one of the values randomly.</p>

<p>As a side note, due to how it is set up, it is impossible for it to be totally correct in many cases, though it will reach relative closeness.</p>
",,0,2016-08-03T17:58:32.980,,246,2016-08-03T18:05:13.723,2016-08-03T18:05:13.723,,244.0,,244.0,240.0,2,3,,,,62.27,10.44,8.95,0.0,0.0,56.0,This answer requests a practical example of how one might be used which I will attempt to provide in addition to the other answers They seem to due a very good job of explaining what a genetic algorithm is So this will give an example Lets say you have a neural network although they are not the only application of it which from some given inputs will yield some outputs A genetic algorithm can create a population of these and by seeing which output is the best breed and kill off members of the population Eventually this should optimise the neural network if it is complicated enough Here is a demonstration Ive made which despite being badly coded might help you understand httpkhrabanasgithubioprojectsevoevohtml Hit the evolve button and mess around with the goals It uses a simple genetic algorithm to breed mutate and decide between which of the population survive Depending on how the input variables are set the network will be able to get to some level of closeness to themIn this fashion wthe population will likely eventually become a homogenenous group whose outputs resemble the goals The genetic algorithm is trying to create a neural network of sorts that by taking in RGB will yield an output color First it generates a random population It then by taking 3 random members from the population selecting the one with the lowest fitness and removing it from the population The fitness is equal to the difference in the top goal squared the difference in the bottom goal squared It then breeds the two remaining ones together and adds the child to the same place in the population as the dead member When mating occurs there is a chance a mutation will occur This mutation will change one of the values randomly As a side note due to how it is set up it is impossible for it to be totally correct in many cases though it will reach relative closeness
,2.0,"<p>In 1969, Seymour Papert and Marvin Minsky showed that Perceptrons could not learn the XOR function.  </p>

<p>This was solved by the backpropagation network with at least one hidden layer. This type of network can learn the XOR function.</p>

<p>I believe I was once taught that every problem that could be learnt by a backpropagation neural network with multiple hidden layers, could also be learnt by a backpropagation neural network with a single hidden layer. (Although possible a nonlinear activation function was required).</p>

<p>However, it is unclear to me what the limits are to backpropagation neural networks themselves. Which patterns <strong>cannot</strong> be learnt by a backpropgation neural network?</p>
",,2,2016-08-03T18:05:24.997,1.0,247,2016-08-04T21:05:44.587,,,,,66.0,,1,5,<neural-networks><machine-learning><backpropagation><learning-theory>,What are the limits to what can be learnt using a backpropagation neural network?,71.0,64.41,12.98,9.27,0.0,0.0,12.0,In 1969 Seymour Papert and Marvin Minsky showed that Perceptrons could not learn the XOR function This was solved by the backpropagation network with at least one hidden layer This type of network can learn the XOR function I believe I was once taught that every problem that could be learnt by a backpropagation neural network with multiple hidden layers could also be learnt by a backpropagation neural network with a single hidden layer Although possible a nonlinear activation function was required However it is unclear to me what the limits are to backpropagation neural networks themselves Which patterns cannot be learnt by a backpropgation neural network
,1.0,"<p>Over the last 50 years, the rise/fall/rise in popularity of neural nets has acted as something of a 'barometer' for AI research.</p>

<p>It's clear from the questions on this site that people are interested in applying Deep Learning (DL) to a wide variety of difficult problems.</p>

<p>I therefore have two questions:</p>

<ol>
<li>Practitioners - What do you find to be the main obstacles to
applying DL 'out of the box' to your problem? </li>
<li>Researchers - What
techniques do you use (or have developed) that might help address
practical issues? Are they within DL or do they offer an
alternative approach?</li>
</ol>
",,2,2016-08-03T18:05:26.937,,248,2016-12-12T23:22:08.810,2016-08-04T14:09:54.380,,95.0,,42.0,,1,6,<deep-learning>,Issues with and alternatives to Deep Learning approaches?,235.0,60.24,11.14,9.65,0.0,0.0,20.0,Over the last 50 years the risefallrise in popularity of neural nets has acted as something of a barometer for AI research Its clear from the questions on this site that people are interested in applying Deep Learning DL to a wide variety of difficult problems I therefore have two questions Practitioners What do you find to be the main obstacles to applying DL out of the box to your problem Researchers What techniques do you use or have developed that might help address practical issues Are they within DL or do they offer an alternative approach
1286.0,1.0,"<p>Is it possible for <em>unsupervised learning</em> to learn about high-level, class-specific features given only unlabelled images? For example detecting human or animal faces? If so, how?</p>
",,2,2016-08-03T18:12:22.133,,249,2016-08-04T06:25:20.620,2016-08-04T00:31:01.093,,8.0,,8.0,,1,2,<image-recognition><unsupervised-learning>,Is it possible for 'unsupervised learning' model to recognize features on unlabelled images?,39.0,37.26,14.75,13.78,0.0,0.0,7.0,Is it possible for unsupervised learning to learn about highlevel classspecific features given only unlabelled images For example detecting human or animal faces If so how
,,"<p>First up, those images (even the first few) aren't complete trash despite being junk to humans; they're actually finely tuned with various advanced techniques, including another neural network.</p>

<blockquote>
  <p>The deep neural network is the pre-trained network modeled on AlexNet provided by <a href=""https://github.com/BVLC/caffe"">Caffe</a>. To evolve images, both the directly encoded and indirectly encoded images, we use the <a href=""https://github.com/jbmouret/sferes2"">Sferes</a> evolutionary framework. The entire code base to conduct the evolutionary experiments can be download [sic] <a href=""https://github.com/Evolving-AI-Lab/fooling"">here</a>. The code for the images produced by gradient ascent is available <a href=""https://github.com/Evolving-AI-Lab/fooling/tree/master/caffe/ascent"">here</a>.</p>
</blockquote>

<p>Images that are actually random junk were correctly recognized as nothing meaningful:</p>

<blockquote>
  <p>In response to an unrecognizable image, the networks could have output a low confidence for each of the 1000 classes, instead of an extremely high confidence value for one of the classes. In fact, they do just that for randomly generated images (e.g. those in generation 0 of the evolutionary run)</p>
</blockquote>

<p>The original goal of the researchers was to use the neural networks to automatically generate images that look like the real things (by getting the recognizer's feedback and trying to change the image to get a more confident result), but they ended up creating the above art. Notice how even in the static-like images there are little splotches - usually near the center - which, it's fair to say, are triggering the recognition.</p>

<blockquote>
  <p>We were not trying to produce adversarial, unrecognizable images. Instead, we were trying to produce recognizable images, but these unrecognizable images emerged.</p>
</blockquote>

<p>Evidently, these images had just the right distinguishing features to match what the AI looked for in pictures. The ""paddle"" image does have a paddle-like shape, the ""bagel"" is round and the right color, the ""projector"" image is a camera-lens-like thing, the ""computer keyboard"" is a bunch of rectangles (like the individual keys), and the ""chainlink fence"" legitimately looks like a chain-link fence to me.</p>

<blockquote>
  <p>Figure 8. Evolving images to match DNN classes produces a tremendous diversity of images. Shown are images selected to showcase diversity from 5 evolutionary runs. The diversity suggests that the images are non-random, but that instead evolutions producing [sic] discriminative features of each target class.</p>
</blockquote>

<p>Further reading: <a href=""http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf"">the original paper</a> (large PDF)</p>
",,0,2016-08-03T18:18:58.077,,250,2016-08-03T18:18:58.077,,,,,75.0,92.0,2,21,,,,33.14,14.16,9.93,0.0,0.0,77.0,First up those images even the first few arent complete trash despite being junk to humans theyre actually finely tuned with various advanced techniques including another neural network The deep neural network is the pretrained network modeled on AlexNet provided by Caffe To evolve images both the directly encoded and indirectly encoded images we use the Sferes evolutionary framework The entire code base to conduct the evolutionary experiments can be download sic here The code for the images produced by gradient ascent is available here Images that are actually random junk were correctly recognized as nothing meaningful In response to an unrecognizable image the networks could have output a low confidence for each of the 1000 classes instead of an extremely high confidence value for one of the classes In fact they do just that for randomly generated images eg those in generation 0 of the evolutionary run The original goal of the researchers was to use the neural networks to automatically generate images that look like the real things by getting the recognizers feedback and trying to change the image to get a more confident result but they ended up creating the above art Notice how even in the staticlike images there are little splotches usually near the center which its fair to say are triggering the recognition We were not trying to produce adversarial unrecognizable images Instead we were trying to produce recognizable images but these unrecognizable images emerged Evidently these images had just the right distinguishing features to match what the AI looked for in pictures The paddle image does have a paddlelike shape the bagel is round and the right color the projector image is a cameralenslike thing the computer keyboard is a bunch of rectangles like the individual keys and the chainlink fence legitimately looks like a chainlink fence to me Figure 8 Evolving images to match DNN classes produces a tremendous diversity of images Shown are images selected to showcase diversity from 5 evolutionary runs The diversity suggests that the images are nonrandom but that instead evolutions producing sic discriminative features of each target class Further reading the original paper large PDF
,,"<p>Fuzzy logic is an alternative to boolean logic.  </p>

<p>In boolean logic, every statement is either true or false. Usually ""1"" is used to represent ""true"" and ""0"" is used to represent ""false"".</p>

<p>Fuzzy logic is an extension of this concept, where statements do not have such absolute values. Instead, the truth value is a real number in the range [0..1]. </p>

<p>As such, fuzzy logic is a real-number version of multi-valued logic.</p>
",,0,2016-08-03T18:35:44.733,,251,2016-08-04T02:52:30.443,2016-08-04T02:52:30.443,,66.0,,66.0,,5,0,,,,67.96,9.79,9.34,0.0,0.0,24.0,Fuzzy logic is an alternative to boolean logic In boolean logic every statement is either true or false Usually 1 is used to represent true and 0 is used to represent false Fuzzy logic is an extension of this concept where statements do not have such absolute values Instead the truth value is a real number in the range 01 As such fuzzy logic is a realnumber version of multivalued logic
,,"Fuzzy logic is a variant of boolean logic, where the values are real numbers between 0 and 1 (inclusive), rather than only the integer numbers 0 and 1. Use this tag for questions  that involve this real-valued logic.",,0,2016-08-03T18:35:44.733,,252,2016-08-04T02:52:57.023,2016-08-04T02:52:57.023,,66.0,,66.0,,4,0,,,,69.11,9.86,11.23,0.0,0.0,7.0,Fuzzy logic is a variant of boolean logic where the values are real numbers between 0 and 1 inclusive rather than only the integer numbers 0 and 1 Use this tag for questions that involve this realvalued logic
,,"<p>In May 2016 Google announced a custom ASIC which was is specifically built for machine learning<sup><a href=""https://en.wikipedia.org/wiki/TensorFlow#DistBelief"" rel=""nofollow"">wiki</a></sup> and tailored for <a href=""https://en.wikipedia.org/wiki/TensorFlow"" rel=""nofollow"">TensorFlow</a>. It is using <a href=""https://en.wikipedia.org/wiki/Tensor_processing_unit"" rel=""nofollow"">tensor processing unit</a> (TPU) which is a programmable microprocessor designed to accelerate artificial neural networks.</p>

<p><a href=""https://web.stanford.edu/group/brainsinsilicon/goals.html"" rel=""nofollow"">NeuroCores</a>, 12x14 sq-mm chips which can be interconnected in a binary tree, see: <a href=""https://en.wikipedia.org/wiki/TensorFlow#DistBelief"" rel=""nofollow"">Neurogrid</a>, a supercomputer which can provide an option for brain simulations.</p>

<p><a href=""https://en.wikipedia.org/wiki/TrueNorth"" rel=""nofollow"">TrueNorth</a>, a neuromorphic CMOS chip produced by IBM, which has 4096 cores in the current chip, each can simulate 256 programmable silicon ""neurons"", giving a total of over a million neurons.</p>

<p>Further readings: <a href=""https://en.wikipedia.org/wiki/Neuromorphic_engineering"" rel=""nofollow"">Neuromorphic engineering</a>, <a href=""https://en.wikipedia.org/wiki/Vision_processing_unit"" rel=""nofollow"">Vision processing unit</a>, <a href=""https://en.wikipedia.org/wiki/Category:AI_accelerators"" rel=""nofollow"">AI accelerators</a></p>

<hr>

<p>As a side note, you can always use an FPGA based piece of hardware which you can implement selected genetic algorithm (GA) directly in hardware. For example the <a href=""https://en.wikipedia.org/wiki/CoDi#Implementation_in_Hardware"" rel=""nofollow"">CoDi model</a> was implemented in the FPGA based CAM-Brain Machine (CBM)<sup><a href=""http://link.springer.com/article/10.1023%2FA%3A1011286308522"" rel=""nofollow"">2001</a></sup>.</p>
",,0,2016-08-03T18:37:03.843,,253,2016-08-06T01:22:37.030,2016-08-06T01:22:37.030,,8.0,,8.0,237.0,2,3,,,,39.16,15.44,11.86,0.0,0.0,28.0,In May 2016 Google announced a custom ASIC which was is specifically built for machine learningwiki and tailored for TensorFlow It is using tensor processing unit TPU which is a programmable microprocessor designed to accelerate artificial neural networks NeuroCores 12x14 sqmm chips which can be interconnected in a binary tree see Neurogrid a supercomputer which can provide an option for brain simulations TrueNorth a neuromorphic CMOS chip produced by IBM which has 4096 cores in the current chip each can simulate 256 programmable silicon neurons giving a total of over a million neurons Further readings Neuromorphic engineering Vision processing unit AI accelerators As a side note you can always use an FPGA based piece of hardware which you can implement selected genetic algorithm GA directly in hardware For example the CoDi model was implemented in the FPGA based CAMBrain Machine CBM2001
,,"<p>Humans know a lot about the real world implicitly.<br>
If someone is said to have ""filled their tank"", it is implied that they went to a gas station, filled the tank of their car, and paid for it.<br>
An AI does not have this knowledge implicitly; it needs to be taught that this is how things work. It may be taught so explicitly by a teacher or it may learn this from scraping the net, but the knowledge does not come from real-world experiences like humans have.<br>
For this reason, acquiring and applying knowledge about the real world is a field of interest in Artificial Intelligence.</p>
",,0,2016-08-03T18:45:06.577,,254,2016-08-04T02:51:34.397,2016-08-04T02:51:34.397,,66.0,,66.0,,5,0,,,,75.34,8.65,7.97,0.0,0.0,14.0,Humans know a lot about the real world implicitly If someone is said to have filled their tank it is implied that they went to a gas station filled the tank of their car and paid for it An AI does not have this knowledge implicitly it needs to be taught that this is how things work It may be taught so explicitly by a teacher or it may learn this from scraping the net but the knowledge does not come from realworld experiences like humans have For this reason acquiring and applying knowledge about the real world is a field of interest in Artificial Intelligence
,,"World knowledge is knowledge about the real world. Humans have implicit knowledge of the real world, but an AI needs to have explicit resources to explain how the real world works. Use this tag for questions about how artificially intelligent systems deal with their need to acquire or apply knowledge about the real world.",,0,2016-08-03T18:45:06.577,,255,2016-08-04T02:52:21.053,2016-08-04T02:52:21.053,,66.0,,66.0,,4,0,,,,61.67,11.42,8.33,0.0,0.0,4.0,World knowledge is knowledge about the real world Humans have implicit knowledge of the real world but an AI needs to have explicit resources to explain how the real world works Use this tag for questions about how artificially intelligent systems deal with their need to acquire or apply knowledge about the real world
,,,,0,2016-08-03T18:58:09.953,,256,2016-08-03T18:58:09.953,2016-08-03T18:58:09.953,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,"Gödels Incompleteness Theorem is a theorem that gives a limit on what can be mathematically derived. It is possible for a formal system to have statements that are true in the system, whose truth cannot be derived in a finite number of steps. Use this tag for questions about how the incompleteness theorem affects artificially intelligent systems.",,0,2016-08-03T18:58:09.953,,257,2016-08-30T19:42:50.347,2016-08-30T19:42:50.347,,29.0,,66.0,,4,0,,,,43.73,12.42,10.95,0.0,0.0,4.0,Gödels Incompleteness Theorem is a theorem that gives a limit on what can be mathematically derived It is possible for a formal system to have statements that are true in the system whose truth cannot be derived in a finite number of steps Use this tag for questions about how the incompleteness theorem affects artificially intelligent systems
268.0,1.0,"<p>On the Wikipedia page we can read the basic structure of an artificial neuron (a model of biological neurons) which consist:</p>

<ul>
<li>Dendrites - acts as the input vector,</li>
<li>Soma - acts as the summation function,</li>
<li>Axon - gets its signal from the summation behavior which occurs inside the soma.</li>
</ul>

<p>I've checked <a href=""https://en.wikipedia.org/wiki/Deep_learning"" rel=""nofollow"">Deep learning</a> wiki page, but I couldn't find any references to dendrites, soma or axons.</p>

<p>So my question is, which type of artificial neural network implements or can mimic such model most closely?</p>
",,0,2016-08-03T19:03:20.587,,258,2016-08-03T23:08:25.990,,,,,8.0,,1,1,<artificial-neuron>,Which ANN can mimic biological neurons the most?,16.0,44.07,12.31,11.8,0.0,0.0,16.0,On the Wikipedia page we can read the basic structure of an artificial neuron a model of biological neurons which consist Dendrites acts as the input vector Soma acts as the summation function Axon gets its signal from the summation behavior which occurs inside the soma Ive checked Deep learning wiki page but I couldnt find any references to dendrites soma or axons So my question is which type of artificial neural network implements or can mimic such model most closely
,,"<p>Declarative programming is a computer programming paradigm where the focus is on declaring what should be accomplished, rather than explaining how it should be accomplished.  </p>

<p>A good example of a declarative programming language is Prolog, which is based upon predicate logic. Here is a simple Prolog program:</p>

<pre><code>dog(brutus).
dog(pluto).
dog(X):-barks(X).
</code></pre>

<p>This program states (declares) 3 things: (1) that brutus is dog, (2) that pluto is a dog, and (3) that if something is a dog, it barks.<br>
(Note that the names of the dogs are in small letters; Prolog uses capital letters to identify variables).</p>

<p>We can now ask this program to tell us who barks:</p>

<pre><code>barks(X)
</code></pre>

<p>and it will give us the solutions:</p>

<pre><code>X = brutus ;
X = pluto ;
no more solutions
</code></pre>

<p>Note how we did not tell our program <strong>how</strong> to arrive at this result; this is done by the Prolog system itself.  </p>

<p>Declarative programming, by focusing on the ""what"" rather than the ""how"", is useful for building knowledge bases.</p>
",,0,2016-08-03T19:27:06.623,,259,2016-08-04T02:53:38.350,2016-08-04T02:53:38.350,,66.0,,66.0,,5,0,,,,54.76,10.68,9.64,97.0,0.0,33.0,Declarative programming is a computer programming paradigm where the focus is on declaring what should be accomplished rather than explaining how it should be accomplished A good example of a declarative programming language is Prolog which is based upon predicate logic Here is a simple Prolog program This program states declares 3 things 1 that brutus is dog 2 that pluto is a dog and 3 that if something is a dog it barks Note that the names of the dogs are in small letters Prolog uses capital letters to identify variables We can now ask this program to tell us who barks and it will give us the solutions Note how we did not tell our program how to arrive at this result this is done by the Prolog system itself Declarative programming by focusing on the what rather than the how is useful for building knowledge bases
,,"Declarative programming is a programming paradigm where the focus is on what must be accomplished, rather than how it is to be accomplished. Hence it is more about ""declaring"" than about implementing algorithms. Use this tag for questions about how declarative programming is used in AI systems.",,0,2016-08-03T19:27:06.623,,260,2016-08-23T00:15:33.337,2016-08-23T00:15:33.337,,145.0,,66.0,,4,0,,,,47.08,13.16,9.45,0.0,0.0,6.0,Declarative programming is a programming paradigm where the focus is on what must be accomplished rather than how it is to be accomplished Hence it is more about declaring than about implementing algorithms Use this tag for questions about how declarative programming is used in AI systems
,,"<p>Problem solving in artficial intelligence is the study of how an AI can solve a given problem. </p>

<p>The usual approach to problem solving is state search. The problem was described as an initial state, conditions for a final state, and a set of transition rules. A transition rule changes takes a state as input and outputs a new state.<br>
The solution of the problem then consists of applying the right transitions, until a state is reached where that satisfies the condition for a final state.</p>

<p>As a concrete example, there is the problem of the Farmer, the Goat, the Cabbage and the Wolf. The farmer must row each of these to the other side of a river, but his boat is only big enough that he can transport only one of them at a time. If he leaves the goat with the cabbage, the goat will eat the cabbage; if he leaves the wolf with the goat, the wolf will eat the goat.<br>
The initial state has the farmer, cabbage, goat and wolf on one side of the river. A final state has them all on the other side. The transition rules are all ""row the cabbage OR the goat OR the wolf from the current side to the other"".  </p>

<p>There are several state search algorithms, where the purpose is to arrive at a final state in an efficient manner.  </p>
",,0,2016-08-03T19:57:38.687,,262,2016-08-04T02:52:35.130,2016-08-04T02:52:35.130,,66.0,,66.0,,5,0,,,,68.91,7.6,8.3,0.0,0.0,27.0,Problem solving in artficial intelligence is the study of how an AI can solve a given problem The usual approach to problem solving is state search The problem was described as an initial state conditions for a final state and a set of transition rules A transition rule changes takes a state as input and outputs a new state The solution of the problem then consists of applying the right transitions until a state is reached where that satisfies the condition for a final state As a concrete example there is the problem of the Farmer the Goat the Cabbage and the Wolf The farmer must row each of these to the other side of a river but his boat is only big enough that he can transport only one of them at a time If he leaves the goat with the cabbage the goat will eat the cabbage if he leaves the wolf with the goat the wolf will eat the goat The initial state has the farmer cabbage goat and wolf on one side of the river A final state has them all on the other side The transition rules are all row the cabbage OR the goat OR the wolf from the current side to the other There are several state search algorithms where the purpose is to arrive at a final state in an efficient manner
,,Problem solving is a field of artificial intelligence that focuses on how problems (usually puzzles) should be solved. ,,0,2016-08-03T19:57:38.687,,263,2016-08-04T02:54:00.790,2016-08-04T02:54:00.790,,66.0,,66.0,,4,0,,,,44.75,14.96,12.42,0.0,0.0,3.0,Problem solving is a field of artificial intelligence that focuses on how problems usually puzzles should be solved
,,"<p>It depends what you mean by 'develop themselves' - in a rather limited sense, an online machine learning approach such as Genetic Algorithms 'develops itself' to provide better solutions.</p>

<p>There is already a theoretical model that represents the <em>ultimate</em> concept of development: Juergen Schmidhuber's <a href=""http://arxiv.org/abs/cs/0309048"" rel=""nofollow"">Goedel Machine</a> is constructed so as to self-modify when it can prove that this modification is optimal.</p>
",,0,2016-08-03T20:03:22.657,,264,2016-08-03T20:03:22.657,,,,,42.0,172.0,2,0,,,,24.11,16.49,13.28,0.0,0.0,11.0,It depends what you mean by develop themselves in a rather limited sense an online machine learning approach such as Genetic Algorithms develops itself to provide better solutions There is already a theoretical model that represents the ultimate concept of development Juergen Schmidhubers Goedel Machine is constructed so as to selfmodify when it can prove that this modification is optimal
,,"<p>Not exactly a detective mystery, but according to a slide dated June 2012 from a NSA PowerPoint presentation (see: Glenn Greenwald’s site), NSA used some kind of <em>Skynet</em> AI technology to analyze and detect suspicious patterns from location and communication data in order to create a watch list of suspected terrorists. This helped to track associated members of Al-Qa’ida as well as the Muslim Brotherhood. And I'm sure their AI solved a lot of mysteries and found some controversial figures.</p>

<p>Source: <a href=""https://theintercept.com/2015/05/08/u-s-government-designated-prominent-al-jazeera-journalist-al-qaeda-member-put-watch-list/"" rel=""nofollow"">U.S. Government Designated Prominent Al Jazeera Journalist as a member of AI Qaeda</a></p>

<p>For more details check: <a href=""https://theintercept.com/document/2015/05/08/skynet-courier/"" rel=""nofollow"">SKYNET: Courier Detection via Machine Learning</a> for courier detection data and charts generated by analyzing GSM metadata using machine learning algorithms. Also <a href=""https://theintercept.com/document/2015/05/08/skynet-applying-advanced-cloud-based-behavior-analytics/"" rel=""nofollow"">Applying Advanced Cloud-based Behavior Analytics</a>.</p>
",,1,2016-08-03T20:03:26.733,,265,2016-08-03T20:03:26.733,,,,,8.0,241.0,2,0,,,,37.64,15.21,12.33,0.0,0.0,18.0,Not exactly a detective mystery but according to a slide dated June 2012 from a NSA PowerPoint presentation see Glenn Greenwald’s site NSA used some kind of Skynet AI technology to analyze and detect suspicious patterns from location and communication data in order to create a watch list of suspected terrorists This helped to track associated members of AlQa’ida as well as the Muslim Brotherhood And Im sure their AI solved a lot of mysteries and found some controversial figures Source US Government Designated Prominent Al Jazeera Journalist as a member of AI Qaeda For more details check SKYNET Courier Detection via Machine Learning for courier detection data and charts generated by analyzing GSM metadata using machine learning algorithms Also Applying Advanced Cloudbased Behavior Analytics
,,"<p>While I'm not familiar with any explicit statements regarding what a Multilayer Perceptron (MLP) <em>cannot</em> learn, I can provide some further detail on the positive statements you made about MLP capabilities:</p>

<p>A MLP with a single hidden layer is capable of what is commonly termed <a href=""https://en.wikipedia.org/wiki/Universal_approximation_theorem"" rel=""nofollow"">'Universal Function Approximation'</a>, i.e. it can approximate any bounded continuous function to an arbitrary degree of accuracy. With two hidden layers, the boundness restriction is removed <a href=""http://link.springer.com/article/10.1007/BF02551274"" rel=""nofollow"">[Cybenko, 1988]</a>.</p>

<p><a href=""http://www.sciencedirect.com/science/article/pii/089360809190009T"" rel=""nofollow"">This paper</a> goes on to demonstrate that this is true for a wide range of activation functions (not necessarily nonlinear). 3 layer MLPs are also capable of representing any boolean function (although they may require an exponential number of neurons).</p>

<p>See also <a href=""http://cstheory.stackexchange.com/questions/7894/universal-function-approximation"">this interesting answer</a> on CS SE about other Universal approximators.</p>
",,0,2016-08-03T21:08:03.797,,266,2016-08-03T21:08:03.797,,,,,42.0,247.0,2,2,,,,41.9,14.62,11.24,0.0,0.0,23.0,While Im not familiar with any explicit statements regarding what a Multilayer Perceptron MLP cannot learn I can provide some further detail on the positive statements you made about MLP capabilities A MLP with a single hidden layer is capable of what is commonly termed Universal Function Approximation ie it can approximate any bounded continuous function to an arbitrary degree of accuracy With two hidden layers the boundness restriction is removed Cybenko 1988 This paper goes on to demonstrate that this is true for a wide range of activation functions not necessarily nonlinear 3 layer MLPs are also capable of representing any boolean function although they may require an exponential number of neurons See also this interesting answer on CS SE about other Universal approximators
,,"<p>The following post has a bit of math, which I hope helps to explain the problem better. Unfortunately it seems, this SE site does not support LaTex:</p>

<p>Document summarization is very much an open problem in AI research. One way this task is currently handled is called ""extractive summarization"". The basic strategy is as follows: Split this document into sentences and we will present as a summary a subset of sentences which together cover all the important details in the post. Assign sentence i, 1&lt;=i&lt;=n, a variable z_i \in {0,1}, where z_i = 1 indicates the sentence was selected and z_i = 0 means the sentence was left out. Then, z_i z_j = 1 if and only if both sentences were chosen. We will also define the importance of each sentence w_i for sentence i and interaction terms w_{ij} between sentences i and j. </p>

<p>Let x_i be the feature vectors for sentence i. w_i = w(x_i) captures how important it is to include this sentence (or the topics covered by it) while w_ij = w(x_i,x_j) indicates the amount of overlap between sentences in our summary. Finally we put all this in a minimization problem:</p>

<p>maximize_{z_i} \sum_{i} w_i z_i - w_{ij} z_i z_j 
s.t. z_i = 0 or 1</p>

<p>This tries to maximize the total weight of the sentences covered and tries to minimize the amount of overlap. This is an integer programming problem similar to finding the lowest weight independent set in a graph and many techniques exist to solve such problems.</p>

<p>This design, in my opinion, captures the fundamental problems in text summarization and can be extended in many ways. We will discuss those in a bit, but first we need to completely specify the features w. w_i = w(x_i) could be a function only of the sentence i, but it could also depends on the place of the sentence in the document or its context (Is the sentence at the beginning of a paragraph? Does it share common words with the title? What is its length? Does it mention any proper nouns? etc)</p>

<p>w_ij = w(x_i,x_j) is a similarity measure. It measures how much repetition there will be if we include both words in the sentence. It can be defined by looking at common words between sentences. We can also extract topics or concepts from each sentence and see how many are common between them, and use language features like pronouns to see if one sentence expands on another.</p>

<p>To improve the design, first, we could do <em>keyphrase extraction</em>, i.e. identify key phrases in the text and choose to define the above problem in terms of those instead of trying to pick sentences. That is a similar problem to what Google does to summarize news articles in their search results, but I am not aware of the details of their approach. We could also break the sentences up further into concepts and try to establish the semantic meaning of the sentences ( Ponzo and Fila are people P1 and P2, a mall is a place P, P1 and P2 went to the place P at time T (day). Mode of transport walking.... and so on). To do this, we would need to use a semantic ontology or other common-sense knowledge database. However, all the parts of this last semantic classification problem are open and I have not seen anyone make satisfactory progress on it yet. </p>

<p>We could also tweak the loss function above so that instead of the setting the tradeoff between the sentence importance w_i and the diversity score w_ij by hand, we could learn it from data. One way to do this is to use Conditional Random Fields to model the data, but many others surely exist.</p>

<p>I hope this answer explained the basic problems that need to be solved to make progress towards good summarization systems. This is an active field of research and you will find the most recent papers via Google Scholar, but first read the <a href=""https://en.wikipedia.org/wiki/Automatic_summarization"" rel=""nofollow"">Wikipedia page</a> to learn the relevant terms</p>
",,0,2016-08-03T22:41:48.160,,267,2016-08-03T22:48:30.993,2016-08-03T22:48:30.993,,130.0,,130.0,157.0,2,3,,,,68.7,9.75,8.4,0.0,0.0,141.0,The following post has a bit of math which I hope helps to explain the problem better Unfortunately it seems this SE site does not support LaTex Document summarization is very much an open problem in AI research One way this task is currently handled is called extractive summarization The basic strategy is as follows Split this document into sentences and we will present as a summary a subset of sentences which together cover all the important details in the post Assign sentence i 1ltiltn a variable zi in 01 where zi 1 indicates the sentence was selected and zi 0 means the sentence was left out Then zi zj 1 if and only if both sentences were chosen We will also define the importance of each sentence wi for sentence i and interaction terms wij between sentences i and j Let xi be the feature vectors for sentence i wi wxi captures how important it is to include this sentence or the topics covered by it while wij wxixj indicates the amount of overlap between sentences in our summary Finally we put all this in a minimization problem maximizezi sumi wi zi wij zi zj st zi 0 or 1 This tries to maximize the total weight of the sentences covered and tries to minimize the amount of overlap This is an integer programming problem similar to finding the lowest weight independent set in a graph and many techniques exist to solve such problems This design in my opinion captures the fundamental problems in text summarization and can be extended in many ways We will discuss those in a bit but first we need to completely specify the features w wi wxi could be a function only of the sentence i but it could also depends on the place of the sentence in the document or its context Is the sentence at the beginning of a paragraph Does it share common words with the title What is its length Does it mention any proper nouns etc wij wxixj is a similarity measure It measures how much repetition there will be if we include both words in the sentence It can be defined by looking at common words between sentences We can also extract topics or concepts from each sentence and see how many are common between them and use language features like pronouns to see if one sentence expands on another To improve the design first we could do keyphrase extraction ie identify key phrases in the text and choose to define the above problem in terms of those instead of trying to pick sentences That is a similar problem to what Google does to summarize news articles in their search results but I am not aware of the details of their approach We could also break the sentences up further into concepts and try to establish the semantic meaning of the sentences Ponzo and Fila are people P1 and P2 a mall is a place P P1 and P2 went to the place P at time T day Mode of transport walking and so on To do this we would need to use a semantic ontology or other commonsense knowledge database However all the parts of this last semantic classification problem are open and I have not seen anyone make satisfactory progress on it yet We could also tweak the loss function above so that instead of the setting the tradeoff between the sentence importance wi and the diversity score wij by hand we could learn it from data One way to do this is to use Conditional Random Fields to model the data but many others surely exist I hope this answer explained the basic problems that need to be solved to make progress towards good summarization systems This is an active field of research and you will find the most recent papers via Google Scholar but first read the Wikipedia page to learn the relevant terms
,,"<p>ANN research does not try to model biological neurons, as the aim is to achieve better performance at prediction tasks. However, there is a body of literature in neuroscience that looks at <a href=""https://en.wikipedia.org/wiki/Models_of_neural_computation"" rel=""nofollow"">Computational models of neurons</a>. Neurons are complicated cells and our understanding of neurons is still not complete. </p>
",,0,2016-08-03T23:08:25.990,,268,2016-08-03T23:08:25.990,,,,,130.0,258.0,2,2,,,,46.47,13.11,10.89,0.0,0.0,5.0,ANN research does not try to model biological neurons as the aim is to achieve better performance at prediction tasks However there is a body of literature in neuroscience that looks at Computational models of neurons Neurons are complicated cells and our understanding of neurons is still not complete
,,"<p><a href=""http://python.org"" rel=""nofollow"">Python</a> is a <a href=""https://wiki.python.org/moin/Why%20is%20Python%20a%20dynamic%20language%20and%20also%20a%20strongly%20typed%20language"" rel=""nofollow"">dynamic and strongly typed</a> programming language that is used for <a href=""http://python.org/about/apps"" rel=""nofollow"">a wide range of applications</a>. It is a general-purpose, high-level programming language that is designed to emphasize usability.</p>

<p>Python allows programmers to express concepts in fewer lines of code than would be possible in many other languages such as <a href=""/questions/tagged/c"" class=""post-tag"" title=""show questions tagged &#39;c&#39;"" rel=""tag"">c</a>, and the language has constructs intended to be used to create clear programs in a variety of domains.</p>

<p>Two similar but incompatible versions of Python are in widespread use (2 and 3). Please consider mentioning the version and implementation that you are using when asking a question about Python.</p>

<p>Python supports multiple programming paradigms, including object-oriented, imperative and functional programming styles. It features a fully dynamic type system and automatic memory management, similar to that of <a href=""/questions/tagged/scheme"" class=""post-tag"" title=""show questions tagged &#39;scheme&#39;"" rel=""tag"">scheme</a>, <a href=""/questions/tagged/ruby"" class=""post-tag"" title=""show questions tagged &#39;ruby&#39;"" rel=""tag"">ruby</a>, <a href=""/questions/tagged/perl"" class=""post-tag"" title=""show questions tagged &#39;perl&#39;"" rel=""tag"">perl</a> and <a href=""/questions/tagged/tcl"" class=""post-tag"" title=""show questions tagged &#39;tcl&#39;"" rel=""tag"">tcl</a>.</p>

<p>Like other <a href=""http://en.wikipedia.org/wiki/Dynamic_programming_language"" rel=""nofollow"">dynamic languages</a>, Python is often used as a <a href=""http://stackoverflow.com/tags/scripting/info"">scripting language</a>, but is also used in a wide range of non-scripting contexts. Using third-party tools, Python code can be packaged into standalone executable programs. Python interpreters are available for many operating systems.</p>

<p><a href=""/questions/tagged/cpython"" class=""post-tag"" title=""show questions tagged &#39;cpython&#39;"" rel=""tag"">cpython</a>, the reference implementation of Python, is free and open source software and has a community-based development model, as do nearly all of its alternative implementations. There are a wide variety of implementations more suited for specific environments or tasks.</p>

<p>The philosophy of Python is succinctly formulated in <a href=""http://python.org/dev/peps/pep-0020"" rel=""nofollow""><em>The Zen of Python</em></a> written by Tim Peters, which can be revealed by issuing this command at the interactive interpreter:</p>

<pre><code>&gt;&gt;&gt; import this
</code></pre>

<p>The documentation can also be accessed offline for your installation of Python in the following manner:</p>

<ol>
<li>Going into <code>Your_Python_install_dir/Doc</code>. There is a complete Python documentation present for the version of Python installed on your computer.</li>
<li>Running <code>pydoc x</code> or <code>python -m pydoc x</code> from the command prompt or terminal displays documentation for module <code>x</code>.</li>
</ol>

<p>Unlike many other languages Python uses an indentation based syntax and this may take some getting used to for programmers familiar with braces for syntax.</p>

<pre><code>&gt;&gt;&gt; from __future__ import braces
  File ""&lt;stdin&gt;"", line 1
SyntaxError: not a chance
</code></pre>

<p>To help with the transition it is a recommendation to use a properly configured text-editor created for programmers or an IDE. Python comes with a basic IDE called <a href=""http://docs.python.org/2/library/idle.html"" rel=""nofollow"">IDLE</a> to get you started. Other popular examples are the charity-ware <a href=""/questions/tagged/vim"" class=""post-tag"" title=""show questions tagged &#39;vim&#39;"" rel=""tag"">vim</a>, the free GNU <a href=""/questions/tagged/emacs"" class=""post-tag"" title=""show questions tagged &#39;emacs&#39;"" rel=""tag"">emacs</a>, <a href=""/questions/tagged/eclipse"" class=""post-tag"" title=""show questions tagged &#39;eclipse&#39;"" rel=""tag"">eclipse</a>+<a href=""/questions/tagged/pydev"" class=""post-tag"" title=""show questions tagged &#39;pydev&#39;"" rel=""tag"">pydev</a> or <a href=""/questions/tagged/pycharm"" class=""post-tag"" title=""show questions tagged &#39;pycharm&#39;"" rel=""tag"">pycharm</a>. Take a look at this <a href=""http://en.wikipedia.org/wiki/List_of_integrated_development_environments_for_Python#Python"" rel=""nofollow"">IDE comparison list</a> for many other alternatives.</p>

<hr>

<h2>Tagging recommendation:</h2>

<p>Use the <a href=""/questions/tagged/python"" class=""post-tag"" title=""show questions tagged &#39;python&#39;"" rel=""tag"">python</a> tag for all Python related questions. If you believe your question includes issues specific to individual versions, use <a href=""/questions/tagged/python-3.x"" class=""post-tag"" title=""show questions tagged &#39;python-3.x&#39;"" rel=""tag"">python-3.x</a> or <a href=""/questions/tagged/python-2.7"" class=""post-tag"" title=""show questions tagged &#39;python-2.7&#39;"" rel=""tag"">python-2.7</a> in addition to the main <a href=""/questions/tagged/python"" class=""post-tag"" title=""show questions tagged &#39;python&#39;"" rel=""tag"">python</a> tag. If you believe your question may be even more specific, you can include a version specific tag such as <a href=""/questions/tagged/python-3.5"" class=""post-tag"" title=""show questions tagged &#39;python-3.5&#39;"" rel=""tag"">python-3.5</a>.</p>

<p>Also, consider including the tag for the specific implementation if you are using one other than <a href=""/questions/tagged/cpython"" class=""post-tag"" title=""show questions tagged &#39;cpython&#39;"" rel=""tag"">cpython</a> - the use of <a href=""/questions/tagged/cpython"" class=""post-tag"" title=""show questions tagged &#39;cpython&#39;"" rel=""tag"">cpython</a> is assumed unless explicitly stated otherwise.</p>

<hr>

<h2>References</h2>

<ul>
<li>Official documentation for the current stable versions: <a href=""http://docs.python.org/2.7/"" rel=""nofollow"">2.7.x</a> and <a href=""http://docs.python.org/3.5/"" rel=""nofollow"">3.5.x</a>.</li>
<li>Release notes for the current stable versions: <a href=""https://www.python.org/downloads/release/python-2711/"" rel=""nofollow"">2.7.11</a> and <a href=""https://www.python.org/download/releases/3.5.1"" rel=""nofollow"">3.5.1</a>.</li>
<li><a href=""http://en.wikipedia.org/wiki/Python_%28programming_language%29"" rel=""nofollow"">Python (programming language)</a> (Wikipedia)</li>
<li><a href=""http://wiki.python.org/moin/BeginnersGuide/Programmers"" rel=""nofollow"">Python for Programmers</a></li>
<li><a href=""http://www.tutorialspoint.com/python/python_quick_guide.htm"" rel=""nofollow"">Python - Quick Guide</a></li>
<li><a href=""https://intellipaat.com/tutorial/python-tutorial/introduction/"" rel=""nofollow"">Getting started with Python</a></li>
<li><a href=""http://docs.python.org/3.5/howto/pyporting.html"" rel=""nofollow"">Porting Python 2 Code to Python 3</a></li>
<li>The non-profit <a href=""http://www.python.org/psf/"" rel=""nofollow"">Python Software Foundation</a> manages <a href=""http://en.wikipedia.org/wiki/CPython"" rel=""nofollow"">CPython</a>.</li>
<li><a href=""http://www.python.org/psf/"" rel=""nofollow"">PSF</a> License Agreement for Python <a href=""http://docs.python.org/2.7/license.html"" rel=""nofollow"">2.7.x</a> and <a href=""http://docs.python.org/3.5/license.html"" rel=""nofollow"">3.5.x</a></li>
<li><a href=""http://stackoverflow.com/documentation/python"">Stackoverflow Documentation</a></li>
</ul>

<hr>

<h2>Popular <a href=""http://wiki.python.org/moin/WebFrameworks"" rel=""nofollow"">web frameworks</a> based on Python</h2>

<p>If your question has to do with any of these frameworks, please ensure you include the appropriate tag.</p>

<ul>
<li><p><a href=""http://www.djangoproject.com"" rel=""nofollow""><strong>Django</strong></a> <a href=""/questions/tagged/django"" class=""post-tag"" title=""show questions tagged &#39;django&#39;"" rel=""tag"">django</a></p>

<p>The Web framework for perfectionists (with deadlines). Django makes it easier to build better Web apps more quickly and with less code. Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. It lets you build high-performing, elegant Web applications quickly. Django focuses on automating as much as possible and adhering to the DRY (Don't Repeat Yourself) principle.</p></li>
<li><p><a href=""http://flask.pocoo.org"" rel=""nofollow""><strong>Flask</strong></a> <a href=""/questions/tagged/flask"" class=""post-tag"" title=""show questions tagged &#39;flask&#39;"" rel=""tag"">flask</a></p>

<p>Flask is a micro-framework for Python based on Werkzeug, Jinja 2 and good intentions.</p></li>
<li><p><a href=""http://www.tornadoweb.org/"" rel=""nofollow""><strong>Tornado</strong></a> <a href=""/questions/tagged/tornado"" class=""post-tag"" title=""show questions tagged &#39;tornado&#39;"" rel=""tag"">tornado</a></p>

<p>Tornado is a Python web framework and asynchronous networking library. By using non-blocking network I/O, Tornado can scale to tens of thousands of open connections, making it ideal for long polling, WebSockets, and other applications that require a long-lived connection to each user.</p></li>
<li><p><a href=""http://www.cherrypy.org"" rel=""nofollow""><strong>CherryPy</strong></a> <a href=""/questions/tagged/cherrypy"" class=""post-tag"" title=""show questions tagged &#39;cherrypy&#39;"" rel=""tag"">cherrypy</a></p>

<p>CherryPy is a pythonic, object-oriented web framework that enables developers to build web applications in much the same way they would build any other object-oriented Python program. This results in smaller source code developed in less time. CherryPy has been in use for over 7 years and it is being used in production by many sites, from the simplest to the most demanding.</p></li>
<li><p><a href=""http://www.pylonsproject.org/projects/pyramid/about"" rel=""nofollow""><strong>Pyramid</strong></a> <a href=""/questions/tagged/pyramid"" class=""post-tag"" title=""show questions tagged &#39;pyramid&#39;"" rel=""tag"">pyramid</a></p>

<p>A lightweight Web framework emphasizing flexibility and rapid development. It combines the very best ideas from the worlds of Ruby, Python and Perl, providing a structured but extremely flexible Python web framework. It's also one of the first projects to leverage the emerging WSGI standard, which allows extensive re-use and flexibility but only if you need it.</p></li>
<li><p><a href=""http://webpy.org"" rel=""nofollow""><strong>web.py</strong></a> <a href=""/questions/tagged/web.py"" class=""post-tag"" title=""show questions tagged &#39;web.py&#39;"" rel=""tag"">web.py</a></p>

<p>web.py is a web framework for Python that is as simple as it is powerful. web.py is in the public domain; you can use it for whatever purpose with absolutely no restrictions. web.py lets you write web apps in Python.</p></li>
<li><p><a href=""http://grok.zope.org"" rel=""nofollow""><strong>Grok</strong></a> <a href=""/questions/tagged/grok"" class=""post-tag"" title=""show questions tagged &#39;grok&#39;"" rel=""tag"">grok</a></p>

<p>Built on the existing Zope 3 libraries, but aims to provide an easier learning curve and a more agile development experience. Grok does this by placing an emphasis on convention over configuration and DRY (Don't Repeat Yourself).</p></li>
<li><p><a href=""http://bottlepy.org/docs/dev/index.html"" rel=""nofollow""><strong>Bottle</strong></a> <a href=""/questions/tagged/bottle"" class=""post-tag"" title=""show questions tagged &#39;bottle&#39;"" rel=""tag"">bottle</a></p>

<p>Bottle is a fast, simple and lightweight WSGI micro web-framework for Python. It is distributed as a single file module and has no dependencies other than the Python Standard Library.</p></li>
</ul>

<h2>Popular Mathematical/Scientific computing libraries in Python</h2>

<ul>
<li><p><a href=""http://www.numpy.org"" rel=""nofollow""><strong>NumPy</strong></a> <a href=""/questions/tagged/numpy"" class=""post-tag"" title=""show questions tagged &#39;numpy&#39;"" rel=""tag"">numpy</a></p>

<p>NumPy is the fundamental package for scientific computing with Python. It contains among other things:</p>

<ul>
<li>a powerful N-dimensional array object</li>
<li>sophisticated (broadcasting) functions</li>
<li>tools for integrating C/C++ and Fortran code</li>
<li>useful linear algebra, Fourier transform, and random number capabilities</li>
</ul>

<p>These features also make it possible to use NumPy in general-purpose database applications.</p></li>
<li><p><a href=""http://scipy.org"" rel=""nofollow""><strong>SciPy</strong></a> <a href=""/questions/tagged/scipy"" class=""post-tag"" title=""show questions tagged &#39;scipy&#39;"" rel=""tag"">scipy</a></p>

<p>SciPy is an open source library for the Python programming language consisting of mathematical algorithms and functions often used in science and engineering. SciPy includes algorithms and tools for tasks such as optimization, clustering, discrete Fourier transforms, linear algebra, signal processing and multi-dimensional image processing. SciPy is closely related to NumPy and depends on many NumPy functions, including a multidimensional array that is used as the basic data structure in SciPy.</p></li>
<li><p><a href=""http://matplotlib.org/"" rel=""nofollow""><strong>matplotlib</strong></a> <a href=""/questions/tagged/matplotlib"" class=""post-tag"" title=""show questions tagged &#39;matplotlib&#39;"" rel=""tag"">matplotlib</a></p>

<p>matplotlib is a plotting library for the Python programming language and its NumPy numerical mathematics extension. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like wxPython, Qt, or GTK. There is also a procedural ""pylab"" interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB.</p></li>
<li><p><a href=""http://pandas.pydata.org/"" rel=""nofollow""><strong>pandas</strong></a> <a href=""/questions/tagged/pandas"" class=""post-tag"" title=""show questions tagged &#39;pandas&#39;"" rel=""tag"">pandas</a></p>

<p>pandas, the Python Data Analysis Library, is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.</p></li>
<li><p><a href=""http://deeplearning.net/software/theano/"" rel=""nofollow""><strong>theano</strong></a> <a href=""/questions/tagged/theano"" class=""post-tag"" title=""show questions tagged &#39;theano&#39;"" rel=""tag"">theano</a></p>

<p>Theano is a Python-C-based library widely-used library suitable for highly computational mathematical tasks due to the optimizations it does on the interface Python code making it highly optimized using its C-based routines. It is a very popular library for machine-learning researchers as well. It features a highly optimized automatic differentiation, easing the implementations of highly complicated functions and computing their gradients without any errors.</p></li>
<li><p><a href=""http://www.blender.org/"" rel=""nofollow""><strong>Blender</strong></a> <a href=""/questions/tagged/blender"" class=""post-tag"" title=""show questions tagged &#39;blender&#39;"" rel=""tag"">blender</a></p>

<p>Blender is a free and open source 3D animation suite. It supports the entirety of the 3D pipeline—modeling, rigging, animation, simulation, rendering, compositing and motion tracking, even video editing and game creation.</p></li>
<li><p><a href=""http://scikit-learn.org/stable/"" rel=""nofollow""><strong>scikit-learn</strong></a> <a href=""/questions/tagged/scikit-learn"" class=""post-tag"" title=""show questions tagged &#39;scikit-learn&#39;"" rel=""tag"">scikit-learn</a></p>

<p>scikit-learn is a free and open source machine learning library written in Python. It supports training and testing many different kinds of machine learning models, along with some basic data processing techniques.</p></li>
</ul>

<hr>

<h2>Community</h2>

<h3>Chat Rooms</h3>

<p>Chat about Python with other Stack Overflow users in the <a href=""http://chat.stackoverflow.com/rooms/6/python"">Python chat room</a>.</p>

<h3>Other Sites</h3>

<ul>
<li><a href=""http://python.org/community/lists/#tutor"" rel=""nofollow"">Tutor mailing list</a></li>
<li><a href=""http://python.org/community/lists/#python-help"" rel=""nofollow"">python-help mailing list</a></li>
<li><a href=""http://www.pycon.org"" rel=""nofollow"">PyCon</a></li>
<li><a href=""http://www.pythonweekly.com"" rel=""nofollow"">Python Weekly</a></li>
<li><a href=""http://pycoders.com"" rel=""nofollow"">Pycoder's Weekly</a></li>
<li><a href=""https://groups.google.com/forum/#!forum/comp.lang.python"" rel=""nofollow"">Python Google Group</a></li>
</ul>

<hr>

<h2>Free Python programming Books</h2>

<ul>
<li><a href=""http://en.wikibooks.org/wiki/Non-Programmer%27s_Tutorial_for_Python_2.6"" rel=""nofollow"">Wikibooks' Non-Programmers Tutorial for Python 2.6</a></li>
<li><a href=""https://en.wikibooks.org/wiki/Non-Programmer%27s_Tutorial_for_Python_3"" rel=""nofollow"">Wikibooks' Non-Programmers Tutorial for Python 3</a></li>
<li><a href=""http://docs.python.org/tutorial"" rel=""nofollow"">The Official Python Tutorial</a></li>
<li><a href=""http://www.itmaybeahack.com/homepage/books/python.html"" rel=""nofollow"">Building Skills in Python Version 2.6</a> (Steven F. Lott)</li>
<li><a href=""http://www.swaroopch.org/notes/Python"" rel=""nofollow"">A Byte of Python</a> (Swaroop C H.)</li>
<li><a href=""http://www.brpreiss.com/books/opus7/html/book.html"" rel=""nofollow"">Data Structures and Algorithms in Python</a> (Bruno R. Preiss)</li>
<li><a href=""http://interactivepython.org/runestone/static/pythonds/index.html"" rel=""nofollow"">Problem Solving with Algorithms and Data Structures using python</a> (Brad Miller and David Ranum)</li>
<li><a href=""http://www.diveintopython.net"" rel=""nofollow"">Dive into Python</a></li>
<li><a href=""http://www.diveinto.org/python3"" rel=""nofollow"">Dive into Python 3</a></li>
<li><a href=""http://www.greenteapress.com/thinkpython/thinkCSpy"" rel=""nofollow"">How to Think Like a Computer Scientist: Learning with Python</a> (Allen Downey, Jeff Elkner and Chris Meyers)</li>
<li><a href=""http://inventwithpython.com"" rel=""nofollow"">Invent Your Own Computer Games With Python</a> (Al Sweigart)</li>
<li><a href=""http://learnpythonthehardway.org"" rel=""nofollow"">Learn Python The Hard Way</a> (Zed A. Shaw)</li>
<li><a href=""http://inventwithpython.com/pygame"" rel=""nofollow"">Making Games with Python &amp; Pygame</a> (Albert Sweigart)</li>
<li><a href=""http://www.nltk.org/book"" rel=""nofollow"">Natural Language Processing with Python</a> (Steven Bird, Ewan Klein, and Edward Loper)</li>
<li><a href=""http://openbookproject.net/pybiblio"" rel=""nofollow"">Python Bibliotheca</a></li>
<li><a href=""http://openbookproject.net/py4fun"" rel=""nofollow"">Python for Fun</a> (Chris Meyers)</li>
<li><a href=""http://www.briggs.net.nz/snake-wrangling-for-kids.html"" rel=""nofollow"">Snake Wrangling For Kids</a> (Jason R. Briggs)</li>
<li><a href=""http://www.greenteapress.com/thinkpython/thinkpython.pdf"" rel=""nofollow"">Think Python (PDF file)</a> (Allen Downey)</li>
<li><a href=""http://python3porting.com"" rel=""nofollow"">Porting to Python 3</a> (Lennart Regebro)</li>
</ul>

<hr>

<h2>Interactive Python learning</h2>

<ul>
<li><a href=""http://pythonmonk.com"" rel=""nofollow"">Python Monk</a> - Interactive Python learning in the browser</li>
<li><a href=""http://www.codecademy.com/tracks/python"" rel=""nofollow"">Codeacademy</a> - Learn the fundamentals of Python and dynamic programming</li>
<li><a href=""http://www.codeskulptor.org"" rel=""nofollow"">CodeSkulptor</a> - Interactive online IDEfor Python programming</li>
<li><a href=""https://www.coursera.org/course/interactivepython"" rel=""nofollow"">Coursera</a> - Online course for introduction to interactive Python programming</li>
<li><a href=""http://www.checkio.org"" rel=""nofollow"">CheckiO</a> - Game world you can explore using your Python programming skills</li>
<li><a href=""http://www.repl.it/languages/Python"" rel=""nofollow"">Repl.it</a> - Online interpreter for Python that it allow saving code for later demonstration</li>
<li><a href=""https://www.jetbrains.com/pycharm-edu/"" rel=""nofollow"">PyCharm Edu</a> - Desktop application that offers interactive Python learning.</li>
<li><a href=""http://interactivepython.org/"" rel=""nofollow"">Interactive Python</a> - Includes a modified, interactive version of How to Think Like a Computer Scientist.</li>
</ul>

<hr>

<h2>Python Online Courses</h2>

<ul>
<li><a href=""https://class.coursera.org/pythonlearn-001/auth/auth_redirector?type=login&amp;subtype=normal"" rel=""nofollow"">Programming for Everybody</a> - Introduction to programming using Python.</li>
<li><a href=""https://class.coursera.org/interactivepython-004/auth/auth_redirector?type=login&amp;subtype=normal"" rel=""nofollow"">An Introduction to Interactive Programming in Python</a> - The name explains itself.</li>
</ul>

<hr>

<h2>Python Video Tutorials</h2>

<ul>
<li><a href=""https://www.youtube.com/watch?v=4Mf0h3HphEA&amp;list=PLEA1FEF17E1E5C0DA"" rel=""nofollow"">Programming in Python</a></li>
<li><a href=""https://www.youtube.com/watch?v=GE3IHS1wAZI&amp;list=PL_RGaFnxSHWpX_byHyTEj9hecPngl2DqR"" rel=""nofollow"">Python for Beginners</a></li>
</ul>

<hr>

<h2>Python for Scientists</h2>

<ul>
<li><a href=""http://nbviewer.ipython.org/gist/rpmuller/5920182"" rel=""nofollow"">A Crash Course in Python for Scientists</a></li>
</ul>
",,0,2016-08-03T23:13:25.643,,269,2016-08-04T02:53:41.727,2016-08-04T02:53:41.727,,5.0,,5.0,,5,0,,,,35.98,15.72,9.77,182.0,0.0,279.0,Python is a dynamic and strongly typed programming language that is used for a wide range of applications It is a generalpurpose highlevel programming language that is designed to emphasize usability Python allows programmers to express concepts in fewer lines of code than would be possible in many other languages such as c and the language has constructs intended to be used to create clear programs in a variety of domains Two similar but incompatible versions of Python are in widespread use 2 and 3 Please consider mentioning the version and implementation that you are using when asking a question about Python Python supports multiple programming paradigms including objectoriented imperative and functional programming styles It features a fully dynamic type system and automatic memory management similar to that of scheme ruby perl and tcl Like other dynamic languages Python is often used as a scripting language but is also used in a wide range of nonscripting contexts Using thirdparty tools Python code can be packaged into standalone executable programs Python interpreters are available for many operating systems cpython the reference implementation of Python is free and open source software and has a communitybased development model as do nearly all of its alternative implementations There are a wide variety of implementations more suited for specific environments or tasks The philosophy of Python is succinctly formulated in The Zen of Python written by Tim Peters which can be revealed by issuing this command at the interactive interpreter The documentation can also be accessed offline for your installation of Python in the following manner Going into There is a complete Python documentation present for the version of Python installed on your computer Running or from the command prompt or terminal displays documentation for module Unlike many other languages Python uses an indentation based syntax and this may take some getting used to for programmers familiar with braces for syntax To help with the transition it is a recommendation to use a properly configured texteditor created for programmers or an IDE Python comes with a basic IDE called IDLE to get you started Other popular examples are the charityware vim the free GNU emacs eclipsepydev or pycharm Take a look at this IDE comparison list for many other alternatives Tagging recommendation Use the python tag for all Python related questions If you believe your question includes issues specific to individual versions use python3x or python27 in addition to the main python tag If you believe your question may be even more specific you can include a version specific tag such as python35 Also consider including the tag for the specific implementation if you are using one other than cpython the use of cpython is assumed unless explicitly stated otherwise References Official documentation for the current stable versions 27x and 35x Release notes for the current stable versions 2711 and 351 Python programming language Wikipedia Python for Programmers Python Quick Guide Getting started with Python Porting Python 2 Code to Python 3 The nonprofit Python Software Foundation manages CPython PSF License Agreement for Python 27x and 35x Stackoverflow Documentation Popular web frameworks based on Python If your question has to do with any of these frameworks please ensure you include the appropriate tag Django django The Web framework for perfectionists with deadlines Django makes it easier to build better Web apps more quickly and with less code Django is a highlevel Python Web framework that encourages rapid development and clean pragmatic design It lets you build highperforming elegant Web applications quickly Django focuses on automating as much as possible and adhering to the DRY Dont Repeat Yourself principle Flask flask Flask is a microframework for Python based on Werkzeug Jinja 2 and good intentions Tornado tornado Tornado is a Python web framework and asynchronous networking library By using nonblocking network IO Tornado can scale to tens of thousands of open connections making it ideal for long polling WebSockets and other applications that require a longlived connection to each user CherryPy cherrypy CherryPy is a pythonic objectoriented web framework that enables developers to build web applications in much the same way they would build any other objectoriented Python program This results in smaller source code developed in less time CherryPy has been in use for over 7 years and it is being used in production by many sites from the simplest to the most demanding Pyramid pyramid A lightweight Web framework emphasizing flexibility and rapid development It combines the very best ideas from the worlds of Ruby Python and Perl providing a structured but extremely flexible Python web framework Its also one of the first projects to leverage the emerging WSGI standard which allows extensive reuse and flexibility but only if you need it webpy webpy webpy is a web framework for Python that is as simple as it is powerful webpy is in the public domain you can use it for whatever purpose with absolutely no restrictions webpy lets you write web apps in Python Grok grok Built on the existing Zope 3 libraries but aims to provide an easier learning curve and a more agile development experience Grok does this by placing an emphasis on convention over configuration and DRY Dont Repeat Yourself Bottle bottle Bottle is a fast simple and lightweight WSGI micro webframework for Python It is distributed as a single file module and has no dependencies other than the Python Standard Library Popular MathematicalScientific computing libraries in Python NumPy numpy NumPy is the fundamental package for scientific computing with Python It contains among other things a powerful Ndimensional array object sophisticated broadcasting functions tools for integrating CC and Fortran code useful linear algebra Fourier transform and random number capabilities These features also make it possible to use NumPy in generalpurpose database applications SciPy scipy SciPy is an open source library for the Python programming language consisting of mathematical algorithms and functions often used in science and engineering SciPy includes algorithms and tools for tasks such as optimization clustering discrete Fourier transforms linear algebra signal processing and multidimensional image processing SciPy is closely related to NumPy and depends on many NumPy functions including a multidimensional array that is used as the basic data structure in SciPy matplotlib matplotlib matplotlib is a plotting library for the Python programming language and its NumPy numerical mathematics extension It provides an objectoriented API for embedding plots into applications using generalpurpose GUI toolkits like wxPython Qt or GTK There is also a procedural pylab interface based on a state machine like OpenGL designed to closely resemble that of MATLAB pandas pandas pandas the Python Data Analysis Library is an open source BSDlicensed library providing highperformance easytouse data structures and data analysis tools for the Python programming language theano theano Theano is a PythonCbased library widelyused library suitable for highly computational mathematical tasks due to the optimizations it does on the interface Python code making it highly optimized using its Cbased routines It is a very popular library for machinelearning researchers as well It features a highly optimized automatic differentiation easing the implementations of highly complicated functions and computing their gradients without any errors Blender blender Blender is a free and open source 3D animation suite It supports the entirety of the 3D pipeline—modeling rigging animation simulation rendering compositing and motion tracking even video editing and game creation scikitlearn scikitlearn scikitlearn is a free and open source machine learning library written in Python It supports training and testing many different kinds of machine learning models along with some basic data processing techniques Community Chat Rooms Chat about Python with other Stack Overflow users in the Python chat room Other Sites Tutor mailing list pythonhelp mailing list PyCon Python Weekly Pycoders Weekly Python Google Group Free Python programming Books Wikibooks NonProgrammers Tutorial for Python 26 Wikibooks NonProgrammers Tutorial for Python 3 The Official Python Tutorial Building Skills in Python Version 26 Steven F Lott A Byte of Python Swaroop C H Data Structures and Algorithms in Python Bruno R Preiss Problem Solving with Algorithms and Data Structures using python Brad Miller and David Ranum Dive into Python Dive into Python 3 How to Think Like a Computer Scientist Learning with Python Allen Downey Jeff Elkner and Chris Meyers Invent Your Own Computer Games With Python Al Sweigart Learn Python The Hard Way Zed A Shaw Making Games with Python amp Pygame Albert Sweigart Natural Language Processing with Python Steven Bird Ewan Klein and Edward Loper Python Bibliotheca Python for Fun Chris Meyers Snake Wrangling For Kids Jason R Briggs Think Python PDF file Allen Downey Porting to Python 3 Lennart Regebro Interactive Python learning Python Monk Interactive Python learning in the browser Codeacademy Learn the fundamentals of Python and dynamic programming CodeSkulptor Interactive online IDEfor Python programming Coursera Online course for introduction to interactive Python programming CheckiO Game world you can explore using your Python programming skills Replit Online interpreter for Python that it allow saving code for later demonstration PyCharm Edu Desktop application that offers interactive Python learning Interactive Python Includes a modified interactive version of How to Think Like a Computer Scientist Python Online Courses Programming for Everybody Introduction to programming using Python An Introduction to Interactive Programming in Python The name explains itself Python Video Tutorials Programming in Python Python for Beginners Python for Scientists A Crash Course in Python for Scientists
,,,,0,2016-08-03T23:13:25.643,,270,2016-08-03T23:13:25.643,2016-08-03T23:13:25.643,,-1.0,,-1.0,,4,0,,,,,,,,,,
,,"<p>An important question that does not yet have a satisfactory answer in neural network research is how DNNs come up with the predictions they offer. DNNs effectively work (though not exactly) by matching patches in the images to a ""dictionary"" of patches, one stored in each neuron (see <a href=""http://research.google.com/archive/unsupervised_icml2012.html"">the youtube cat paper</a>). Thus, it may not have a high level view of the image since it only looks at patches, and images are usually downscaled to much lower resolution to obtain the results in current generation systems. Methods which look at how the components of the image interact may be able to avoid these problems.</p>

<p>Some questions to ask for this work are: How confident were the networks when they made these predictions? How much volume do such adversarial images occupy in the space of all images?</p>

<p>Some work I am aware of in this regard comes from Dhruv Batra and Devi Parikh's Lab at Virginia Tech who look into this for question answering systems: <a href=""http://arxiv.org/pdf/1606.07356v1.pdf"">Analyzing the Behavior of Visual Question Answering Models</a> and <a href=""https://filebox.ece.vt.edu/~dbatra/papers/gmpb_icmlvis16.pdf"">Interpreting Visual Question Answering models</a>. </p>

<p>More such work is needed, and just as the human visual system does also get fooled by such ""optical illusions"", these problems may be unavoidable if we use DNNs, though AFAIK nothing is yet known either way, theoretically or empirically. </p>
",,0,2016-08-03T23:25:53.977,,271,2016-08-03T23:25:53.977,,,,,130.0,92.0,2,7,,,,52.12,11.38,10.69,0.0,0.0,26.0,An important question that does not yet have a satisfactory answer in neural network research is how DNNs come up with the predictions they offer DNNs effectively work though not exactly by matching patches in the images to a dictionary of patches one stored in each neuron see the youtube cat paper Thus it may not have a high level view of the image since it only looks at patches and images are usually downscaled to much lower resolution to obtain the results in current generation systems Methods which look at how the components of the image interact may be able to avoid these problems Some questions to ask for this work are How confident were the networks when they made these predictions How much volume do such adversarial images occupy in the space of all images Some work I am aware of in this regard comes from Dhruv Batra and Devi Parikhs Lab at Virginia Tech who look into this for question answering systems Analyzing the Behavior of Visual Question Answering Models and Interpreting Visual Question Answering models More such work is needed and just as the human visual system does also get fooled by such optical illusions these problems may be unavoidable if we use DNNs though AFAIK nothing is yet known either way theoretically or empirically
,,"<p>To have any chance at answering this, you'd first need a rigorous definition of ""true artificial intelligence"", which we don't have.  And even if you had that, the best answer would probably be ""nobody knows.""  We don't even understand exactly how human intelligence (which is probably the best model of intelligence we have available to study) works.  What we do know (or think we know) is that ANN's are at best a very superficial replica of brain function.  It may turn out that they're absolutely the wrong path to achieving ""true artificial intelligence"" although I expect most people would be surprised if that turned out to be the case.</p>

<p>What probably wouldn't be so surprising would be if some other technique emerged which is better than ANN's, OR if it turns out that you need an ensemble of techniques.  Personally I think it's close to self-evident that the brain works largely in a probabilistic fashion, but it's also clear that we do sometimes use symbolic processing / deductive logic / rules / etc.  And right now, ANN's don't give you much in the way of reasoning, deduction, etc.  So we may ultimately find that we have to combine a probabilistic approach like ANN's with other techniques - maybe Inductive Logic Programming or something of that nature. </p>
",,0,2016-08-04T00:57:01.833,,272,2016-08-04T00:57:01.833,,,,,33.0,233.0,2,2,,,,55.98,11.73,9.2,0.0,0.0,44.0,To have any chance at answering this youd first need a rigorous definition of true artificial intelligence which we dont have And even if you had that the best answer would probably be nobody knows We dont even understand exactly how human intelligence which is probably the best model of intelligence we have available to study works What we do know or think we know is that ANNs are at best a very superficial replica of brain function It may turn out that theyre absolutely the wrong path to achieving true artificial intelligence although I expect most people would be surprised if that turned out to be the case What probably wouldnt be so surprising would be if some other technique emerged which is better than ANNs OR if it turns out that you need an ensemble of techniques Personally I think its close to selfevident that the brain works largely in a probabilistic fashion but its also clear that we do sometimes use symbolic processing deductive logic rules etc And right now ANNs dont give you much in the way of reasoning deduction etc So we may ultimately find that we have to combine a probabilistic approach like ANNs with other techniques maybe Inductive Logic Programming or something of that nature
,2.0,"<p>Have there been any studies which attempted to use AI algorithms to detect human thoughts or emotions based on brain activity, such as using <a href=""https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface#EEG-based"" rel=""nofollow"">BCI/EEG devices</a>?</p>

<p>By this, I mean simple guesses such as whether the person was happy or angry, or what object (e.g. banana, car) they were thinking about.</p>

<p>If so, did any of those studies show some degree of success?</p>
",,5,2016-08-04T01:39:20.737,2.0,1274,2016-08-23T15:25:23.607,2016-08-17T13:03:50.990,,145.0,,8.0,,1,-1,<research><signal-processing>,Are there any studies which attempt to use AI to guess the human emotion based on the brainwaves?,97.0,72.36,9.51,9.93,0.0,0.0,13.0,Have there been any studies which attempted to use AI algorithms to detect human thoughts or emotions based on brain activity such as using BCIEEG devices By this I mean simple guesses such as whether the person was happy or angry or what object eg banana car they were thinking about If so did any of those studies show some degree of success
,,,,0,2016-08-04T04:07:42.000,,1275,2016-08-04T04:07:42.000,2016-08-04T04:07:42.000,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions about algorithm's used in creating AIs.,,0,2016-08-04T04:07:42.000,,1276,2016-08-12T06:40:17.923,2016-08-12T06:40:17.923,,145.0,,145.0,,4,0,,,,63.36,14.0,13.9,0.0,0.0,2.0,For questions about algorithms used in creating AIs
,,,,0,2016-08-04T04:10:33.107,,1277,2016-08-04T04:10:33.107,2016-08-04T04:10:33.107,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,,,0,2016-08-04T04:10:33.107,,1278,2016-08-04T04:10:33.107,2016-08-04T04:10:33.107,,-1.0,,-1.0,,4,0,,,,,,,,,,
,,,,0,2016-08-04T04:18:57.577,,1279,2016-08-04T04:18:57.577,2016-08-04T04:18:57.577,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,,,0,2016-08-04T04:18:57.577,,1280,2016-08-04T04:18:57.577,2016-08-04T04:18:57.577,,-1.0,,-1.0,,4,0,,,,,,,,,,
,,"<p>The <strong>Turing test</strong> was designed in 1950 by Alan Turing, to evaluate a machine's ability to exhibit <a href=""/questions/tagged/natural-language"" class=""post-tag"" title=""show questions tagged &#39;natural-language&#39;"" rel=""tag"">natural-language</a>. He introduced the test in his essay Computing Machinery and Intelligence, which can be read online <a href=""http://www.loebner.net/Prizef/TuringArticle.html"" rel=""nofollow"">here</a>. </p>

<p>The test has a human and an AI have a conversation, which is limited to on-screen text, and then a judge will look over the transcript, and try to see if they can tell who was who in the conversation.</p>
",,0,2016-08-04T04:38:36.787,,1281,2016-08-30T19:44:09.247,2016-08-30T19:44:09.247,,29.0,,145.0,,5,0,,,,54.56,9.7,9.51,0.0,0.0,11.0,The Turing test was designed in 1950 by Alan Turing to evaluate a machines ability to exhibit naturallanguage He introduced the test in his essay Computing Machinery and Intelligence which can be read online here The test has a human and an AI have a conversation which is limited to onscreen text and then a judge will look over the transcript and try to see if they can tell who was who in the conversation
,,"For questions about the Turing Test, a test designed to see whether a third-party person can identify, from a written transcript, who was the AI and who was the human is an human/AI conversation.",,0,2016-08-04T04:38:36.787,,1282,2016-08-30T19:44:22.377,2016-08-30T19:44:22.377,,29.0,,145.0,,4,0,,,,53.89,10.92,11.36,0.0,0.0,6.0,For questions about the Turing Test a test designed to see whether a thirdparty person can identify from a written transcript who was the AI and who was the human is an humanAI conversation
,,,,0,2016-08-04T04:41:55.367,,1283,2016-08-04T04:41:55.367,2016-08-04T04:41:55.367,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,"practice of decomposing a complex spoken sentence it into smaller lexical segments taking the meaning to each segment, and combining those meanings according to language and grammar rules to understand its meaning",,0,2016-08-04T04:41:55.367,,1284,2016-08-04T13:18:18.887,2016-08-04T13:18:18.887,,1256.0,,1256.0,,4,0,,,,30.54,16.31,14.6,0.0,0.0,1.0,practice of decomposing a complex spoken sentence it into smaller lexical segments taking the meaning to each segment and combining those meanings according to language and grammar rules to understand its meaning
1315.0,1.0,"<p>Has there been any attempts to deploy AI with blockchain technology? </p>

<p>Are there any decentralized examples of AI networks with no central point of control with AI nodes acting independently (but according to a codified set of rules) creating, validating and storing the same shared decentralized database in many locations around the world?</p>
",,0,2016-08-04T05:07:03.323,1.0,1285,2016-08-04T14:23:37.070,,,,,1256.0,,1,7,<untagged>,Artificial Intelligence on the blockchain,86.0,36.12,14.45,12.1,0.0,0.0,5.0,Has there been any attempts to deploy AI with blockchain technology Are there any decentralized examples of AI networks with no central point of control with AI nodes acting independently but according to a codified set of rules creating validating and storing the same shared decentralized database in many locations around the world
,,"<p>I am assuming each image contains a single object.</p>

<p>It is possible, however, it is not as easy as you might think. Firstly, you need extract as many features as possible: original image, <a href=""https://en.wikipedia.org/wiki/Local_binary_patterns"" rel=""nofollow"">LBP</a>, <a href=""https://en.wikipedia.org/wiki/Scale-invariant_feature_transform"" rel=""nofollow"">SIFT</a>, <a href=""https://en.wikipedia.org/wiki/Image_moment#Moment_invariants"" rel=""nofollow"">moments</a>, contour descriptors to name a few. Than concatenate these features into a single feature vector. After this step, use clustering. You will need a lot of samples to compensate for the number of features. After clustering, use a correlation method to find which features are related to each cluster.</p>

<p>If you need features to classify within a cluster, you could do a second clustering with full set of features and apply the same method. The features that are selected for a cluster will not be suitable to classify within the cluster.</p>
",,0,2016-08-04T06:25:20.620,,1286,2016-08-04T06:25:20.620,,,,,210.0,249.0,2,1,,,,65.62,10.08,9.68,0.0,0.0,20.0,I am assuming each image contains a single object It is possible however it is not as easy as you might think Firstly you need extract as many features as possible original image LBP SIFT moments contour descriptors to name a few Than concatenate these features into a single feature vector After this step use clustering You will need a lot of samples to compensate for the number of features After clustering use a correlation method to find which features are related to each cluster If you need features to classify within a cluster you could do a second clustering with full set of features and apply the same method The features that are selected for a cluster will not be suitable to classify within the cluster
,,"<ul>
<li><strong>MLP</strong>: uses dot products (between inputs and weights) and <a href=""https://en.wikipedia.org/wiki/Sigmoid_function"" rel=""nofollow noreferrer"">sigmoidal activation functions</a> (or other monotonic functions such as <a href=""https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"" rel=""nofollow noreferrer"">ReLU</a>) and training is usually done through backpropagation for all layers (which can be as many as you want). This type of neural network is used in deep learning with the help of many techniques (such as dropout or batch normalization);</li>
<li><strong>RBF</strong>: uses Euclidean distances (between inputs and weights, which can be viewed as centers) and (usually) <a href=""https://en.wikipedia.org/wiki/Gaussian_function"" rel=""nofollow noreferrer"">Gaussian activation functions</a> (which could be multivariate), which makes neurons more locally sensitive. Thus, RBF neurons have maximum activation when the center/weights are equal to the inputs (look at the image below). Due to this property, RBF neural networks are good for novelty detection (if each neuron is centered on a training example, inputs far away from all neurons constitute novel patterns) but not so good at extrapolation. Also, RBFs may use backpropagation for learning, or hybrid approaches with unsupervised learning in the hidden layer (they usually have just 1 hidden layer). Finally, RBFs make it easier to <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.294.5088&amp;rep=rep1&amp;type=pdf"" rel=""nofollow noreferrer"">grow new neurons</a> during training.</li>
</ul>

<p><a href=""https://i.stack.imgur.com/9u5fQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9u5fQ.jpg"" alt=""enter image description here""></a></p>
",,3,2016-08-04T06:43:19.760,,1287,2016-08-04T07:03:45.203,2016-08-04T07:03:45.203,,144.0,,144.0,227.0,2,5,,,,49.79,14.4,10.34,0.0,0.0,38.0,MLP uses dot products between inputs and weights and sigmoidal activation functions or other monotonic functions such as ReLU and training is usually done through backpropagation for all layers which can be as many as you want This type of neural network is used in deep learning with the help of many techniques such as dropout or batch normalization RBF uses Euclidean distances between inputs and weights which can be viewed as centers and usually Gaussian activation functions which could be multivariate which makes neurons more locally sensitive Thus RBF neurons have maximum activation when the centerweights are equal to the inputs look at the image below Due to this property RBF neural networks are good for novelty detection if each neuron is centered on a training example inputs far away from all neurons constitute novel patterns but not so good at extrapolation Also RBFs may use backpropagation for learning or hybrid approaches with unsupervised learning in the hidden layer they usually have just 1 hidden layer Finally RBFs make it easier to grow new neurons during training
,1.0,"<p>In their famous book entitled ""<em>Perceptrons: An Introduction to Computational Geometry</em>"", Minsky and Papert show that a perceptron can't solve the XOR problem. This contributed to the first AI winter, resulting in funding cuts for neural networks. However, now we know that a multilayer perceptron can solve the XOR problem easily.</p>

<p>Backprop wasn't known at the time, but did they know about manually building multilayer perceptrons? Did Minsky &amp; Papert know that multilayer perceptrons could solve XOR at the time they wrote the book, albeit not knowing how to train it?</p>
",,4,2016-08-04T07:34:06.557,,1288,2016-08-07T09:42:52.590,2016-08-04T08:13:56.207,,144.0,,144.0,,1,2,<neural-networks><history>,Did Minsky & Papert know that multilayer perceptrons could solve XOR?,73.0,61.46,13.46,9.4,0.0,0.0,17.0,In their famous book entitled Perceptrons An Introduction to Computational Geometry Minsky and Papert show that a perceptron cant solve the XOR problem This contributed to the first AI winter resulting in funding cuts for neural networks However now we know that a multilayer perceptron can solve the XOR problem easily Backprop wasnt known at the time but did they know about manually building multilayer perceptrons Did Minsky amp Papert know that multilayer perceptrons could solve XOR at the time they wrote the book albeit not knowing how to train it
,2.0,"<p>According to wikipedia <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"" rel=""nofollow noreferrer"">Artificial general intelligence(AGI)</a></p>

<blockquote>
  <p>Artificial general intelligence (AGI) is the intelligence of a
  (hypothetical) machine that could successfully perform any
  intellectual task that a human being can. </p>
</blockquote>

<p>According to below image todays artifical intellgence is same as that of a lizards.</p>

<p><a href=""https://i.stack.imgur.com/gddKB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gddKB.jpg"" alt=""enter image description here""></a></p>

<p>Lets assume(or not) that within 10-20 years we humans are successful in creating a AGI or AGIs. As AGI has the same intelligence and <a href=""http://futurism.com/scientist-claims-to-be-on-the-verge-of-making-an-ai-that-feels-true-emotions/"" rel=""nofollow noreferrer"">emotions</a> as that of humans because according to wikipedia definition it can perform same intellectual task of a human. Then can we destroy an AGI without its consent? Do this be considered as murder?</p>
",,1,2016-08-04T07:41:16.090,,1289,2017-03-11T00:14:25.140,,,,,39.0,,1,1,<ethics>,Can we destroy artificial general intelligence without its consent?,60.0,37.5,13.45,10.25,0.0,0.0,15.0,According to wikipedia Artificial general intelligenceAGI Artificial general intelligence AGI is the intelligence of a hypothetical machine that could successfully perform any intellectual task that a human being can According to below image todays artifical intellgence is same as that of a lizards Lets assumeor not that within 1020 years we humans are successful in creating a AGI or AGIs As AGI has the same intelligence and emotions as that of humans because according to wikipedia definition it can perform same intellectual task of a human Then can we destroy an AGI without its consent Do this be considered as murder
,2.0,"<p>Deep Mind has published a lot of works on deep learning in the last years, most of them state-of-the-art on their respective tasks. But how much of this work has actually been reproduced by the AI community? For instance, the Neural Turing Machine paper seems to be very hard to reproduce, according to other researchers.</p>
",,3,2016-08-04T07:43:33.010,,1290,2016-10-28T09:00:13.520,,,,,144.0,,1,3,<neural-networks><deep-learning><research>,How much of Deep Mind's work is actually reproducible?,134.0,69.82,10.85,9.42,0.0,0.0,9.0,Deep Mind has published a lot of works on deep learning in the last years most of them stateoftheart on their respective tasks But how much of this work has actually been reproduced by the AI community For instance the Neural Turing Machine paper seems to be very hard to reproduce according to other researchers
,,"<p><em>I tend to think this question is border-line and may get close. A few comments for now, though.</em></p>

<hr>

<p>There are (at least) two issues with reproducing the work of a company like DeepMind:</p>

<ul>
<li>Technicalities missing from publications.</li>
<li>Access to the same level of data.</li>
</ul>

<p>Technicalities should be workable. Some people have reproduced some of the <a href=""https://github.com/kristjankorjus/Replicating-DeepMind"" rel=""nofollow"">Atari</a> gaming stunts. AlphaGo is seemingly more complex and will require more work, yet that should be feasible at some point in the future (individuals may lack computing resources today).</p>

<p>Data can be more tricky. Several companies open their data sets, but data is also the nerve of the competition...</p>
",,2,2016-08-04T07:58:33.453,,1292,2016-08-04T07:58:33.453,,,,,169.0,1290.0,2,3,,,,59.6,10.88,9.63,0.0,0.0,20.0,I tend to think this question is borderline and may get close A few comments for now though There are at least two issues with reproducing the work of a company like DeepMind Technicalities missing from publications Access to the same level of data Technicalities should be workable Some people have reproduced some of the Atari gaming stunts AlphaGo is seemingly more complex and will require more work yet that should be feasible at some point in the future individuals may lack computing resources today Data can be more tricky Several companies open their data sets but data is also the nerve of the competition
,,"<p>Firstly, an AGI could conceivably exhibit all of the observable properties of intelligence without being conscious. Although that may seem counter-intuitive, at present we have no physical theory that allows us to detect consciousness (philosophically speaking, a <a href=""http://plato.stanford.edu/entries/zombies/"" rel=""nofollow noreferrer"">'Zombie'</a> is indistinguishable from a non-Zombie - see the writing of <a href=""http://rads.stackoverflow.com/amzn/click/0316180661"" rel=""nofollow noreferrer"">Daniel Dennett</a> and <a href=""http://consc.net/books/tcm/"" rel=""nofollow noreferrer"">David Chalmers</a> for more on this). Destroying a non-conscious entity has the same moral cost as destroying a chair.</p>

<p>Also, note that 'destroy' doesn't necessarily mean the same for entities with <em>persistent substrate</em>, i.e. their 'brain state' can be reversibly serialized to some other storage medium and/or multiple copies of them can co-exist. So if by 'destroy' we simply mean 'switch off', then an AGI might conceivably be reassured of a subsequent re-awakening. Douglas Hofstadter gives an interesting description of such an 'episodic consciousness' in <a href=""http://themindi.blogspot.co.uk/2007/02/chapter-26-conversation-with-einsteins.html"" rel=""nofollow noreferrer"">""A Conversation with Einstein's Brain""</a></p>

<p>If by 'destroy', we mean 'irrevocably erase with no chance of re-awakening', then (unless we have a physical test which proves it is <em>not</em> conscious) destroying an entity with a seemingly human-level awareness is clearly morally tantamount to murder. To believe otherwise would be <em>substrate-ist</em> - a moral stance which may one day be seen as antiquated as racism.</p>
",,3,2016-08-04T08:06:28.653,,1293,2016-12-03T07:46:15.380,2016-12-03T07:46:15.380,,3989.0,,42.0,1289.0,2,4,,,,37.84,15.44,11.09,0.0,0.0,52.0,Firstly an AGI could conceivably exhibit all of the observable properties of intelligence without being conscious Although that may seem counterintuitive at present we have no physical theory that allows us to detect consciousness philosophically speaking a Zombie is indistinguishable from a nonZombie see the writing of Daniel Dennett and David Chalmers for more on this Destroying a nonconscious entity has the same moral cost as destroying a chair Also note that destroy doesnt necessarily mean the same for entities with persistent substrate ie their brain state can be reversibly serialized to some other storage medium andor multiple copies of them can coexist So if by destroy we simply mean switch off then an AGI might conceivably be reassured of a subsequent reawakening Douglas Hofstadter gives an interesting description of such an episodic consciousness in A Conversation with Einsteins Brain If by destroy we mean irrevocably erase with no chance of reawakening then unless we have a physical test which proves it is not conscious destroying an entity with a seemingly humanlevel awareness is clearly morally tantamount to murder To believe otherwise would be substrateist a moral stance which may one day be seen as antiquated as racism
1313.0,1.0,"<p>Geoffrey Hinton has been researching something he calls ""capsules theory"" in neural networks. What is this and how does it work?</p>
",,0,2016-08-04T08:28:06.277,2.0,1294,2016-08-04T18:48:54.517,,,,,144.0,,1,4,<neural-networks>,"How does Hinton's ""capsules theory"" work?",183.0,77.74,11.05,10.17,0.0,0.0,4.0,Geoffrey Hinton has been researching something he calls capsules theory in neural networks What is this and how does it work
1299.0,1.0,"<p>During my research, I've stumbled upon ""complex-valued neural networks"", which are neural networks that work with complex-valued inputs (probably weights too). What are the advantages (or simply the applications) of this kind of neural network over real-valued neural networks?</p>
",,0,2016-08-04T08:32:36.610,3.0,1295,2016-08-04T10:28:59.957,,,,,144.0,,1,7,<neural-networks>,What are the advantages of complex-valued neural networks?,66.0,43.22,18.27,11.89,0.0,0.0,14.0,During my research Ive stumbled upon complexvalued neural networks which are neural networks that work with complexvalued inputs probably weights too What are the advantages or simply the applications of this kind of neural network over realvalued neural networks
,1.0,"<p>The <a href=""http://fabelier.org/novelty-search-and-open-ended-evolution-by-ken-stanley/"" rel=""nofollow"">author</a> claims that guiding evolution by novelty alone (without explicit goals) can solve problems even better than using explicit goals. In other words, using a novelty measure as a fitness function for a genetic algorithm works better than a goal-directed fitness function. How is that possible?</p>
",,0,2016-08-04T08:39:43.740,,1296,2016-08-04T11:10:36.090,,,,,144.0,,1,2,<genetic-algorithms>,"How does ""novelty search"" work?",35.0,55.54,13.74,11.13,0.0,0.0,7.0,The author claims that guiding evolution by novelty alone without explicit goals can solve problems even better than using explicit goals In other words using a novelty measure as a fitness function for a genetic algorithm works better than a goaldirected fitness function How is that possible
1327.0,2.0,"<p>Quote from this <a href=""http://meta.ai.stackexchange.com/a/46/8"">Eric's meta post</a> about modelling and implementation:</p>

<blockquote>
  <p>They are not exactly the same, although strongly related. This was a very difficult lesson to learn among mathematicians and early programmers, notably in the 70s (mathematical proofs can demand a lot of non-trivial programming work to make them ""computable"", as in runnable on a computer).</p>
</blockquote>

<p>If they're not the same, what is the difference?</p>

<p>How we can say when we're talking about AI implementation, and when about modelling? It's suggested above it's not easy task. So where we can draw the line when we talk about it?</p>

<p>I'm asking in general, not specifically for this site, that's why I haven't posted question in meta</p>
",,0,2016-08-04T09:22:21.360,,1297,2016-08-08T04:44:18.407,2016-08-04T09:28:43.907,,8.0,,8.0,,1,2,<models><comparison><implementation>,How to distinguish AI modeling from implementation?,39.0,63.29,11.31,9.53,0.0,0.0,27.0,Quote from this Erics meta post about modelling and implementation They are not exactly the same although strongly related This was a very difficult lesson to learn among mathematicians and early programmers notably in the 70s mathematical proofs can demand a lot of nontrivial programming work to make them computable as in runnable on a computer If theyre not the same what is the difference How we can say when were talking about AI implementation and when about modelling Its suggested above its not easy task So where we can draw the line when we talk about it Im asking in general not specifically for this site thats why I havent posted question in meta
,,"<p>I might be wrong, but I do not believe that something of the scope you describe would be possible with the current state of technology. It would require a lot of things which are still in relatively early stages of research.</p>

<p>For one, just extracting relevant information from text is a huge task by itself. Doubly so with a novel which contains a large amount of unimportant details.</p>

<p>It might perhaps be easier if the input was presented in the form of some sort of list of important facts. But it would still be rather difficult for the AI to connect them and find a solution.</p>

<p>As an example, let's say that we have these two facts:</p>

<ul>
<li>Alice died of a snake bite.</li>
<li>Bob was seen buying a couple of mice recently.</li>
</ul>

<p>To a human, it seems obvious that the mice were bought to feed a venomous snake. However, it would probably require a tremendous effort to teach an AI to make such connections.</p>

<p>Disclaimer: I don't do text processing myself, so I'm not quite up-to-date on the current state-of-the-art. It's possible that some of these things have already been done in some form. If anyone knows more about the subject, please correct me if I'm wrong.</p>
",,2,2016-08-04T09:52:17.587,,1298,2016-08-04T09:59:50.663,2016-08-04T09:59:50.663,,30.0,,30.0,241.0,2,3,,,,72.26,8.64,8.39,0.0,0.0,32.0,I might be wrong but I do not believe that something of the scope you describe would be possible with the current state of technology It would require a lot of things which are still in relatively early stages of research For one just extracting relevant information from text is a huge task by itself Doubly so with a novel which contains a large amount of unimportant details It might perhaps be easier if the input was presented in the form of some sort of list of important facts But it would still be rather difficult for the AI to connect them and find a solution As an example lets say that we have these two facts Alice died of a snake bite Bob was seen buying a couple of mice recently To a human it seems obvious that the mice were bought to feed a venomous snake However it would probably require a tremendous effort to teach an AI to make such connections Disclaimer I dont do text processing myself so Im not quite uptodate on the current stateoftheart Its possible that some of these things have already been done in some form If anyone knows more about the subject please correct me if Im wrong
,,"<p>According to <a href=""http://link.springer.com/chapter/10.1007/3-540-44989-2_118"">this paper</a>, complex-valued ANNs (C-ANNs) can solve problems such as XOR and symmetry detection with a smaller number of layers than real ANNs (for both of these a 2 layer C-ANN suffices, whereas a 3-layer R-ANN is required).</p>

<p>I believe that it is still an open question as to how useful this result is in practice (e.g. whether it actually makes finding the right topology easier), so at present the key practical advantage of C-ANNs is when they are a closer model for the problem domain.</p>

<p>Application areas are then where complex values arise naturally, e.g. in optics, signal processing/FFT or electrical engineering.</p>
",,5,2016-08-04T10:28:59.957,,1299,2016-08-04T10:28:59.957,,,,,42.0,1295.0,2,6,,,,58.62,11.84,10.84,0.0,0.0,25.0,According to this paper complexvalued ANNs CANNs can solve problems such as XOR and symmetry detection with a smaller number of layers than real ANNs for both of these a 2 layer CANN suffices whereas a 3layer RANN is required I believe that it is still an open question as to how useful this result is in practice eg whether it actually makes finding the right topology easier so at present the key practical advantage of CANNs is when they are a closer model for the problem domain Application areas are then where complex values arise naturally eg in optics signal processingFFT or electrical engineering
,,"<p>As explained in an answer to <a href=""http://ai.stackexchange.com/questions/240/what-exactly-are-genetic-algorithms-and-what-sort-of-problems-are-they-good-for/244#244"">this AI SE question</a>, GAs are 'satisficers' rather than 'optimizers' and tend not to explore 'outlying' regions of the search space. Rather, the population tends to cluster in regions that are 'fairly good' according to the fitness function.</p>

<p>In contrast, I believe the thinking is that novelty affords a kind of dynamic fitness, tending to push the population away from previously discovered areas.</p>
",,0,2016-08-04T11:10:36.090,,1300,2016-08-04T11:10:36.090,,,,,42.0,1296.0,2,1,,,,48.13,13.12,11.18,0.0,0.0,15.0,As explained in an answer to this AI SE question GAs are satisficers rather than optimizers and tend not to explore outlying regions of the search space Rather the population tends to cluster in regions that are fairly good according to the fitness function In contrast I believe the thinking is that novelty affords a kind of dynamic fitness tending to push the population away from previously discovered areas
1337.0,1.0,"<p>Given pictures with multiple features such as faces, can single AI algorithm detect all of them, or for better reliability is it preferred to use separate instances?</p>

<p>In other words I'm talking about attempt of finding all possible human faces on the same picture by a single neural network.</p>
",,2,2016-08-04T12:10:26.593,,1301,2016-08-04T21:07:01.750,2016-08-04T21:07:01.750,,127.0,,8.0,,1,1,<deep-network><algorithm><image-recognition>,Do you need single or multiple networks to detect multiple faces?,32.0,46.61,11.67,12.59,0.0,0.0,5.0,Given pictures with multiple features such as faces can single AI algorithm detect all of them or for better reliability is it preferred to use separate instances In other words Im talking about attempt of finding all possible human faces on the same picture by a single neural network
1305.0,2.0,"<p>I read some information<sup>1</sup> about attempts to build neural networks in the PHP programming language. Personally I think PHP is not the right language to do so at all probably because it's a high-level language, I assume low level language are way more suitable for AI in terms of performance and scalability. </p>

<p>Is there a good/logical reason why you should or shouldn't use PHP as a language to write AI in?</p>

<p><em><sup>1</sup></em> <a href=""http://www.developer.com/lang/php/creating-neural-networks-in-php.html"" rel=""nofollow"">http://www.developer.com/lang/php/creating-neural-networks-in-php.html</a> and <a href=""http://stackoverflow.com/questions/2303357/are-there-any-artificial-intelligence-projects-in-php-out-there"">http://stackoverflow.com/questions/2303357/are-there-any-artificial-intelligence-projects-in-php-out-there</a> </p>
",2016-08-04T14:55:53.457,3,2016-08-04T12:28:07.427,,1303,2016-10-31T09:06:48.290,,,,,217.0,,1,1,<neural-networks><programming-languages>,Can PHP be considered as a serious programming language for AI?,172.0,27.01,22.33,9.2,0.0,0.0,37.0,I read some information1 about attempts to build neural networks in the PHP programming language Personally I think PHP is not the right language to do so at all probably because its a highlevel language I assume low level language are way more suitable for AI in terms of performance and scalability Is there a goodlogical reason why you should or shouldnt use PHP as a language to write AI in 1 httpwwwdevelopercomlangphpcreatingneuralnetworksinphphtml and httpstackoverflowcomquestions2303357arethereanyartificialintelligenceprojectsinphpoutthere
,,"<p><em>Question on-topicness questionable, but...</em></p>

<hr>

<p>The most logical reason why PHP is unsuited for neural networks is that PHP is, well, intended to be used for server side webpages. It can connect to various external resources, such as databases, via native language features. It is very much a glue language, and not a processing language. PHP is also mostly stateless, only allowing you to store state in either clients, file storage or databases.</p>

<p>As such, it's <strong>not</strong> suitable for this sort of thing - not because PHP is a high level language, but rather because it's so request based and focused towards creating pages to serve to clients.</p>

<p>That won't stop people from trying, though - there are various esoteric programming languages out there in which regular programming would be an insane task or not possible at all - but from a ease of development perspective, making a neural network in PHP makes no sense.</p>
",,1,2016-08-04T12:35:10.167,,1305,2016-10-31T09:06:48.290,2016-10-31T09:06:48.290,,74.0,,74.0,1303.0,2,5,,,,58.01,11.66,10.46,0.0,0.0,28.0,Question ontopicness questionable but The most logical reason why PHP is unsuited for neural networks is that PHP is well intended to be used for server side webpages It can connect to various external resources such as databases via native language features It is very much a glue language and not a processing language PHP is also mostly stateless only allowing you to store state in either clients file storage or databases As such its not suitable for this sort of thing not because PHP is a high level language but rather because its so request based and focused towards creating pages to serve to clients That wont stop people from trying though there are various esoteric programming languages out there in which regular programming would be an insane task or not possible at all but from a ease of development perspective making a neural network in PHP makes no sense
1312.0,1.0,"<p>I've found <a href=""http://link.springer.com/chapter/10.1007%2F978-1-4613-1009-9_2"" rel=""nofollow"">this old scientific paper from 1988</a> about introduction of AI into nuclear power fields.</p>

<p>Were or still are there any dangers by application of such algorithm? Are nuclear power plants or human life in risk if the algorithm will fail?</p>

<p>Especially applications to the core, like cooling systems and other components which can be affected in negative way.</p>
",,4,2016-08-04T12:40:22.857,,1306,2016-08-04T14:10:15.490,2016-08-04T14:10:15.490,,8.0,,8.0,,1,1,<applications>,What are the dangers of AI applications for nuclear industry?,24.0,56.25,11.3,9.91,0.0,0.0,6.0,Ive found this old scientific paper from 1988 about introduction of AI into nuclear power fields Were or still are there any dangers by application of such algorithm Are nuclear power plants or human life in risk if the algorithm will fail Especially applications to the core like cooling systems and other components which can be affected in negative way
,,"<p>Actually, yes. Remember, that due to the history of PHP development, some very good things has formed what we have now:</p>

<ul>
<li><p>From a simple/laggy/limited interpreter in PHP 3, we have now three mainstream lines coming one-by-one like v5/v6/v7 with <em>full bytecode</em> supported.   </p></li>
<li><p>In PHP v7 you don't even need a bytecode cache due to HHVM, old Zend VM is a hell-good-debugged and using a cacher like XCache you can achieve a true native execution speed <strong>and</strong> payload</p></li>
<li><p>The PHP language interface allows <strong>any</strong> external C/C++ library <em>just to be added</em> as a module via very simple wrapper that can be written by the person that just red Kerrigan&amp;Richie and Straustrup base books on C and C++. This is amazing feature, exclusive to PHP as far as I know</p></li>
<li><p>In PHP v7 you're welcome to use <em>native</em> multi-threading and even CUDA-based things, if you wish to do it. I did it, so I can confirm that it works</p></li>
</ul>
",,2,2016-08-04T12:50:23.533,,1307,2016-08-04T12:50:23.533,,,,,1263.0,1303.0,2,2,,,,48.61,9.99,9.91,0.0,0.0,32.0,Actually yes Remember that due to the history of PHP development some very good things has formed what we have now From a simplelaggylimited interpreter in PHP 3 we have now three mainstream lines coming onebyone like v5v6v7 with full bytecode supported In PHP v7 you dont even need a bytecode cache due to HHVM old Zend VM is a hellgooddebugged and using a cacher like XCache you can achieve a true native execution speed and payload The PHP language interface allows any external CC library just to be added as a module via very simple wrapper that can be written by the person that just red KerriganampRichie and Straustrup base books on C and C This is amazing feature exclusive to PHP as far as I know In PHP v7 youre welcome to use native multithreading and even CUDAbased things if you wish to do it I did it so I can confirm that it works
,0.0,"<p>How likely AI can fully replace pilots on commercial flights (including take off, landing and parking)?</p>

<p>Since we've self-driving cars already, is it likely to happen to commercial planes as well?</p>
",2016-08-04T16:02:53.503,2,2016-08-04T12:58:01.810,,1308,2016-08-04T12:58:01.810,,,,,8.0,,1,1,<untagged>,Would ever AI replace pilots on commercial flights?,23.0,55.74,13.28,10.52,0.0,0.0,8.0,How likely AI can fully replace pilots on commercial flights including take off landing and parking Since weve selfdriving cars already is it likely to happen to commercial planes as well
,,"<p>Generally agree with @Inquisitive Lurker, but I think we also have a wide range of potential abilities/requirements. As with computer chess or Go, where there's a big difference between ""beating an honest novice human "" and ""beating all humans""; there's a big difference between solving a simple kids' mystery and a complex adult novel.</p>

<p>So I don't think there would be any problem writing a program that could solve a problem that is listed as a list of statements, or laid out as a (very young) children's book. However something like an Agatha Christie or John Le Carre's ""Tinker Tailor Soldier Spy"" (relatively simple solution, but the story is told in a complex manner) are far in the future.</p>

<p>Sometimes an alternative approach might work. For example a neural network could probably solve all Colombo mysteries at the ""Who did it?"" level without a full ""Why?"" explanation, after only reading a few Colombo mysteries. The same is true for most kids!</p>
",,0,2016-08-04T13:40:05.177,,1310,2016-08-04T13:40:05.177,,,,,132.0,241.0,2,2,,,,53.41,11.13,10.14,0.0,0.0,37.0,Generally agree with Inquisitive Lurker but I think we also have a wide range of potential abilitiesrequirements As with computer chess or Go where theres a big difference between beating an honest novice human and beating all humans theres a big difference between solving a simple kids mystery and a complex adult novel So I dont think there would be any problem writing a program that could solve a problem that is listed as a list of statements or laid out as a very young childrens book However something like an Agatha Christie or John Le Carres Tinker Tailor Soldier Spy relatively simple solution but the story is told in a complex manner are far in the future Sometimes an alternative approach might work For example a neural network could probably solve all Colombo mysteries at the Who did it level without a full Why explanation after only reading a few Colombo mysteries The same is true for most kids
,,"<p>As per this <a href=""http://www.dailymail.co.uk/sciencetech/article-2095214/As-scientists-discover-translate-brainwaves-words--Could-machine-read-innermost-thoughts.html"" rel=""nofollow"">site</a></p>

<blockquote>
  <p>Researchers recorded the complex patterns of electrical activity generated by someone’s brain, as the subject listened to someone talking.
  By feeding those brainwave patterns into a computer, they were able to translate them back into actual words — the same words that the volunteer had been hearing.</p>
  
  <p><strong>The scientists behind the work believe they can now go further and read the unspoken thoughts of people using electrodes placed against the brain.</strong></p>
  
  <p>In the experiment, each patient listened to a recording of spoken words for five to ten minutes, while the net of electrodes placed under their skull monitored activity in a part of the brain involved in understanding speech called <a href=""https://en.wikipedia.org/wiki/Wernicke%27s_area"" rel=""nofollow"">Wernicke’s area</a>.</p>
  
  <p>In one experiment, volunteers looked at black-and-white photographs while the scanner monitored activity in part of the brain that handles vision called the primary visual cortex.
  A computer predicted accurately the image that the person was looking at purely from the pattern of brain activity.</p>
</blockquote>

<p>So AI might be able to read our emotions as well in near future.</p>

<p>I found that <a href=""http://electronics.howstuffworks.com/gadgets/high-tech-gadgets/google-glass-detect-emotions.htm"" rel=""nofollow"">google glasses can detect people's emotion</a> via facial expression, voice tone e.t.c, (just like us), obviously not what they are thinking in their brain.</p>
",,1,2016-08-04T14:00:13.253,,1311,2016-08-23T15:25:23.607,2016-08-23T15:25:23.607,,145.0,,72.0,1274.0,2,5,,,,48.74,12.89,10.53,0.0,0.0,23.0,As per this site Researchers recorded the complex patterns of electrical activity generated by someone’s brain as the subject listened to someone talking By feeding those brainwave patterns into a computer they were able to translate them back into actual words — the same words that the volunteer had been hearing The scientists behind the work believe they can now go further and read the unspoken thoughts of people using electrodes placed against the brain In the experiment each patient listened to a recording of spoken words for five to ten minutes while the net of electrodes placed under their skull monitored activity in a part of the brain involved in understanding speech called Wernicke’s area In one experiment volunteers looked at blackandwhite photographs while the scanner monitored activity in part of the brain that handles vision called the primary visual cortex A computer predicted accurately the image that the person was looking at purely from the pattern of brain activity So AI might be able to read our emotions as well in near future I found that google glasses can detect peoples emotion via facial expression voice tone etc just like us obviously not what they are thinking in their brain
,,"<p>Any technology in the nuclear industry represents variance--it may be an improvement in safety or efficiency, or it may contain some unseen defect that allows a catastrophe to happen.</p>

<p>But the simple <em>possibility</em> of harm isn't enough to swing the decision one way or the other. The application of AI methods--whether to the real-time control of plant variables, or the early detection of problems, or to the design of plants and their components--seems likely to be as beneficial as in other realms.</p>

<p>For example, check out the <a href=""http://www.lasar.polimi.it/?page_id=798"" rel=""nofollow"">publication list</a> of a lab active in this area. Their paper I'm most familiar with is one in which they build a fault detector paired with a fault library classifier, so that the operators can be alerted not just that something is abnormal but what fault has probably occurred. This is done in such a way that standardized plants (such as, say, the French nuclear system) can share records with each other, meaning that <em>any</em> plant has the experience of <em>every</em> plant at their fingertips.</p>
",,0,2016-08-04T14:04:35.743,,1312,2016-08-04T14:04:35.743,,,,,10.0,1306.0,2,2,,,,50.8,11.56,10.38,0.0,0.0,25.0,Any technology in the nuclear industry represents varianceit may be an improvement in safety or efficiency or it may contain some unseen defect that allows a catastrophe to happen But the simple possibility of harm isnt enough to swing the decision one way or the other The application of AI methodswhether to the realtime control of plant variables or the early detection of problems or to the design of plants and their componentsseems likely to be as beneficial as in other realms For example check out the publication list of a lab active in this area Their paper Im most familiar with is one in which they build a fault detector paired with a fault library classifier so that the operators can be alerted not just that something is abnormal but what fault has probably occurred This is done in such a way that standardized plants such as say the French nuclear system can share records with each other meaning that any plant has the experience of every plant at their fingertips
,,"<p>It appears to not be published yet; the best available online are <a href=""http://cseweb.ucsd.edu/~gary/cs200/s12/Hinton.pdf"" rel=""nofollow"">these slides</a> for <a href=""https://www.youtube.com/watch?v=TFIMqt0yT2I"" rel=""nofollow"">this talk</a>. (Several people reference an earlier talk with <a href=""http://techtv.mit.edu/collections/bcs/videos/30698-what-s-wrong-with-convolutional-nets"" rel=""nofollow"">this link</a>, but sadly it's broken at time of writing this answer.)</p>

<p>My impression is that it's an attempt to formalize and abstract the creation of subnetworks inside a neural network. That is, if you look at a standard neural network, layers are fully connected (that is, every neuron in layer 1 has access to every layer in neuron 0, and is itself accessed by every neuron in layer 2). But this isn't obviously useful; one might instead have, say, <em>n</em> parallel stacks of layers (the 'capsules') that each specializes on some separate task (which may itself require more than one layer to complete successfully).</p>

<p>If I'm imagining its results correctly, this more sophisticated graph topology seems like something that could easily increase both the effectiveness and the interpretability of the resulting network.</p>
",,0,2016-08-04T14:16:57.367,,1313,2016-08-04T18:48:54.517,2016-08-04T18:48:54.517,,10.0,,10.0,1294.0,2,6,,,,44.88,12.36,10.17,0.0,0.0,30.0,It appears to not be published yet the best available online are these slides for this talk Several people reference an earlier talk with this link but sadly its broken at time of writing this answer My impression is that its an attempt to formalize and abstract the creation of subnetworks inside a neural network That is if you look at a standard neural network layers are fully connected that is every neuron in layer 1 has access to every layer in neuron 0 and is itself accessed by every neuron in layer 2 But this isnt obviously useful one might instead have say n parallel stacks of layers the capsules that each specializes on some separate task which may itself require more than one layer to complete successfully If Im imagining its results correctly this more sophisticated graph topology seems like something that could easily increase both the effectiveness and the interpretability of the resulting network
1316.0,3.0,"<p>How much processing power is needed to emulate the human brain? More specifically, the neural simulation, such as communication between the neurons and processing certain data in real-time.</p>

<p>I understand that this may be a bit of speculation and it's not possible to be accurate, but I'm sure there is some data available or research studies which attempted to estimate it based on our current understanding of the human brain.</p>
",,1,2016-08-04T14:18:10.547,,1314,2016-08-17T13:03:43.787,2016-08-17T13:03:43.787,,145.0,,8.0,,1,4,<hardware><neuromorphic-computing>,How powerful a computer is required to simulate the human brain?,115.0,39.37,12.6,10.66,0.0,0.0,9.0,How much processing power is needed to emulate the human brain More specifically the neural simulation such as communication between the neurons and processing certain data in realtime I understand that this may be a bit of speculation and its not possible to be accurate but Im sure there is some data available or research studies which attempted to estimate it based on our current understanding of the human brain
,,"<p><a href=""https://en.wikipedia.org/wiki/Swarm_intelligence"">Swarm intelligence</a> is the term for systems where relatively simple agents work together to solve a complicated problem in a decentralized fashion.</p>

<p>In general, distributed computing methods are very important for dealing with problems at scale, and many of them embrace decentralization in a deep way. (Given the reality of hardware failure and the massive size of modern datasets relative to individual nodes, the less work is passed through a central bottleneck, the better.) While there are people interested in doing computation on the blockchain, it seems to me like it's unlikely to be competitive with computation in dedicated clusters (like AWS).</p>
",,1,2016-08-04T14:23:37.070,,1315,2016-08-04T14:23:37.070,,,,,10.0,1285.0,2,6,,,,28.67,14.39,11.87,0.0,0.0,14.0,Swarm intelligence is the term for systems where relatively simple agents work together to solve a complicated problem in a decentralized fashion In general distributed computing methods are very important for dealing with problems at scale and many of them embrace decentralization in a deep way Given the reality of hardware failure and the massive size of modern datasets relative to individual nodes the less work is passed through a central bottleneck the better While there are people interested in doing computation on the blockchain it seems to me like its unlikely to be competitive with computation in dedicated clusters like AWS
,,"<p>H+ magazine wrote an estimate <a href=""http://hplusmagazine.com/2009/04/07/brain-chip/"" rel=""nofollow"">in 2009</a> that seems broadly comparable to other things I've seen; they think the human brain is approximately 37 petaflops. A supercomputer larger than that 37 petaflop estimate <a href=""https://en.wikipedia.org/wiki/Sunway_TaihuLight"" rel=""nofollow"">exists today</a>.</p>

<p>But emulation is hard. See <a href=""http://stackoverflow.com/questions/471973/what-makes-building-game-console-emulators-so-hard"">this SO question about hardware emulation</a> or <a href=""http://www.tested.com/tech/gaming/2712-why-perfect-hardware-snes-emulation-requires-a-3ghz-cpu/"" rel=""nofollow"">this article</a> on emulating the SNES, in which they require <strong>140 times</strong> the processing power of the SNES chip to get it right. <a href=""http://gizmodo.com/an-83-000-processor-supercomputer-only-matched-one-perc-1045026757"" rel=""nofollow"">This 2013 article</a> claims that a second of human brain activity took 40 minutes to emulate on a 10 petaflop computer (a <em>2400-times</em> slowdown, not the 4-times slowdown one might naively expect).</p>

<p>And all this assumes that neurons are relatively simple objects! It could be that the amount of math we have to do to model a single neuron is actually much more than the flops estimate above. Or it could be the case that dramatic simplifications can be made, and if we knew what the brain was actually trying to accomplish we could do it much more cleanly and simply. (One advantage that ANNs have, for example, is that they are doing computations with much more precision than we expect biological neurons to have. But this means emulation is <em>harder</em>, not easier, while replacement <em>is</em> easier.)</p>
",,0,2016-08-04T14:35:16.053,,1316,2016-08-04T14:35:16.053,,,,,10.0,1314.0,2,5,,,,58.92,10.62,9.32,0.0,0.0,26.0,H magazine wrote an estimate in 2009 that seems broadly comparable to other things Ive seen they think the human brain is approximately 37 petaflops A supercomputer larger than that 37 petaflop estimate exists today But emulation is hard See this SO question about hardware emulation or this article on emulating the SNES in which they require 140 times the processing power of the SNES chip to get it right This 2013 article claims that a second of human brain activity took 40 minutes to emulate on a 10 petaflop computer a 2400times slowdown not the 4times slowdown one might naively expect And all this assumes that neurons are relatively simple objects It could be that the amount of math we have to do to model a single neuron is actually much more than the flops estimate above Or it could be the case that dramatic simplifications can be made and if we knew what the brain was actually trying to accomplish we could do it much more cleanly and simply One advantage that ANNs have for example is that they are doing computations with much more precision than we expect biological neurons to have But this means emulation is harder not easier while replacement is easier
,,"<p>Not just how much, but what kind of processing power : there're specially-crafted <a href=""http://www.research.ibm.com/cognitive-computing/neurosynaptic-chips.shtml#fbid=V01grppeOIs"" rel=""nofollow"">dedicated chips</a>, and it has a <a href=""http://www.dailymail.co.uk/sciencetech/article-3516047/IBM-reveals-brain-supercomputer-neurosynaptic-chip-replicate-16-million-neurons-work-using-hearing-aid-battery.html"" rel=""nofollow"">practical applications</a>, so it's not a lab-only project</p>
",,0,2016-08-04T14:36:01.980,,1317,2016-08-04T14:36:01.980,,,,,1263.0,1314.0,2,2,,,,45.09,14.22,10.39,0.0,0.0,8.0,Not just how much but what kind of processing power therere speciallycrafted dedicated chips and it has a practical applications so its not a labonly project
1820.0,1.0,"<p><strong>The Situation:</strong>
A self-driving car is traveling at it's maximum speed, 25 mph (40 km/h), in the middle of an empty street with the ability to change lanes on both sides. There are two passengers, one in the front and another in the back.</p>

<p>Someone jumps from the side of the road directly into the path of the car. A collision would occur in 50 meters. <a href=""http://www.brake.org.uk/rsw/15-facts-a-resources/facts/1255-speed"" rel=""nofollow"">Breaking distance</a> at this speed is about 24m.</p>

<p><strong>The Question:</strong> Is it known how the current implementation of the Google Car AI would react, or is it currently a matter of speculation? A step-by-step explanation of the AI's decisioning process would be preferred.</p>

<p><strong>Possible Answers:</strong> The car could activate its brakes immediately, coming to a halt as quickly as possible. This would be sooner than a human could stop, as people require time to recognize the possibility of a collision, and then physically slam on the brake. (<em>thinking distance</em>).</p>

<p>Alternatively, the car could continue traveling forward, processing the situation. (Similar to a humans <em>thinking distance</em>). The person may continue to move, either out of the way, or still into danger of being hit. In this case, the car may decide to change lanes in an attempt to pass around the person.</p>

<p>Lastly and most unlikely, the car will not alter its course and proceed to drive forward.</p>

<p><sup>Do not attempt to do it to check;)</sup></p>
",,5,2016-08-04T15:10:41.230,,1318,2016-09-01T01:52:21.400,2016-08-31T23:17:29.040,,1812.0,,8.0,,1,1,<self-driving><decision-theory>,What would happen if someone jumped in the front of a Google car?,249.0,72.76,9.74,9.19,0.0,0.0,45.0,The Situation A selfdriving car is traveling at its maximum speed 25 mph 40 kmh in the middle of an empty street with the ability to change lanes on both sides There are two passengers one in the front and another in the back Someone jumps from the side of the road directly into the path of the car A collision would occur in 50 meters Breaking distance at this speed is about 24m The Question Is it known how the current implementation of the Google Car AI would react or is it currently a matter of speculation A stepbystep explanation of the AIs decisioning process would be preferred Possible Answers The car could activate its brakes immediately coming to a halt as quickly as possible This would be sooner than a human could stop as people require time to recognize the possibility of a collision and then physically slam on the brake thinking distance Alternatively the car could continue traveling forward processing the situation Similar to a humans thinking distance The person may continue to move either out of the way or still into danger of being hit In this case the car may decide to change lanes in an attempt to pass around the person Lastly and most unlikely the car will not alter its course and proceed to drive forward Do not attempt to do it to check
,,"<p>The simple answer is ""no, they aren't limited to images"": CNNs are also being used for natural language processing. (See <a href=""http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"" rel=""nofollow"">here</a> for an introduction.)</p>

<p>I haven't seen them applied to graphical data yet, but I haven't looked; there are some obvious things to try and so I'm optimistic that it would work.</p>
",,0,2016-08-04T15:22:05.297,,1319,2016-08-04T15:22:05.297,,,,,10.0,70.0,2,3,,,,62.38,10.09,9.96,0.0,0.0,15.0,The simple answer is no they arent limited to images CNNs are also being used for natural language processing See here for an introduction I havent seen them applied to graphical data yet but I havent looked there are some obvious things to try and so Im optimistic that it would work
1324.0,2.0,"<p>Artificial intelligence is present in many games, both current and older games. How can such intelligence understand what to do? I mean, how can it behave like a human in a game, allowing you to play against itself, or that AI plays against itself?</p>

<p>In games like Age of Empires, for example.</p>
",2016-08-15T14:22:03.317,1,2016-08-04T15:25:37.293,2.0,1320,2016-08-13T03:12:52.877,2016-08-09T20:36:48.157,,145.0,,173.0,,1,1,<gaming>,How does artificial intelligence work in games?,87.0,66.74,8.63,9.75,0.0,0.0,9.0,Artificial intelligence is present in many games both current and older games How can such intelligence understand what to do I mean how can it behave like a human in a game allowing you to play against itself or that AI plays against itself In games like Age of Empires for example
,,"<p>Most of the existing AI bots which can play games use deep search from possible space and choose the best move. This is done by most of the chess, Go, Tic-Tac-Toe, etc bots.</p>

<p>However, there has been a recent breakthrough where (deep)neural nets with deep search techniques like monte-carlo search, etc; which might be more human-like and demonstrate a much more complex game behaviour than the above bots. One such example is the Google's Alpha-Go bot.</p>
",,3,2016-08-04T15:33:07.120,,1321,2016-08-04T15:33:07.120,,,,,101.0,1320.0,2,2,,,,69.11,11.43,8.32,0.0,0.0,18.0,Most of the existing AI bots which can play games use deep search from possible space and choose the best move This is done by most of the chess Go TicTacToe etc bots However there has been a recent breakthrough where deepneural nets with deep search techniques like montecarlo search etc which might be more humanlike and demonstrate a much more complex game behaviour than the above bots One such example is the Googles AlphaGo bot
,,"<p>There are a number of good answers here explaining what genetic algorithms are, and giving example applications. I'm adding some general purpose advice on what they are good for, but also cases where you should NOT use them. If my tone seems harsh, it is because using GAs in any of the cases in the Not Appropriate section will lead to your paper being <em>instantly</em> rejected from any top-tier journal. </p>

<p>First, your problem MUST be an optimization problem. You need to define a ""fitness function"" that you are trying to optimize and you need to have a way to measure it.</p>

<p>Good: </p>

<ul>
<li><strong>Crossover functions are easy to define and natural</strong>: When dealing with certain kinds of data, crossover/mutation functions might be easy to define. For example strings (eg. DNA or gene sequences) can be mutated easily by splicing two candidate strings to obtain a new one (this is why nature uses genetic algorithms!). Trees (like phylogenetic trees or parse trees) can be spliced too, by replacing a branch of one tree with a branch from another. Shapes (like airplane wings or boat shapes) can be mutated easily by drawing a grid on the shape and combining different grid sections from the parents to obtain a child. Usually this means your problem is composed of different parts and putting together parts from distinct solutions is a valid candidate solution.

<ul>
<li>This means that if your problem is defined in a vector space where the coordinates don't have any special meaning, GAs are not a good choice. If it is hard to formulate your problem as a GA, it is not worth it.</li>
</ul></li>
<li><strong>Black Box evaluation</strong>: If for a candidate, your fitness function is evaluated outside the computer, GAs are a good idea. For example, if you are testing a wing shape in an air tunnel, genetic algorithms will help you generate good candidate shapes to try. 

<ul>
<li><strong>Exception: Simulations</strong>. If your fitness function is measuring how well a nozzle design performs and requires simulating the fluid dynamics for each nozzle shape, GAs may work well for you. They may also work if you are simulating a physical system through time and are interested in how well your design performs over the course of the operation eg. <a href=""https://www.youtube.com/watch?v=dRthdBr46cs"" rel=""nofollow"">modelling locomotion patterns</a>. However, methods that use partial differential equations as constraints are being developed in the literature, eg. <a href=""https://www.siam.org/meetings/op08/Heinkenschloss.pdf"" rel=""nofollow"">PDE constrained optimization</a>, so this may change in the future.</li>
</ul></li>
</ul>

<p>Not Appropriate:</p>

<ul>
<li><strong>You can calculate a gradient</strong> for your function: If you have access to the gradient of your function, you can do gradient descent, which is in general much more efficient than GAs. Gradient descent may have issues with local minima (as will GAs) but many methods have been studied to mitigate this. </li>
<li><strong>You know the fitness function in closed form</strong>: Then, you can probably calculate the gradient. Many languages have libraries supporting <a href=""https://en.wikipedia.org/wiki/Automatic_differentiation"" rel=""nofollow"">automatic differentiation</a>, so you don't even need to do it manually. If your function is not differentiable, then you can use <a href=""https://en.wikipedia.org/wiki/Subgradient_method"" rel=""nofollow"">subgradient descent</a>.</li>
<li>Your optimization problem is of a known form, like a <strong>linear program or a quadratic program</strong>: GAs (and black box optimization methods in general) are very inefficient in terms of the number of candidates they need to evaluate, and are best avoided if possible. </li>
<li><strong>Your solution space is small</strong>: If you can grid your search space efficiently, you can guarantee that you have found the best solution, and can make contour plots of the solution space to see if there is a region you need to explore further.</li>
</ul>

<p>Finally, if you are considering a GA, consider more recent work in Evolutionary Strategies. I am biased towards <a href=""https://en.wikipedia.org/wiki/CMA-ES"" rel=""nofollow"">CMA-ES</a>, which I think is a good simple algorithm that captures the notion of a gradient in the fitness landscape in a way that traditional GAs do not.</p>
",,1,2016-08-04T15:48:10.133,,1322,2016-08-04T18:01:36.313,2016-08-04T18:01:36.313,,130.0,,130.0,240.0,2,3,,,,57.81,11.02,9.21,0.0,0.0,88.0,There are a number of good answers here explaining what genetic algorithms are and giving example applications Im adding some general purpose advice on what they are good for but also cases where you should NOT use them If my tone seems harsh it is because using GAs in any of the cases in the Not Appropriate section will lead to your paper being instantly rejected from any toptier journal First your problem MUST be an optimization problem You need to define a fitness function that you are trying to optimize and you need to have a way to measure it Good Crossover functions are easy to define and natural When dealing with certain kinds of data crossovermutation functions might be easy to define For example strings eg DNA or gene sequences can be mutated easily by splicing two candidate strings to obtain a new one this is why nature uses genetic algorithms Trees like phylogenetic trees or parse trees can be spliced too by replacing a branch of one tree with a branch from another Shapes like airplane wings or boat shapes can be mutated easily by drawing a grid on the shape and combining different grid sections from the parents to obtain a child Usually this means your problem is composed of different parts and putting together parts from distinct solutions is a valid candidate solution This means that if your problem is defined in a vector space where the coordinates dont have any special meaning GAs are not a good choice If it is hard to formulate your problem as a GA it is not worth it Black Box evaluation If for a candidate your fitness function is evaluated outside the computer GAs are a good idea For example if you are testing a wing shape in an air tunnel genetic algorithms will help you generate good candidate shapes to try Exception Simulations If your fitness function is measuring how well a nozzle design performs and requires simulating the fluid dynamics for each nozzle shape GAs may work well for you They may also work if you are simulating a physical system through time and are interested in how well your design performs over the course of the operation eg modelling locomotion patterns However methods that use partial differential equations as constraints are being developed in the literature eg PDE constrained optimization so this may change in the future Not Appropriate You can calculate a gradient for your function If you have access to the gradient of your function you can do gradient descent which is in general much more efficient than GAs Gradient descent may have issues with local minima as will GAs but many methods have been studied to mitigate this You know the fitness function in closed form Then you can probably calculate the gradient Many languages have libraries supporting automatic differentiation so you dont even need to do it manually If your function is not differentiable then you can use subgradient descent Your optimization problem is of a known form like a linear program or a quadratic program GAs and black box optimization methods in general are very inefficient in terms of the number of candidates they need to evaluate and are best avoided if possible Your solution space is small If you can grid your search space efficiently you can guarantee that you have found the best solution and can make contour plots of the solution space to see if there is a region you need to explore further Finally if you are considering a GA consider more recent work in Evolutionary Strategies I am biased towards CMAES which I think is a good simple algorithm that captures the notion of a gradient in the fitness landscape in a way that traditional GAs do not
1325.0,2.0,"<p><a href=""http://cs.stackexchange.com/a/60535/54605"">At a related question in Computer Science SE</a>, a user told:</p>

<blockquote>
  <p>Neural networks typically require a large training set.</p>
</blockquote>

<p>Is there a way to define the boundaries of the ""optimal"" size of a training set in general case?</p>

<p>When I was learning about fuzzy logic, I've heard some rules of thumb that involved examining the mathematical composition of the problem and using that to define the number of fuzzy sets.</p>

<p>Is there such a method that can be applicable for an already defined neural network topology? </p>
",,6,2016-08-04T15:49:13.793,,1323,2016-08-05T08:26:38.290,2016-08-05T08:26:38.290,,29.0,,1270.0,,1,2,<neural-networks><training><optimization>,Is there a way to define the boundaries of the optimal size of a training set?,29.0,49.65,10.33,10.58,0.0,0.0,10.0,At a related question in Computer Science SE a user told Neural networks typically require a large training set Is there a way to define the boundaries of the optimal size of a training set in general case When I was learning about fuzzy logic Ive heard some rules of thumb that involved examining the mathematical composition of the problem and using that to define the number of fuzzy sets Is there such a method that can be applicable for an already defined neural network topology
,,"<p>There are many different kinds of AI used in games; AI for historical board games (like chess or Go) tends to be much better than AI for computer games (such as Starcraft or Civilization), in large part because there's more academic interest in developing strategies for those games.</p>

<p>The basic structure of a game-playing AI is that it takes in game state inputs and outputs an action; typically, the internals also contain some sort of goal and some sort of future prediction.</p>

<p>But beyond that, there's tremendous amounts of variability. Some AI are little more than scripted reflexes, some are built like control systems, some do actual optimization and forward thinking.</p>

<p>Getting into the details of <em>how</em> the many different approaches work is probably beyond the scope of this site, though.</p>
",,0,2016-08-04T16:04:43.973,,1324,2016-08-04T16:04:43.973,,,,,10.0,1320.0,2,5,,,,44.88,12.25,10.6,0.0,0.0,20.0,There are many different kinds of AI used in games AI for historical board games like chess or Go tends to be much better than AI for computer games such as Starcraft or Civilization in large part because theres more academic interest in developing strategies for those games The basic structure of a gameplaying AI is that it takes in game state inputs and outputs an action typically the internals also contain some sort of goal and some sort of future prediction But beyond that theres tremendous amounts of variability Some AI are little more than scripted reflexes some are built like control systems some do actual optimization and forward thinking Getting into the details of how the many different approaches work is probably beyond the scope of this site though
,,"<p>For a finite value to be 'optimal,' typically you need some benefit from more paired up with some cost for more, and eventually the lines cross because the benefit decreases and the cost increases.</p>

<p>Most models will have a reduction in error with more training data, that asymptotically approaches the best the model can do. See this image (from <a href=""http://blog.revolutionanalytics.com/2015/09/why-big-data-learning-curves.html"" rel=""nofollow noreferrer"">here</a>) as an example:</p>

<p><a href=""https://i.stack.imgur.com/Jx1AZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jx1AZ.png"" alt=""Decreasing error with increasing training set size""></a></p>

<p>The costs of training data are also somewhat obvious; data is costly to obtain, to store, and to move. (Assuming model complexity stays constant, the actual cost of storing, moving, and using the model remains the same, since the weights in the model are just being tuned.)</p>

<p>So at some point the slope of the error-reduction curve becomes horizontal enough that more data points are costlier than they're worth, and that's the optimal amount of training data.</p>
",,0,2016-08-04T16:09:38.327,,1325,2016-08-04T16:09:38.327,,,,,10.0,1323.0,2,3,,,,51.52,11.38,9.88,0.0,0.0,26.0,For a finite value to be optimal typically you need some benefit from more paired up with some cost for more and eventually the lines cross because the benefit decreases and the cost increases Most models will have a reduction in error with more training data that asymptotically approaches the best the model can do See this image from here as an example The costs of training data are also somewhat obvious data is costly to obtain to store and to move Assuming model complexity stays constant the actual cost of storing moving and using the model remains the same since the weights in the model are just being tuned So at some point the slope of the errorreduction curve becomes horizontal enough that more data points are costlier than theyre worth and thats the optimal amount of training data
,,"<p>In general, the larger the training set, the better. See <a href=""http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf"" rel=""nofollow"">The Unreasonable effectiveness of Data</a>, though this article is quite dated (written in 2009). Xavier Amatriain, a researcher at Netflix has a <a href=""https://www.quora.com/In-machine-learning-is-more-data-always-better-than-better-algorithms"" rel=""nofollow"">Quora answer</a> where he discusses that more data can sometimes hurt algorithms. </p>

<p>For deep neural networks in particular, it does not seem that we have hit these limits yet. </p>
",,0,2016-08-04T16:15:38.400,,1326,2016-08-04T16:15:38.400,,,,,130.0,1323.0,2,2,,,,64.51,11.42,10.6,0.0,0.0,11.0,In general the larger the training set the better See The Unreasonable effectiveness of Data though this article is quite dated written in 2009 Xavier Amatriain a researcher at Netflix has a Quora answer where he discusses that more data can sometimes hurt algorithms For deep neural networks in particular it does not seem that we have hit these limits yet
,,"<p>One good way of differentiating modelling and implementation is to consider that models occupy a much higher level of abstraction. </p>

<p>To continue with the mathematical example: even though experimental mathematics might be dependent on computation, the program can be considered as one possible realization of the necessary conditions of a more abstract existence proof.</p>

<p>Over the last 25 years, software engineering methodologies have become quite good at separating models and implementations, e.g. by using interfaces/typeclasses/abstract base classes to define constraints on behavior that is concretely realized by the implementation of derived classes.</p>

<p>AI has always been a battle between the <a href=""https://en.wikipedia.org/wiki/Neats_vs._scruffies"" rel=""nofollow"">'neats and the scruffies'</a>. Neats tend to prefer working 'top down' from clean abstractions, 'scruffies' like to work 'bottom up', and 'bang the bits' of the implementation together, to see what happens.</p>

<p>Of course, in practice, interplay between both styles is necessary, but AI <em>as a science</em> progresses when we abstract mechanisms away from specific implementations into their most general (and hence re-useable) form.</p>
",,0,2016-08-04T16:24:09.050,,1327,2016-08-04T16:24:09.050,,,,,42.0,1297.0,2,4,,,,30.8,16.6,11.06,0.0,0.0,33.0,One good way of differentiating modelling and implementation is to consider that models occupy a much higher level of abstraction To continue with the mathematical example even though experimental mathematics might be dependent on computation the program can be considered as one possible realization of the necessary conditions of a more abstract existence proof Over the last 25 years software engineering methodologies have become quite good at separating models and implementations eg by using interfacestypeclassesabstract base classes to define constraints on behavior that is concretely realized by the implementation of derived classes AI has always been a battle between the neats and the scruffies Neats tend to prefer working top down from clean abstractions scruffies like to work bottom up and bang the bits of the implementation together to see what happens Of course in practice interplay between both styles is necessary but AI as a science progresses when we abstract mechanisms away from specific implementations into their most general and hence reuseable form
,,"<p>Human brain contains about 100 billions neurons (10^11) and about hundred trillions synapses ($10^14). Each neuron can fire about 100 times a second. If we model brain as a simple neural network, then it would be equivalent to machine that requires 1016 calculations per second and 1013 bits of memory.</p>

<p>From <a href=""https://en.wikipedia.org/wiki/The_Singularity_Is_Near#The_brain"">Wikipedia</a></p>

<blockquote>
  <p>Kurzweil introduces the idea of ""uploading"" a specific brain with every mental process intact, to be instantiated on a ""suitably powerful computational substrate"". He writes that general modeling requires 1016 calculations per second and 1013 bits of memory, but then explains uploading requires additional detail, perhaps as many as 1019 cps and 1018 bits. Kurzweil says the technology to do this will be available by 2040.</p>
</blockquote>

<p>According to this two site <a href=""http://www.extremetech.com/extreme/163051-simulating-1-second-of-human-brain-activity-takes-82944-processors"">here</a>:</p>

<blockquote>
  <p>Using the NEST software framework, the team led by Markus Diesmann and Abigail Morrison succeeded in creating an artificial neural network of 1.73 billion nerve cells connected by 10.4 trillion synapses. While impressive, this is only a fraction of the neurons every human brain contains. Scientists believe we all carry 80-100 billion nerve cells</p>
  
  <p>It took 40 minutes with the combined muscle of 82,944 processors in K computer to get just 1 second of biological brain processing time. While running, the simulation ate up about 1PB of system memory as each synapse was modeled individually.</p>
</blockquote>

<p>Computing power will continue to ramp up while transistors scale down, which could make true neural simulations possible in real time with supercomputers.</p>

<p><a href=""https://en.wikipedia.org/wiki/SpiNNaker"">SpiNNaker</a> is a manycore computer architecture designed to <a href=""https://en.wikipedia.org/wiki/Human_Brain_Project"">simulate the human brain</a>. It is planned to use 1 million ARM processors (currently .5 million). The completed design will holds 100,000 cores</p>

<p>In this <a href=""https://www.youtube.com/watch?v=2e06C-yUwlc"">video</a> they showed a completed rack with 100,000 cores emulating 25 million neurons (at ¼ the efficiency—it will eventually run 1,000 neurons per core). </p>
",,2,2016-08-04T17:15:56.423,,1328,2016-08-04T17:15:56.423,,,,,72.0,1314.0,2,5,,,,52.49,13.4,10.53,0.0,0.0,46.0,Human brain contains about 100 billions neurons 1011 and about hundred trillions synapses 1014 Each neuron can fire about 100 times a second If we model brain as a simple neural network then it would be equivalent to machine that requires 1016 calculations per second and 1013 bits of memory From Wikipedia Kurzweil introduces the idea of uploading a specific brain with every mental process intact to be instantiated on a suitably powerful computational substrate He writes that general modeling requires 1016 calculations per second and 1013 bits of memory but then explains uploading requires additional detail perhaps as many as 1019 cps and 1018 bits Kurzweil says the technology to do this will be available by 2040 According to this two site here Using the NEST software framework the team led by Markus Diesmann and Abigail Morrison succeeded in creating an artificial neural network of 173 billion nerve cells connected by 104 trillion synapses While impressive this is only a fraction of the neurons every human brain contains Scientists believe we all carry 80100 billion nerve cells It took 40 minutes with the combined muscle of 82944 processors in K computer to get just 1 second of biological brain processing time While running the simulation ate up about 1PB of system memory as each synapse was modeled individually Computing power will continue to ramp up while transistors scale down which could make true neural simulations possible in real time with supercomputers SpiNNaker is a manycore computer architecture designed to simulate the human brain It is planned to use 1 million ARM processors currently 5 million The completed design will holds 100000 cores In this video they showed a completed rack with 100000 cores emulating 25 million neurons at ¼ the efficiency—it will eventually run 1000 neurons per core
,,"<p>What might be classed as AI has of course changed over the years, but landmarks and research breakthroughs include:</p>

<ul>
<li>Babbagge's <a href=""https://en.wikipedia.org/wiki/Difference_engine"" rel=""nofollow"">Difference Engine</a> (~1823) for tabulating/interpolating polynomials.</li>
<li>Frank Rosenblatt's 1957 invention of the <a href=""http://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf"" rel=""nofollow"">Perceptron</a>.</li>
<li><p>John McCarthy's invention of <a href=""https://en.wikipedia.org/wiki/Lisp_(programming_language)"" rel=""nofollow"">Lisp</a> in the late 1950s.</p></li>
<li><p>Arthur Samuel's 1959 <a href=""http://link.springer.com/chapter/10.1007/978-3-319-30668-1_9"" rel=""nofollow"">checkers player</a>, which famously improved by playing against itself (it would have been nice if that had destroyed the myth about a program only being as 'intelligent' as its creator).</p></li>
<li><p>Newell and Simon's 1959 <a href=""http://bitsavers.informatik.uni-stuttgart.de/pdf/rand/ipl/P-1584_Report_On_A_General_Problem-Solving_Program_Feb59.pdf"" rel=""nofollow"">General Problem Solver</a> applied Means-Ends analysis to solve a range of problems expressed as Horn clauses.</p></li>
<li><p>Davis, Putnam et al: 1962 invention of the <a href=""https://en.wikipedia.org/wiki/DPLL_algorithm"" rel=""nofollow"">DPLL algorithm</a> which still forms the core of modern SAT-based theorem provers.</p></li>
<li><p>Lawrence Fogel et al: 1966 book <a href=""http://rads.stackoverflow.com/amzn/click/B0000CNARU"" rel=""nofollow"">Artificial Intelligence through Simulated Evolution</a>.</p></li>
<li><p>Rechenberg and Schwefel: 1960s development of <a href=""https://en.wikipedia.org/wiki/Evolution_strategy"" rel=""nofollow"">Evolutionsstrategie</a> - an Evolutionary Computation approach using mutation and a form of Darwinian 'survival of the fittest'.</p></li>
<li><p>Lotfi Zadeh's 1965 invention of <a href=""https://people.eecs.berkeley.edu/~zadeh/papers/Fuzzy%20Sets-Information%20and%20Control-1965.pdf"" rel=""nofollow"">Fuzzy Logic</a>.</p></li>
<li><p>John Holland's 1975 book <a href=""http://dl.acm.org/citation.cfm?id=531075"" rel=""nofollow"">""Adaptation in Natural and Artificial Systems""</a> which introduced Genetic Algorithms.</p></li>
<li><p>The 1980 <a href=""http://aitopics.org/sites/default/files/classic/Webber-Nilsson-Readings/Rdgs-NW-Erman-Hayes-Roth-Lesser-Reddy.pdf"" rel=""nofollow"">Hearsay II</a> Blackboard Architecture by Hayes-Roth et al.</p></li>
<li><p>The 1980s invention of the <a href=""http://dl.acm.org/citation.cfm?id=104293"" rel=""nofollow"">backpropagation algorithm</a> for Mutlilayer Perceptrons by Rumelhart, Hinton et al.</p></li>
</ul>
",,0,2016-08-04T17:34:17.233,,1329,2016-08-04T17:34:17.233,,,,,42.0,64.0,2,3,,,,46.57,16.7,11.36,0.0,0.0,43.0,What might be classed as AI has of course changed over the years but landmarks and research breakthroughs include Babbagges Difference Engine 1823 for tabulatinginterpolating polynomials Frank Rosenblatts 1957 invention of the Perceptron John McCarthys invention of Lisp in the late 1950s Arthur Samuels 1959 checkers player which famously improved by playing against itself it would have been nice if that had destroyed the myth about a program only being as intelligent as its creator Newell and Simons 1959 General Problem Solver applied MeansEnds analysis to solve a range of problems expressed as Horn clauses Davis Putnam et al 1962 invention of the DPLL algorithm which still forms the core of modern SATbased theorem provers Lawrence Fogel et al 1966 book Artificial Intelligence through Simulated Evolution Rechenberg and Schwefel 1960s development of Evolutionsstrategie an Evolutionary Computation approach using mutation and a form of Darwinian survival of the fittest Lotfi Zadehs 1965 invention of Fuzzy Logic John Hollands 1975 book Adaptation in Natural and Artificial Systems which introduced Genetic Algorithms The 1980 Hearsay II Blackboard Architecture by HayesRoth et al The 1980s invention of the backpropagation algorithm for Mutlilayer Perceptrons by Rumelhart Hinton et al
,,,,0,2016-08-04T18:08:05.010,,1330,2016-08-04T18:08:05.010,2016-08-04T18:08:05.010,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,"For questions about Artificial General Intelligence (AGI), a hypothetical machine that can perform any intellectual task that a human being can.",,0,2016-08-04T18:08:05.010,,1331,2016-08-30T19:43:34.407,2016-08-30T19:43:34.407,,29.0,,8.0,,4,0,,,,24.78,16.94,13.7,0.0,0.0,4.0,For questions about Artificial General Intelligence AGI a hypothetical machine that can perform any intellectual task that a human being can
,1.0,"<p>How important is true (non-<a href=""https://en.wikipedia.org/wiki/Pseudorandomness"" rel=""nofollow"" title=""pseudo"">pseudo</a>) randomness in Artificial Intelligence designs? Is there any chance that pseudo-randomness could be a barrier to more successful designs?</p>
",,0,2016-08-04T18:26:31.213,,1332,2016-08-04T18:56:47.850,,,,,46.0,,1,4,<ai-design>,How important is true randomness in AI designs?,40.0,33.92,17.62,10.81,0.0,0.0,6.0,How important is true nonpseudo randomness in Artificial Intelligence designs Is there any chance that pseudorandomness could be a barrier to more successful designs
1370.0,2.0,"<p>Complex AI that learns lexical-semantic content and its meaning (such as collection of words, their structure and dependencies) such as <em>Watson</em> takes terabytes of disk space.</p>

<p>Lets assume <em>DeepQA</em>-like AI consumed whole Wikipedia of size 10G which took the same amount of structured and unstructured stored content.</p>

<p>Will learning another 10G of different encyclopedia (different topics in the same language) take the same amount of data? Or will the AI reuse the existing structured and take less than half (like 1/10 of it) additional space?</p>
",,0,2016-08-04T18:31:33.580,,1333,2016-08-18T00:46:03.973,2016-08-17T13:52:57.853,,75.0,,8.0,,1,0,<watson><storage>,Does learning content from additional encyclopedias consume much less amount of storage?,26.0,49.96,13.69,10.26,0.0,0.0,14.0,Complex AI that learns lexicalsemantic content and its meaning such as collection of words their structure and dependencies such as Watson takes terabytes of disk space Lets assume DeepQAlike AI consumed whole Wikipedia of size 10G which took the same amount of structured and unstructured stored content Will learning another 10G of different encyclopedia different topics in the same language take the same amount of data Or will the AI reuse the existing structured and take less than half like 110 of it additional space
1342.0,2.0,"<p>Is there any simple explanation how <em>Watson</em> finds and scores evidence after gathering massive evidence and analyzing the data?</p>

<p>In other words, how does it know which precise answer it needs to return?</p>
",,0,2016-08-04T18:40:44.093,,1334,2016-08-17T13:52:23.383,2016-08-17T13:52:23.383,,75.0,,8.0,,1,2,<watson>,How does Watson find and evaluate its evidence to the answer?,50.0,46.27,11.77,11.63,0.0,0.0,3.0,Is there any simple explanation how Watson finds and scores evidence after gathering massive evidence and analyzing the data In other words how does it know which precise answer it needs to return
,,"<p>Randomness is typically the best one can do with ignorance, rather than a source of strength in its own right.</p>

<p>For example, the primary use of randomness in statistics is random assignment (A/B testing, randomized controlled trials, etc.). The reason to do this is to make the influence of confounders independent from the influence of the factor under investigation.</p>

<p>But randomness only works for this <em>in expectation</em>. If we actually knew what the confounders were, we could do a paired assignment (or a similar scheme) that ensured the various groups were matched <em>as well as possible</em>, instead of us just not knowing ahead of time which way the bias went.</p>

<hr>

<p>There are some cases where pseudorandomness, rather than full randomness, will impair training AI designs. A simple example would be a case where you want to randomly initialize weights in a network where the number of parameters exceeds the periodicity of the RNG; this means that while you have as many possible networks as there are possible unique seeds, you can't actually visit the entire weight space that you wanted to sample over.</p>

<p>I don't think any of those cases are limiting factors, however. Having truly random stochastic gradient descent instead of pseudorandom stochastic gradient descent doesn't seem like it would make a serious difference in the trajectory of AI designs.</p>
",,2,2016-08-04T18:56:47.850,,1335,2016-08-04T18:56:47.850,,,,,10.0,1332.0,2,4,,,,46.51,12.54,9.86,0.0,0.0,29.0,Randomness is typically the best one can do with ignorance rather than a source of strength in its own right For example the primary use of randomness in statistics is random assignment AB testing randomized controlled trials etc The reason to do this is to make the influence of confounders independent from the influence of the factor under investigation But randomness only works for this in expectation If we actually knew what the confounders were we could do a paired assignment or a similar scheme that ensured the various groups were matched as well as possible instead of us just not knowing ahead of time which way the bias went There are some cases where pseudorandomness rather than full randomness will impair training AI designs A simple example would be a case where you want to randomly initialize weights in a network where the number of parameters exceeds the periodicity of the RNG this means that while you have as many possible networks as there are possible unique seeds you cant actually visit the entire weight space that you wanted to sample over I dont think any of those cases are limiting factors however Having truly random stochastic gradient descent instead of pseudorandom stochastic gradient descent doesnt seem like it would make a serious difference in the trajectory of AI designs
,,"<p>AFAIK, normally detection algorithms work in a sub-window of the image and not the whole of it. For example, for a specific size and orientation you slide a sub-window on the image and extract sub-images. Then you apply your algorithm on every sub-image for detection and report the size-and-orientations with positive results.</p>

<p>You can have a single neural network for face detection in this case or you might want to have different detectors for different orientation or any other feature, that is your decision.</p>

<p>There is also the technique of <a href=""https://en.wikipedia.org/wiki/Ensembles_of_classifiers"" rel=""nofollow"">Combining Classifiers</a> by which you can improve the decision of single classifiers by combining them.</p>

<p><a href=""https://en.wikipedia.org/wiki/Ensemble_learning"" rel=""nofollow"">Ensemble Learning</a> is another way in which your classifiers are not trained independently but rather together. In fact, the well-known <a href=""https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework"" rel=""nofollow"">object detector of Viola and Jones</a> uses such a technique.</p>
",,0,2016-08-04T19:36:06.863,,1337,2016-08-04T19:56:38.653,2016-08-04T19:56:38.653,,143.0,,143.0,1301.0,2,3,,,,43.43,12.94,10.56,0.0,0.0,18.0,AFAIK normally detection algorithms work in a subwindow of the image and not the whole of it For example for a specific size and orientation you slide a subwindow on the image and extract subimages Then you apply your algorithm on every subimage for detection and report the sizeandorientations with positive results You can have a single neural network for face detection in this case or you might want to have different detectors for different orientation or any other feature that is your decision There is also the technique of Combining Classifiers by which you can improve the decision of single classifiers by combining them Ensemble Learning is another way in which your classifiers are not trained independently but rather together In fact the wellknown object detector of Viola and Jones uses such a technique
,,"<p>Until we can make a quantum computer with a lot more qubits, the potential to further develop AI will remain just that.</p>

<p>D-Wave (which currently has a qubit count of over 1,000) is an <em>adiabatic quantum computer</em>, not a general purpose quantum computer. It is restricted to certain optimization problems (at which its effectiveness <a href=""https://en.wikipedia.org/wiki/D-Wave_Systems#Reception"" rel=""nofollow"">has reportedly been doubted</a> by one of the originators of the theory on which it is based).</p>

<p>Suppose that we could build a 32 qubit general purpose quantum computer (twice as big as current models, as far as I'm aware). This would still mean that only 2<sup>32</sup> possibilities exist in superposition. This is a space small enough to be explored exhaustively for many problems. Hence, there are perhaps not so many problems for which any of the known quantum algorithms (e.g. <a href=""https://en.wikipedia.org/wiki/Shor%27s_algorithm"" rel=""nofollow"">Shor</a>, <a href=""https://en.wikipedia.org/wiki/Grover%27s_algorithm"" rel=""nofollow"">Grover</a>) would be useful for that number of bits.</p>
",,0,2016-08-04T20:12:38.983,,1338,2016-08-11T10:27:02.020,2016-08-11T10:27:02.020,,29.0,,42.0,36.0,2,4,,,,61.56,10.61,9.43,0.0,0.0,25.0,Until we can make a quantum computer with a lot more qubits the potential to further develop AI will remain just that DWave which currently has a qubit count of over 1000 is an adiabatic quantum computer not a general purpose quantum computer It is restricted to certain optimization problems at which its effectiveness has reportedly been doubted by one of the originators of the theory on which it is based Suppose that we could build a 32 qubit general purpose quantum computer twice as big as current models as far as Im aware This would still mean that only 232 possibilities exist in superposition This is a space small enough to be explored exhaustively for many problems Hence there are perhaps not so many problems for which any of the known quantum algorithms eg Shor Grover would be useful for that number of bits
,,"<p>There has been previous research with promising results cited at length in the following recent article, and although they have limited training data, here is some <a href=""http://uaf46365.ddns.uark.edu/SarahStolze_Thesis.pdf"" rel=""nofollow noreferrer"">impressive research for an undergraduate thesis at the University of Arkansas</a> which extends that research using an artificial neural network on enhancing a classifying algorithm's capacity to facilitate unspoken, or imagined, speech recognition by collecting and analyzing a large dataset of simultaneous EEG signal and video data streams. </p>

<blockquote>
  <p>Imagined speech (unspoken speech, silent speech, or covert speech) is
  the process by which one thinks about a word, or “hears” the word in
  one’s head, in the absence of any vocalization or physical movement
  indicating the word. Though there exists evidence that it is possible
  for imagined speech information to be captured and interpreted. To
  facilitate imagined speech, a Brain-to-Computer Interface (BCI) must
  be implemented to provide silent communication abilities directly
  between the two entities. One of the most popular methods for
  interfacing directly between a human brain and a computer is through
  electroencephalographic signals.</p>
  
  <p>Researchers have created models capable of achieving 70 - 90%
  predictive accuracy in recognizing patterns in EEG data;
  however, the accuracy of current methods for unspoken speech
  recognition is not yet sufficient to enable fluid communication
  between humans and machines.</p>
</blockquote>

<p>High Level Experiment Design</p>

<blockquote>
  <p>the subjects were asked to imagine a specific word or feeling (label).
  The subjects responded to a set of uniform verbal cues describing the
  set of labels as well as the desired individual label to imagine. The
  data was then processed in order to minimize the effects of irrelevant
  signal activity, or noise. Additionally the data was processed to
  minimize its volume while still maintaining the core “information” in
  the data. The condensed dataset was created by dropping irrelevant
  information from the EEG device and applying principal component
  analysis (PCA) to the video stream data. Once the data was processed
  and assembled into the correct format, cross-validation using a random
  forest algorithm was performed on the control group of EEG signals
  alone and on the hypothesis group consisting of both EEG and video
  data. The predictive accuracy measurements obtained from the
  cross-validation experiments were used as metrics to evaluate the
  success of the hypothesis.</p>
</blockquote>

<p>The results show a notable improvement classifying thoughts when in conjunction with the video streams.</p>

<p><a href=""https://i.stack.imgur.com/LH6sI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LH6sI.png"" alt=""graph of predictive accuracy""></a></p>
",,0,2016-08-04T20:15:37.607,,1339,2016-08-04T20:15:37.607,,,,,62.0,1274.0,2,4,,,,26.85,15.61,11.27,0.0,0.0,42.0,There has been previous research with promising results cited at length in the following recent article and although they have limited training data here is some impressive research for an undergraduate thesis at the University of Arkansas which extends that research using an artificial neural network on enhancing a classifying algorithms capacity to facilitate unspoken or imagined speech recognition by collecting and analyzing a large dataset of simultaneous EEG signal and video data streams Imagined speech unspoken speech silent speech or covert speech is the process by which one thinks about a word or “hears” the word in one’s head in the absence of any vocalization or physical movement indicating the word Though there exists evidence that it is possible for imagined speech information to be captured and interpreted To facilitate imagined speech a BraintoComputer Interface BCI must be implemented to provide silent communication abilities directly between the two entities One of the most popular methods for interfacing directly between a human brain and a computer is through electroencephalographic signals Researchers have created models capable of achieving 70 90 predictive accuracy in recognizing patterns in EEG data however the accuracy of current methods for unspoken speech recognition is not yet sufficient to enable fluid communication between humans and machines High Level Experiment Design the subjects were asked to imagine a specific word or feeling label The subjects responded to a set of uniform verbal cues describing the set of labels as well as the desired individual label to imagine The data was then processed in order to minimize the effects of irrelevant signal activity or noise Additionally the data was processed to minimize its volume while still maintaining the core “information” in the data The condensed dataset was created by dropping irrelevant information from the EEG device and applying principal component analysis PCA to the video stream data Once the data was processed and assembled into the correct format crossvalidation using a random forest algorithm was performed on the control group of EEG signals alone and on the hypothesis group consisting of both EEG and video data The predictive accuracy measurements obtained from the crossvalidation experiments were used as metrics to evaluate the success of the hypothesis The results show a notable improvement classifying thoughts when in conjunction with the video streams
,,"<p><a href=""http://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(04)00243-8?_returnURL=http%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS1364661304002438%3Fshowall%3Dtrue"" rel=""nofollow"">This article</a> gives a description of mirror neurons in terms of Hebbian learning, a mechanism that has been widely used in AI. I don't know whether the formulation given in the article has ever actually been implemented computationally.</p>
",,0,2016-08-04T20:48:35.637,,1340,2016-08-04T20:48:35.637,,,,,42.0,156.0,2,4,,,,43.73,12.94,11.64,0.0,0.0,4.0,This article gives a description of mirror neurons in terms of Hebbian learning a mechanism that has been widely used in AI I dont know whether the formulation given in the article has ever actually been implemented computationally
,,"<p>MLP's can theoretically approximate any bounded, continuous function. There's no guarantee for a discontinuous function. There are plenty of important discontinuous functions, like, say, the prime counting function.</p>

<p>The <a href=""https://en.wikipedia.org/wiki/Prime-counting_function"" rel=""nofollow"">prime counting function</a> pi(n) is simply equal to the number of primes less than or equal to n. It has a discontinuity about each prime p, so good luck trying to approximate this with a neural network!</p>

<p>However, this function is extensively studied and extremely important in number theory. See the <a href=""https://en.wikipedia.org/wiki/Riemann_hypothesis"" rel=""nofollow"">Riemann hypothesis</a>.</p>
",,0,2016-08-04T21:05:44.587,,1341,2016-08-04T21:05:44.587,,,,,127.0,247.0,2,1,,,,51.14,13.73,9.8,0.0,0.0,17.0,MLPs can theoretically approximate any bounded continuous function Theres no guarantee for a discontinuous function There are plenty of important discontinuous functions like say the prime counting function The prime counting function pin is simply equal to the number of primes less than or equal to n It has a discontinuity about each prime p so good luck trying to approximate this with a neural network However this function is extensively studied and extremely important in number theory See the Riemann hypothesis
,,"<p>Watson starts off by searching its massive database of sources for stuff that might be pertinent to the question. Next, it searches through all of the search results and turns them into candidate answers. For example, if one of the search results is an article, Watson might pick the title of the article as a possible answer. After finding all of these candidate answers, it proceeds to iteratively score them to determine which one is best.</p>

<p>The scoring process is very complicated, and involves finding supporting evidence for each answer, and then combining many different scoring algorithms to determine which candidate answer is the best. You can read a more detailed (but still very conceptual) overview <a href=""http://www.aaai.org/Magazine/Watson/watson.php"" rel=""nofollow"">here</a>, by the creators of Watson.</p>
",,0,2016-08-04T21:40:31.970,,1342,2016-08-04T21:40:31.970,,,,,127.0,1334.0,2,3,,,,50.87,12.24,10.08,0.0,0.0,15.0,Watson starts off by searching its massive database of sources for stuff that might be pertinent to the question Next it searches through all of the search results and turns them into candidate answers For example if one of the search results is an article Watson might pick the title of the article as a possible answer After finding all of these candidate answers it proceeds to iteratively score them to determine which one is best The scoring process is very complicated and involves finding supporting evidence for each answer and then combining many different scoring algorithms to determine which candidate answer is the best You can read a more detailed but still very conceptual overview here by the creators of Watson
,,,,0,2016-08-04T21:41:23.037,,1343,2016-08-04T21:41:23.037,2016-08-04T21:41:23.037,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions about machine learning (ml) and the related concepts with respect to AI.,,0,2016-08-04T21:41:23.037,,1344,2016-12-13T19:07:28.240,2016-12-13T19:07:28.240,,3836.0,,3836.0,,4,0,,,,74.19,12.35,12.23,0.0,0.0,3.0,For questions about machine learning ml and the related concepts with respect to AI
,,"<p>The following survey article by researchers from IIT Bombay summarizes recent advances in sarcasm detection: <a href=""https://arxiv.org/abs/1602.03426"" rel=""nofollow"">Arxiv link</a>.</p>

<p>In reference to your question, I do not think it is considered either extraordinarily difficult or open-ended. While it does introduce ambiguity that computers cannot yet handle, Humans are easily able to understand sarcasm, and are thus able to label datasets for sarcasm detection.</p>
",,0,2016-08-04T21:45:48.653,,1345,2016-08-04T21:45:48.653,,,,,130.0,198.0,2,4,,,,33.95,15.43,12.93,0.0,0.0,8.0,The following survey article by researchers from IIT Bombay summarizes recent advances in sarcasm detection Arxiv link In reference to your question I do not think it is considered either extraordinarily difficult or openended While it does introduce ambiguity that computers cannot yet handle Humans are easily able to understand sarcasm and are thus able to label datasets for sarcasm detection
,,,,0,2016-08-04T21:55:57.107,,1346,2016-08-04T21:55:57.107,2016-08-04T21:55:57.107,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions relating to an AI's ability to speak naturally.,,0,2016-08-04T21:55:57.107,,1347,2016-08-11T19:10:15.940,2016-08-11T19:10:15.940,,145.0,,145.0,,4,0,,,,52.87,11.4,10.45,0.0,0.0,2.0,For questions relating to an AIs ability to speak naturally
1355.0,3.0,"<p>Isaac Asimov's famous <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics"">Three Laws of Robotics</a> originated in the context of Asimov's science fiction stories. In those stories, the three laws serve as a safety measure, in order to avoid untimely or manipulated situations from exploding in havoc.</p>

<p>More often than not, Asimov's narratives would find a way to break them, leading the writer to make several modifications to the laws themselves. For instance, in some of his stories, he <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#First_Law_modified"">modified the First Law</a>, <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Zeroth_Law_added"">added a Fourth (or Zeroth) Law</a>, or even <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Removal_of_the_Three_Laws"">removed all Laws altogether</a>.</p>

<p>However, it is easy to argue that, in popular culture, and even in the field of AI research itself, the Laws of Robotics are taken quite seriously. Ignoring the side problem of the different, subjective, and mutually-exclusive interpretations of the laws, are there any arguments proving the laws themselves intrinsically flawed by their design, or, alternatively, strong enough for use in reality? Likewise, has a better, stricter security heuristics set being designed for the purpose?</p>
",,3,2016-08-04T23:49:01.983,2.0,1348,2016-08-25T16:00:00.820,2016-08-23T10:05:46.817,,145.0,,71.0,,1,16,<asimovs-laws>,"Are Asimov's Laws flawed by design, or are they feasible in practice?",258.0,39.57,13.52,10.92,0.0,0.0,33.0,Isaac Asimovs famous Three Laws of Robotics originated in the context of Asimovs science fiction stories In those stories the three laws serve as a safety measure in order to avoid untimely or manipulated situations from exploding in havoc More often than not Asimovs narratives would find a way to break them leading the writer to make several modifications to the laws themselves For instance in some of his stories he modified the First Law added a Fourth or Zeroth Law or even removed all Laws altogether However it is easy to argue that in popular culture and even in the field of AI research itself the Laws of Robotics are taken quite seriously Ignoring the side problem of the different subjective and mutuallyexclusive interpretations of the laws are there any arguments proving the laws themselves intrinsically flawed by their design or alternatively strong enough for use in reality Likewise has a better stricter security heuristics set being designed for the purpose
,,,,0,2016-08-05T01:17:49.000,,1352,2016-08-05T01:17:49.000,2016-08-05T01:17:49.000,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,,,0,2016-08-05T01:17:49.000,,1353,2016-08-05T01:17:49.000,2016-08-05T01:17:49.000,,-1.0,,-1.0,,4,0,,,,,,,,,,
2143.0,4.0,"<p>Are there any modern techniques of generating <strong>textual</strong> CAPTCHA (so person needs to type the right text) challenges which can easily <a href=""http://ai.stackexchange.com/q/92/8"">fool AI</a> with some visual obfuscation methods, but at the same time human can solve them without any struggle?</p>

<p>For example I'm talking about plain ability of <strong>recognising text embedded into image</strong> (without considering any external plugins like flash or java, image classification, etc.) and re-typing the text that has been written or something similar.</p>

<p>I guess adding noise, gradient, rotating letters or changing colours are not reliable methods any more, since they can be quickly broken.</p>

<p>Any suggestions or research has been done?</p>
",,3,2016-08-05T01:45:17.633,2.0,1354,2016-10-13T09:33:07.423,2016-10-08T09:58:23.427,,8.0,,8.0,,1,5,<image-recognition><research><ocr>,"Are there any textual CAPTCHA challenges which can fool AI, but not human?",276.0,41.7,13.52,11.6,0.0,0.0,17.0,Are there any modern techniques of generating textual CAPTCHA so person needs to type the right text challenges which can easily fool AI with some visual obfuscation methods but at the same time human can solve them without any struggle For example Im talking about plain ability of recognising text embedded into image without considering any external plugins like flash or java image classification etc and retyping the text that has been written or something similar I guess adding noise gradient rotating letters or changing colours are not reliable methods any more since they can be quickly broken Any suggestions or research has been done
,,"<p>Asimov's laws are not strong enough to be used in practice. Strength isn't even a consideration, when considering that since they're written in English words would first have to be interpreted subjectively to have any meaning at all. You can find a good discussion of this <a href=""https://youtu.be/7PKx3kS7f4A"">here</a>.</p>

<p>To transcribe an excerpt:</p>

<blockquote>
  <p>How do you define these things? How do you define ""human"", without first having to take a stand on almost every issue. And if ""human"" wasn't hard enough, you then have to define ""harm"", and you've got the same problem again. Almost any really solid unambiguous definitions you give for those words&mdash;that don't rely on human intuition&mdash;result in weird quirks of philosophy, leading to your AI doing something you really don't want it to do.</p>
</blockquote>

<p>One can easily imagine that Asimov was smart enough to know this and was more interested in story-writing than designing real-world AI control protocols.</p>

<p>In the novel <a href=""https://en.wikipedia.org/wiki/Neuromancer#Plot_summary"">Neuromancer</a>, it was suggested that AIs could possibly serve as checks against each other. Ray Kurzweil's impending <a href=""https://en.wikipedia.org/wiki/Technological_singularity"">Singularity</a>, or the possibility of hyperintelligent AGIs otherwise, might not leave much of a possibility for humans to control AIs at all, leaving peer-regulation as the only feasible possibility.</p>

<p>It's worth noting that Eliezer Yudkowsky and others ran an <a href=""http://www.yudkowsky.net/singularity/aibox/"">experiment</a> wherein Yudkowsky played the role of a superintelligent AI with the ability to speak, but no other connection outside of a locked box. The challengers were tasked simply with keeping the AI in the box at all costs. Yudkowsky escaped both times.</p>
",,4,2016-08-05T01:55:51.737,,1355,2016-08-05T02:00:59.683,2016-08-05T02:00:59.683,,46.0,,46.0,1348.0,2,10,,,,51.78,12.59,9.92,0.0,0.0,46.0,Asimovs laws are not strong enough to be used in practice Strength isnt even a consideration when considering that since theyre written in English words would first have to be interpreted subjectively to have any meaning at all You can find a good discussion of this here To transcribe an excerpt How do you define these things How do you define human without first having to take a stand on almost every issue And if human wasnt hard enough you then have to define harm and youve got the same problem again Almost any really solid unambiguous definitions you give for those wordsmdashthat dont rely on human intuitionmdashresult in weird quirks of philosophy leading to your AI doing something you really dont want it to do One can easily imagine that Asimov was smart enough to know this and was more interested in storywriting than designing realworld AI control protocols In the novel Neuromancer it was suggested that AIs could possibly serve as checks against each other Ray Kurzweils impending Singularity or the possibility of hyperintelligent AGIs otherwise might not leave much of a possibility for humans to control AIs at all leaving peerregulation as the only feasible possibility Its worth noting that Eliezer Yudkowsky and others ran an experiment wherein Yudkowsky played the role of a superintelligent AI with the ability to speak but no other connection outside of a locked box The challengers were tasked simply with keeping the AI in the box at all costs Yudkowsky escaped both times
,,"<p>There has been a recent work in the same domain where neural networks(CNNs to be accurate) are used for the same purpose. Some info. about the research is:</p>

<blockquote>
  <p>To learn that context, the paper describes a method by which the
  neural network finds the user’s “embeddings” — i.e. contextual cues
  like the content of previous tweets, related interests and accounts,
  and so on. It uses these various factors to plot the user with others,
  and (ideally) finds that they form relatively well-defined groups.</p>
</blockquote>

<p>So, the paper uses CNNs, word and user embeddings for detecting sarcasm in text. There is also a <a href=""https://techcrunch.com/2016/08/04/this-neural-network-tries-to-tell-if-youre-being-sarcastic-online/"" rel=""nofollow"">Techcrunch article</a> on that.</p>

<p>The paper uses sentiment of the tweet and compares with that of the other similar tweets:</p>

<blockquote>
  <p>If the sentiment of the tweet seems to disagree with the bulk of what
  is expressed by similar users, there’s a good chance sarcasm is being
  employed.</p>
</blockquote>

<p><a href=""http://arxiv.org/pdf/1607.00976v2.pdf"" rel=""nofollow"">Link to the paper</a></p>
",,0,2016-08-05T04:52:42.947,,1356,2016-08-05T04:52:42.947,,,,,101.0,198.0,2,1,,,,69.11,10.44,9.36,0.0,0.0,23.0,There has been a recent work in the same domain where neural networksCNNs to be accurate are used for the same purpose Some info about the research is To learn that context the paper describes a method by which the neural network finds the user’s “embeddings” — ie contextual cues like the content of previous tweets related interests and accounts and so on It uses these various factors to plot the user with others and ideally finds that they form relatively welldefined groups So the paper uses CNNs word and user embeddings for detecting sarcasm in text There is also a Techcrunch article on that The paper uses sentiment of the tweet and compares with that of the other similar tweets If the sentiment of the tweet seems to disagree with the bulk of what is expressed by similar users there’s a good chance sarcasm is being employed Link to the paper
1359.0,1.0,"<p>Can an AI program have an EQ (Emotional intelligence or emotional quotient)?</p>

<p>In other words, can the EQ of an AI program be measured?</p>

<p>If EQ is more problematic to measure than IQ (at least with a standard applicaple to both humans and AI programs), why is that the case?</p>
",,1,2016-08-05T05:40:23.683,,1357,2016-08-05T12:43:38.537,,,,,1278.0,,1,9,<emotional-intelligence><intelligence-testing>,How can the EQ of an AI program be measured?,62.0,71.44,8.06,8.57,0.0,0.0,9.0,Can an AI program have an EQ Emotional intelligence or emotional quotient In other words can the EQ of an AI program be measured If EQ is more problematic to measure than IQ at least with a standard applicaple to both humans and AI programs why is that the case
1361.0,1.0,"<p>I have heard about this concept in a reddit post about Alpha Go. I have trued to go through the paper and the article, but could not really make sense of the algorithm.</p>

<p>So, can someone give a easy-to-understand explanation of how the Monte-Carlo search algorithm work and how is it being used in building game-playing AI bots?</p>
",,3,2016-08-05T07:03:50.110,,1358,2016-08-09T18:57:36.527,2016-08-09T18:57:36.527,,8.0,,101.0,,1,10,<gaming><monte-carlo-search>,"How does ""Monte-Carlo search"" work?",123.0,68.81,9.63,8.95,0.0,0.0,9.0,I have heard about this concept in a reddit post about Alpha Go I have trued to go through the paper and the article but could not really make sense of the algorithm So can someone give a easytounderstand explanation of how the MonteCarlo search algorithm work and how is it being used in building gameplaying AI bots
,,"<p>The answer to your question is ""In principle, yes"" - in it's most general form, EQ testing is just a specific case of the Turing test (""How would you feel about ... ?""). </p>

<p>To see why meaningful EQ tests might be difficult to achieve, consider the following two possible tests:</p>

<p>At one extreme of complexity, the film 'Blade Runner' famously shows a test to distinguish between human and android on the basis of responses to emotionally-charged questions.</p>

<p>If you tried asking these questions (or even much simpler ones) to a modern chatbot, you'd likely quickly conclude that you were not talking to a person.</p>

<p>The problem with assessing EQ is that the more emotionally sophisticated the test, the more general the AI system is likely have to be, in order to turn the input into a meaningful representation.</p>

<p>At the other extreme from the above, suppose that an EQ test was phrased in an extremely structured way, with the structured input provided by a human. In such a case, success at an 'EQ test' is not really grounded in the real-world.</p>

<p>In an essay entitled ""The ineradicable Eliza effect and its dangers"", Douglas Hofstadter gives the following example, in which the ACME program is claimed (not by Hofstadter) to 'understand' analogy.</p>

<blockquote>
  <p>Here the computer learns about a fellow named Sluggo taking his wife Jane and
  his good buddy Buck to a bar, where things take their natural course and Jane
  winds up pregnant by Buck. She has the baby but doesn't want it, and so, aided
  by her husband, she drowns the baby in a river, thus ""neatly solving ""the problem""
  of Bambi.</p>
</blockquote>

<p>This story is presented to ACME in the following form:</p>

<pre><code>ql: (neglectful-husband (Sluggo))
q2: (lonely-and-sex-starved-wife (Jane-Doe))
q3: (macho-ladykiller (Buck-Stag))
q4: (poor-innocent-little-fetus (Bambi))
q5: (takes-out-to-local-bar (Sluggo Jane-Doe Buck-Stag))
...
q11: (neatly-solves-the-problem-of (Jane-Doe Bambi))
q12: (cause (ql0 q11))
</code></pre>

<p>Suppose the program were to be asked if Jane Doe's behavior was moral. Complex compound emotional concepts such as 'neglectful', 'lonely' and 'innocent' are here simply predicates, not available to the AI for deeper introspective examination. They could just as easily be replaced by labels such as as 'bling-blang-blong15657'. </p>

<p>So in one sense, the absence of success at an EQ test with any depth is indicative of the general problem currently facing AI: the inability to define (or otherwise learn) meaningful representations of subtle complexities of the human world, which is a lot more complex than being able to recognize videos of cats.</p>
",,0,2016-08-05T08:24:24.657,,1359,2016-08-05T12:43:38.537,2016-08-05T12:43:38.537,,42.0,,42.0,1357.0,2,5,,,,49.99,11.73,10.38,292.0,0.0,81.0,The answer to your question is In principle yes in its most general form EQ testing is just a specific case of the Turing test How would you feel about To see why meaningful EQ tests might be difficult to achieve consider the following two possible tests At one extreme of complexity the film Blade Runner famously shows a test to distinguish between human and android on the basis of responses to emotionallycharged questions If you tried asking these questions or even much simpler ones to a modern chatbot youd likely quickly conclude that you were not talking to a person The problem with assessing EQ is that the more emotionally sophisticated the test the more general the AI system is likely have to be in order to turn the input into a meaningful representation At the other extreme from the above suppose that an EQ test was phrased in an extremely structured way with the structured input provided by a human In such a case success at an EQ test is not really grounded in the realworld In an essay entitled The ineradicable Eliza effect and its dangers Douglas Hofstadter gives the following example in which the ACME program is claimed not by Hofstadter to understand analogy Here the computer learns about a fellow named Sluggo taking his wife Jane and his good buddy Buck to a bar where things take their natural course and Jane winds up pregnant by Buck She has the baby but doesnt want it and so aided by her husband she drowns the baby in a river thus neatly solving the problem of Bambi This story is presented to ACME in the following form Suppose the program were to be asked if Jane Does behavior was moral Complex compound emotional concepts such as neglectful lonely and innocent are here simply predicates not available to the AI for deeper introspective examination They could just as easily be replaced by labels such as as blingblangblong15657 So in one sense the absence of success at an EQ test with any depth is indicative of the general problem currently facing AI the inability to define or otherwise learn meaningful representations of subtle complexities of the human world which is a lot more complex than being able to recognize videos of cats
1369.0,1.0,"<p>DNNs are typically used to classify things (of course) but can we let them go wild with sounds and then tell them if we think it sounds good or not? I'd like to think after a training class has been made (perhaps comparing the output to an existing song) we could get an NN that has a basic concept of music.</p>

<p>Timing would be an issue; I'm not sure how feasible this is. A strongly weighted input attached to all hidden layers perhaps? Use it as the bias?</p>

<p>Is this even slightly feasible? </p>
",,2,2016-08-05T08:51:50.657,2.0,1360,2016-08-18T21:36:41.180,2016-08-18T21:36:41.180,,145.0,,1284.0,,1,6,<deep-network><machine-learning>,Has any research been done on DNN Music?,78.0,89.58,7.02,8.31,0.0,0.0,13.0,DNNs are typically used to classify things of course but can we let them go wild with sounds and then tell them if we think it sounds good or not Id like to think after a training class has been made perhaps comparing the output to an existing song we could get an NN that has a basic concept of music Timing would be an issue Im not sure how feasible this is A strongly weighted input attached to all hidden layers perhaps Use it as the bias Is this even slightly feasible
,,"<p><a href=""https://en.wikipedia.org/wiki/Monte_Carlo_method"">Monte Carlo method</a> is an approach where you generate a large number of random values or simulations and form some sort of conlusions based on the general patterns, such as the means and variances.</p>

<p>As an example, you could use it for <a href=""https://en.wikipedia.org/wiki/Numerical_weather_prediction"">weather forecasts</a>. Predicting long-term weather is quite difficult, because it is a chaotic system where small changes can lead to very different results. Using Monte Carlo methods, you could run a large number of simulations, each with slightly different atmospheric changes. Then you can analyze the results and for example calculate the probability of rain on a given day based on how many simulations ended up with rain. </p>

<p>As for the use of Monte Carlo in Alpha Go, they seem to be using the so-called <a href=""https://en.wikipedia.org/wiki/Monte_Carlo_tree_search"">Monte Carlo Tree Search</a>. In this approach, you make a tree of possible moves, a few turns into the future, and try to find the best sequence. However, since the number of possible moves in the game of go is very large, you won't be able to explore very far ahead. This means that some of the moves which look good now might turn out to be bad later. </p>

<p>So, in the Monte Carlo Tree Search, you pick a promising sequence of moves and run one or more simulations of how the game might proceed from that point. Then you can use the results of that simulation to get a better idea of how good that specific sequence of moves really is and you update the tree accordingly. Repeat as needed until you find a good move.</p>

<p>If you want more information or to look at some illustrations, I found an interesting paper on the topic: C. Browne et al., A Survey of Monte Carlo Tree Search Methods (<a href=""http://repository.essex.ac.uk/4117/1/MCTS-Survey.pdf"">open repository</a> / <a href=""http://dx.doi.org/10.1109/TCIAIG.2012.2186810"">permanent link (paywalled)</a>)</p>
",,0,2016-08-05T09:32:09.223,,1361,2016-08-05T09:32:09.223,,,,,30.0,1358.0,2,9,,,,68.1,8.99,8.79,0.0,0.0,38.0,Monte Carlo method is an approach where you generate a large number of random values or simulations and form some sort of conlusions based on the general patterns such as the means and variances As an example you could use it for weather forecasts Predicting longterm weather is quite difficult because it is a chaotic system where small changes can lead to very different results Using Monte Carlo methods you could run a large number of simulations each with slightly different atmospheric changes Then you can analyze the results and for example calculate the probability of rain on a given day based on how many simulations ended up with rain As for the use of Monte Carlo in Alpha Go they seem to be using the socalled Monte Carlo Tree Search In this approach you make a tree of possible moves a few turns into the future and try to find the best sequence However since the number of possible moves in the game of go is very large you wont be able to explore very far ahead This means that some of the moves which look good now might turn out to be bad later So in the Monte Carlo Tree Search you pick a promising sequence of moves and run one or more simulations of how the game might proceed from that point Then you can use the results of that simulation to get a better idea of how good that specific sequence of moves really is and you update the tree accordingly Repeat as needed until you find a good move If you want more information or to look at some illustrations I found an interesting paper on the topic C Browne et al A Survey of Monte Carlo Tree Search Methods open repository permanent link paywalled
1365.0,1.0,"<p>How do I avoid my gradient descent algorithm into falling into the ""local minima"" trap while backpropogating on my neural network?</p>

<p>Are there any methods which help me avoid it?</p>
",2016-08-05T18:32:58.937,4,2016-08-05T10:39:31.520,1.0,1362,2016-08-05T11:00:14.427,,,,,101.0,,1,4,<neural-networks><backpropagation><gradient-descent>,"How to avoid falling into the ""local minima"" trap?",64.0,56.25,10.55,11.75,0.0,0.0,4.0,How do I avoid my gradient descent algorithm into falling into the local minima trap while backpropogating on my neural network Are there any methods which help me avoid it
,3.0,"<p>A neural network is a directed weighted graph. These can be represented by a (sparse) matrix. Doing so can expose some elegant properties of the network.</p>

<p>Is this technique beneficial for examining neural networks?</p>
",,2,2016-08-05T10:49:39.557,,1363,2017-02-16T19:38:44.487,,,,,1283.0,,1,8,<neural-networks>,Is it beneficial to represent a neural net as a matrix?,115.0,54.39,11.33,10.56,0.0,0.0,6.0,A neural network is a directed weighted graph These can be represented by a sparse matrix Doing so can expose some elegant properties of the network Is this technique beneficial for examining neural networks
,,"<p>It depends on the type of neural networks you are dealing with.</p>

<p>For medium sized neural nets, the matrix approach is a very good way to do quick computations and even backpropogation of errors. One can even exploit sparse matrixes for understanding the sparse architecture of some neural nets.</p>

<p>But, for very large neural nets, using matrix computations would be computationally very intensive. So, relevant methods like graph-based stores, etc are used for them depending on the purpose and the architecture.</p>
",,0,2016-08-05T10:58:53.260,,1364,2017-02-16T19:38:44.487,2017-02-16T19:38:44.487,,101.0,,101.0,1363.0,2,4,,,,46.57,13.05,9.9,0.0,0.0,11.0,It depends on the type of neural networks you are dealing with For medium sized neural nets the matrix approach is a very good way to do quick computations and even backpropogation of errors One can even exploit sparse matrixes for understanding the sparse architecture of some neural nets But for very large neural nets using matrix computations would be computationally very intensive So relevant methods like graphbased stores etc are used for them depending on the purpose and the architecture
,,"<p>There are several elementary techniques to try and move a search out of the basin of attraction of local optima. They include:</p>

<ul>
<li>Probabalistically accepting worse solutions in the hope that this
will jump out of the current basin (like Metropolis-Hastings acceptance in Simulated Annealing). </li>
<li>Maintaining a list of recently-encountered states (or attributes thereof) and not returning
to a recently-encountered one (like Tabu Search). </li>
<li>Performing a random walk of a length determined by the current state of the search (an explicit 'Diversification strategy', e.g. as used in 'Reactive Tabu Search').</li>
</ul>

<p>See the excellent (and free online) book <a href=""https://cs.gmu.edu/~sean/book/metaheuristics/"">'Essentials of Metaheuristics'</a> by Sean Luke for more details on these kind of techniques and some rules of thumb about when and how to use them.</p>
",,0,2016-08-05T11:00:14.427,,1365,2016-08-05T11:00:14.427,,,,,42.0,1362.0,2,5,,,,42.41,14.16,9.56,0.0,0.0,28.0,There are several elementary techniques to try and move a search out of the basin of attraction of local optima They include Probabalistically accepting worse solutions in the hope that this will jump out of the current basin like MetropolisHastings acceptance in Simulated Annealing Maintaining a list of recentlyencountered states or attributes thereof and not returning to a recentlyencountered one like Tabu Search Performing a random walk of a length determined by the current state of the search an explicit Diversification strategy eg as used in Reactive Tabu Search See the excellent and free online book Essentials of Metaheuristics by Sean Luke for more details on these kind of techniques and some rules of thumb about when and how to use them
,,"<p>For large ANNs, something equivalent to a 'sparse matrix format' is used in practice.</p>

<p>In contrast to what is said in another answer given, considering an ANN as a graph doesn't actually buy very much, for two reasons:</p>

<ol>
<li><p>The backpropagation algorithm can usefully be
defined in terms of matrix operations. <a href=""http://briandolhansky.com/blog/2014/10/30/artificial-neural-networks-matrix-form-part-5"" rel=""nofollow"">This page</a> gives a
readable and comprehensive description.</p></li>
<li><p>All real-valued matrices can be represented as graphs, but the converse is clearly not the case. So while it is true that an ANN can be considered as a special case of a graph data structure, making that specialization explicit in matrix form is more efficient.</p></li>
</ol>
",,0,2016-08-05T11:15:40.017,,1366,2016-08-05T19:12:14.883,2016-08-05T19:12:14.883,,42.0,,42.0,1363.0,2,4,,,,50.36,11.66,10.29,0.0,0.0,15.0,For large ANNs something equivalent to a sparse matrix format is used in practice In contrast to what is said in another answer given considering an ANN as a graph doesnt actually buy very much for two reasons The backpropagation algorithm can usefully be defined in terms of matrix operations This page gives a readable and comprehensive description All realvalued matrices can be represented as graphs but the converse is clearly not the case So while it is true that an ANN can be considered as a special case of a graph data structure making that specialization explicit in matrix form is more efficient
,,"<p>An easy method would be to use a poem (short one, two paragraphs) then give an one line test question regarding the emotional states of the poem, that an AI won't or can't be programmed to understand.</p>
",,10,2016-08-05T11:55:03.030,,1367,2016-08-05T15:56:46.897,2016-08-05T15:56:46.897,,29.0,,1282.0,1354.0,2,0,,,,50.84,9.01,9.31,0.0,0.0,7.0,An easy method would be to use a poem short one two paragraphs then give an one line test question regarding the emotional states of the poem that an AI wont or cant be programmed to understand
,,"<p>The first thing is to define what is a «good» and a «bad» sound. This is an extremely tricky issue, since the networks need <em>numeric</em> inputs. And music is whole bunch of numbers.</p>

<p>I know from people doing research in identifying <em>how similar</em> two sounds are, and imitation, say: you hear a sound and try to make another that sounds like it. Like when you hum a song or similar. That is by no means easy. These guys are using something similar to feature extraction, with Fourier transforms and energy and <a href=""https://en.wikipedia.org/wiki/Music_information_retrieval"" rel=""nofollow"">such things</a>. They feed the networks with the (selected) features and... Train. </p>

<p>Now, to return to your original question: *What do you present as <em>target</em> during training?* You can present different <em>types</em> of music as categories and classify (I couldn't help but think on <a href=""https://link.springer.com/article/10.3758/BF03192900"" rel=""nofollow"">this research with fish</a>). Or <strong>you</strong> define categories of music <strong>you</strong> like and see if the network can classify them ;)</p>

<p>One basic decision here is how long you get a piece of sound. Since it is needed to analyse frequency, this is a key issue. Since you talked about DNN, I was wondering if you wanted to do it <em>online</em>, as a stream, in which case I don't have the slightest idea where to begin, other than do it <em>after a little while</em>.</p>

<hr>

<p>Other idea: I remember a little sketch in <a href=""http://www.bbc.co.uk/programmes/b012xppj"" rel=""nofollow"">this series</a> about a researcher that makes use of the relations between peaks in the Fourier spectrum in order to differentiate noise from music.</p>
",,0,2016-08-05T12:05:13.230,,1369,2016-08-05T12:05:13.230,,,,,70.0,1360.0,2,5,,,,70.23,8.64,8.76,0.0,0.0,40.0,The first thing is to define what is a «good» and a «bad» sound This is an extremely tricky issue since the networks need numeric inputs And music is whole bunch of numbers I know from people doing research in identifying how similar two sounds are and imitation say you hear a sound and try to make another that sounds like it Like when you hum a song or similar That is by no means easy These guys are using something similar to feature extraction with Fourier transforms and energy and such things They feed the networks with the selected features and Train Now to return to your original question What do you present as target during training You can present different types of music as categories and classify I couldnt help but think on this research with fish Or you define categories of music you like and see if the network can classify them One basic decision here is how long you get a piece of sound Since it is needed to analyse frequency this is a key issue Since you talked about DNN I was wondering if you wanted to do it online as a stream in which case I dont have the slightest idea where to begin other than do it after a little while Other idea I remember a little sketch in this series about a researcher that makes use of the relations between peaks in the Fourier spectrum in order to differentiate noise from music
,,"<p>It seems easy for this to be sublinear growth or superlinear growth, depending on context. </p>

<p>If we imagine the space of the complex AI as split into two parts--the context model and the content model (that is, information and structure that is expected to be shared across entries vs. information and structure that is local to particular entries), then expanding the source material means we don't have much additional work to do on the context model, but whether the additional piece of the content model is larger or smaller depends on how connected the new material is to the old material.</p>

<p>That is, one of the reasons why Watson takes many times the space of its source material is because it stores links between objects, which one would expect to grow with roughly order <em>n</em> squared. If there are many links between the old and new material, then we should expect it to roughly quadruple in size instead of double; if the old material and new material are mostly unconnected and roughly the same in topology, then we expect the model to roughly double; if the new material is mostly unconnected to the old material and also mostly unconnected to itself, then we expect the model to not grow by much.</p>
",,1,2016-08-05T13:55:54.380,,1370,2016-08-05T13:55:54.380,,,,,10.0,1333.0,2,3,,,,37.1,10.87,9.62,0.0,0.0,21.0,It seems easy for this to be sublinear growth or superlinear growth depending on context If we imagine the space of the complex AI as split into two partsthe context model and the content model that is information and structure that is expected to be shared across entries vs information and structure that is local to particular entries then expanding the source material means we dont have much additional work to do on the context model but whether the additional piece of the content model is larger or smaller depends on how connected the new material is to the old material That is one of the reasons why Watson takes many times the space of its source material is because it stores links between objects which one would expect to grow with roughly order n squared If there are many links between the old and new material then we should expect it to roughly quadruple in size instead of double if the old material and new material are mostly unconnected and roughly the same in topology then we expect the model to roughly double if the new material is mostly unconnected to the old material and also mostly unconnected to itself then we expect the model to not grow by much
,,"<p>If anything, multiple intelligences are much more obvious in AI than in other fields, because we haven't yet unlocked how to do transfer between domains.</p>

<p>As an example, AlphaGo is very, very good at playing Go, but it's got basically nothing in the way of bodily-kinesthetic intelligence. But other teams have built software to control robots that does have bodily-kinesthetic intelligence, while not being good at the tasks that AlphaGo excels at.</p>

<p>This sort of modular intelligence is typically referred to as 'narrow AI,' whereas we use the term 'general AI' (or AGI, for Artificial General Intelligence) to refer to intelligence that we've built that can do roughly as many different kinds of things as people can do.</p>
",,0,2016-08-05T13:59:51.080,,1371,2016-08-05T13:59:51.080,,,,,10.0,179.0,2,2,,,,41.53,12.95,10.59,0.0,0.0,23.0,If anything multiple intelligences are much more obvious in AI than in other fields because we havent yet unlocked how to do transfer between domains As an example AlphaGo is very very good at playing Go but its got basically nothing in the way of bodilykinesthetic intelligence But other teams have built software to control robots that does have bodilykinesthetic intelligence while not being good at the tasks that AlphaGo excels at This sort of modular intelligence is typically referred to as narrow AI whereas we use the term general AI or AGI for Artificial General Intelligence to refer to intelligence that weve built that can do roughly as many different kinds of things as people can do
,,"<p>""Current artificial intelligence research"" is a pretty broad field. From where I sit, in a mostly CS realm, people are focused on narrow intelligence that can do economically relevant work on narrow tasks. (That is, predicting when components will fail, predicting which ads a user will click on, and so on.)</p>

<p>For those sorts of tools, the generality of a formalism like AIXI is a weakness instead of a strength. You don't need to take an AI that could in theory compute anything, and then slowly train it to focus on what you want, when you could just directly shape a tool that is the mirror of your task.</p>

<p>I'm not as familiar with AGI research itself, but my impression is that AIXI is, to some extent, the simplest idea that could work--it takes all the hard part and pushes it into computation, so it's 'just an engineering challenge.' (This is the bit about 'finding approximations to AIXI.') The question then becomes, is starting at AIXI and trying to approximate down a more or less fruitful research path than starting at something small and functional, and trying to build up?</p>

<p>My impression is the latter is much more common, but again, I only see a small corner of this space.</p>
",,0,2016-08-05T14:05:19.287,,1372,2016-08-05T14:05:19.287,,,,,10.0,145.0,2,4,,,,64.75,9.99,9.08,0.0,0.0,40.0,Current artificial intelligence research is a pretty broad field From where I sit in a mostly CS realm people are focused on narrow intelligence that can do economically relevant work on narrow tasks That is predicting when components will fail predicting which ads a user will click on and so on For those sorts of tools the generality of a formalism like AIXI is a weakness instead of a strength You dont need to take an AI that could in theory compute anything and then slowly train it to focus on what you want when you could just directly shape a tool that is the mirror of your task Im not as familiar with AGI research itself but my impression is that AIXI is to some extent the simplest idea that could workit takes all the hard part and pushes it into computation so its just an engineering challenge This is the bit about finding approximations to AIXI The question then becomes is starting at AIXI and trying to approximate down a more or less fruitful research path than starting at something small and functional and trying to build up My impression is the latter is much more common but again I only see a small corner of this space
,,"<p>AIXI is really a conceptual framework. All the hard work of actually compressing the environment still remains.</p>

<p>To further discuss the question raised in Matthew Graves answer: given our current limited level of ability to represent complex environments, it seems to me that it doesn't make a lot of practical difference whether you start with AIXI as defining the 'top' of the system and working down (e.g. via supposedly generalized compression methods) or start at the 'bottom' and try solve problems in a single domain via domain-specific methods that (you hope) can subsequently be abstracted to provide cross-domain compression.</p>
",,0,2016-08-05T14:25:48.467,,1373,2016-08-05T14:25:48.467,,,,,42.0,145.0,2,2,,,,46.3,14.39,11.88,0.0,0.0,18.0,AIXI is really a conceptual framework All the hard work of actually compressing the environment still remains To further discuss the question raised in Matthew Graves answer given our current limited level of ability to represent complex environments it seems to me that it doesnt make a lot of practical difference whether you start with AIXI as defining the top of the system and working down eg via supposedly generalized compression methods or start at the bottom and try solve problems in a single domain via domainspecific methods that you hope can subsequently be abstracted to provide crossdomain compression
,2.0,"<p>Would it be ethical to implement AI for self-defence for public walking robots which are exposed to dangers such as violence and crime such as robbery (of parts), damage or abduction?</p>

<p>What would be pros and cons of such AI behavior? Is it realistic, or it won't be taken into account for some obvious reasons?</p>

<p>Like pushing back somebody when somebody start pushing it first (AI will say: he pushed me first), or running away on crowded street in case algorithm will detect risk of abduction.</p>
",,1,2016-08-05T16:51:49.833,,1376,2016-08-05T17:08:27.273,2016-08-05T17:07:35.410,,8.0,,8.0,,1,1,<ethics><decision-theory><robots>,Is it ethical to implement self-defence for street walking AI robots?,42.0,66.57,10.1,9.84,0.0,0.0,14.0,Would it be ethical to implement AI for selfdefence for public walking robots which are exposed to dangers such as violence and crime such as robbery of parts damage or abduction What would be pros and cons of such AI behavior Is it realistic or it wont be taken into account for some obvious reasons Like pushing back somebody when somebody start pushing it first AI will say he pushed me first or running away on crowded street in case algorithm will detect risk of abduction
,,"<p>The question mentions ""walking robot"", but it may be illustrative to re-frame the discussion in terms of self-driving cars, because: </p>

<ul>
<li>It gives a common point of reference, rather than everyone having their own separate vision of how vulnerable/powerful a kung-fu walking robot might be.</li>
<li>We already know a lot about societal attitudes to car theft.</li>
<li>Given that autonomous vehicles will soon be mainstream, the morality of the question is then more of a pressing issue.</li>
</ul>

<p>So, should a self-driving car run someone over (likely killing them) if they try to steal it? I'm hoping that few people would argue that it should.</p>

<p>Should it attempt to do a lesser amount of damage (say, calculated to hopefully only break a leg)?</p>

<p>Again, I'd argue not. The main reason for saying this is that our decision-making algorithms are simply not sufficiently context aware to be able to decide whether theft or harm is the intent. To concretely illustrate this: a recent fatality arose because a self-driving Tesla <a href=""http://www.livescience.com/55273-first-self-driving-car-fatality.html"" rel=""nofollow"">was oblivious to context</a> to the extent that it couldn't distinguish between a high-sided van and empty space.</p>

<p>Under those circumstances, it's probably best not to allow commercial autonomous systems to cause physical damage (even to inanimate objects). </p>

<p>'Running away' (or rather, 'driving away', in the case of the car) is another matter: driving is what it's designed to do.</p>
",,0,2016-08-05T17:06:27.950,,1377,2016-08-05T17:06:27.950,,,,,42.0,1376.0,2,2,,,,50.67,12.18,10.83,0.0,0.0,51.0,The question mentions walking robot but it may be illustrative to reframe the discussion in terms of selfdriving cars because It gives a common point of reference rather than everyone having their own separate vision of how vulnerablepowerful a kungfu walking robot might be We already know a lot about societal attitudes to car theft Given that autonomous vehicles will soon be mainstream the morality of the question is then more of a pressing issue So should a selfdriving car run someone over likely killing them if they try to steal it Im hoping that few people would argue that it should Should it attempt to do a lesser amount of damage say calculated to hopefully only break a leg Again Id argue not The main reason for saying this is that our decisionmaking algorithms are simply not sufficiently context aware to be able to decide whether theft or harm is the intent To concretely illustrate this a recent fatality arose because a selfdriving Tesla was oblivious to context to the extent that it couldnt distinguish between a highsided van and empty space Under those circumstances its probably best not to allow commercial autonomous systems to cause physical damage even to inanimate objects Running away or rather driving away in the case of the car is another matter driving is what its designed to do
,,"<p>It depends on whether the loss of the robot would end up causing harm to humans.</p>

<p>If the robot was supposed to be watching for a suspected terrorist attack to start taking place (so it could alert authorities or halt the attack), it would be very bad if somebody dismantled the robot or otherwise stopped it from carrying out its mission. In that case, the device would be certainly justified in stopping humans from injuring it in any meaningful way.</p>

<p>A robot carrying classified information should probably be similarly willing to protect itself, since the spread of such data could bring harm to a state or a lot of people.</p>

<p>If an AI-enabled device was just walking the streets in the course of carrying out some mundane task, I think it would be hard to justify allowing the robot to incapacitate a human attacker. After all, it was made - presumably - to serve humans.</p>

<p>No matter whether the AI was programmed to defend itself, people couldn't just impede or damage it with impunity. <a href=""https://malegislature.gov/laws/generallaws/partiv/titlei/chapter266/section127"" rel=""nofollow"">Intentional destruction</a> of another person's property (including public property) is almost certainly a crime, as is <a href=""http://law.justia.com/codes/georgia/2010/title-16/chapter-10/article-2/16-10-24"" rel=""nofollow"">intentional obstruction of law enforcement</a>. It wouldn't have to be up to each robot to defend itself; it could just send information about the perpetrator to C&amp;C before its demise.</p>
",,0,2016-08-05T17:08:27.273,,1378,2016-08-05T17:08:27.273,,,,,75.0,1376.0,2,2,,,,55.37,11.38,10.05,0.0,0.0,29.0,It depends on whether the loss of the robot would end up causing harm to humans If the robot was supposed to be watching for a suspected terrorist attack to start taking place so it could alert authorities or halt the attack it would be very bad if somebody dismantled the robot or otherwise stopped it from carrying out its mission In that case the device would be certainly justified in stopping humans from injuring it in any meaningful way A robot carrying classified information should probably be similarly willing to protect itself since the spread of such data could bring harm to a state or a lot of people If an AIenabled device was just walking the streets in the course of carrying out some mundane task I think it would be hard to justify allowing the robot to incapacitate a human attacker After all it was made presumably to serve humans No matter whether the AI was programmed to defend itself people couldnt just impede or damage it with impunity Intentional destruction of another persons property including public property is almost certainly a crime as is intentional obstruction of law enforcement It wouldnt have to be up to each robot to defend itself it could just send information about the perpetrator to CampC before its demise
1380.0,1.0,"<p>Is there any risk in the near future of replacing all encyclopedias with Watson-like AI where knowledge is accessible by everybody through <a href=""https://watson-api-explorer.mybluemix.net/"" rel=""nofollow"">API</a>?</p>

<p><sup>Something similar happened in the future in <a href=""https://en.wikipedia.org/wiki/The_Time_Machine_(2002_film)"" rel=""nofollow""><strong>The Time Machine</strong> movie from 2002</a>.</sup></p>

<p>Obviously maintaining 40 million articles and keeping it up-to-date and consistent could be beyond brain power of few thousands of active editors. Not to mention thousands of other encyclopedias including paperback version or large number of books used by universities which needs to be updated every year by a huge number of people.</p>

<p>What are the pros and cons of such a change?</p>
",2016-08-12T15:19:24.397,1,2016-08-05T17:11:34.880,,1379,2016-08-11T12:21:13.530,2016-08-11T12:21:13.530,,145.0,,8.0,,1,-2,<watson>,How likely is it that Watson-like AI will replace Wikipedia-like encyclopedias?,42.0,51.38,12.13,10.2,0.0,0.0,8.0,Is there any risk in the near future of replacing all encyclopedias with Watsonlike AI where knowledge is accessible by everybody through API Something similar happened in the future in The Time Machine movie from 2002 Obviously maintaining 40 million articles and keeping it uptodate and consistent could be beyond brain power of few thousands of active editors Not to mention thousands of other encyclopedias including paperback version or large number of books used by universities which needs to be updated every year by a huge number of people What are the pros and cons of such a change
,,"<p>I get the impression that (perhaps even more than Bluemix) this is what the <a href=""https://www.wolfram.com/language/elementary-introduction/"" rel=""nofollow"">Wolfram Language</a> is looking to offer in the longer term.</p>

<p>Seems to me that the main pros and cons are two sides of the same coin:</p>

<p>With Wikipedia, there's no 'search filter' between you and the text. Adding an algorithmic level of indirection between the user and the knowledge that they're looking for is subject to hidden biases.</p>

<p>If those biases are intended in your best interests, and the search is context-sensitive enough to present you with information in the form that is most useful and digestible to you, then this is a good thing. Otherwise, not. Like many topics in AI, problems arise because we're simply not that good at modelling human context yet.</p>

<p>Of course, we're already subject to this <a href=""https://en.wikipedia.org/wiki/Filter_bubble"" rel=""nofollow"">filter bubble</a> effect via search engines and social media. The current consensus seems to be that even more of this would not be a good thing for society.</p>
",,0,2016-08-05T17:30:07.273,,1380,2016-08-05T17:37:44.953,2016-08-05T17:37:44.953,,42.0,,42.0,1379.0,2,3,,,,64.64,10.22,9.8,0.0,0.0,24.0,I get the impression that perhaps even more than Bluemix this is what the Wolfram Language is looking to offer in the longer term Seems to me that the main pros and cons are two sides of the same coin With Wikipedia theres no search filter between you and the text Adding an algorithmic level of indirection between the user and the knowledge that theyre looking for is subject to hidden biases If those biases are intended in your best interests and the search is contextsensitive enough to present you with information in the form that is most useful and digestible to you then this is a good thing Otherwise not Like many topics in AI problems arise because were simply not that good at modelling human context yet Of course were already subject to this filter bubble effect via search engines and social media The current consensus seems to be that even more of this would not be a good thing for society
1382.0,1.0,"<p>I've watched the <a href=""https://www.youtube.com/watch?v=LY7x2Ihqjmc"" rel=""nofollow"">Sunspring</a> video which didn't make any sense to me (a lot of nonsense monologues), mainly because it was created by Jetson AI.</p>

<p>What was the mechanism of creating such screenplay?</p>

<p>On what criteria was it trained? What was the goal or motivation in terms of training criteria of defining when text does make sense? And what was missed (that it's so bad) and how possibly this could be improved?</p>
",,0,2016-08-05T17:38:23.270,,1381,2016-08-06T01:44:14.583,2016-08-06T01:44:14.583,,130.0,,8.0,,1,2,<algorithm>,How does the Jetson AI write its screenplays?,57.0,73.78,9.27,8.96,0.0,0.0,13.0,Ive watched the Sunspring video which didnt make any sense to me a lot of nonsense monologues mainly because it was created by Jetson AI What was the mechanism of creating such screenplay On what criteria was it trained What was the goal or motivation in terms of training criteria of defining when text does make sense And what was missed that its so bad and how possibly this could be improved
,,"<p>It <a href=""http://benjamin.wtf/"" rel=""nofollow"">appears to use</a> Recurrent NNs (RNNs) that have a 'Long Short-Term Memory' (LTSM) architecture.</p>

<p><a href=""https://medium.com/artists-and-machine-intelligence/adventures-in-narrated-reality-6516ff395ba3#.5lvtgribl"" rel=""nofollow"">Here's a summary</a> of the development process that the author, Ross Goodwin, went through to create it.</p>

<p>It seems to me (and is also observed in the above link) that the output is rather poor - simply comparable to what one might expect from Markov chains, a technique that is over 100 years old.</p>

<p>I haven't dug deeply into the technique, so I could be misktaken, but perhaps one of the reasons that it's so bad is that (as far as I can see), the model-building process is essentially <em>lexical</em> - i.e. it is linking together tokens (words) without any more informed language model to guide it. In particular, the generated output doesn't seem to know anything about the functional roles played by objects (chairs are supporting objects, used by humans for sitting on etc), which is something that might be fairly readily incorporated.</p>
",,0,2016-08-05T17:52:10.297,,1382,2016-08-05T18:23:38.027,2016-08-05T18:23:38.027,,42.0,,42.0,1381.0,2,5,,,,53.34,11.38,10.57,0.0,0.0,38.0,It appears to use Recurrent NNs RNNs that have a Long ShortTerm Memory LTSM architecture Heres a summary of the development process that the author Ross Goodwin went through to create it It seems to me and is also observed in the above link that the output is rather poor simply comparable to what one might expect from Markov chains a technique that is over 100 years old I havent dug deeply into the technique so I could be misktaken but perhaps one of the reasons that its so bad is that as far as I can see the modelbuilding process is essentially lexical ie it is linking together tokens words without any more informed language model to guide it In particular the generated output doesnt seem to know anything about the functional roles played by objects chairs are supporting objects used by humans for sitting on etc which is something that might be fairly readily incorporated
1385.0,2.0,"<p>This <a href=""http://blog.claymcleod.io/2016/06/01/The-truth-about-Deep-Learning/"" rel=""nofollow"">article</a> suggests that deep learning is not designed to produce the universal algorithm and cannot be used to create such a complex systems.</p>

<p>First of all it requires huge amounts of computing power, time and effort to train the algorithm the right way and adding extra layers doesn't really help to solve complex problems which cannot be easily predicted.</p>

<p>Secondly some tasks are extremely difficult or impossible to solve using DNN, like solving a <a href=""http://ai.stackexchange.com/q/154/8"">math</a> equations, predicting <a href=""http://ai.stackexchange.com/q/225/8"">pseudo-random lists</a>, <a href=""http://ai.stackexchange.com/q/168/8"">fluid mechanics</a>, guessing encryption algorithms, or <a href=""http://ai.stackexchange.com/q/205/8"">decompiling</a> unknown formats, because there is no simple mapping between input and output.</p>

<p>So I'm asking, are there any alternative learning algorithms as powerful as deep architectures for general purpose problem solving? Which can solve more variety of problems, than ""deep"" architectures cannot?</p>
",,6,2016-08-05T18:22:41.003,1.0,1384,2016-08-07T07:11:09.847,2016-08-06T01:45:40.897,,8.0,,8.0,,1,2,<deep-network><comparison><architecture>,"Are there any learning algorithms as powerful as ""deep"" architectures?",86.0,36.83,15.5,11.89,0.0,0.0,19.0,This article suggests that deep learning is not designed to produce the universal algorithm and cannot be used to create such a complex systems First of all it requires huge amounts of computing power time and effort to train the algorithm the right way and adding extra layers doesnt really help to solve complex problems which cannot be easily predicted Secondly some tasks are extremely difficult or impossible to solve using DNN like solving a math equations predicting pseudorandom lists fluid mechanics guessing encryption algorithms or decompiling unknown formats because there is no simple mapping between input and output So Im asking are there any alternative learning algorithms as powerful as deep architectures for general purpose problem solving Which can solve more variety of problems than deep architectures cannot
,,"<p>Have you read the book <a href=""http://golibgen.io/view.php?id=1397686"" rel=""nofollow noreferrer"">The Master Algorithm:</a> by Pedro Domingos?</p>

<p>He discusses the present day machine learning algorithms... Their strengths, weaknesses and applications...</p>

<ul>
<li>Deep Neural Network</li>
<li>Genetic Algorithm</li>
<li>Bayesian Network </li>
<li>Support Vector Machine</li>
<li>Inverse Deduction </li>
</ul>

<p><a href=""https://i.stack.imgur.com/9HpIP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9HpIP.png"" alt=""enter image description here""></a></p>
",,1,2016-08-05T18:41:59.757,,1385,2016-08-07T07:11:09.847,2016-08-07T07:11:09.847,,157.0,,157.0,1384.0,2,4,,,,45.42,18.3,13.29,0.0,0.0,9.0,Have you read the book The Master Algorithm by Pedro Domingos He discusses the present day machine learning algorithms Their strengths weaknesses and applications Deep Neural Network Genetic Algorithm Bayesian Network Support Vector Machine Inverse Deduction
,,"<p>Deep learning is actually pretty useful (relative to other techniques) <em>precisely when there is no simple mapping between input and output</em>, and features from the raw input need to be aggregated and combined in complex ways by successive layers to form the output.</p>

<p>As I pointed out in my answer to the <a href=""http://ai.stackexchange.com/questions/205/how-to-write-c-decompiler-using-ai"">AI SE decompilation question</a>, there is recent DL research which takes a natural language description as input and <a href=""http://arxiv.org/pdf/1510.07211.pdf"" rel=""nofollow"">generates program text as output</a>. Despite working in this general research area, I was personally surprised by this - the problem is significantly harder than the 'AI math' link you provide above.</p>
",,0,2016-08-05T18:44:20.053,,1386,2016-08-05T18:44:20.053,,,,,42.0,1384.0,2,3,,,,37.27,12.54,11.87,0.0,0.0,11.0,Deep learning is actually pretty useful relative to other techniques precisely when there is no simple mapping between input and output and features from the raw input need to be aggregated and combined in complex ways by successive layers to form the output As I pointed out in my answer to the AI SE decompilation question there is recent DL research which takes a natural language description as input and generates program text as output Despite working in this general research area I was personally surprised by this the problem is significantly harder than the AI math link you provide above
,,"<blockquote>
  <p>""the human mind is a battleground of higher level goals and lower level goals ""<br>— Marvin Minsky paraphrasing Sigmund Freud</p>
</blockquote>

<p>I argue that in general human agents try to maximise a hierarchy of performance measures.</p>

<h1>performance measures of humans</h1>

<ul>
<li><p>Survival of genetic data </p>

<ul>
<li><p>Energy supply and Water</p></li>
<li><p>Sex</p>

<ul>
<li><em>myriad subgoals....</em></li>
</ul></li>
</ul></li>
</ul>

<p>Mysterious mental mechanisms which neuroscientists do not understand yet force the average human agent to maximise various evaluation metrics.
With the overarching goal of <strong>survival of genetic information</strong>. Successful genes are immortal. We are still under the yoke of an ancient genetic algorithm.</p>

<p>These measures are optimised throughout a humans life time. A 30 year old agent is better at survival than a 10 year old agent. A 30 year old agent makes fewer mistakes.</p>

<p>We remember our mistakes. Mistakes are burned into our memory by high levels of neurotransmitters (and reinforcing of synapses) so we don't make them again.</p>

<p>We attempt to optimise a swarm of subgoals that are all connected in one way or another to the main goal <strong>gene survival</strong>.</p>

<ul>
<li><p>status</p></li>
<li><p>money</p></li>
<li><p>education </p></li>
<li><p>happiness</p></li>
</ul>
",,0,2016-08-05T19:31:15.263,,1387,2016-08-05T23:21:31.713,2016-08-05T23:21:31.713,,29.0,,157.0,6.0,2,2,,,,49.21,12.17,10.64,0.0,0.0,20.0,the human mind is a battleground of higher level goals and lower level goals — Marvin Minsky paraphrasing Sigmund Freud I argue that in general human agents try to maximise a hierarchy of performance measures performance measures of humans Survival of genetic data Energy supply and Water Sex myriad subgoals Mysterious mental mechanisms which neuroscientists do not understand yet force the average human agent to maximise various evaluation metrics With the overarching goal of survival of genetic information Successful genes are immortal We are still under the yoke of an ancient genetic algorithm These measures are optimised throughout a humans life time A 30 year old agent is better at survival than a 10 year old agent A 30 year old agent makes fewer mistakes We remember our mistakes Mistakes are burned into our memory by high levels of neurotransmitters and reinforcing of synapses so we dont make them again We attempt to optimise a swarm of subgoals that are all connected in one way or another to the main goal gene survival status money education happiness
,,"<p>One obstacle to the development of AI is the fundamental limitations of computer memory. Computers, at a fundamental level, can only work with bits. This limits the type of information that they can describe.</p>

<p>EDIT:</p>

<p>The precise nature and complexity of human memory isn't fully understood, but I would argue that at the very least, human memory is well adapted for the types of tasks that humans perform. Thus, computer memory, even if theoretically capable of representing everything that human memory can, is  probably inefficient and poorly structured for such a task.  </p>
",,3,2016-08-05T20:08:43.130,,1389,2016-08-08T17:36:35.903,2016-08-08T17:36:35.903,,127.0,,127.0,60.0,2,-2,,,,44.34,12.88,10.56,0.0,0.0,14.0,One obstacle to the development of AI is the fundamental limitations of computer memory Computers at a fundamental level can only work with bits This limits the type of information that they can describe EDIT The precise nature and complexity of human memory isnt fully understood but I would argue that at the very least human memory is well adapted for the types of tasks that humans perform Thus computer memory even if theoretically capable of representing everything that human memory can is probably inefficient and poorly structured for such a task
1412.0,2.0,"<p>Is there any research which study application of AI into chemistry which can predict the output of certain chemical reactions.</p>

<p>So for example, you train the AI about current compounds, substances, structures and their products and chemical reactions from the existing <a href=""http://opendata.stackexchange.com/q/3553/3082"">dataset</a> (basically what produce what). Then you give the task to find how to create a gold or silver from group of available substances. Then the algorithm will find the chemical reactions (successfully predicting new one which weren't in the dataset) and gives the results. Maybe the gold is not a good example, but the practical scenario would be creation of drugs which are cheaper to create by using much more simpler processes or synthesizing some substances for the first time for drug industries.</p>

<p>Was there any successful research attempting to achieve that using deep learning algorithms?</p>
",,0,2016-08-05T20:47:50.290,,1390,2016-08-11T10:15:24.027,2016-08-08T08:43:15.913,,8.0,,8.0,,1,2,<deep-learning><research><prediction>,Predicting chemical reactions using AI,110.0,48.13,13.58,10.27,0.0,0.0,15.0,Is there any research which study application of AI into chemistry which can predict the output of certain chemical reactions So for example you train the AI about current compounds substances structures and their products and chemical reactions from the existing dataset basically what produce what Then you give the task to find how to create a gold or silver from group of available substances Then the algorithm will find the chemical reactions successfully predicting new one which werent in the dataset and gives the results Maybe the gold is not a good example but the practical scenario would be creation of drugs which are cheaper to create by using much more simpler processes or synthesizing some substances for the first time for drug industries Was there any successful research attempting to achieve that using deep learning algorithms
1399.0,2.0,"<p>Assume that I want to solve an issue with neural network that either I can't fit to already existing topologies (perceptron, Konohen, etc) or I'm simply not aware of the existence of those or I'm unable to understand their mechanics and I rely on my own instead.</p>

<p>How can I deconstruct a problem to find a corresponding neural network topology? By this I don't mean only the size of certain layers, but the number of them, the type of activation functions, the number and the direction of connections, and so on.</p>

<p>I'm a beginner, yet I realized that in some topologies (or, at least in perceptrons) it is very hard if not impossible to understand the inner mechanics as the neurons of the hidden layers don't express any mathematically meaningful context.</p>
",,0,2016-08-05T21:29:37.880,1.0,1391,2016-08-06T14:44:14.077,,,,,1270.0,,1,8,<neural-networks>,"How can I plan the topology of a neural network for a given ""random"" problem?",72.0,46.64,10.92,10.21,0.0,0.0,22.0,Assume that I want to solve an issue with neural network that either I cant fit to already existing topologies perceptron Konohen etc or Im simply not aware of the existence of those or Im unable to understand their mechanics and I rely on my own instead How can I deconstruct a problem to find a corresponding neural network topology By this I dont mean only the size of certain layers but the number of them the type of activation functions the number and the direction of connections and so on Im a beginner yet I realized that in some topologies or at least in perceptrons it is very hard if not impossible to understand the inner mechanics as the neurons of the hidden layers dont express any mathematically meaningful context
1450.0,1.0,"<p>For example there is <a href=""https://en.wikipedia.org/wiki/MNIST_database"" rel=""nofollow"">the MNIST database</a> which is used to test artificial neural network (ANN), however it's not so challenging, because some hierarchical systems of convolutional neural networks manages to get an error rate of 0.23 percent.</p>

<p>Are there any similar, especially the most challenging tasks with dataset which are used as benchmark tests to challenge the AI which are fairly reliable and it's possible to pass, but most AAN are struggling to achieve the lower error rate?</p>
",,2,2016-08-06T00:24:44.493,1.0,1392,2016-08-08T13:58:28.580,2016-08-08T08:41:45.330,,8.0,,8.0,,1,1,<image-recognition><deep-learning><datasets>,What are the most challenging tasks aiming to achieve the lowest error rate?,38.0,31.38,12.95,11.39,0.0,0.0,11.0,For example there is the MNIST database which is used to test artificial neural network ANN however its not so challenging because some hierarchical systems of convolutional neural networks manages to get an error rate of 023 percent Are there any similar especially the most challenging tasks with dataset which are used as benchmark tests to challenge the AI which are fairly reliable and its possible to pass but most AAN are struggling to achieve the lower error rate
,1.0,"<p>This <a href=""http://repository.supsi.ch/5145/1/IDSIA-04-12.pdf"" rel=""nofollow"">study</a> (pages 7-8) shows an attempt at recognizing the traffic signs with lower error rates by using multi-column deep neural networks </p>

<p>Are Google cars using similar techniques of predicting signs using DNN, or are they using some other method?</p>
",,1,2016-08-06T00:37:24.067,,1393,2016-10-09T11:49:20.453,2016-08-18T20:10:33.393,,145.0,,8.0,,1,2,<deep-network><image-recognition><self-driving><classification><cars>,How do Google cars recognize the traffic signs?,210.0,30.88,13.47,12.73,0.0,0.0,6.0,This study pages 78 shows an attempt at recognizing the traffic signs with lower error rates by using multicolumn deep neural networks Are Google cars using similar techniques of predicting signs using DNN or are they using some other method
1408.0,2.0,"<p>I'd like to know whether there were attempts to simulate the whole brain, I'm not talking only about some <a href=""http://ai.stackexchange.com/q/237/8"">ANN on microchips</a>, but brain simulations.</p>
",,0,2016-08-06T01:27:03.500,1.0,1394,2016-08-23T10:49:14.587,2016-08-17T13:03:48.270,,145.0,,8.0,,1,2,<neuromorphic-computing>,Are there any artificial neuromorphic systems which can mimic the brain?,61.0,54.56,11.55,10.56,0.0,0.0,5.0,Id like to know whether there were attempts to simulate the whole brain Im not talking only about some ANN on microchips but brain simulations
,,"<p>Neuromorphic engineering offers various of ways of reproducing the brain’s processing ability.</p>

<p>The recent technology can include IBM's multi-artificial-neuron computer, the world's first artificial nanoscale stochastic phase-change neurons<sup><a href=""http://arstechnica.com/gadgets/2016/08/ibm-phase-change-neurons/?"" rel=""nofollow"">article</a></sup>. Check the: <a href=""http://www.nature.com/nnano/journal/v11/n8/full/nnano.2016.70.html"" rel=""nofollow"">Stochastic phase-change neurons</a> study.</p>

<p>Other can include</p>

<ul>
<li><p><a href=""http://web.stanford.edu/group/brainsinsilicon/neurogrid.html"" rel=""nofollow"">Neurogrid</a>, built by Brains in Silicon at Stanford University is another example for brain simulation. It uses analog computation to emulate ion-channel activity. It emulates neurons using digital circuitry designed to maximize spiking throughput<sup><a href=""https://en.wikipedia.org/wiki/Neuromorphic_engineering#Examples"" rel=""nofollow"">wiki</a></sup>.</p></li>
<li><p><a href=""https://en.wikipedia.org/wiki/SpiNNaker"" rel=""nofollow"">SpiNNaker</a>, which is a manycore computer to simulate the human brain (see <a href=""https://en.wikipedia.org/wiki/Human_Brain_Project"" rel=""nofollow"">Human Brain Project</a>).</p></li>
<li><p><a href=""https://en.wikipedia.org/wiki/SyNAPSE"" rel=""nofollow"">SyNAPSE</a>, a DARPA neuromorphic machine technology, that scales to biological levels. Each chip can have over a million of electronic “neurons” and 256 million electronic synapses between neurons. In 2014 the 5.4 billion transistor chip had one of the highest transistor counts of any chip ever produced. The program is undertaken by HRL, HP and IBM.</p></li>
</ul>
",,0,2016-08-06T01:27:03.500,,1395,2016-08-06T01:27:03.500,,,,,8.0,1394.0,2,3,,,,34.12,17.33,12.23,0.0,0.0,28.0,Neuromorphic engineering offers various of ways of reproducing the brain’s processing ability The recent technology can include IBMs multiartificialneuron computer the worlds first artificial nanoscale stochastic phasechange neuronsarticle Check the Stochastic phasechange neurons study Other can include Neurogrid built by Brains in Silicon at Stanford University is another example for brain simulation It uses analog computation to emulate ionchannel activity It emulates neurons using digital circuitry designed to maximize spiking throughputwiki SpiNNaker which is a manycore computer to simulate the human brain see Human Brain Project SyNAPSE a DARPA neuromorphic machine technology that scales to biological levels Each chip can have over a million of electronic “neurons” and 256 million electronic synapses between neurons In 2014 the 54 billion transistor chip had one of the highest transistor counts of any chip ever produced The program is undertaken by HRL HP and IBM
1406.0,3.0,"<p>On <a href=""https://en.wikipedia.org/wiki/Artificial_intelligence"">the wikipedia page</a> about AI, we can read:</p>

<blockquote>
  <p>Optical character recognition is no longer perceived as an exemplar of ""artificial intelligence"" having become a routine technology.</p>
</blockquote>

<p>On the other hand, the <a href=""https://en.wikipedia.org/wiki/MNIST_database"">MNIST</a> database of handwritten digits is especially designed for training and testing neural networks and their error rates (see: <a href=""https://en.wikipedia.org/wiki/MNIST_database#Classifiers"">Classifiers</a>).</p>

<p>So why does the above quote state that OCR is no longer exemplar of AI?</p>
",,0,2016-08-06T01:57:43.263,,1396,2016-08-08T03:21:22.243,2016-08-07T19:07:57.220,,145.0,,8.0,,1,11,<ocr>,Why can't OCR be perceived as a good example of AI?,395.0,40.38,13.23,11.34,0.0,0.0,11.0,On the wikipedia page about AI we can read Optical character recognition is no longer perceived as an exemplar of artificial intelligence having become a routine technology On the other hand the MNIST database of handwritten digits is especially designed for training and testing neural networks and their error rates see Classifiers So why does the above quote state that OCR is no longer exemplar of AI
1452.0,2.0,"<p><a href=""https://en.wikipedia.org/wiki/Minimum_intelligent_signal_test"" rel=""nofollow"">MIST</a> is a quantiative test of humanness, consisting of ~80k propositions such as:</p>

<ul>
<li>Is Earth a planet?</li>
<li>Is the sun bigger than my foot?</li>
<li>Do people sometimes lie?</li>
<li>etc.</li>
</ul>

<p>Have any AI attempted and passed this test to date?</p>
",,0,2016-08-06T02:04:19.343,2.0,1397,2016-09-06T15:23:41.650,2016-08-15T14:44:41.513,,145.0,,8.0,,1,11,<history><intelligence-testing><turing-test><chat-bots>,Are there any AI that have passed the MIST test so far?,222.0,78.45,7.4,8.58,0.0,0.0,8.0,MIST is a quantiative test of humanness consisting of 80k propositions such as Is Earth a planet Is the sun bigger than my foot Do people sometimes lie etc Have any AI attempted and passed this test to date
,,"<p>I'm not sure if predicting MNIST can be really considered as an AI task. AI problems can be usually framed under the context of an agent acting in an environment. Neural nets and machine learning techniques in general do not have to deal with this framing. Classifiers for example, are learning a mapping between two spaces. Though one could argue that you <em>can</em> frame OCR/image classification as an AI problem - the classifier is the agent, each prediction it makes is an action, and it receives rewards based on its classification accuracy - this is rather unnatural and different from problems that are commonly considered AI problems.</p>
",,0,2016-08-06T05:02:50.820,,1398,2016-08-06T05:02:50.820,,,,,1305.0,1396.0,2,3,,,,50.16,12.01,10.54,0.0,0.0,12.0,Im not sure if predicting MNIST can be really considered as an AI task AI problems can be usually framed under the context of an agent acting in an environment Neural nets and machine learning techniques in general do not have to deal with this framing Classifiers for example are learning a mapping between two spaces Though one could argue that you can frame OCRimage classification as an AI problem the classifier is the agent each prediction it makes is an action and it receives rewards based on its classification accuracy this is rather unnatural and different from problems that are commonly considered AI problems
,,"<p>I think in this case, you'll probably want to use a genetic algorithm to generate a topology rather than working on your own. I personally like <a href=""http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf"" rel=""nofollow"">NEAT Paper</a> (NeuroEvolution of Augmemting Topologies).
(<a href=""http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf"" rel=""nofollow"">NEAT Paper</a>)</p>

<p>The original NEAT paper involves evolving weights for connections, but if you only want a topology, you can use a weighting algorithm instead. You can also mix activation functions if you aren't sure which to use. <a href=""http://blog.otoro.net/2016/05/07/backprop-neat/"" rel=""nofollow"">Here</a> is an example of using backpropagation and multiple neuron types.</p>
",,0,2016-08-06T05:40:41.717,,1399,2016-08-06T14:44:14.077,2016-08-06T14:44:14.077,,1306.0,,1306.0,1391.0,2,6,,,,46.57,12.12,10.68,0.0,0.0,14.0,I think in this case youll probably want to use a genetic algorithm to generate a topology rather than working on your own I personally like NEAT Paper NeuroEvolution of Augmemting Topologies NEAT Paper The original NEAT paper involves evolving weights for connections but if you only want a topology you can use a weighting algorithm instead You can also mix activation functions if you arent sure which to use Here is an example of using backpropagation and multiple neuron types
,,"<p>Another answer mentions <a href=""https://www.cs.ucf.edu/~kstanley/neat.html"" rel=""nofollow"">NEAT</a> to generate network weights/topologies. Here's a <a href=""http://doc.gold.ac.uk/aisb50/AISB50-S11/AISB50-S11-Turner-paper.pdf"" rel=""nofollow"">nice paper</a> on an alternative approach to NEAT, which also gives a short summary of neuroevolution techniques. It uses <a href=""http://www.cartesiangp.co.uk/"" rel=""nofollow"">Cartesian Genetic Programming</a> to evolve a multiple activation functions.</p>
",,0,2016-08-06T06:33:58.880,,1400,2016-08-06T06:33:58.880,,,,,42.0,1391.0,2,4,,,,24.74,17.1,13.82,0.0,0.0,6.0,Another answer mentions NEAT to generate network weightstopologies Heres a nice paper on an alternative approach to NEAT which also gives a short summary of neuroevolution techniques It uses Cartesian Genetic Programming to evolve a multiple activation functions
1403.0,2.0,"<p>It is possible of normal code to prove that it is correct using mathematical techniques, and that is often done to ensure that some parts are bug-free. </p>

<p>Can we also prove that a piece of code in AI software will cause it to never turn against us, i.e. that the AI is <a href=""https://en.wikipedia.org/wiki/Friendly_artificial_intelligence"" rel=""nofollow"">friendly</a>? Has there any research been done towards this?</p>
",,0,2016-08-06T07:08:20.203,,1401,2016-08-06T07:51:00.567,,,,,29.0,,1,2,<friendly-ai>,Can we prove that a piece of code in AI software will cause it to never turn against us?,39.0,72.97,7.07,8.53,0.0,0.0,8.0,It is possible of normal code to prove that it is correct using mathematical techniques and that is often done to ensure that some parts are bugfree Can we also prove that a piece of code in AI software will cause it to never turn against us ie that the AI is friendly Has there any research been done towards this
,,"<p>Although OCR is now a mainstream technology, it remains true that none our methods genuinely have the recognition facilities of a 5 year old (claimed success with CAPTCHAs notwithstanding). We don't know how to achieve this using well-understood techniques, so OCR should still rightfully be considered an AI problem.</p>

<p>To see why this might be so, it is illuminating to read the essay
<a href=""https://web.stanford.edu/group/SHR/4-2/text/hofstadter.html"">""On seeing A's and seeing AS""</a> by Douglas Hofstadter.</p>

<p>With respect to a point made in another answer, the agent framing is a useful one insofar as it motivates success in increasingly complex environments. However, there are many hard problems (e.g. Bongard) that don't need to be stated in such a fashion. </p>
",,0,2016-08-06T07:11:39.830,,1402,2016-08-06T07:11:39.830,,,,,42.0,1396.0,2,7,,,,60.45,11.72,10.08,0.0,0.0,22.0,Although OCR is now a mainstream technology it remains true that none our methods genuinely have the recognition facilities of a 5 year old claimed success with CAPTCHAs notwithstanding We dont know how to achieve this using wellunderstood techniques so OCR should still rightfully be considered an AI problem To see why this might be so it is illuminating to read the essay On seeing As and seeing AS by Douglas Hofstadter With respect to a point made in another answer the agent framing is a useful one insofar as it motivates success in increasingly complex environments However there are many hard problems eg Bongard that dont need to be stated in such a fashion
,,"<p>Unfortunately, this is extremely unlikely.</p>

<p>It is nearly impossible to make statements about the behaviour of software in general. This is due to the <a href=""https://en.wikipedia.org/wiki/Halting_problem"">Halting problem</a>, which shows that it is impossible to prove whether a program will stop for any given input. From this result, many other things have been shown to be unprovable.</p>

<p>The question whether a piece of code is friendly, can very likely be reduced to a variant of the halting problem.<br>
An AI that operates in the real world, which is a requirement for ""friendliness"" to have a meaning, would need to be Turing complete. Input from the real world cannot be reliably interpreted using regular or context-free languages.</p>

<p>Proofs of correctness work for small code snippets, with clearly defined inputs and outputs. They show that an algorithm produces the mathematically right output, given the right input.<br>
But these are about situations that can be defined with mathematical rigour.</p>

<p>""Friendliness"" isn't a rigidly defined concept, which already makes it difficult to prove anything about it. On top of that, ""friendliness"" is about how the AI relates to the real world, which is an environment whose input to the AI is highly unpredictable.</p>

<p>The best we can hope for, is that an AI can be programmed to have safeguards, and that the code will raise warning flags if unethical behaviour becomes likely - that AI's are programmed defensively.</p>
",,4,2016-08-06T07:24:41.043,,1403,2016-08-06T07:51:00.567,2016-08-06T07:51:00.567,,66.0,,66.0,1401.0,2,5,,,,53.41,11.6,9.58,0.0,0.0,36.0,Unfortunately this is extremely unlikely It is nearly impossible to make statements about the behaviour of software in general This is due to the Halting problem which shows that it is impossible to prove whether a program will stop for any given input From this result many other things have been shown to be unprovable The question whether a piece of code is friendly can very likely be reduced to a variant of the halting problem An AI that operates in the real world which is a requirement for friendliness to have a meaning would need to be Turing complete Input from the real world cannot be reliably interpreted using regular or contextfree languages Proofs of correctness work for small code snippets with clearly defined inputs and outputs They show that an algorithm produces the mathematically right output given the right input But these are about situations that can be defined with mathematical rigour Friendliness isnt a rigidly defined concept which already makes it difficult to prove anything about it On top of that friendliness is about how the AI relates to the real world which is an environment whose input to the AI is highly unpredictable The best we can hope for is that an AI can be programmed to have safeguards and that the code will raise warning flags if unethical behaviour becomes likely that AIs are programmed defensively
1407.0,1.0,"<p>In <a href=""http://arxiv.org/pdf/1606.00652.pdf"" rel=""nofollow"">this paper</a>, a proposal is given for what death could mean for Artificial Intelligence. </p>

<p>What does this mean using English only? I understand that mathematical notation is useful for giving a precise definition, but I'd like to understand what the definition really means. </p>
",,0,2016-08-06T07:27:52.287,1.0,1404,2016-08-30T16:31:10.013,2016-08-30T16:31:10.013,,29.0,,29.0,,1,3,<research><definitions><death>,What is meant by death in this paper?,631.0,48.09,12.46,10.47,0.0,0.0,6.0,In this paper a proposal is given for what death could mean for Artificial Intelligence What does this mean using English only I understand that mathematical notation is useful for giving a precise definition but Id like to understand what the definition really means
,,"<p>Here are some examples of recent work on verifying certain properties of autonomous systems <a href=""https://www.cs.york.ac.uk/circus/RoboCalc-event/courses/"" rel=""nofollow"">[RoboCheck]</a>.</p>

<p>However, to achieve the same kind of thing for the notion of 'friendly' using formal verification (i.e. 'proving correctness using mathematical techniques'),
it would (at the least) seem necessary to be able to express 'friendly' within a logical formalism, (i.e. as a predicate testable within a model-checker, so that we can be sure a system never enters an undesirable state).</p>

<p>However, it's not immediately clear that 'friendly' has a more specific definition than 'a desire not to harm humans', so much more low-level detail is needed.</p>

<p>Some previous work in this general area that might be useful in this respect include:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Deontic_logic"" rel=""nofollow"">Deontic Logic</a> - a logical calculus of obligations.</li>
<li><a href=""http://www.jfsowa.com/ikl/McCarthy89"" rel=""nofollow"">Elephant 2000</a> - John McCarthy's description of a promise-based programming language.</li>
</ul>
",,0,2016-08-06T07:40:02.787,,1405,2016-08-06T07:40:02.787,,,,,42.0,1401.0,2,3,,,,43.73,14.62,11.23,0.0,0.0,41.0,Here are some examples of recent work on verifying certain properties of autonomous systems RoboCheck However to achieve the same kind of thing for the notion of friendly using formal verification ie proving correctness using mathematical techniques it would at the least seem necessary to be able to express friendly within a logical formalism ie as a predicate testable within a modelchecker so that we can be sure a system never enters an undesirable state However its not immediately clear that friendly has a more specific definition than a desire not to harm humans so much more lowlevel detail is needed Some previous work in this general area that might be useful in this respect include Deontic Logic a logical calculus of obligations Elephant 2000 John McCarthys description of a promisebased programming language
,,"<p>Whenever a problem becomes solvable by a computer, people start arguing that it does not require intelligence. John McCarthy is often quoted: ""As soon as it works, no one calls it AI anymore"" (<a href=""http://cacm.acm.org/magazines/2012/1/144824-artificial-intelligence-past-and-future/fulltext"" rel=""nofollow"">Referenced in CACM</a>).</p>

<p>One of my teachers in college said that in the 1950's, a professor was asked what he thought was intelligent for a machine. The professor reputedly answered that if a vending machine gave him the right change, that would be intelligent.</p>

<p>Later, playing chess was considered intelligent. However, computers can now defeat grandmasters at chess, and people are no longer saying that it is a form of intelligence. </p>

<p>Now we have OCR. It's already stated in <a href=""http://ai.stackexchange.com/a/1402/66"">another answer</a> that our methods do not have the recognition facilities of a 5 year old. As soon as this is achieved, people will say ""meh, that's not intelligence, a 5 year old can do that!""</p>

<p>A psychological bias, a need to state that we are somehow superior to machines, is at the basis of this.</p>
",,2,2016-08-06T07:44:40.520,,1406,2016-08-08T03:21:22.243,2016-08-08T03:21:22.243,,169.0,,66.0,1396.0,2,12,,,,62.88,9.74,8.79,0.0,0.0,32.0,Whenever a problem becomes solvable by a computer people start arguing that it does not require intelligence John McCarthy is often quoted As soon as it works no one calls it AI anymore Referenced in CACM One of my teachers in college said that in the 1950s a professor was asked what he thought was intelligent for a machine The professor reputedly answered that if a vending machine gave him the right change that would be intelligent Later playing chess was considered intelligent However computers can now defeat grandmasters at chess and people are no longer saying that it is a form of intelligence Now we have OCR Its already stated in another answer that our methods do not have the recognition facilities of a 5 year old As soon as this is achieved people will say meh thats not intelligence a 5 year old can do that A psychological bias a need to state that we are somehow superior to machines is at the basis of this
,,"<p>The authors do actually give an English definition in terms of the well-known agent formulation of AI:</p>

<blockquote>
  <p>We intend this usage to be intuitive: death means that one sees
  no more percepts, and takes no more actions.</p>
</blockquote>

<p>It would seem that this becomes possible for a reinforcement learning agent such as AIXI in a formulation that uses <em>semi-measures</em> of probability (which need not sum up to 1), rather than the more traditional notion.</p>
",,1,2016-08-06T08:07:18.500,,1407,2016-08-06T08:07:18.500,,,,,42.0,1404.0,2,7,,,,42.89,11.5,10.64,0.0,0.0,10.0,The authors do actually give an English definition in terms of the wellknown agent formulation of AI We intend this usage to be intuitive death means that one sees no more percepts and takes no more actions It would seem that this becomes possible for a reinforcement learning agent such as AIXI in a formulation that uses semimeasures of probability which need not sum up to 1 rather than the more traditional notion
,,"<p>Vernor Vinge said that if we can scan a human brain and then simulate it: We can run it at 1000 times the speed. The brain will be able to do 1000 years of thinking in 1 year ect. </p>

<p>At this stage in history we have the computer power.</p>

<p>The trouble lies in cutting a brain up and scanning the 100 billion neurons and 12 million kilometres of axons and 100000 billion synapses.
And piecing together the connectome from all the data.</p>

<p>Sebastian Seung at MIT is working on automating this scanning process with machine learning. By gathering training data from thousands of people playing his <a href=""https://en.wikipedia.org/wiki/Eyewire"" rel=""nofollow noreferrer"">Eyewire game</a></p>

<p>Henry Markram in Europe tried to do something similar with his <a href=""https://en.wikipedia.org/wiki/Blue_Brain_Project"" rel=""nofollow noreferrer"">Blue Brain Project</a>.
He attempted to simulate the neocortical column of a rat. The EU gave him a billion euros to do this. Unfortunately he has been heavily criticised by the Neuroscience community. They claim that we don't know the physiology  well enough to make a valid simulation.</p>

<p>Check out his <a href=""https://www.youtube.com/watch?v=LS3wMC2BpxU"" rel=""nofollow noreferrer"">Ted Talk</a>.</p>

<p>In the 1970s Sydney Brenner achieved a <em>full brain scan</em> of a C Elegans worm. This worm has one of the simplest biological neural networks having only 302 neurons.
Here is a picture of its connectome:<a href=""https://i.stack.imgur.com/cAZ49.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cAZ49.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/siHt8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/siHt8.png"" alt=""enter image description here""></a></p>

<p>An accurate computer simulation of this worm would be a major stepping stone to uploading a human brain.</p>
",,0,2016-08-06T08:59:35.103,,1408,2016-08-06T09:37:48.850,2016-08-06T09:37:48.850,,157.0,,157.0,1394.0,2,4,,,,73.07,8.98,9.35,0.0,0.0,18.0,Vernor Vinge said that if we can scan a human brain and then simulate it We can run it at 1000 times the speed The brain will be able to do 1000 years of thinking in 1 year ect At this stage in history we have the computer power The trouble lies in cutting a brain up and scanning the 100 billion neurons and 12 million kilometres of axons and 100000 billion synapses And piecing together the connectome from all the data Sebastian Seung at MIT is working on automating this scanning process with machine learning By gathering training data from thousands of people playing his Eyewire game Henry Markram in Europe tried to do something similar with his Blue Brain Project He attempted to simulate the neocortical column of a rat The EU gave him a billion euros to do this Unfortunately he has been heavily criticised by the Neuroscience community They claim that we dont know the physiology well enough to make a valid simulation Check out his Ted Talk In the 1970s Sydney Brenner achieved a full brain scan of a C Elegans worm This worm has one of the simplest biological neural networks having only 302 neurons Here is a picture of its connectome An accurate computer simulation of this worm would be a major stepping stone to uploading a human brain
,,"<p>Matrix representation is beneficial for implementing neural networks in silicon.</p>

<p>But for examining neural networks empirically it is sometimes good to visualise the synapse weight values as images or videos: <a href=""https://www.youtube.com/watch?v=AgkfIQ4IGaM"" rel=""nofollow noreferrer"">Jason Yosinski's </a> exploration of a convolution neural network. The network seems to have a ""filter"" that just detects shoulders. A bit like a lock that only opens when it recognises the pattern of shoulders.<a href=""https://i.stack.imgur.com/4g4gF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4g4gF.png"" alt=""enter image description here""></a></p>
",,0,2016-08-06T09:15:09.163,,1409,2016-08-06T10:12:34.840,2016-08-06T10:12:34.840,,157.0,,157.0,1363.0,2,1,,,,46.78,14.21,11.34,0.0,0.0,8.0,Matrix representation is beneficial for implementing neural networks in silicon But for examining neural networks empirically it is sometimes good to visualise the synapse weight values as images or videos Jason Yosinskis exploration of a convolution neural network The network seems to have a filter that just detects shoulders A bit like a lock that only opens when it recognises the pattern of shoulders
,3.0,"<p>We can measure the power of the machine with the number of operation per second or the frequency of the processor. But does units similar of IQ for humans exist for a AI?<br/>
I'm asking for a unit which can give countable result so something different from a Turing Test which only give a binary result.</p>
",,0,2016-08-06T10:46:10.100,1.0,1410,2016-08-08T18:25:11.147,2016-08-06T14:14:05.420,,10.0,,98.0,,1,4,<machine-learning><classification><intelligence-testing>,Do specific units exists for measuring the intelligence of a machine?,41.0,60.95,8.01,10.77,0.0,0.0,4.0,We can measure the power of the machine with the number of operation per second or the frequency of the processor But does units similar of IQ for humans exist for a AI Im asking for a unit which can give countable result so something different from a Turing Test which only give a binary result
,,"<p>One of the challenges of AI is defining Intelligence.
If we could precisely define general intelligence then we could program it into a computer. After all an algorithm is a process so well defined that it can be run on a computer.</p>

<p>Narrow AI can be evaluated on its success at achieving goals in an environment. In domains such as computer vision and speech recognition narrow AI algorithms can be easily evaluated.</p>

<p>Many universities curate narrow AI tests. Fei-Fei Li a professor at Stanford who directs the Artificial Intelligence lab there organises the annual ImageNet Challenge. In 2012 Geoffrey Hinton famously won the competition by building a Deep Neural Network that could recognize pictures more accurately than humans can.</p>

<p>To my knowledge the testers commonly use <a href=""https://en.wikipedia.org/wiki/Precision_and_recall"" rel=""nofollow"">Precision and recall</a> evaluation metrics</p>
",,0,2016-08-06T11:57:19.053,,1411,2016-08-06T14:07:22.407,2016-08-06T14:07:22.407,,157.0,,157.0,1410.0,2,3,,,,48.2,12.4,11.35,0.0,0.0,9.0,One of the challenges of AI is defining Intelligence If we could precisely define general intelligence then we could program it into a computer After all an algorithm is a process so well defined that it can be run on a computer Narrow AI can be evaluated on its success at achieving goals in an environment In domains such as computer vision and speech recognition narrow AI algorithms can be easily evaluated Many universities curate narrow AI tests FeiFei Li a professor at Stanford who directs the Artificial Intelligence lab there organises the annual ImageNet Challenge In 2012 Geoffrey Hinton famously won the competition by building a Deep Neural Network that could recognize pictures more accurately than humans can To my knowledge the testers commonly use Precision and recall evaluation metrics
,,"<p>Yes, many people have worked on this sort of thing, due to its obvious industrial applications (most of the ones I'm familiar with are in the pharmaceutical industry). Here's <a href=""https://arxiv.org/abs/1305.7074"">a paper from 2013</a> that claims good results; following the trail of <a href=""https://scholar.google.com/scholar?cites=10630711614897084406&amp;as_sdt=5,44&amp;sciodt=0,44&amp;hl=en"">papers that cited it</a> will likely give you more recent work. </p>
",,0,2016-08-06T14:04:28.247,,1412,2016-08-06T14:04:28.247,,,,,10.0,1390.0,2,5,,,,62.01,10.91,10.39,0.0,0.0,9.0,Yes many people have worked on this sort of thing due to its obvious industrial applications most of the ones Im familiar with are in the pharmaceutical industry Heres a paper from 2013 that claims good results following the trail of papers that cited it will likely give you more recent work
,,"<p>Shane Legg and Marcus Hutter <a href=""http://www.vetta.org/documents/42.pdf"" rel=""nofollow"">proposed one</a> in 2006. The main descriptive quotes (see the paper for the actual formula):</p>

<blockquote>
  <p>Intelligence measures an agent’s general ability to achieve goals in a wide range of environments</p>
  
  <p>...</p>
  
  <p>It is clear by construction that universal intelligence measures the general ability of an agent to perform well in a very wide range of environments, as required by our informal definition of intelligence given earlier. The definition places no restrictions on the internal workings of the agent; it only requires that the agent is capable of generating output and receiving input which includes a reward signal.</p>
</blockquote>
",,0,2016-08-06T14:17:28.310,,1413,2016-08-06T14:17:28.310,,,,,10.0,1410.0,2,3,,,,28.98,13.64,11.45,0.0,0.0,11.0,Shane Legg and Marcus Hutter proposed one in 2006 The main descriptive quotes see the paper for the actual formula Intelligence measures an agent’s general ability to achieve goals in a wide range of environments It is clear by construction that universal intelligence measures the general ability of an agent to perform well in a very wide range of environments as required by our informal definition of intelligence given earlier The definition places no restrictions on the internal workings of the agent it only requires that the agent is capable of generating output and receiving input which includes a reward signal
,,"<p>The other answers are correct that machine IQ test results are currently <strong>not</strong> indicative of machine intelligence. One of the surprising facts of human intelligence is that performance on almost all cognitive tasks are correlated with each other; that is, there is such a thing as 'general smartness' and IQ tests attempt to measure that thing.</p>

<p>People <em>have</em> built programs that take IQ tests, however, and some of them perform quite well. Raven's Progressive Matrices, a visual pattern recognition IQ test, is an easy target for AI (see <a href=""https://www.researchgate.net/publication/288211280_Solving_Raven&#39;s_IQ-tests_An_AI_and_cognitive_modeling_approach"" rel=""nofollow"">this paper</a> as representative) and another group <a href=""http://arxiv.org/abs/1509.03390"" rel=""nofollow"">has constructed an AI</a> that performs about as well as a 4 year old on the verbal intelligence portion of a standard childhood IQ test.</p>
",,0,2016-08-06T14:22:56.640,,1414,2016-08-06T14:22:56.640,,,,,10.0,41.0,2,2,,,,41.03,12.14,10.65,0.0,0.0,15.0,The other answers are correct that machine IQ test results are currently not indicative of machine intelligence One of the surprising facts of human intelligence is that performance on almost all cognitive tasks are correlated with each other that is there is such a thing as general smartness and IQ tests attempt to measure that thing People have built programs that take IQ tests however and some of them perform quite well Ravens Progressive Matrices a visual pattern recognition IQ test is an easy target for AI see this paper as representative and another group has constructed an AI that performs about as well as a 4 year old on the verbal intelligence portion of a standard childhood IQ test
1418.0,1.0,"<p>In the mid 1980s, Rodney Brooks famously created the foundations of ""the new AI"". The central claim was that the symbolist approach of 'Good Old Fashioned AI' (GOFAI) had failed by attempting to 'cream cognition off the top', and that <em>embodied cognition</em> was required, i.e. built from the bottom up in a 'hierarchy of competances' (e.g. basic locomotion -> wandering around -> actively foraging) etc.</p>

<p>I imagine most AI researchers would agree that the 'embodied cognition' perspective has now (at least tacitly) supplanted GOFAI as the mainstream.</p>

<p>My question takes the form of a thought experiment and asks: ""Which (if any)  aspects of 'embodied' can be relaxed/omitted before we lose something essential for AGI?""</p>
",,0,2016-08-06T17:24:50.083,1.0,1415,2016-08-06T18:19:44.323,,,,,42.0,,1,5,<agi><gofai><embodied-cognition>,What kind of body (if any) does intelligence require?,170.0,52.49,13.81,10.63,0.0,0.0,39.0,In the mid 1980s Rodney Brooks famously created the foundations of the new AI The central claim was that the symbolist approach of Good Old Fashioned AI GOFAI had failed by attempting to cream cognition off the top and that embodied cognition was required ie built from the bottom up in a hierarchy of competances eg basic locomotion wandering around actively foraging etc I imagine most AI researchers would agree that the embodied cognition perspective has now at least tacitly supplanted GOFAI as the mainstream My question takes the form of a thought experiment and asks Which if any aspects of embodied can be relaxedomitted before we lose something essential for AGI
,0.0,"<p>In other words, which existing reinforcement method learns in fewest episodes? <a href=""http://www.jmlr.org/papers/volume3/brafman02a/brafman02a.pdf"" rel=""nofollow"">R-Max</a> comes to mind, but its very old and I'd like to know if there is something better now.</p>
",,0,2016-08-06T17:35:02.143,,1416,2016-08-06T17:35:02.143,,,,,144.0,,1,2,<algorithm><research><reinforcement-learning>,What is the current state-of-the-art in Reinforcement Learning regarding data efficiency?,42.0,73.17,9.56,9.64,0.0,0.0,6.0,In other words which existing reinforcement method learns in fewest episodes RMax comes to mind but its very old and Id like to know if there is something better now
,,"<p>The images that you provided may be unrecognizable for us. They are actually the images that we recognize but evolved  using the <a href=""https://github.com/sferes2/sferes2"" rel=""noreferrer"">Sferes</a> evolutionary framework.</p>

<p>While these images are almost impossible for humans to label with anything but abstract arts, the Deep Neural Network will label them to be familiar objects with 99.99% confidence.</p>

<p>This result highlights differences between how DNNs and humans recognize objects. Images are either directly () or indirectly
() encoded</p>

<p>According to this <a href=""https://youtu.be/M2IebCN9Ht4"" rel=""noreferrer"">video</a></p>

<blockquote>
  <p>Changing an image originally correctly classified in a way imperceptible to humans can cause the cause DNN to classify it as something else.</p>
  
  <p>In the image below the number at the bottom are the images are supposed to look like the digits
  But the network believes the images at the top (the one like white noise) are real digits with 99.99% certainty.</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/Jx1wX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Jx1wX.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>The main reason why these are easily fooled is because Deep Neural Network does not see the world in the same way as human vision. We use the whole image to identify things while DNN depends on the features. As long as DNN detects certain features, it will classify the image as a familiar object it has been trained on.
  The researchers proposed one way to prevent such fooling by adding the fooling images to the dataset in a new class and training DNN on the enlarged dataset. In the experiment, the confidence score decreases significantly for ImageNet AlexNet. It is not easy to fool the retrained DNN this time. But when the researchers applied such method to MNIST LeNet, evolution still produces many unrecognizable images with confidence scores of 99.99%.</p>
</blockquote>

<p>More details <a href=""http://www.evolvingai.org/fooling"" rel=""noreferrer"">here</a>, <a href=""http://www.kdnuggets.com/2015/01/deep-learning-can-be-easily-fooled.html"" rel=""noreferrer"">here</a> and <a href=""http://www.kdnuggets.com/2015/01/deep-learning-can-be-easily-fooled.html"" rel=""noreferrer"">here</a>.</p>
",,0,2016-08-06T17:41:35.647,,1417,2016-08-11T10:04:20.437,2016-08-11T10:04:20.437,,29.0,,72.0,92.0,2,9,,,,51.58,11.66,9.83,0.0,0.0,31.0,The images that you provided may be unrecognizable for us They are actually the images that we recognize but evolved using the Sferes evolutionary framework While these images are almost impossible for humans to label with anything but abstract arts the Deep Neural Network will label them to be familiar objects with 9999 confidence This result highlights differences between how DNNs and humans recognize objects Images are either directly or indirectly encoded According to this video Changing an image originally correctly classified in a way imperceptible to humans can cause the cause DNN to classify it as something else In the image below the number at the bottom are the images are supposed to look like the digits But the network believes the images at the top the one like white noise are real digits with 9999 certainty The main reason why these are easily fooled is because Deep Neural Network does not see the world in the same way as human vision We use the whole image to identify things while DNN depends on the features As long as DNN detects certain features it will classify the image as a familiar object it has been trained on The researchers proposed one way to prevent such fooling by adding the fooling images to the dataset in a new class and training DNN on the enlarged dataset In the experiment the confidence score decreases significantly for ImageNet AlexNet It is not easy to fool the retrained DNN this time But when the researchers applied such method to MNIST LeNet evolution still produces many unrecognizable images with confidence scores of 9999 More details here here and here
,,"<p>This is something of an orthogonal answer, but I think Brooks didn't go about his idea the right way. That is, <a href=""https://en.wikipedia.org/wiki/Subsumption_architecture"" rel=""nofollow"">subsumption architecture</a> is one in which the 'autopilot' is <em>replaced</em> by a more sophisticated system when necessary. (All pieces receive the raw sensory inputs, and output actions, some of which turn off or on other systems.)</p>

<p>But a better approach is the normal hierarchical control approach, in which the target of a lower level system is the output of a higher level system. That is, the targeted joint angle of a robot leg is determined by the system that is trying to optimize the velocity, which is determined by a system that is trying to optimize the trajectory, which is determined by a system that is trying to optimize the target position, and so on.</p>

<p>This allows for increasing level of complexity while maintaining detail and system reusability.</p>

<hr>

<p>That said, I don't think you actually need what one would naively call 'embodied cognition' in order to get the bottom-up hierarchy of competencies that Brooks is right to point towards. The core feature is the wide array of inputs and outputs, which are understood in a hierarchical fashion that allows systems to be chained together vertically. I think you could get a functional general intelligence whose only inputs and outputs involve going through an Ethernet cable, and doesn't have anything like a traditional body that it actuates or senses through. (This is a claim that the hierarchical structure is what matters, not the content of what we use that structure for.)</p>

<p>(The main place to look for more, I think, is actually a book about <em>human</em> cognition, called The Control of Perception by William T. Powers.)</p>
",,0,2016-08-06T18:19:44.323,,1418,2016-08-06T18:19:44.323,,,,,10.0,1415.0,2,4,,,,53.55,11.44,9.78,0.0,0.0,42.0,This is something of an orthogonal answer but I think Brooks didnt go about his idea the right way That is subsumption architecture is one in which the autopilot is replaced by a more sophisticated system when necessary All pieces receive the raw sensory inputs and output actions some of which turn off or on other systems But a better approach is the normal hierarchical control approach in which the target of a lower level system is the output of a higher level system That is the targeted joint angle of a robot leg is determined by the system that is trying to optimize the velocity which is determined by a system that is trying to optimize the trajectory which is determined by a system that is trying to optimize the target position and so on This allows for increasing level of complexity while maintaining detail and system reusability That said I dont think you actually need what one would naively call embodied cognition in order to get the bottomup hierarchy of competencies that Brooks is right to point towards The core feature is the wide array of inputs and outputs which are understood in a hierarchical fashion that allows systems to be chained together vertically I think you could get a functional general intelligence whose only inputs and outputs involve going through an Ethernet cable and doesnt have anything like a traditional body that it actuates or senses through This is a claim that the hierarchical structure is what matters not the content of what we use that structure for The main place to look for more I think is actually a book about human cognition called The Control of Perception by William T Powers
,,"<p>I can offer two (at first sight, conflicting) perspectives on this:</p>

<p>Firstly:</p>

<p><em>If the letter string 'abc' becomes 'abd' what would ""doing the same thing"" to 'ijk' look like?</em></p>

<p>This is just one example of a problem (so-called 'letterstring analogy problems') that is not easily framed as an optimization problem - there are a range of answers that appear compelling to humans, each for it's own structurally-specific reason. Some of the subtleties of these kind of problems are discussed in detail <a href=""http://cognitrn.psych.indiana.edu/rgoldsto/courses/concepts/copycat.pdf"" rel=""nofollow"">here</a>.</p>

<p>Secondly:</p>

<p>Here's a <em>very</em> high-level perspective on AGI in which <a href=""http://arxiv.org/abs/cs/0309048"" rel=""nofollow"">optimization plays a key part</a>.</p>

<p>It's not at all clear how these two very different scales of approach might be reconciled. As someone who does optimization research for a living, I'd be inclined to say that, certainly for all <em>current, practical</em> purposes, AGI can't really be treated as an optimization problem, since most interesting activities don't readily lend themselves to description via a cost function.</p>
",,0,2016-08-06T18:38:06.953,,1419,2016-08-06T19:03:40.640,2016-08-06T19:03:40.640,,42.0,,42.0,104.0,2,3,,,,45.09,13.29,10.19,0.0,0.0,40.0,I can offer two at first sight conflicting perspectives on this Firstly If the letter string abc becomes abd what would doing the same thing to ijk look like This is just one example of a problem socalled letterstring analogy problems that is not easily framed as an optimization problem there are a range of answers that appear compelling to humans each for its own structurallyspecific reason Some of the subtleties of these kind of problems are discussed in detail here Secondly Heres a very highlevel perspective on AGI in which optimization plays a key part Its not at all clear how these two very different scales of approach might be reconciled As someone who does optimization research for a living Id be inclined to say that certainly for all current practical purposes AGI cant really be treated as an optimization problem since most interesting activities dont readily lend themselves to description via a cost function
1421.0,4.0,"<p>Are there any research teams which attempted to create or have already created an AI robot which can be as close to intelligent as these found in <a href=""https://en.wikipedia.org/wiki/Ex_Machina_(film)""><em>Ex Machina</a></em> or <em><a href=""https://en.wikipedia.org/wiki/I,_Robot_(film)"">I, Robot</em></a> movies?</p>

<p>I'm not talking about full awareness, but an artificial being which can make its own decisions and physical and intellectual tasks that a human being can do?</p>
",,0,2016-08-06T18:38:50.160,4.0,1420,2016-08-28T20:41:04.140,2016-08-23T14:16:20.303,,145.0,,8.0,,1,8,<research><agi><robots>,How close are we to creating Ex Machina?,733.0,49.49,9.88,9.86,0.0,0.0,5.0,Are there any research teams which attempted to create or have already created an AI robot which can be as close to intelligent as these found in Ex Machina or I Robot movies Im not talking about full awareness but an artificial being which can make its own decisions and physical and intellectual tasks that a human being can do
,,"<p>We are absolutely nowhere near, nor do we have any idea how to bridge the gap between what we can currently do and what is depicted in these films.</p>

<p>The current trend for DL approaches (coupled with the emergence of data science as a mainstream discipline) has led to a lot of popular interest in AI.</p>

<p>However, researchers and practitioners would do well to learn the lessons of the 'AI Winter' and not engage in hubris or read too much into current successes.</p>

<p>For example:</p>

<ul>
<li>Success in transfer learning is very limited. </li>
<li>The 'hard problem' (i.e. presenting the 'raw, unwashed environment' to the machine and having it come up with a solution from scratch) is not being
addressed by DL to the extent that it is popularly portrayed: expert human knowledge is still required to help decide how the input should be framed, tune parameters, interpret output etc.</li>
</ul>

<p>Someone who has enthusiasm for AGI would hopefully agree that the 'hard problem' is actually the only one that matters. Some years ago, a famous cognitive scientist said ""We have yet to successfully represent <em>even a single concept</em> on a computer"". </p>

<p>In my opinion, recent research trends have done little to change this.</p>

<p>All of this perhaps sounds pessimistic - it's not intended to. None of us want another AI Winter, so we should challenge (and be honest about) the limits of our current techniques rather than mythologizing them.</p>
",,0,2016-08-06T18:54:48.577,,1421,2016-08-06T19:04:52.397,2016-08-06T19:04:52.397,,42.0,,42.0,1420.0,2,20,,,,58.11,10.91,10.39,0.0,0.0,40.0,We are absolutely nowhere near nor do we have any idea how to bridge the gap between what we can currently do and what is depicted in these films The current trend for DL approaches coupled with the emergence of data science as a mainstream discipline has led to a lot of popular interest in AI However researchers and practitioners would do well to learn the lessons of the AI Winter and not engage in hubris or read too much into current successes For example Success in transfer learning is very limited The hard problem ie presenting the raw unwashed environment to the machine and having it come up with a solution from scratch is not being addressed by DL to the extent that it is popularly portrayed expert human knowledge is still required to help decide how the input should be framed tune parameters interpret output etc Someone who has enthusiasm for AGI would hopefully agree that the hard problem is actually the only one that matters Some years ago a famous cognitive scientist said We have yet to successfully represent even a single concept on a computer In my opinion recent research trends have done little to change this All of this perhaps sounds pessimistic its not intended to None of us want another AI Winter so we should challenge and be honest about the limits of our current techniques rather than mythologizing them
,,"<blockquote>
  <p>""heavier-than-air flying machines are impossible"" <em>_ Lord Kelvin 1895</em> </p>
</blockquote>

<p>7 years later the Wright brothers built one.</p>

<hr>

<p>Currently we have many powerful narrow AI (good at special tasks) but we have no idea how to unify them into a single system like in a biological brain. </p>
",,0,2016-08-06T20:30:10.680,,1422,2016-08-07T07:02:48.480,2016-08-07T07:02:48.480,,157.0,,157.0,1420.0,2,0,,,,57.1,11.49,10.72,0.0,0.0,9.0,heavierthanair flying machines are impossible Lord Kelvin 1895 7 years later the Wright brothers built one Currently we have many powerful narrow AI good at special tasks but we have no idea how to unify them into a single system like in a biological brain
1440.0,2.0,"<p>We, humans, during following multiple processes (e.g. reading while listening to music) memorize information from less focused sources with worse efficiency than we do from our main concentration.</p>

<p>Do such things exist in case of artificial intelligences? I doubt, for example that neural networks obtain such features, but I may be wrong.</p>
",,0,2016-08-06T22:59:43.413,2.0,1423,2016-08-18T14:27:26.763,2016-08-18T14:27:26.763,,8.0,,1270.0,,1,9,<structured-data>,"Is there any artificial intelligence that possesses ""concentration""?",98.0,49.82,13.85,11.27,0.0,0.0,11.0,We humans during following multiple processes eg reading while listening to music memorize information from less focused sources with worse efficiency than we do from our main concentration Do such things exist in case of artificial intelligences I doubt for example that neural networks obtain such features but I may be wrong
,,"<p>Yes, there were successful attempts at predicting the interaction between molecules and biological proteins which have been used to identify potential treatments by using <a href=""https://en.wikipedia.org/wiki/Convolutional_neural_network#Drug_discovery"" rel=""nofollow"">convolutional neural networks</a>.</p>

<p>For example in 2015, the first deep learning neural network has been created for structure-based drug design which trains 3-dimensional representation of chemical interactions which works similar to how image recognition works (composing smaller features into larger, complex structures).<sup><a href=""https://en.wikipedia.org/wiki/Convolutional_neural_network#Drug_discovery"" rel=""nofollow"">wiki</a></sup></p>

<p><sup>Study: <a href=""https://arxiv.org/abs/1510.02855"" rel=""nofollow"">AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery</a></sup></p>

<hr>

<p>Another approach is to use evolutionary artificial neural networks which can achieve great optimization results.</p>

<p>Further more, the <a href=""https://arxiv.org/pdf/1502.00193.pdf"" rel=""nofollow"">paper from 2015</a> demonstrated heurisic <a href=""http://link.springer.com/article/10.1007/s12293-012-0075-1"" rel=""nofollow"">chemical reaction optimization</a> (CRO) which is inspired by the natural of chemical reactions (e.g. transforming the unstable substances to the stable onces). Simulation results shows that CRO outperforms many evolutionary algorithms by population-based metaheuristics mimicking the transition of molecules and their interactions in a chemical reaction.</p>

<p>Sample pseudocode algorithm for predicting synthesis given ω1, ω2 (from the above paper):</p>

<pre><code> 1: for all Matrices and vectors m in ω′ do
 2:     for all Elements e in m do
 3:         Generate a real r between 0 and 1
 4:         if r &gt; 0.5 then
 5:             e =counterpart in m1
 6:         else
 7:             e =counterpart in m2
 8:         end if
 9:     end for
10: end for
</code></pre>

<p>which is used to generate a new solution ω′ based on two given solutions ω1 and ω2.</p>
",,0,2016-08-06T23:23:52.457,,1424,2016-08-11T10:15:24.027,2016-08-11T10:15:24.027,,8.0,,8.0,1390.0,2,1,,,,20.11,19.44,12.58,294.0,0.0,28.0,Yes there were successful attempts at predicting the interaction between molecules and biological proteins which have been used to identify potential treatments by using convolutional neural networks For example in 2015 the first deep learning neural network has been created for structurebased drug design which trains 3dimensional representation of chemical interactions which works similar to how image recognition works composing smaller features into larger complex structureswiki Study AtomNet A Deep Convolutional Neural Network for Bioactivity Prediction in Structurebased Drug Discovery Another approach is to use evolutionary artificial neural networks which can achieve great optimization results Further more the paper from 2015 demonstrated heurisic chemical reaction optimization CRO which is inspired by the natural of chemical reactions eg transforming the unstable substances to the stable onces Simulation results shows that CRO outperforms many evolutionary algorithms by populationbased metaheuristics mimicking the transition of molecules and their interactions in a chemical reaction Sample pseudocode algorithm for predicting synthesis given ω1 ω2 from the above paper which is used to generate a new solution ω′ based on two given solutions ω1 and ω2
1443.0,1.0,"<p>How can a swarm of small robots (like Kilobots) walking close to each other achieve collaboration without bumping into each other? For example, one study shows <a href=""http://science.sciencemag.org/content/345/6198/795.abstract"" rel=""nofollow"">programmable self-assembly in a thousand-robot swarm</a> (see <a href=""http://robohub.org/thousand-robot-swarm-self-assembles-into-arbitrary-shapes/"" rel=""nofollow"">article</a> &amp; <a href=""https://vimeo.com/103329200"" rel=""nofollow"">video</a>) which are moving without GPS-like system and by measuring distances to neighbours. This was achieved, because the robots were very slow.</p>

<p>Is there any way that similar robots can achieve much more efficient and quicker assembly by using more complex techniques of coordination? Not by walking around clock-wise (which I guess was the easiest way), but I mean using some more sophisticated way. Because waiting half a day (~11h) to create a simple star shape using a thousand-robot swarm is way too long!</p>
",,0,2016-08-07T00:06:45.913,,1426,2016-08-07T19:10:08.217,2016-08-07T19:10:08.217,,145.0,,8.0,,1,5,<robots><multi-agent-systems>,How can thousand-robot swarm coordinate their moves without bumping into each other?,61.0,51.38,13.11,10.06,0.0,0.0,25.0,How can a swarm of small robots like Kilobots walking close to each other achieve collaboration without bumping into each other For example one study shows programmable selfassembly in a thousandrobot swarm see article amp video which are moving without GPSlike system and by measuring distances to neighbours This was achieved because the robots were very slow Is there any way that similar robots can achieve much more efficient and quicker assembly by using more complex techniques of coordination Not by walking around clockwise which I guess was the easiest way but I mean using some more sophisticated way Because waiting half a day 11h to create a simple star shape using a thousandrobot swarm is way too long
1428.0,1.0,"<p>On Watson wiki page we can read:</p>

<blockquote>
  <p>In healthcare, Watson's natural language, hypothesis generation, and evidence-based learning capabilities allow it to function as a clinical decision support system for use by medical professionals.</p>
</blockquote>

<p>How exactly such AI can help doctors to diagnose the diseases?</p>
",,0,2016-08-07T00:55:36.657,,1427,2016-08-07T00:55:36.657,,,,,8.0,,1,0,<watson><healthcare>,How Watson can help to make medical diagnoses?,17.0,23.77,15.78,12.98,0.0,0.0,8.0,On Watson wiki page we can read In healthcare Watsons natural language hypothesis generation and evidencebased learning capabilities allow it to function as a clinical decision support system for use by medical professionals How exactly such AI can help doctors to diagnose the diseases
,,"<p>Watson can make its diagnosis based on the patient's data and comparing it to the data from million of other studies.</p>

<p>For example having enough genetic data and the right algorithms, its AI computing capability demonstrated the huge potential of data analysis based on which it can used for everything from diagnosing rare illnesses to prescribing perfect dosages of medicine based on the patient's personal genetic makeup. Of course there are still plenty of challenges which need to be overcome before it can be used in mainstream medicine. </p>

<p>Recently Watson was able to diagnose rare form of leukemia after treatment was proved ineffective. It was fed in with patient’s genetic data and compared to data from other 20 million oncological studies.</p>

<p>Sources:</p>

<ul>
<li><a href=""http://www.ndtv.com/health/artificial-intelligence-used-to-detect-rare-leukemia-type-in-japan-1440789"" rel=""nofollow"">Artificial Intelligence Used To Detect Rare Leukemia Type In Japan</a> (2016)</li>
<li><a href=""http://siliconangle.com/blog/2016/08/05/watson-correctly-diagnoses-woman-after-doctors-were-stumped/"" rel=""nofollow"">Watson correctly diagnoses woman after doctors were stumped</a> (2016)</li>
</ul>
",,0,2016-08-07T00:55:36.657,,1428,2016-08-07T00:55:36.657,,,,,8.0,1427.0,2,0,,,,38.96,13.87,11.26,0.0,0.0,13.0,Watson can make its diagnosis based on the patients data and comparing it to the data from million of other studies For example having enough genetic data and the right algorithms its AI computing capability demonstrated the huge potential of data analysis based on which it can used for everything from diagnosing rare illnesses to prescribing perfect dosages of medicine based on the patients personal genetic makeup Of course there are still plenty of challenges which need to be overcome before it can be used in mainstream medicine Recently Watson was able to diagnose rare form of leukemia after treatment was proved ineffective It was fed in with patient’s genetic data and compared to data from other 20 million oncological studies Sources Artificial Intelligence Used To Detect Rare Leukemia Type In Japan 2016 Watson correctly diagnoses woman after doctors were stumped 2016
1430.0,1.0,"<p>Recently White House published the article: <a href=""https://www.whitehouse.gov/blog/2016/05/03/preparing-future-artificial-intelligence"" rel=""nofollow"">Preparing for the Future of Artificial Intelligence</a> which says that government is working to leverage AI for public good and toward a more effective government.</p>

<p>I'm especially interested how AI can help with computational sustainability, environmental management and Earth's ecosystem such as biological conservation?</p>
",,0,2016-08-07T01:32:32.137,1.0,1429,2016-08-07T03:12:17.000,,,,,8.0,,1,1,<biology>,How machine learning can help with sustainable development and biological conservation?,87.0,3.8,19.21,12.14,0.0,0.0,6.0,Recently White House published the article Preparing for the Future of Artificial Intelligence which says that government is working to leverage AI for public good and toward a more effective government Im especially interested how AI can help with computational sustainability environmental management and Earths ecosystem such as biological conservation
,,"<p>There are variety of aspects where AI can help for a public good. Future studies of computational methods can contribute to sustainable management ecosystem by its data acquisition, interpretation, integration and model fitting.</p>

<p><a href=""http://web.engr.oregonstate.edu/~tgd/"" rel=""nofollow"">Prof. Tom Dietterich</a> is a leader in combining computer science and ecological sciences to build new discipline of Ecosystem Informatics which studies methods for collecting, analyzing and visualizing data on the structure and function of ecosystems.</p>

<p>His group is involved in many aspects of ecosystem, such as:</p>

<ul>
<li>Models that can predict species distribution and their presence/absence elsewhere in order to create species distribution and migration/dispersal maps (such as <a href=""https://dataone.org/"" rel=""nofollow"">DataONE Datanet</a>, <a href=""http://ebird.org/content/ebird/"" rel=""nofollow"">eBird project</a>, <a href=""http://birdcast.info/"" rel=""nofollow"">BirdCast</a>) .</li>
<li>Bio-economic models which require solving large spatio-temporal optimization problems under uncertainty.</li>
<li>Ecosystem prediction problems which require integrating heterogeneous data sources.</li>
<li>Algorithms for deployment (sensor placement), cleaning and analysis of sensor network data of resulting data to increase agricultural productivity (Project <a href=""http://tahmo.org/"" rel=""nofollow"">TAHMO</a>), like deployment of 20,000 hydro-meteorological stations in Africa (e.g. computational problem where to place it).</li>
<li>Systems for capturing, imaging, and sorting bugs combined with general image processing/machine learning/pattern recognition tools for counting and classifying them (<a href=""http://web.engr.oregonstate.edu/~tgd/bugid/"" rel=""nofollow"">BugID project</a>). The goal is to develop algorithms for automating biodiversity based on the visual pattern recognition by using computer vision method.</li>
</ul>

<p>For further information about this work, check the following resources, see:</p>

<ul>
<li>(video) <a href=""https://www.youtube.com/watch?v=FjHBFWwOdIk"" rel=""nofollow"">""Challenges for Machine Learning in Computational Sustainability"" (CRCS)</a> (2013)</li>
<li>(slides) <a href=""http://cra.org/ccc/wp-content/uploads/sites/2/2016/06/Thomas-Dietterich-AI-slides.pdf"" rel=""nofollow"">Tom Dietterich, Understanding and Managing Ecosystems through AI</a> (2013)</li>
<li>(study) <a href=""http://rspb.royalsocietypublishing.org/content/282/1808/20142984"" rel=""nofollow"">Adapting environmental management to uncertain but inevitable change</a> (2015)</li>
</ul>
",,0,2016-08-07T01:32:32.137,,1430,2016-08-07T03:12:17.000,2016-08-07T03:12:17.000,,8.0,,8.0,1429.0,2,1,,,,15.0,21.18,12.73,0.0,0.0,61.0,There are variety of aspects where AI can help for a public good Future studies of computational methods can contribute to sustainable management ecosystem by its data acquisition interpretation integration and model fitting Prof Tom Dietterich is a leader in combining computer science and ecological sciences to build new discipline of Ecosystem Informatics which studies methods for collecting analyzing and visualizing data on the structure and function of ecosystems His group is involved in many aspects of ecosystem such as Models that can predict species distribution and their presenceabsence elsewhere in order to create species distribution and migrationdispersal maps such as DataONE Datanet eBird project BirdCast Bioeconomic models which require solving large spatiotemporal optimization problems under uncertainty Ecosystem prediction problems which require integrating heterogeneous data sources Algorithms for deployment sensor placement cleaning and analysis of sensor network data of resulting data to increase agricultural productivity Project TAHMO like deployment of 20000 hydrometeorological stations in Africa eg computational problem where to place it Systems for capturing imaging and sorting bugs combined with general image processingmachine learningpattern recognition tools for counting and classifying them BugID project The goal is to develop algorithms for automating biodiversity based on the visual pattern recognition by using computer vision method For further information about this work check the following resources see video Challenges for Machine Learning in Computational Sustainability CRCS 2013 slides Tom Dietterich Understanding and Managing Ecosystems through AI 2013 study Adapting environmental management to uncertain but inevitable change 2015
1438.0,1.0,"<p>When AI has some narrow domain such as chess where it can outperform the world's human masters of chess, does it make it a superintelligence or not?</p>
",2016-08-12T15:18:43.397,1,2016-08-07T01:51:28.157,0.0,1431,2016-08-07T07:04:07.740,,,,,8.0,,1,-4,<definitions><deep-blue>,Is Deep Blue superintelligent or not?,50.0,60.99,9.23,8.48,0.0,0.0,3.0,When AI has some narrow domain such as chess where it can outperform the worlds human masters of chess does it make it a superintelligence or not
,2.0,"<p>Suppose my goal is to collaborate and create an advanced AI, for instance one that resembles a human being and the project would be on the frontier of AI research, what kind of skills would I need?</p>

<p>I am talking about specific things like what university program should I complete to enter and be competent in the field. Here are some of the things that I thought about, just to exemplify what I mean:</p>

<ul>
<li>Computer sciences: obviously the AI is built on computers, it wouldn't hurt to know how computers work, but some low level stuff and machine specific things does not seem essential, I may be wrong of course.</li>
<li>Psychology: if AI resembles human beings, knowledge of human cognition would probably be useful, although I do not imagine neurology on a cellular level or complicated psychological quirks typical to human beings like the Oedipus complex would be relevant, but again, I may be wrong.</li>
</ul>
",,1,2016-08-07T02:08:34.803,,1432,2016-08-07T14:18:25.840,,,,,1321.0,,1,5,<research>,What kind of education/expertise is required for researchers in AI?,80.0,40.55,10.92,9.94,0.0,0.0,18.0,Suppose my goal is to collaborate and create an advanced AI for instance one that resembles a human being and the project would be on the frontier of AI research what kind of skills would I need I am talking about specific things like what university program should I complete to enter and be competent in the field Here are some of the things that I thought about just to exemplify what I mean Computer sciences obviously the AI is built on computers it wouldnt hurt to know how computers work but some low level stuff and machine specific things does not seem essential I may be wrong of course Psychology if AI resembles human beings knowledge of human cognition would probably be useful although I do not imagine neurology on a cellular level or complicated psychological quirks typical to human beings like the Oedipus complex would be relevant but again I may be wrong
,2.0,"<p><a href=""https://www.whitehouse.gov/webform/rfi-preparing-future-artificial-intelligence"" rel=""nofollow"">White House published the information</a> about AI which requests mentions about 'the most important research gaps in AI that must be addressed to advance this field and benefit the public'.</p>

<p>What are these exactly?</p>
",2016-08-08T00:43:26.470,6,2016-08-07T02:33:51.350,2.0,1433,2016-08-13T03:11:49.510,2016-08-13T03:11:49.510,,8.0,,8.0,,1,1,<research><ethics><social><reasoning>,What are the most pressing fundamental questions and gaps in AI research?,83.0,54.22,12.64,10.05,0.0,0.0,4.0,White House published the information about AI which requests mentions about the most important research gaps in AI that must be addressed to advance this field and benefit the public What are these exactly
,,"<p>According to IBM Research organization in the response to White House as part of <a href=""https://www.whitehouse.gov/webform/rfi-preparing-future-artificial-intelligence"" rel=""nofollow"">preparing for the future of Artificial Intelligence</a>, AI depends upon many long-term advances, not only from AI researchers, but from many interdisciplinary teams of experts from many disciplines, including the following challenges:</p>

<ul>
<li><p><strong>Machine learning and reasoning.</strong></p>

<p>Currently AI systems use supervised learning using huge amount of dataset of labeled data for training. This is very different to how humans learn by creating concepts, relationship, common sense reasoning which gives ability to learn much without too much data. Therefore machine learning with common-sense reasoning capabilities should be researched further more.</p></li>
<li><p><strong>Decision techniques.</strong></p>

<p>Current AI-based systems have very limited ability for making decisions, therefore new techniques must be developed (e.g. modeling systemic risks, analyzing tradeoffs, detecting anomalies in context, analyzing data while preserving privacy).</p></li>
<li><p><strong>Domain-specific AI systems.</strong></p>

<p>The current AI-based system is lack of abilities to understand the variety of domains of human expertise (such as medicine, engineering, law and many more). The systems should be able to perform professional-level tasks such as designing problems, experiments, managing contradictions, negotiating, etc.</p></li>
<li><p><strong>Data assurance and trust.</strong></p>

<p>The current AI-based systems require huge amounts of data and their behaviour directly depends on the quality of this data which can be biased, incomplete or compromised. This can be expensive and time consuming especially where it is used for safety critical systems which potentially can be very dangerous.</p></li>
<li><p><strong>Radically efficient computing infrastructure.</strong></p>

<p>The current AI-based systems require unprecedented workloads and computing power which require development of new computing architectures (such as neuromorphic).</p></li>
<li><p><strong>Interpretability and explanations.</strong></p>

<p>For people to follow AI suggestions, they need to trust systems, and this is only when they are capable of knowing users' intents, priorities, reasoning and they can learn from their mistakes. These capabilities are required in many business domains and professionals</p></li>
<li><p><strong>Value alignment and ethics.</strong></p>

<p>Humans can share the common knowledge of how the world function, the machine cannot. They can fail by having unintended and unexpected behaviour only because humans did not specify the right goals for them or them omitted essential training details. The systems should be able to correct specification of the goals and avoid unintended and undesired consequences in the behaviour.</p></li>
<li><p><strong>Social AI.</strong></p>

<p>The AI-based systems should be able to work closely to humans in their professional and personal life, therefore they should have significant social capabilities, because they can impact on our emotions and our decision making capabilities. Also sophisticated natural language capabilities will need to be developed to allow a natural interaction and dialog between humans and machines.</p></li>
</ul>

<p>Source: <a href=""http://research.ibm.com/cognitive-computing/ostp/document4.shtml"" rel=""nofollow"">Fundamental questions in AI research, and the most important research gaps (RFI questions 5 and 6)</a></p>
",,0,2016-08-07T02:33:51.350,,1434,2016-08-07T02:33:51.350,,,,,8.0,1433.0,2,1,,,,34.97,17.0,11.03,0.0,0.0,70.0,According to IBM Research organization in the response to White House as part of preparing for the future of Artificial Intelligence AI depends upon many longterm advances not only from AI researchers but from many interdisciplinary teams of experts from many disciplines including the following challenges Machine learning and reasoning Currently AI systems use supervised learning using huge amount of dataset of labeled data for training This is very different to how humans learn by creating concepts relationship common sense reasoning which gives ability to learn much without too much data Therefore machine learning with commonsense reasoning capabilities should be researched further more Decision techniques Current AIbased systems have very limited ability for making decisions therefore new techniques must be developed eg modeling systemic risks analyzing tradeoffs detecting anomalies in context analyzing data while preserving privacy Domainspecific AI systems The current AIbased system is lack of abilities to understand the variety of domains of human expertise such as medicine engineering law and many more The systems should be able to perform professionallevel tasks such as designing problems experiments managing contradictions negotiating etc Data assurance and trust The current AIbased systems require huge amounts of data and their behaviour directly depends on the quality of this data which can be biased incomplete or compromised This can be expensive and time consuming especially where it is used for safety critical systems which potentially can be very dangerous Radically efficient computing infrastructure The current AIbased systems require unprecedented workloads and computing power which require development of new computing architectures such as neuromorphic Interpretability and explanations For people to follow AI suggestions they need to trust systems and this is only when they are capable of knowing users intents priorities reasoning and they can learn from their mistakes These capabilities are required in many business domains and professionals Value alignment and ethics Humans can share the common knowledge of how the world function the machine cannot They can fail by having unintended and unexpected behaviour only because humans did not specify the right goals for them or them omitted essential training details The systems should be able to correct specification of the goals and avoid unintended and undesired consequences in the behaviour Social AI The AIbased systems should be able to work closely to humans in their professional and personal life therefore they should have significant social capabilities because they can impact on our emotions and our decision making capabilities Also sophisticated natural language capabilities will need to be developed to allow a natural interaction and dialog between humans and machines Source Fundamental questions in AI research and the most important research gaps RFI questions 5 and 6
1439.0,1.0,"<p>Is there any methods by which artificial intelligence use recursion(s) to solve a certain issue or to keep up working and calculating?</p>
",,1,2016-08-07T03:49:20.143,1.0,1436,2016-08-07T08:41:03.237,,,,,1270.0,,1,4,<math>,Is recursion used in practice to improve performance of AI systems?,35.0,40.69,12.53,10.47,0.0,0.0,3.0,Is there any methods by which artificial intelligence use recursions to solve a certain issue or to keep up working and calculating
,,"<p><a href=""http://karpathy.github.io/2016/05/31/rl/"" rel=""nofollow"">Andrej Karpathy's blog</a> has a tutorial on getting a neural network to learn pong with reinforcement learning. His commentary on the current state of the field is interesting.</p>

<p>He also provides a whole bunch of links (David Silver's <a href=""http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"" rel=""nofollow"">course</a> catches my eye). <a href=""https://www.youtube.com/watch?v=2pWv7GOvuf0"" rel=""nofollow"">Here is a working link to the lecture videos.</a></p>

<p>Here are demos of DeepMinds game playing.
Get links to the papers at Andrej Karpathy's blog above
- <a href=""https://www.youtube.com/watch?v=MAMuNUixKJ8"" rel=""nofollow"">rat fps</a></p>

<ul>
<li><a href=""http://livestream.com/oxuni/StracheyLectureDrDemisHassabis/videos/113380152"" rel=""nofollow"">nice demos at 19 minutes into this</a></li>
</ul>
",,0,2016-08-07T06:30:33.053,,1437,2016-08-08T08:34:38.777,2016-08-08T08:34:38.777,,157.0,,157.0,52.0,2,1,,,,75.4,9.32,9.6,0.0,0.0,11.0,Andrej Karpathys blog has a tutorial on getting a neural network to learn pong with reinforcement learning His commentary on the current state of the field is interesting He also provides a whole bunch of links David Silvers course catches my eye Here is a working link to the lecture videos Here are demos of DeepMinds game playing Get links to the papers at Andrej Karpathys blog above rat fps nice demos at 19 minutes into this
,,"<p>There are three typical use cases for the phrase 'superintelligent':</p>

<ol>
<li><p>Something that is at least as smart as a human for <em>every</em> task.</p></li>
<li><p>Something that is smart enough to improve itself on a fundamental level.</p></li>
<li><p>Something that is smarter than a human at a <em>single</em> task.</p></li>
</ol>

<p>Most uses that I see are definition 2, but the other two are also somewhat common. I typically follow I. J. Good and use <a href=""https://en.wikipedia.org/wiki/I._J._Good#Research_and_publications"" rel=""nofollow"">ultraintelligence</a> for the first definition. (There are lots of arguments that something that fits definition 1 is likely to fit definition 2 as well, but this doesn't seem to be logically necessary.)</p>

<p>Obviously, Deep Blue only counts for definition 3.</p>
",,0,2016-08-07T07:04:07.740,,1438,2016-08-07T07:04:07.740,,,,,10.0,1431.0,2,3,,,,65.93,9.74,8.63,0.0,0.0,18.0,There are three typical use cases for the phrase superintelligent Something that is at least as smart as a human for every task Something that is smart enough to improve itself on a fundamental level Something that is smarter than a human at a single task Most uses that I see are definition 2 but the other two are also somewhat common I typically follow I J Good and use ultraintelligence for the first definition There are lots of arguments that something that fits definition 1 is likely to fit definition 2 as well but this doesnt seem to be logically necessary Obviously Deep Blue only counts for definition 3
,,"<p>To my knowledge, recursion does not play a strong role in the definition of modern AI techniques, although it does feature used in Lovasz's definition of 'Local Search' and Kurzweil is <a href=""http://www.kurzweilai.net/a-formula-for-intelligence-the-recursive-paradigm"">certainly an advocate</a>. </p>

<p>Recursion can be seen as an elegant 'architectural factorization' - building complexity by combining the results of smaller, similar patterns previously encountered. Computationally, <a href=""http://stackoverflow.com/questions/931762/can-every-recursion-be-converted-into-iteration"">recursion can always be converted into iteration</a> so this form of elegance is really mainly of use in helping to make designs more comprehensible.</p>

<p>GOFAI algorithms that were traditionally defined using recursion include depth- and breath- first search and means-ends analysis (used in Newell and Simon's General Problem Solver).</p>

<p>With respect to performance, while many functions can be very economically <em>defined</em> using recursion, the naive version of such definitions can be inefficient.</p>

<p><a href=""http://rayhightower.com/blog/2014/04/12/recursion-and-memoization/"">This page gives</a> an example in which recursive version of the Fibbonnacci function, which has asymptotic execution time n^1.6, which is reduced to n by the use of memoization.</p>
",,1,2016-08-07T07:11:56.880,,1439,2016-08-07T08:41:03.237,2016-08-07T08:41:03.237,,42.0,,42.0,1436.0,2,5,,,,31.82,16.02,11.99,0.0,0.0,28.0,To my knowledge recursion does not play a strong role in the definition of modern AI techniques although it does feature used in Lovaszs definition of Local Search and Kurzweil is certainly an advocate Recursion can be seen as an elegant architectural factorization building complexity by combining the results of smaller similar patterns previously encountered Computationally recursion can always be converted into iteration so this form of elegance is really mainly of use in helping to make designs more comprehensible GOFAI algorithms that were traditionally defined using recursion include depth and breath first search and meansends analysis used in Newell and Simons General Problem Solver With respect to performance while many functions can be very economically defined using recursion the naive version of such definitions can be inefficient This page gives an example in which recursive version of the Fibbonnacci function which has asymptotic execution time n16 which is reduced to n by the use of memoization
,,"<p>Douglas Hofstadter's <a href=""http://cognitrn.psych.indiana.edu/rgoldsto/courses/concepts/copycat.pdf"">CopyCat</a> architecture for solving letter-string analogy problems was deliberately engineered to maintain a semantically-informed notion of 'salience', i.e. given a variety of competing possibilities, tend to maintain interest in the one that is most compelling. Although the salience value of (part of) a solution is ultimately represented numerically, the means by which it determined is broadly intended to correspond (at least functionally) to the way 'selective attention' might operate in human cognition.</p>
",,0,2016-08-07T07:23:17.390,,1440,2016-08-07T07:50:46.827,2016-08-07T07:50:46.827,,42.0,,42.0,1423.0,2,5,,,,12.56,18.86,12.54,0.0,0.0,18.0,Douglas Hofstadters CopyCat architecture for solving letterstring analogy problems was deliberately engineered to maintain a semanticallyinformed notion of salience ie given a variety of competing possibilities tend to maintain interest in the one that is most compelling Although the salience value of part of a solution is ultimately represented numerically the means by which it determined is broadly intended to correspond at least functionally to the way selective attention might operate in human cognition
,,"<p>One way of illustrating the deficiencies of many of our current approaches <em>at once</em> is to consider how well it is possible to represent (equivalently, learn) commonsense knowledge. </p>

<p>In this area, the Winograd Schema Challenge has been proposed by <a href=""http://www.cs.toronto.edu/~hector/Papers/winograd.pdf"" rel=""nofollow"">Levesque</a>, in which each problem is given as input natural language text containing an ambiguous pronoun:</p>

<blockquote>
  <p>Babar wonders how he can get new clothing. Luckily, a very rich old man who has always been fond of little elephants understands right away that he is longing for a fine suit.</p>
</blockquote>

<p>Here, the program is asked to decide if 'he' in ""he is longing for a fine suit"" refers to Babar or the old man. Several thousand such questions <a href=""http://commonsensereasoning.org/winograd.html"" rel=""nofollow"">have been collated</a> and proposed as a more quantifiable alternative to the Turing test.</p>

<p>Despite the fact that the input domain is natural language, success here is undeniably a pre-requisite for AGI and (as implied in my answer <a href=""http://ai.stackexchange.com/questions/1376/is-it-ethical-to-implement-self-defence-for-street-walking-ai-robots"">here</a>) for being able to interact ethically with the human world.</p>
",,0,2016-08-07T07:39:38.143,,1441,2016-08-07T07:39:38.143,,,,,42.0,1433.0,2,2,,,,52.02,11.03,10.74,0.0,0.0,22.0,One way of illustrating the deficiencies of many of our current approaches at once is to consider how well it is possible to represent equivalently learn commonsense knowledge In this area the Winograd Schema Challenge has been proposed by Levesque in which each problem is given as input natural language text containing an ambiguous pronoun Babar wonders how he can get new clothing Luckily a very rich old man who has always been fond of little elephants understands right away that he is longing for a fine suit Here the program is asked to decide if he in he is longing for a fine suit refers to Babar or the old man Several thousand such questions have been collated and proposed as a more quantifiable alternative to the Turing test Despite the fact that the input domain is natural language success here is undeniably a prerequisite for AGI and as implied in my answer here for being able to interact ethically with the human world
,,"<p>There does not appear to be an historicial consensus on this. </p>

<p>The <a href=""https://en.wikipedia.org/wiki/Perceptrons_(book)"" rel=""nofollow"">Wikipedia page on the Perceptrons book</a> (which does not come down on either side) gives an argument that the ability of MLPs to compute any Boolean function was widely known at the time (at the very least to McCulloch and Pitts).</p>

<p>However, <a href=""http://harveycohen.net/image/perceptron.html"" rel=""nofollow"">this page</a> gives an account by someone present at the MIT AI lab in 1974, claiming that this was not common knowledge there, alluding to documentation in ""Artificial Intelligence Progress Report: Research at the Laboratory in Vision, Language, and other problems of Intelligence"" (p31-32) which is claimed to support this.</p>
",,0,2016-08-07T09:42:52.590,,1442,2016-08-07T09:42:52.590,,,,,42.0,1288.0,2,4,,,,44.71,12.43,11.13,0.0,0.0,18.0,There does not appear to be an historicial consensus on this The Wikipedia page on the Perceptrons book which does not come down on either side gives an argument that the ability of MLPs to compute any Boolean function was widely known at the time at the very least to McCulloch and Pitts However this page gives an account by someone present at the MIT AI lab in 1974 claiming that this was not common knowledge there alluding to documentation in Artificial Intelligence Progress Report Research at the Laboratory in Vision Language and other problems of Intelligence p3132 which is claimed to support this
,,"<p>There has been quite a few approaches to achieve such kind of distributed coordination. I present here one of them, for its generality and simplicity (that makes it easy to remember too). But first, the general idea behind these approaches is pretty interesting, around a mechanism called <a href=""https://en.wikipedia.org/wiki/Stigmergy"">stigmergy</a>.</p>

<p>Stigmergy is a behaviour coordination mechanism mediated by the environment. It was first described for termites, and the most famous example pertains to ants. Ants form trails when going out for food, but they often do not interact <em>directly</em>. It turns out they leave pheromones on the ground as they walk away from their hills. The pheromone allows them to find their way home, and it also guides where they are going: If they find a pheromone trace from one of their peers, they follow it and their own pheromones <em>add up</em>, reinforcing the signal of the trail. In stage, more and more ants get ""together as they move"", forming a trail. Coordination has been achieved.</p>

<p>Among the various implementation derived from stigmergy, there is the ""field-based motion coordination model"" (FBMCM). The idea is to create a (maybe virtual) environment that maintains some states of the world. Each object registers in the environment a signal that is maximum at the object position (its edges) and then decreases with distance. Moving objects (e.g. robots) each emit a signal relayed by the environment. They can then sense each other's field and act accordingly: E.g., when signals are strong, move away; when weak, it is safe to get closer, etc. Several complex group moves have been demonstrated in software simulators (platoon formation in games, drill simulations) and with robots. The benefit of this approach is that it can be cheap to compute even complex behaviours. For example, avoiding clashes requires simple ""logic"" code based on summing-up nearby fields value. FBMCM is pretty slick, used in video-games, but hard to implement in physical settings (to my <em>dating</em> knowledge), as it can be challenging to build a reliable environment. See for example the work from <a href=""https://www.researchgate.net/publication/2949751_Field-based_Motion_Coordination_In_Quake_3_Arena"">Mamei and Zambonelli</a>, as well as one of the first industrial implementations for robots by <a href=""https://www.researchgate.net/publication/221456459_Decentralized_control_of_E%27GV_transportation_systems"">Weyns et al.</a>. Note that the implementation for robots required significant work on the environment infrastructure, made somewhat more feasible as it was a controlled warehouse.</p>

<p>The advantage of stigmergy-like models is that they are often <em>simple</em> and <em>resilient</em>: You can lose an ant without impact on the food-finding trail. On the downside, these models are usually <em>slow</em>, as the coordination takes time to emerge from indirect interactions. This can be improved upon by adding extra direct interactions (e.g. empowering ants with a GPS and a grocery store map, or just a magnetic-North sense).</p>

<p>In practice, these models can collapse if the environment implementation is not reliable. It can be difficult for robots or, say, self-driving cars, if they expect some transponders put on their way, as these devices are expensive to set and maintain, and they can be broken or stolen. It would be better to endow robots with radars, sonars or other proximity sensors to implement stigmergic models. One related example is the decision by Tesla to add radars to its cars, instead of assuming reliable transponders on the road (<em>note</em>: This is just a parallel; there is no official relation).</p>

<p>Other implementations and related models are, for example, tuple-based coordination languages such as <a href=""https://en.wikipedia.org/wiki/Linda_(coordination_language)"">Linda</a>, and network protocols like <a href=""https://pdos.csail.mit.edu/papers/chord:sigcomm01/chord_sigcomm.pdf"">Chord</a>. As you see, these works are not necessarily in the ""AI domain"".</p>
",,0,2016-08-07T10:13:49.460,,1443,2016-08-07T10:13:49.460,,,,,169.0,1426.0,2,8,,,,53.71,12.24,9.43,0.0,0.0,115.0,There has been quite a few approaches to achieve such kind of distributed coordination I present here one of them for its generality and simplicity that makes it easy to remember too But first the general idea behind these approaches is pretty interesting around a mechanism called stigmergy Stigmergy is a behaviour coordination mechanism mediated by the environment It was first described for termites and the most famous example pertains to ants Ants form trails when going out for food but they often do not interact directly It turns out they leave pheromones on the ground as they walk away from their hills The pheromone allows them to find their way home and it also guides where they are going If they find a pheromone trace from one of their peers they follow it and their own pheromones add up reinforcing the signal of the trail In stage more and more ants get together as they move forming a trail Coordination has been achieved Among the various implementation derived from stigmergy there is the fieldbased motion coordination model FBMCM The idea is to create a maybe virtual environment that maintains some states of the world Each object registers in the environment a signal that is maximum at the object position its edges and then decreases with distance Moving objects eg robots each emit a signal relayed by the environment They can then sense each others field and act accordingly Eg when signals are strong move away when weak it is safe to get closer etc Several complex group moves have been demonstrated in software simulators platoon formation in games drill simulations and with robots The benefit of this approach is that it can be cheap to compute even complex behaviours For example avoiding clashes requires simple logic code based on summingup nearby fields value FBMCM is pretty slick used in videogames but hard to implement in physical settings to my dating knowledge as it can be challenging to build a reliable environment See for example the work from Mamei and Zambonelli as well as one of the first industrial implementations for robots by Weyns et al Note that the implementation for robots required significant work on the environment infrastructure made somewhat more feasible as it was a controlled warehouse The advantage of stigmergylike models is that they are often simple and resilient You can lose an ant without impact on the foodfinding trail On the downside these models are usually slow as the coordination takes time to emerge from indirect interactions This can be improved upon by adding extra direct interactions eg empowering ants with a GPS and a grocery store map or just a magneticNorth sense In practice these models can collapse if the environment implementation is not reliable It can be difficult for robots or say selfdriving cars if they expect some transponders put on their way as these devices are expensive to set and maintain and they can be broken or stolen It would be better to endow robots with radars sonars or other proximity sensors to implement stigmergic models One related example is the decision by Tesla to add radars to its cars instead of assuming reliable transponders on the road note This is just a parallel there is no official relation Other implementations and related models are for example tuplebased coordination languages such as Linda and network protocols like Chord As you see these works are not necessarily in the AI domain
,,"<p>Concentration, perhaps easier to grasp as ""focus"" or ""attention"", has quite some history in AI. user217281728 mentions CopyCat, and there was work with neural networks in the 80s as well (e.g. from <a href=""http://link.springer.com/article/10.1007%2FBF00363973"" rel=""nofollow"">Fukushima</a>, creator of the Neocognitron).</p>

<p>More recently, <em>attention</em> in neural networks is <a href=""http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/"" rel=""nofollow"">gaining momentum</a>. The mechanisms are applied to learning in deep neural networks.</p>
",,0,2016-08-07T11:19:28.940,,1444,2016-08-07T23:51:51.180,2016-08-07T23:51:51.180,,169.0,,169.0,1423.0,2,4,,,,60.11,13.84,10.4,0.0,0.0,17.0,Concentration perhaps easier to grasp as focus or attention has quite some history in AI user217281728 mentions CopyCat and there was work with neural networks in the 80s as well eg from Fukushima creator of the Neocognitron More recently attention in neural networks is gaining momentum The mechanisms are applied to learning in deep neural networks
,,"<p>Yes - it would seem that it is now possible to achieve more is required from the example you've given this paper describes a DL solution to a considerably harder problem - <a href=""http://arxiv.org/pdf/1510.07211.pdf"" rel=""nofollow"">generating the source code for a program described in natural language</a>.</p>

<p>Both of these can be described as regression problems (i.e. the goal is to minimize some loss function on the validation set), but the search space in the natural language case is much bigger.</p>
",,0,2016-08-07T12:21:12.527,,1445,2016-08-07T12:21:12.527,,,,,42.0,154.0,2,1,,,,54.56,10.86,9.72,0.0,0.0,10.0,Yes it would seem that it is now possible to achieve more is required from the example youve given this paper describes a DL solution to a considerably harder problem generating the source code for a program described in natural language Both of these can be described as regression problems ie the goal is to minimize some loss function on the validation set but the search space in the natural language case is much bigger
,0.0,"<p>The Von Neumann's <a href=""https://en.wikipedia.org/wiki/Minimax_theorem"" rel=""nofollow"">Minimax theorem</a> gives the conditions that make the <a href=""https://en.wikipedia.org/wiki/Max%E2%80%93min_inequality"" rel=""nofollow"">max-min inequality</a> an equality.</p>

<p>I understand the max-min inequality, basically <code>min(max(f))&gt;=max(min(f))</code>.</p>

<p>The Von Neumann's theorem states that, for the inequality to become an equality <code>f(.,y)</code> should always be convex for given y and <code>f(x,.)</code> should always be concave for given x, which also makes sense.</p>

<p><a href=""https://www.youtube.com/watch?v=m-EewaiFhF0&amp;list=PLAwxTw4SYaPnidDwo9e2c7ixIsu_pdSNp&amp;index=61"" rel=""nofollow"">This video</a> says that for a zero-sum perfect information game, the Von Neumann's theorem always holds, so that minimax always equal to maximin, which I did not quite follow.</p>

<p><strong>Questions</strong><br>
Why zero-sum perfect information games satisfy the conditions of Von Neumann's theorem?<br>
If we relax the rules to be non-zero-sum or non-perfect information, how would the conditions change?</p>
",2016-08-16T17:59:47.460,5,2016-08-07T12:57:29.583,,1446,2016-08-11T14:43:03.157,2016-08-11T14:43:03.157,,185.0,,185.0,,1,3,<gaming><search><minimax><game-theory>,Illustration of Von Neumann's Minimax theorem in games?,65.0,44.24,13.69,9.82,41.0,0.0,24.0,The Von Neumanns Minimax theorem gives the conditions that make the maxmin inequality an equality I understand the maxmin inequality basically The Von Neumanns theorem states that for the inequality to become an equality should always be convex for given y and should always be concave for given x which also makes sense This video says that for a zerosum perfect information game the Von Neumanns theorem always holds so that minimax always equal to maximin which I did not quite follow Questions Why zerosum perfect information games satisfy the conditions of Von Neumanns theorem If we relax the rules to be nonzerosum or nonperfect information how would the conditions change
,,"<p>One of the comments suggests a PhD in machine learning. As a full-time AI researcher myself, I'd say that would certainly be one useful option.</p>

<p>However, in order <a href=""http://ai.stackexchange.com/questions/1420/how-close-are-we-to-creating-ex-machina"">make much-needed progress</a>, AI needs avoid falling into the trap of thinking that currently fashionable methods are any kind of 'silver bullet'. There's some danger that a PhD that heads straight into (say) some sub-sub-sub area of DL would end up imposing too much bias on the student's subsequent perspective.</p>

<p>AI research is an essentially multi-disciplinary activity. Other possible backgrounds therefore include:</p>

<ul>
<li><p>Mathematics or physics (to first degree or PhD level). A strong background in either of these never did anyone any harm. People who are competent in these fields tend to be able to turn their abilities to new domains relatively easily. </p></li>
<li><p>Software Engineering. One of the things that AI needs are integrative architectures for knowledge engineering. <a href=""http://www.cogsys.wiai.uni-bamberg.de/teaching/materials/AI-Light-Bulbb.htm"">Here's why</a>. I believe that one of the reasons that we haven't yet managed to do <a href=""http://ai.stackexchange.com/questions/1396/why-ocr-cannot-be-perceived-as-good-example-of-ai"">OCR at the level of a 5 year old</a> is that we've yet to accept that we have to 'build a sledgehammer to crack a nut'. Software architects are used to managing large-scale complexity, so they may be able to help.</p></li>
<li><p>Cognitive Science, Psychology, Cognitive Linguistics. The reasons here are obvious.</p></li>
</ul>

<p>Above all, I personally think that a good AI researcher should be creative, inquisitive and prepared to question received wisdom, all of which are more important in practice than specifics of their background.</p>
",,0,2016-08-07T12:59:43.577,,1447,2016-08-07T13:04:53.037,2016-08-07T13:04:53.037,,42.0,,42.0,1432.0,2,7,,,,53.61,12.18,10.29,0.0,0.0,46.0,One of the comments suggests a PhD in machine learning As a fulltime AI researcher myself Id say that would certainly be one useful option However in order make muchneeded progress AI needs avoid falling into the trap of thinking that currently fashionable methods are any kind of silver bullet Theres some danger that a PhD that heads straight into say some subsubsub area of DL would end up imposing too much bias on the students subsequent perspective AI research is an essentially multidisciplinary activity Other possible backgrounds therefore include Mathematics or physics to first degree or PhD level A strong background in either of these never did anyone any harm People who are competent in these fields tend to be able to turn their abilities to new domains relatively easily Software Engineering One of the things that AI needs are integrative architectures for knowledge engineering Heres why I believe that one of the reasons that we havent yet managed to do OCR at the level of a 5 year old is that weve yet to accept that we have to build a sledgehammer to crack a nut Software architects are used to managing largescale complexity so they may be able to help Cognitive Science Psychology Cognitive Linguistics The reasons here are obvious Above all I personally think that a good AI researcher should be creative inquisitive and prepared to question received wisdom all of which are more important in practice than specifics of their background
,,"<p>Research on AI seems to be getting wider these days (2016). First, ""obvious"" few departments (no order):</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Computer_science"" rel=""nofollow"">Computer Science</a> (e.g. computation theory, algorithms): AI researchers there assume that intelligence is a kind of computation, under various forms (e.g. a neural network, a logic system).</li>
<li><a href=""https://en.wikipedia.org/wiki/Software_engineering"" rel=""nofollow"">Software Engineering</a>: Assuming we find a good model for AI, how do you make it? This is what the engineer will want to figure out. And it can be hard to map mathematical models to an engineered piece.</li>
<li>Statistics and Probabilities (more specific than just Mathematics, which is also close to Computer Science): This is about Data Science, notably as a foundation to Machine Learning, the most active branch in AI---which ""just"" covers the <em>learning</em> part.</li>
<li>Physics: This is particularly relevant now for hardware (see below).</li>
<li>Neuro Science: Understand how the brain works, as an inspiration to create an artificial one, is the home for Connectionists. Recently, Hassabis and his team at Google Deepmind made several breakthroughs related to reinforcement learning, memory, attention, etc.</li>
</ul>

<p>Recently Electric Engineering is getting a lot of light, together with the related branches of Physics. Several public and private laboratories focus on ""brain chips"". To name a few: IBM (who's working on that for some time already), Nvidia, and Facebook. Circa 2010, it became clear that techniques like deep learning require horsepower, thus an increasing focus on creating more powerful, smaller, more energy efficient chips. And on top of that, there is all the work in Quantum Computing.</p>

<p>But the thing is, there seems to be many more fields that are getting involved in AI research. We should mention Chemistry and Biology, as both inspiration and tools to make new models or hardware (e.g. chips that do not use silicon, so they can get smaller).</p>

<p>As for 2016, the above fields are the most active, and promise to remain very active for quite some time. Pick your own depending on your interest, skills, or mere intuition!</p>

<p>To finish, we may be surprised in a few years when we look back at where AI has come from. I believe that if we manage to build an AGI, it will leverage <em>all</em> these fields anyway. I guess the thrill is to be part of the story.</p>
",,0,2016-08-07T14:18:25.840,,1448,2016-08-07T14:18:25.840,,,,,169.0,1432.0,2,4,,,,64.2,11.31,9.36,0.0,0.0,91.0,Research on AI seems to be getting wider these days 2016 First obvious few departments no order Computer Science eg computation theory algorithms AI researchers there assume that intelligence is a kind of computation under various forms eg a neural network a logic system Software Engineering Assuming we find a good model for AI how do you make it This is what the engineer will want to figure out And it can be hard to map mathematical models to an engineered piece Statistics and Probabilities more specific than just Mathematics which is also close to Computer Science This is about Data Science notably as a foundation to Machine Learning the most active branch in AIwhich just covers the learning part Physics This is particularly relevant now for hardware see below Neuro Science Understand how the brain works as an inspiration to create an artificial one is the home for Connectionists Recently Hassabis and his team at Google Deepmind made several breakthroughs related to reinforcement learning memory attention etc Recently Electric Engineering is getting a lot of light together with the related branches of Physics Several public and private laboratories focus on brain chips To name a few IBM whos working on that for some time already Nvidia and Facebook Circa 2010 it became clear that techniques like deep learning require horsepower thus an increasing focus on creating more powerful smaller more energy efficient chips And on top of that there is all the work in Quantum Computing But the thing is there seems to be many more fields that are getting involved in AI research We should mention Chemistry and Biology as both inspiration and tools to make new models or hardware eg chips that do not use silicon so they can get smaller As for 2016 the above fields are the most active and promise to remain very active for quite some time Pick your own depending on your interest skills or mere intuition To finish we may be surprised in a few years when we look back at where AI has come from I believe that if we manage to build an AGI it will leverage all these fields anyway I guess the thrill is to be part of the story
,,"<p>Convolutional neural network can be applied not only for image recognition, but also for video analysis and recognition, natural language processing, natural language processing, in games (e.g. <a href=""https://en.wikipedia.org/wiki/Computer_Go#New_approaches_to_problems"" rel=""nofollow"">Go</a>) or even for <a href=""http://ai.stackexchange.com/a/1424/8"">drug discovery</a> by predicting the interaction between molecules and biological proteins<sup><a href=""https://en.wikipedia.org/wiki/Convolutional_neural_network#Applications"" rel=""nofollow"">wiki</a></sup>.</p>

<p>Therefore it can be used for variety of problems by using convolutional and subsampling layers connected to more fully connected layers. They're easier to train, because have fewer parameters than fully connected networks with the same number of hidden units.<sup><a href=""http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/"" rel=""nofollow"">UFLDL</a></sup></p>
",,0,2016-08-07T17:14:07.107,,1449,2016-08-07T17:14:07.107,,,,,8.0,70.0,2,0,,,,25.29,17.0,12.16,0.0,0.0,13.0,Convolutional neural network can be applied not only for image recognition but also for video analysis and recognition natural language processing natural language processing in games eg Go or even for drug discovery by predicting the interaction between molecules and biological proteinswiki Therefore it can be used for variety of problems by using convolutional and subsampling layers connected to more fully connected layers Theyre easier to train because have fewer parameters than fully connected networks with the same number of hidden unitsUFLDL
,,"<p>Yes. Here are some of the most prominent ones and their respective state-of-the-art errors:</p>

<ul>
<li><a href=""https://www.cs.toronto.edu/~kriz/cifar.html"" rel=""nofollow"">CIFAR-10</a>: ~3.5% error</li>
<li><a href=""https://www.cs.toronto.edu/~kriz/cifar.html"" rel=""nofollow"">CIFAR-100</a>: ~24% error  </li>
<li><a href=""https://cs.stanford.edu/~acoates/stl10/"" rel=""nofollow"">STL-10</a>: ~26% error</li>
<li><a href=""http://ufldl.stanford.edu/housenumbers/"" rel=""nofollow"">SVHN</a>: ~1.7% error</li>
<li><a href=""http://www.image-net.org/"" rel=""nofollow"">ImageNet</a> tasks: the best 2012 classification task solution got 15% top-5 error, better results are currently available</li>
</ul>

<p>You can check an updated list of solutions <a href=""http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"" rel=""nofollow"">here</a>. Also, a more comprehensive list of modern datasets can be found <a href=""http://deeplearning.net/datasets/"" rel=""nofollow"">here</a>.</p>
",,1,2016-08-07T18:09:03.483,,1450,2016-08-08T13:58:28.580,2016-08-08T13:58:28.580,,144.0,,144.0,1392.0,2,4,,,,63.7,13.86,9.86,0.0,0.0,29.0,Yes Here are some of the most prominent ones and their respective stateoftheart errors CIFAR10 35 error CIFAR100 24 error STL10 26 error SVHN 17 error ImageNet tasks the best 2012 classification task solution got 15 top5 error better results are currently available You can check an updated list of solutions here Also a more comprehensive list of modern datasets can be found here
2095.0,1.0,"<p>In October 2014, Dr. Mark Riedl published an approach to testing AI intelligence, called <a href=""http://arxiv.org/pdf/1410.6142v3.pdf"" rel=""nofollow"">the ""Lovelace Test 2.0""</a>, after being inspired by the <a href=""http://kryten.mm.rpi.edu/lovelace.pdf"" rel=""nofollow"">original Lovelace Test</a> (published in 2001). Mark believed that the original Lovelace Test would be impossible to pass, and therefore, suggested a weaker, and more practical version.</p>

<p>The Lovelace Test 2.0 makes the assumption that for an AI to be intelligent, it must exhibit creativity. From the paper itself:</p>

<blockquote>
  <p>The Lovelace 2.0 Test is as follows: artificial agent a is challenged as follows:</p>
  
  <ul>
  <li><p>a must create an artifact o of type t;</p></li>
  <li><p>o must conform to a set of constraints C where ci ∈ C is
  any criterion expressible in natural language;</p></li>
  <li><p>a human evaluator h, having chosen t and C, is satisfied
  that o is a valid instance of t and meets C; and</p></li>
  <li><p>a human referee r determines the combination of t and C
  to not be unrealistic for an average human.</p></li>
  </ul>
</blockquote>

<p>Since it is possible for a human evaluator to come up with some pretty easy constraints for an AI to beat, the human evaluator is then expected to keep coming up with more and more complex constraints for the AI until the AI fails. The point of the Lovelace Test 2.0 is to <em>compare</em> the creativity of different AIs, not to provide a definite dividing line between 'intelligence' and 'nonintelligence' like the Turing Test would.</p>

<p>However, I am curious about whether this test has actually been used in an academic setting, or it is only seen as a thought experiment at the moment. The Lovelace Test seems easy to apply in academic settings (you only need to develop some measurable constraints that you can use to test the artificial agent), but it also may be too subjective (humans can disagree on the merits of certain constraints, and whether a creative artifact produced by an AI actually meets the final result).</p>
",,0,2016-08-07T18:17:11.663,1.0,1451,2016-10-08T01:47:41.487,2016-08-30T19:42:55.163,,29.0,,181.0,,1,14,<history><intelligence-testing>,Has the Lovelace Test 2.0 been successfully used in an academic setting?,257.0,55.17,10.16,9.58,0.0,0.0,46.0,In October 2014 Dr Mark Riedl published an approach to testing AI intelligence called the Lovelace Test 20 after being inspired by the original Lovelace Test published in 2001 Mark believed that the original Lovelace Test would be impossible to pass and therefore suggested a weaker and more practical version The Lovelace Test 20 makes the assumption that for an AI to be intelligent it must exhibit creativity From the paper itself The Lovelace 20 Test is as follows artificial agent a is challenged as follows a must create an artifact o of type t o must conform to a set of constraints C where ci ∈ C is any criterion expressible in natural language a human evaluator h having chosen t and C is satisfied that o is a valid instance of t and meets C and a human referee r determines the combination of t and C to not be unrealistic for an average human Since it is possible for a human evaluator to come up with some pretty easy constraints for an AI to beat the human evaluator is then expected to keep coming up with more and more complex constraints for the AI until the AI fails The point of the Lovelace Test 20 is to compare the creativity of different AIs not to provide a definite dividing line between intelligence and nonintelligence like the Turing Test would However I am curious about whether this test has actually been used in an academic setting or it is only seen as a thought experiment at the moment The Lovelace Test seems easy to apply in academic settings you only need to develop some measurable constraints that you can use to test the artificial agent but it also may be too subjective humans can disagree on the merits of certain constraints and whether a creative artifact produced by an AI actually meets the final result
,,"<p>Yes, although how useful this AI can be is another question entirely.</p>

<p><strong>mpgac</strong> is a ""minimally intelligent AGI"" trained on the GAC-80K corpus of MIST questions. As a result, it should be able to ""minimally"" pass this test. However, being trained on the GAC-80K corpus obviously make it lacking for any practical purposes. From the README:</p>

<blockquote>
  <p>Obviously this should only be capable of producing a minimally intelligent signal when ordinary commonsense questions are asked, of the kind depicted above, using questions which would have made sense to an average human between the years 2000 and 2005. On expert knowledge or current affairs related questions it should perform no better than chance.</p>
</blockquote>

<p>The point of mpgac is to compare it to other AIs that could be built to pass this test. Or as the writer wrote in the README:</p>

<blockquote>
  <p>When scanning the skies how can we tell whether the radio signals detected are from an intelligent source, or are merely just background or sensor noise?</p>
</blockquote>

<p>Ideally, you would want to build a program that is ""better"" than mpgac. In much the same way as ELIZA can be seen as a baseline for the Turing Test, mpgac is the baseline for the MIST test.</p>

<p>The GitHub repo of mpgac (as well as the GAC-80K corpus) is available <a href=""https://github.com/bashrc/mindpix"">here</a>.</p>
",,0,2016-08-07T18:48:12.270,,1452,2016-08-07T18:48:12.270,,,,,181.0,1397.0,2,7,,,,68.5,10.04,9.29,0.0,0.0,32.0,Yes although how useful this AI can be is another question entirely mpgac is a minimally intelligent AGI trained on the GAC80K corpus of MIST questions As a result it should be able to minimally pass this test However being trained on the GAC80K corpus obviously make it lacking for any practical purposes From the README Obviously this should only be capable of producing a minimally intelligent signal when ordinary commonsense questions are asked of the kind depicted above using questions which would have made sense to an average human between the years 2000 and 2005 On expert knowledge or current affairs related questions it should perform no better than chance The point of mpgac is to compare it to other AIs that could be built to pass this test Or as the writer wrote in the README When scanning the skies how can we tell whether the radio signals detected are from an intelligent source or are merely just background or sensor noise Ideally you would want to build a program that is better than mpgac In much the same way as ELIZA can be seen as a baseline for the Turing Test mpgac is the baseline for the MIST test The GitHub repo of mpgac as well as the GAC80K corpus is available here
1456.0,1.0,"<p>Convolutional neural network are leading type of feed-forward artificial neural network for image recognition. Can they be used for real-time image recognition for videos (frame by frame), or it takes too much processing (assuming they're written in C-like language)?</p>

<p>For example for classification of type of animals based on the training from huge dataset.</p>
",,0,2016-08-07T19:49:25.977,1.0,1453,2016-08-16T14:50:49.567,2016-08-07T20:36:00.187,,8.0,,8.0,,1,2,<conv-neural-network><classification><performance><real-time>,Can ConvNets be used for real-time object recognition from video feed?,29.0,36.28,15.19,11.25,0.0,0.0,12.0,Convolutional neural network are leading type of feedforward artificial neural network for image recognition Can they be used for realtime image recognition for videos frame by frame or it takes too much processing assuming theyre written in Clike language For example for classification of type of animals based on the training from huge dataset
,,"<p>No, quantum computers (as understood by mainstream scientists) cannot solve the halting problem. We can already <a href=""http://algorithmicassertions.com/2016/05/22/quirk.html"" rel=""nofollow"">simulate quantum circuits with normal computers</a>; it just takes a really long time when you get a decent number of qubits involved. (Quantum computing provides exponential speedups for some problems.) Therefore, if quantum computers could solve the halting problem, we could solve the halting problem with classical computers by simulating a quantum one, but it's impossible to solve the halting problem with classical computers, so we can't do it with quantum ones either.</p>

<p>There are proponents of hypercomputation - infinite speedups using quantum computers - but the evidence put forward so far is mostly conjecture. Further reading: <a href=""https://arxiv.org/ftp/quant-ph/papers/0512/0512248.pdf"" rel=""nofollow"">Can quantum computing solve classically
unsolvable problems?</a> (PDF), <a href=""http://cs.stackexchange.com/q/6296"">References on comparison between quantum computers and Turing machines</a> (at CS.SE).</p>

<p>Solving the halting problem would make a computer exceptionally powerful. It would conceivably be able to <a href=""https://blogs.msdn.microsoft.com/ericlippert/2011/02/24/never-say-never-part-two/"" rel=""nofollow"">check whether complex theorems are true</a> without necessarily needing to product a mathematical proof. Solving that problem isn't necessary for strong AI, though. Going with the definition of ""strong AI"" as ""where the machine's intellectual capability is functionally equal to a human's"" (<a href=""https://www.ocf.berkeley.edu/~arihuang/academic/research/strongai3.html"" rel=""nofollow"">source</a>), a computer could be able to learn like a human despite not being able to look at a program and see if it halts. I can't magically determine the halting properties of any arbitrary program, yet I'd like to think I'm an intelligent being.</p>
",,0,2016-08-07T23:34:29.837,,1454,2016-08-07T23:34:29.837,,,,,75.0,186.0,2,2,,,,43.32,15.14,10.29,0.0,0.0,48.0,No quantum computers as understood by mainstream scientists cannot solve the halting problem We can already simulate quantum circuits with normal computers it just takes a really long time when you get a decent number of qubits involved Quantum computing provides exponential speedups for some problems Therefore if quantum computers could solve the halting problem we could solve the halting problem with classical computers by simulating a quantum one but its impossible to solve the halting problem with classical computers so we cant do it with quantum ones either There are proponents of hypercomputation infinite speedups using quantum computers but the evidence put forward so far is mostly conjecture Further reading Can quantum computing solve classically unsolvable problems PDF References on comparison between quantum computers and Turing machines at CSSE Solving the halting problem would make a computer exceptionally powerful It would conceivably be able to check whether complex theorems are true without necessarily needing to product a mathematical proof Solving that problem isnt necessary for strong AI though Going with the definition of strong AI as where the machines intellectual capability is functionally equal to a humans source a computer could be able to learn like a human despite not being able to look at a program and see if it halts I cant magically determine the halting properties of any arbitrary program yet Id like to think Im an intelligent being
,,"<p>We are getting there, with as usual some trade-off between quality and speed.</p>

<p>For example <a href=""http://cs231n.stanford.edu/slides/winter1516_lecture8.pdf"" rel=""nofollow noreferrer"">Spatial Localization and Detection lecture</a> shows some benchmarks (mAP = Mean Average Precision, higher is better; FPS = frame per second):</p>

<p><a href=""https://i.stack.imgur.com/AfHt2m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AfHt2m.png"" alt=""Table/Performance: Real-Time Detectors""></a></p>

<p><a href=""https://i.stack.imgur.com/B5Qb9m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B5Qb9m.png"" alt=""Table/Performance: R-CNN, Fast R-CNN, Faster R-CNN""></a></p>
",,0,2016-08-08T04:14:49.693,,1456,2016-08-16T14:50:49.567,2016-08-16T14:50:49.567,,8.0,,4.0,1453.0,2,2,,,,45.76,15.37,11.91,0.0,0.0,10.0,We are getting there with as usual some tradeoff between quality and speed For example Spatial Localization and Detection lecture shows some benchmarks mAP Mean Average Precision higher is better FPS frame per second
,,"<p>In AI (but in general too, I believe), a simplification is that modeling is more akin to Mathematics (and related hard sciences involved, like Physics and... Computer Science), and implementation to Software Engineering.</p>

<p>Let's take a concrete example, really outside of AI: Find the minimum value of a given polynomial, if it exists.</p>

<p>The Mathematician will derivate the polynomial, find the zeros, and checkout convexity to find a minimum (if there is any zero). This procedure is very standard---some will say straightforward. It relies on a body of knowledge and an abstraction level that is appropriate for manual proof.</p>

<p>The Software Engineer approach is actually way longer to explain, and I am going to skip it. The point is that the body of knowledge is related but different: We have to find now a step-by-step procedure for the computer to achieve the result. The Mathematician one could be implemented directly in MathLab, almost verbatim, but we <em>assume</em> MathLab. And to build MathLab, we are back to the problem of making a procedure the computer can execute. We could for example base a procedure on Euler's method to find roots (a ""simple"" approach that closes on roots step after step), etc.</p>

<p>Simple mathematical operations can be quite complex to implement on a computer. Perhaps the most famous is <a href=""https://en.wikipedia.org/wiki/Random_number_generation"" rel=""nofollow"">random-number generation</a>. Mathematically, the concept is pure and clear. Generating an actual random-number is more elusive than it looks, to the point it calls for new <em>models</em> and new <em>implementations</em>...</p>

<p>A concrete example from history: Neural networks. In the 80s and 90s, NNs were weighted graphs that could be executed on computers using graph libraries or similar foundation libs. Choosing the weights was challenging. One day the back-propagation learning model was introduced to automated the choice of weights. The model relied on a procedure dedicated to NNs, using a terminology like partial derivates, gradient descent, chain rules, etc. And later then, clever engineers created libraries to automate the back-propagation procedure. The libraries can be somewhat far from the original model, as engineers learn how to make it computable, even faster (i.e. optimization, approximations/truncations).</p>
",,0,2016-08-08T04:44:18.407,,1457,2016-08-08T04:44:18.407,,,,,169.0,1297.0,2,2,,,,46.88,13.4,9.93,0.0,0.0,79.0,In AI but in general too I believe a simplification is that modeling is more akin to Mathematics and related hard sciences involved like Physics and Computer Science and implementation to Software Engineering Lets take a concrete example really outside of AI Find the minimum value of a given polynomial if it exists The Mathematician will derivate the polynomial find the zeros and checkout convexity to find a minimum if there is any zero This procedure is very standardsome will say straightforward It relies on a body of knowledge and an abstraction level that is appropriate for manual proof The Software Engineer approach is actually way longer to explain and I am going to skip it The point is that the body of knowledge is related but different We have to find now a stepbystep procedure for the computer to achieve the result The Mathematician one could be implemented directly in MathLab almost verbatim but we assume MathLab And to build MathLab we are back to the problem of making a procedure the computer can execute We could for example base a procedure on Eulers method to find roots a simple approach that closes on roots step after step etc Simple mathematical operations can be quite complex to implement on a computer Perhaps the most famous is randomnumber generation Mathematically the concept is pure and clear Generating an actual randomnumber is more elusive than it looks to the point it calls for new models and new implementations A concrete example from history Neural networks In the 80s and 90s NNs were weighted graphs that could be executed on computers using graph libraries or similar foundation libs Choosing the weights was challenging One day the backpropagation learning model was introduced to automated the choice of weights The model relied on a procedure dedicated to NNs using a terminology like partial derivates gradient descent chain rules etc And later then clever engineers created libraries to automate the backpropagation procedure The libraries can be somewhat far from the original model as engineers learn how to make it computable even faster ie optimization approximationstruncations
1460.0,3.0,"<p>Just for the purpose of learning I'd like to classify the likeliness of a tweet being in aggressive language or not. </p>

<p>I was wondering how to approach the problem. I guess I need first train my neural network on a huge dataset of text what aggressive language is. This brings up the question where I would get this data in the first place?</p>

<p>It feels a bit like the chicken and egg problem to me so I wonder how would I approach the problem?</p>
",,0,2016-08-08T07:04:49.487,,1458,2016-08-08T18:19:35.150,,,,,1334.0,,1,3,<classification><datasets>,How to classify language as friendly or aggressive with AI?,45.0,79.8,6.84,7.85,0.0,0.0,6.0,Just for the purpose of learning Id like to classify the likeliness of a tweet being in aggressive language or not I was wondering how to approach the problem I guess I need first train my neural network on a huge dataset of text what aggressive language is This brings up the question where I would get this data in the first place It feels a bit like the chicken and egg problem to me so I wonder how would I approach the problem
,,"<p>I did a little search and couldn't find any database that has ground truth for aggressiveness. This means that you need to build yourself a database. This might be huge undertaking. Take thousands of messages, and classify them by hand whether they are aggressive or not. This part is quite labor intensive.</p>

<p>Second part is much easier at start but would be pain to optimize (both performance and computational cost). I would suggest you to start with Naive Bayes classifier for this job. That is the preferred classifier for spam detection. ANN would probably not work for this case because the data would be a huge sparse vector. Estimated number of words in English is over a million, which means the input layer of your ANN should be able to scale up to that number. Search for sparse vector classification for additional classifier that can be used in these cases.</p>
",,1,2016-08-08T07:49:22.200,,1459,2016-08-08T07:49:22.200,,,,,210.0,1458.0,2,3,,,,74.59,9.91,8.63,0.0,0.0,16.0,I did a little search and couldnt find any database that has ground truth for aggressiveness This means that you need to build yourself a database This might be huge undertaking Take thousands of messages and classify them by hand whether they are aggressive or not This part is quite labor intensive Second part is much easier at start but would be pain to optimize both performance and computational cost I would suggest you to start with Naive Bayes classifier for this job That is the preferred classifier for spam detection ANN would probably not work for this case because the data would be a huge sparse vector Estimated number of words in English is over a million which means the input layer of your ANN should be able to scale up to that number Search for sparse vector classification for additional classifier that can be used in these cases
,,"<p>The answer by <a href=""http://ai.stackexchange.com/users/210/cem-kalyoncu"">Cem Kalyoncu</a> mentions the difficulty of building a ground truth database for aggressiveness.</p>

<p>One alternative approach would be to attempt to operate at the <em>concept level</em>, which would allow the use of pre-existing ontologies such as ConceptNet.</p>

<p><a href=""http://www.cs.stir.ac.uk/~spo/publication/resources/sentiment-analysis.pdf"" rel=""nofollow"">Here's a paper</a> that describes this technique.</p>
",,1,2016-08-08T08:26:24.703,,1460,2016-08-08T13:51:13.120,2016-08-08T13:51:13.120,,42.0,,42.0,1458.0,2,2,,,,47.08,14.15,11.47,0.0,0.0,6.0,The answer by Cem Kalyoncu mentions the difficulty of building a ground truth database for aggressiveness One alternative approach would be to attempt to operate at the concept level which would allow the use of preexisting ontologies such as ConceptNet Heres a paper that describes this technique
1483.0,4.0,"<p>Siri and Cortana communicate pretty much like humans. Unlike Google now which mainly gives us search results when asked some questions (not setting alarms or reminders), Siri and Cortana provide us with an answer, in the same way that a person would do.<br>
So are they actual AI programs or not?</p>

<p>(By ""question"" I don't mean any academic related question or asking routes/ temperature, but rather opinion based question). </p>
",,2,2016-08-08T17:48:43.730,3.0,1461,2016-12-23T22:08:36.177,2016-08-24T14:11:11.543,,72.0,,72.0,,1,15,<emotional-intelligence><intelligence-testing><natural-language><applications>,Are Siri and Cortana AI programs?,401.0,54.02,11.77,11.81,0.0,0.0,15.0,Siri and Cortana communicate pretty much like humans Unlike Google now which mainly gives us search results when asked some questions not setting alarms or reminders Siri and Cortana provide us with an answer in the same way that a person would do So are they actual AI programs or not By question I dont mean any academic related question or asking routes temperature but rather opinion based question
1465.0,4.0,"<p>The question is pretty much the title.</p>

<p>Basically what is the difference between AI and robots?</p>
",,2,2016-08-08T17:53:40.050,,1462,2016-08-10T15:18:21.477,2016-08-09T00:55:45.473,,145.0,,72.0,,1,2,<comparison><robots>,What is the difference between AI and robots?,251.0,71.82,9.3,10.94,0.0,0.0,2.0,The question is pretty much the title Basically what is the difference between AI and robots
,,"<p>I would classify both as having / using elements of AI, yes.  But I wouldn't say either represents a truly ""intelligent"" (in the AGI sense) program.</p>

<p>But here's the rub... as you'll see in other questions asking about definitions of AI, there's a sort of memetic thing where anything that AI begins to do successfully, immediately stops being considered ""AI"".  So AI is always an unreachable state, because it's always ""something humans can do that computers can't"" and once the computer <em>can</em> do it, it isn't AI anymore.   So take that into consideration.</p>
",,0,2016-08-08T18:01:39.743,,1463,2016-08-08T18:01:39.743,,,,,33.0,1461.0,2,5,,,,64.41,10.95,10.06,0.0,0.0,29.0,I would classify both as having using elements of AI yes But I wouldnt say either represents a truly intelligent in the AGI sense program But heres the rub as youll see in other questions asking about definitions of AI theres a sort of memetic thing where anything that AI begins to do successfully immediately stops being considered AI So AI is always an unreachable state because its always something humans can do that computers cant and once the computer can do it it isnt AI anymore So take that into consideration
,,"<p>One:  we don't really know what intelligence is.</p>

<p>Two: we don't truly understand the best model of intelligence we have available (human intelligence) works.</p>

<p>Three: we're trying to replicate human intelligence (to some extent) on hardware which is quite different from the hardware it runs on in reality.</p>

<p>Four: the human brain (our best model of intelligence) is mostly a black-box to us, and it's difficult to probe/introspect it's operation without killing the test subject.  This is, of course, unethical and illegal.  So progress in understanding the brain is very slow. </p>

<p>Combine those factors and you can understand why it's difficult to make progress in AI.  In many ways, you can argue that we're shooting in the dark.   Of course we have made <em>some</em> progress, so we know we're getting some things right.  But without a real comprehensive theory about <em>how</em> AI should/will work, we are reduced to a lot of trial and error and iteration to move forward.</p>
",,0,2016-08-08T18:08:26.123,,1464,2016-08-08T18:08:26.123,,,,,33.0,60.0,2,3,,,,63.8,11.19,8.5,0.0,0.0,37.0,One we dont really know what intelligence is Two we dont truly understand the best model of intelligence we have available human intelligence works Three were trying to replicate human intelligence to some extent on hardware which is quite different from the hardware it runs on in reality Four the human brain our best model of intelligence is mostly a blackbox to us and its difficult to probeintrospect its operation without killing the test subject This is of course unethical and illegal So progress in understanding the brain is very slow Combine those factors and you can understand why its difficult to make progress in AI In many ways you can argue that were shooting in the dark Of course we have made some progress so we know were getting some things right But without a real comprehensive theory about how AI shouldwill work we are reduced to a lot of trial and error and iteration to move forward
,,"<p>Although there are several definitions of ""robot"", an essential feature of everything called ""robot"" is that it is capable of movement. This does not necessarily mean <em>displacement</em>; a robot arm in a factory also moves.</p>

<p>There is a single exception to this rule,  which is bot-programs like chatbots; I will discuss them later.</p>

<p>Artificial Intelligence does not need to move; a chess program can be argued to be an AI, but does not move. </p>

<p>A robot can actually have AI; one of the definitions of robot is that it is a system, capable of <em>autonomous</em> movement. In order to be autonomous, to be able to make decisions of its own, a certain amount of AI may be necessary. </p>

<p>There is one class of ""robots"" that does not move, and does not even have physical presence; bot programs, like chatbots, that operate inside systems. I do not consider them robots, because they are not physical devices operating in the real world. A chatbot can be an AI, however - a good chatbot may have some natural language processing to interact with humans in a way that humans find natural.</p>

<p>To summarize; an AI can exist purely in software. But to be a robot, there must be a moving physical component in the real world.</p>
",,0,2016-08-08T18:14:19.090,,1465,2016-08-08T18:14:19.090,,,,,66.0,1462.0,2,4,,,,60.35,9.4,9.58,0.0,0.0,37.0,Although there are several definitions of robot an essential feature of everything called robot is that it is capable of movement This does not necessarily mean displacement a robot arm in a factory also moves There is a single exception to this rule which is botprograms like chatbots I will discuss them later Artificial Intelligence does not need to move a chess program can be argued to be an AI but does not move A robot can actually have AI one of the definitions of robot is that it is a system capable of autonomous movement In order to be autonomous to be able to make decisions of its own a certain amount of AI may be necessary There is one class of robots that does not move and does not even have physical presence bot programs like chatbots that operate inside systems I do not consider them robots because they are not physical devices operating in the real world A chatbot can be an AI however a good chatbot may have some natural language processing to interact with humans in a way that humans find natural To summarize an AI can exist purely in software But to be a robot there must be a moving physical component in the real world
,,"<p>IBM clearly don't provide all the details / ""secret sauce"" but there is some information out there on how Watson works.  Some of the text search / retrieval stuff uses a technology called <a href=""http://uima.apache.org/"" rel=""nofollow"">UIMA</a> which IBM open-sourced a few years ago.  It also uses Prolog and some custom C++ code.    Some more information can be found <a href=""http://learning.acm.org/webinar/lally.cfm"" rel=""nofollow"">here</a>.</p>
",,0,2016-08-08T18:14:45.737,,1466,2016-08-08T18:14:45.737,,,,,33.0,1334.0,2,2,,,,65.93,9.74,8.91,0.0,0.0,12.0,IBM clearly dont provide all the details secret sauce but there is some information out there on how Watson works Some of the text search retrieval stuff uses a technology called UIMA which IBM opensourced a few years ago It also uses Prolog and some custom C code Some more information can be found here
,,"<p>In the broadest sense, the difference is that non-robotic A(G)I <em>may</em> not be possible because, as per <a href=""http://ai.stackexchange.com/questions/1415/what-kind-of-body-if-any-does-intelligence-require"">this question</a>, it could be that ""Intelligence requires a body"".</p>

<p>More specifically, it could be that there are limitations to what the traditional (well, 1950s style) 'Brain in a vat' notion of an AI is capable of comprehending, in the absence of experience of embodied experience such as force, motion and ""the raw, unawshed world"".</p>
",,0,2016-08-08T18:17:21.233,,1467,2016-08-08T19:14:17.083,2016-08-08T19:14:17.083,,42.0,,42.0,1462.0,2,5,,,,34.94,12.37,10.25,0.0,0.0,21.0,In the broadest sense the difference is that nonrobotic AGI may not be possible because as per this question it could be that Intelligence requires a body More specifically it could be that there are limitations to what the traditional well 1950s style Brain in a vat notion of an AI is capable of comprehending in the absence of experience of embodied experience such as force motion and the raw unawshed world
,,"<p>A simple way to do it would be lexicograpical sentiment analysis.  To do that, you'd need a list of words categorized with a score that reflects ""friendly"" vs ""aggressive"" sentiment.   For an example of setting up a SA system using Spark, see <a href=""http://mammothdata.com/sentiment-analysis-on-enrons-emails-with-apache-spark/"" rel=""nofollow"">this article</a>.  To do what you're talking about, substitute AFINN for a different dataset.  You might have to create said dataset yourself, if there isn't one ""out there"" like you want.</p>

<p>Note that this isn't the most sophisticated technique in the world, but it's been found to be surprisingly effective. </p>
",,0,2016-08-08T18:19:35.150,,1469,2016-08-08T18:19:35.150,,,,,33.0,1458.0,2,2,,,,64.41,10.43,8.86,0.0,0.0,22.0,A simple way to do it would be lexicograpical sentiment analysis To do that youd need a list of words categorized with a score that reflects friendly vs aggressive sentiment For an example of setting up a SA system using Spark see this article To do what youre talking about substitute AFINN for a different dataset You might have to create said dataset yourself if there isnt one out there like you want Note that this isnt the most sophisticated technique in the world but its been found to be surprisingly effective
,,"<p>One thing you'll see quite often, is to declare a correspondence between a system and a human of a given age.  For example ""this program can answer questions about science approximately as well as an average 7 year old"" or something of that nature.  </p>
",,0,2016-08-08T18:25:11.147,,1470,2016-08-08T18:25:11.147,,,,,33.0,1410.0,2,0,,,,49.15,9.63,10.47,0.0,0.0,6.0,One thing youll see quite often is to declare a correspondence between a system and a human of a given age For example this program can answer questions about science approximately as well as an average 7 year old or something of that nature
,,"<p>I am assuming by AI you mean AG(eneral)I, not machine learning or expert systems tuned for specific tasks.</p>

<p>In addition to mindcrime's answer, sometimes we run out of samples to train and sometimes computers became so slow to process enough samples to work in manageable timescales. bpachev mentioned memory but on the surface, our supercomputers have more than enough memory to store a human brain matrix. But we lack the ability to simulate it real time. After we are able to do that, we also need to connect external input, even more processing power is required for that. Even that would not be enough to simulate a human brain fully as biochemistry plays an important role. </p>

<p>One final note would be there is little incentive to develop AGI other than understanding how human mind works. There are classification algorithms, expert systems, knowledge engines that can out-perform even the best humans on specific tasks.</p>
",,4,2016-08-08T19:11:47.130,,1471,2016-08-08T19:11:47.130,,,,,210.0,60.0,2,2,,,,43.63,11.89,10.47,0.0,0.0,19.0,I am assuming by AI you mean AGeneralI not machine learning or expert systems tuned for specific tasks In addition to mindcrimes answer sometimes we run out of samples to train and sometimes computers became so slow to process enough samples to work in manageable timescales bpachev mentioned memory but on the surface our supercomputers have more than enough memory to store a human brain matrix But we lack the ability to simulate it real time After we are able to do that we also need to connect external input even more processing power is required for that Even that would not be enough to simulate a human brain fully as biochemistry plays an important role One final note would be there is little incentive to develop AGI other than understanding how human mind works There are classification algorithms expert systems knowledge engines that can outperform even the best humans on specific tasks
,0.0,"<p>With typical machine learning you would usually use a training data-set to create a model of some kind, and a testing data-set to then test the newly created model. For something like linear regression after the model is created with the training data you now have an equation that you would use to predict the outcome of the set of features in the testing data. You would then take the prediction that the model returned and compare that to the actual data in the testing set. How would a validation set be used here?</p>

<p>With nearest neighbor you would use the training data to create an n-dimensional space that has all the features of the training set. You would then use this space to classify the features in the testing data. Again you would compare these predictions to the actual value of the data. How would a validation set help here as well?</p>
",2016-08-09T15:46:42.237,2,2016-08-08T19:24:16.993,,1472,2016-08-08T19:24:16.993,,,,,1324.0,,1,1,<machine-learning><nearest-neighbor><linear-regression>,How exactly does a validation data-set work work in machine learning?,23.0,69.01,9.17,7.99,0.0,0.0,12.0,With typical machine learning you would usually use a training dataset to create a model of some kind and a testing dataset to then test the newly created model For something like linear regression after the model is created with the training data you now have an equation that you would use to predict the outcome of the set of features in the testing data You would then take the prediction that the model returned and compare that to the actual data in the testing set How would a validation set be used here With nearest neighbor you would use the training data to create an ndimensional space that has all the features of the training set You would then use this space to classify the features in the testing data Again you would compare these predictions to the actual value of the data How would a validation set help here as well
,,"<p>A robot is a physical device capable of independent movement. It has a software component to steer it, but does not necessarily have (artificial) intelligence.</p>

<p>The word ""Robot"" was coined by Czech writer Karel Čapek in his play R.U.R. (Rossum's Universal Robots). It comes from a Czech word meaning labour.</p>

<p>On Artificial Intelligence Stack Exchange, we are interested in how AI relates with these physical devices.<br>
If you are interested in the physical aspect of robots, like the electronical or mechanical aspects, you should visit <a href=""https://robotics.stackexchange.com"">Robotics Stack Exchange</a> instead.</p>
",,0,2016-08-08T19:37:59.710,,1473,2016-08-09T00:55:30.283,2016-08-09T00:55:30.283,,66.0,,66.0,,5,0,,,,50.12,12.92,10.83,0.0,0.0,20.0,A robot is a physical device capable of independent movement It has a software component to steer it but does not necessarily have artificial intelligence The word Robot was coined by Czech writer Karel Čapek in his play RUR Rossums Universal Robots It comes from a Czech word meaning labour On Artificial Intelligence Stack Exchange we are interested in how AI relates with these physical devices If you are interested in the physical aspect of robots like the electronical or mechanical aspects you should visit Robotics Stack Exchange instead
,,"A robot is a physical device capable of independent movement. Use this tag for questions involving the relationship of AI with robots. If you have questions about the physical aspect of robots, you should see if it fits our sister site Robotics, at https://robotics.stackexchange.com.",,0,2016-08-08T19:37:59.710,,1474,2016-08-23T00:18:05.657,2016-08-23T00:18:05.657,,145.0,,66.0,,4,0,,,,48.09,13.91,10.11,0.0,0.0,10.0,A robot is a physical device capable of independent movement Use this tag for questions involving the relationship of AI with robots If you have questions about the physical aspect of robots you should see if it fits our sister site Robotics at httpsroboticsstackexchangecom
1591.0,3.0,"<p>By reinforcement learning, I don't mean the class of machine learning algorithms such as DeepQ, etc. I have in mind the general concept of learning based on rewards and punishment. </p>

<p>Is it possible to create a Strong AI that does not rely on learning by reinforcement, or is reinforcement learning a requirement for artificial intelligence? The existence of rewards and punishment imply the existence of favorable and unfavorable world-states. Must intelligence in general and artificial intelligence in particular have a way of classifying world-states as favorable or unfavorable?  </p>
",,1,2016-08-08T23:46:12.853,,1476,2016-08-12T21:39:28.547,,,,,127.0,,1,3,<philosophy><reinforcement-learning>,Is reinforcement learning needed to create Strong AI?,64.0,36.69,14.9,9.35,0.0,0.0,11.0,By reinforcement learning I dont mean the class of machine learning algorithms such as DeepQ etc I have in mind the general concept of learning based on rewards and punishment Is it possible to create a Strong AI that does not rely on learning by reinforcement or is reinforcement learning a requirement for artificial intelligence The existence of rewards and punishment imply the existence of favorable and unfavorable worldstates Must intelligence in general and artificial intelligence in particular have a way of classifying worldstates as favorable or unfavorable
,0.0,"<p>I'm not talking about mass scale <a href=""https://en.wikipedia.org/wiki/Skynet_(Terminator)"" rel=""nofollow"">Skynet</a> or something, but for example <a href=""http://ethereum.stackexchange.com/"">Ethereum</a> (or <a href=""http://ethereum.stackexchange.com/q/4120/105"">similar</a>) which is a public blockchain-based distributed computing platform (like <a href=""http://ethereum.stackexchange.com/a/762/105"">internet</a>) featuring smart contracts which can be executed on a decentralized virtual machine. They call it a <em>World Computer</em>.</p>

<p>Where there any attempts to use similar blockchain-based technology driven by community (or not) in order to create an artificial intelligence into a decentralized public blockchain-based distributed computing platform where it <a href=""http://www.newsereum.com/newsereum/can-ethereum-shut/"" rel=""nofollow"">cannot be shutdown</a>?</p>
",2016-08-09T18:14:32.743,0,2016-08-09T01:42:41.093,1.0,1477,2016-08-17T15:49:18.400,2016-08-17T15:49:18.400,,10.0,,8.0,,1,0,<untagged>,Were there any attempts to create a global decentralized AI?,24.0,20.01,17.64,11.68,0.0,0.0,14.0,Im not talking about mass scale Skynet or something but for example Ethereum or similar which is a public blockchainbased distributed computing platform like internet featuring smart contracts which can be executed on a decentralized virtual machine They call it a World Computer Where there any attempts to use similar blockchainbased technology driven by community or not in order to create an artificial intelligence into a decentralized public blockchainbased distributed computing platform where it cannot be shutdown
1527.0,2.0,"<p>For example, search engine companies want to classify their image searches into 2 categories (which they already do that) such as: <a href=""https://en.wikipedia.org/wiki/Not_safe_for_work"" rel=""nofollow"">NSFW</a> (nudity, porn, brutality) and safe to view pictures.</p>

<p>How can artificial neural networks achieve that, and at what success rate? Can they be easily mistaken?</p>
",,2,2016-08-09T02:01:58.240,0.0,1478,2016-08-25T09:59:13.270,2016-08-17T19:58:38.663,,145.0,,8.0,,1,1,<image-recognition><classification><conv-neural-network>,How successfully can convnets detect NSFW images?,212.0,55.54,12.76,10.8,0.0,0.0,12.0,For example search engine companies want to classify their image searches into 2 categories which they already do that such as NSFW nudity porn brutality and safe to view pictures How can artificial neural networks achieve that and at what success rate Can they be easily mistaken
1486.0,4.0,"<p>Do scientists or research experts know from the kitchen what is happening inside complex ""deep"" neural network with at least millions of connections firing at an instant? Do they understand the process behind this (e.g. what is happening inside and how it works exactly), or it is a subject of debate?</p>

<p>For example this <a href=""https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf"">study</a> says:</p>

<blockquote>
  <p>However there is no clear understanding of <em>why</em> they perform so well, or <em>how</em> they might be improved.</p>
</blockquote>

<p>So does it mean the scientists actually doesn't know how complex convolutional network models work?</p>
",,4,2016-08-09T02:08:56.663,5.0,1479,2016-08-26T10:34:16.803,2016-08-25T19:51:40.800,,8.0,,8.0,,1,15,<neural-networks><deep-learning><conv-neural-network>,Do scientists know what is happening inside artificial neural networks?,651.0,70.33,11.25,9.49,0.0,0.0,14.0,Do scientists or research experts know from the kitchen what is happening inside complex deep neural network with at least millions of connections firing at an instant Do they understand the process behind this eg what is happening inside and how it works exactly or it is a subject of debate For example this study says However there is no clear understanding of why they perform so well or how they might be improved So does it mean the scientists actually doesnt know how complex convolutional network models work
1741.0,1.0,"<p>Is there any way to estimate how big the neural network would be after training session of 100,000 unlabeled images for unsupervised learning (like in <a href=""https://cs.stanford.edu/~acoates/stl10/"" rel=""nofollow"">STL-10 dataset</a>: 96x96 pixels and color)?</p>

<p>Not the storage space (because this could vary I guess based on the implementation), but specifically how many neurons it could have. It could be an estimate (e.g. in thousand, millions). If it depends, then on what? Are there any figures that can be estimated?</p>
",,0,2016-08-09T02:30:17.240,,1480,2017-02-13T20:48:43.850,2016-08-25T18:48:24.383,,145.0,,8.0,,1,-1,<deep-network><image-recognition><deep-learning><datasets>,How many neurons would a network have after a training of 100k small images?,165.0,67.04,10.66,9.88,0.0,0.0,19.0,Is there any way to estimate how big the neural network would be after training session of 100000 unlabeled images for unsupervised learning like in STL10 dataset 96x96 pixels and color Not the storage space because this could vary I guess based on the implementation but specifically how many neurons it could have It could be an estimate eg in thousand millions If it depends then on what Are there any figures that can be estimated
1699.0,4.0,"<p>For example I'd like to train my neural network to recognize the type of actions (e.g. in commercial movies or some real life videos), so I can ""ask"" my network in which video or movie (and at what frames) somebody was driving a car, kissing, eating, was scared or was talking over the phone.</p>

<p>What are the current successful approaches to that type of problem?</p>
",,5,2016-08-09T02:54:33.677,,1481,2016-08-24T08:48:23.077,2016-08-09T11:29:23.227,,145.0,,8.0,,1,4,<image-recognition><training><action-recognition>,How can action recognition be achieved?,111.0,66.37,9.11,9.81,0.0,0.0,15.0,For example Id like to train my neural network to recognize the type of actions eg in commercial movies or some real life videos so I can ask my network in which video or movie and at what frames somebody was driving a car kissing eating was scared or was talking over the phone What are the current successful approaches to that type of problem
,,"<p>An <a href=""http://www.oxforddictionaries.com/definition/english/artificial-intelligence#artificial-intelligence__2"" rel=""nofollow"" title=""OED - AI"">AI</a> is a computer program designed for tasks normally requiring human <a href=""http://www.oxforddictionaries.com/definition/english/intelligence"" rel=""nofollow"" title=""OED - Intelligence"">intelligence</a> (a human's ability to learn), while a <a href=""http://www.oxforddictionaries.com/definition/english/robot"" rel=""nofollow"" title=""OED - Robot"">robot</a> is a machine that completes complex tasks. An AI could be used to control a robot, but they are very different.</p>

<p>Source: <a href=""http://www.oxforddictionaries.com/"" rel=""nofollow"">Oxford English Dictionary</a>, above links will direct to definitions.</p>
",,1,2016-08-09T06:18:18.593,,1482,2016-08-10T11:11:25.337,2016-08-10T11:11:25.337,,72.0,,1420.0,1462.0,2,0,,,,45.46,11.89,12.09,0.0,0.0,10.0,An AI is a computer program designed for tasks normally requiring human intelligence a humans ability to learn while a robot is a machine that completes complex tasks An AI could be used to control a robot but they are very different Source Oxford English Dictionary above links will direct to definitions
,,"<p>Siri and co. are AI to some extent. The usual label is ""Weak AI"" (also called ""narrow"" or ""soft"" AI). It turns out the <a href=""https://en.wikipedia.org/wiki/Weak_AI"">Wikipedia article on Weak AI</a> explicitly refers to Siri:</p>

<blockquote>
  <p>Siri is a good example of narrow intelligence. Siri operates within a limited pre-defined range, there is no genuine intelligence, no self-awareness, no life despite being a sophisticated example of weak AI. In Forbes (2011), Ted Greenwald wrote: ""The iPhone/Siri marriage represents the arrival of hybrid AI, combining several narrow AI techniques plus access to massive data in the cloud."" AI researcher Ben Goertzel, on his blog in 2010, stated Siri was ""VERY narrow and brittle"" evidenced by annoying results if you ask questions outside the limits of the application.</p>
</blockquote>

<p>Important to note that ""mixing"" Weak AIs does not make a ""stronger"" AI, by some arguments (see Searle's <a href=""https://en.wikipedia.org/wiki/Chinese_room"">Chinese Room</a> argument), but there is no definitive answer yet in 2016.</p>
",,0,2016-08-09T06:38:26.720,,1483,2016-08-09T06:38:26.720,,,,,169.0,1461.0,2,10,,,,52.09,11.89,10.57,0.0,0.0,43.0,Siri and co are AI to some extent The usual label is Weak AI also called narrow or soft AI It turns out the Wikipedia article on Weak AI explicitly refers to Siri Siri is a good example of narrow intelligence Siri operates within a limited predefined range there is no genuine intelligence no selfawareness no life despite being a sophisticated example of weak AI In Forbes 2011 Ted Greenwald wrote The iPhoneSiri marriage represents the arrival of hybrid AI combining several narrow AI techniques plus access to massive data in the cloud AI researcher Ben Goertzel on his blog in 2010 stated Siri was VERY narrow and brittle evidenced by annoying results if you ask questions outside the limits of the application Important to note that mixing Weak AIs does not make a stronger AI by some arguments see Searles Chinese Room argument but there is no definitive answer yet in 2016
,0.0,"<p>I'm playing with an LSTM to generate text. In particular, this one:</p>

<p><a href=""https://raw.githubusercontent.com/fchollet/keras/master/examples/lstm_text_generation.py"" rel=""nofollow"">https://raw.githubusercontent.com/fchollet/keras/master/examples/lstm_text_generation.py</a></p>

<p>It works on quite a big demo text set from Nietzsche and says</p>

<blockquote>
  <p>If you try this script on new data, make sure your corpus
  has at least ~100k characters. ~1M is better.</p>
</blockquote>

<p>This pops up a couple of questions.</p>

<p>A.) If all I want is an AI with a very limited vocabulary where the generate text should be short sentences following a basic pattern.</p>

<p>E.g.</p>

<p><em>I like blue sky with white clouds</em></p>

<p><em>I like yellow fields with some trees</em></p>

<p><em>I like big cities with lots of bars</em></p>

<p>...</p>

<p>Would it then be reasonable to use a much much smaller dataset?</p>

<p>B.) If the dataset really needs to be that big. What if I just repeat the text over and over to reach the recommended minimum? If that would work though, I'd be wondering how that is any different from just taking more iterations of learning with the same shorter text?</p>

<p>Obviously I can play with these two questions myself and in fact I am experimenting with it. One thing I already figured out is that with a shorter text following a basic pattern I can get to a very very low ( ~0.04) quite fast but the predicted text just turns out as gibberish.</p>

<p>My naive explanation for that would be that there are just not enough samples to proof against whether the gibberish actually makes sense or not? But then again I wonder if more iterations or duplicating the content would actually help.</p>

<p>I'm trying to experiment with these questions myself so please don't think I'm just too lazy and are aiming for others to do the work. I'm just looking for more experienced people to give me a better understanding of the mechanics that influence these things.</p>
",,4,2016-08-09T06:55:39.460,1.0,1484,2016-08-09T06:55:39.460,,,,,1334.0,,1,2,<datasets><lstm>,In LSTM text generation can low amount of training data be compensated?,43.0,71.44,9.74,7.98,0.0,0.0,53.0,Im playing with an LSTM to generate text In particular this one httpsrawgithubusercontentcomfcholletkerasmasterexampleslstmtextgenerationpy It works on quite a big demo text set from Nietzsche and says If you try this script on new data make sure your corpus has at least 100k characters 1M is better This pops up a couple of questions A If all I want is an AI with a very limited vocabulary where the generate text should be short sentences following a basic pattern Eg I like blue sky with white clouds I like yellow fields with some trees I like big cities with lots of bars Would it then be reasonable to use a much much smaller dataset B If the dataset really needs to be that big What if I just repeat the text over and over to reach the recommended minimum If that would work though Id be wondering how that is any different from just taking more iterations of learning with the same shorter text Obviously I can play with these two questions myself and in fact I am experimenting with it One thing I already figured out is that with a shorter text following a basic pattern I can get to a very very low 004 quite fast but the predicted text just turns out as gibberish My naive explanation for that would be that there are just not enough samples to proof against whether the gibberish actually makes sense or not But then again I wonder if more iterations or duplicating the content would actually help Im trying to experiment with these questions myself so please dont think Im just too lazy and are aiming for others to do the work Im just looking for more experienced people to give me a better understanding of the mechanics that influence these things
1523.0,1.0,"<p>For example I would like to implement transparent AI in the RTS game which doesn't offer any AI API (like old games), and I'd like to use image recognition algorithm for detecting the objects which can talks to another algorithm which is responsible for the logic.</p>

<p>Given I'd like to use two neural networks, what are the approaches to setup the communication between them? Is it just by exporting result findings of the first algorithm (e.g. using CNN) with list of features which were found on the screen, then use it as input for another network? Or it's more complex than that, or I need to have more than two networks?</p>
",,0,2016-08-09T09:01:52.373,,1485,2016-08-10T11:24:13.367,2016-08-09T18:57:43.590,,8.0,,8.0,,1,0,<neural-networks><conv-neural-network><gaming>,How to separate image recognition from logic?,41.0,65.86,9.46,9.15,0.0,0.0,18.0,For example I would like to implement transparent AI in the RTS game which doesnt offer any AI API like old games and Id like to use image recognition algorithm for detecting the objects which can talks to another algorithm which is responsible for the logic Given Id like to use two neural networks what are the approaches to setup the communication between them Is it just by exporting result findings of the first algorithm eg using CNN with list of features which were found on the screen then use it as input for another network Or its more complex than that or I need to have more than two networks
,,"<p>It depends on what you mean by ""know what is happening"".</p>

<p>Conceptually, yes: ANN perform nonlinear regression. The actual expression represented by the weight matrix/activation function(s) of an ANN can be explicitly expanded in symbolic form (e.g. containing sub-expressions such as 1/1+e^{1/1+e^{...}}).</p>

<p>However, if by 'know' you mean <em>predicting the output of some specific (black box) ANN</em>, by some other means, then the obstacle is the presence of chaos in a ANN that has <a href=""http://sprott.physics.wisc.edu/pubs/paper234.pdf"">high degrees of freedom</a>.</p>

<p>EDIT: Here's some relatively recent work by Hod Lipson on understanding ANNs through  <a href=""http://arxiv.org/pdf/1506.06579.pdf"">visualisation</a>.</p>
",,0,2016-08-09T09:09:32.990,,1486,2016-08-25T18:05:50.620,2016-08-25T18:05:50.620,,42.0,,42.0,1479.0,2,12,,,,55.95,14.09,9.89,0.0,0.0,39.0,It depends on what you mean by know what is happening Conceptually yes ANN perform nonlinear regression The actual expression represented by the weight matrixactivation functions of an ANN can be explicitly expanded in symbolic form eg containing subexpressions such as 11e11e However if by know you mean predicting the output of some specific black box ANN by some other means then the obstacle is the presence of chaos in a ANN that has high degrees of freedom EDIT Heres some relatively recent work by Hod Lipson on understanding ANNs through visualisation
1503.0,1.0,"<p>Were there any successful attempts to replace poor guide dogs used for blind people with AI to achieve similar rate of success? I guess dogs could be easily distracted and not reliable for every situation, and it probably takes less time to train AI, than a dog.</p>
",,2,2016-08-09T09:09:34.990,,1487,2016-08-17T12:03:08.677,2016-08-17T12:03:08.677,,29.0,,8.0,,1,1,<neural-networks><applications>,How close we are to replacing guide dogs with AI?,49.0,56.08,9.7,10.18,0.0,0.0,4.0,Were there any successful attempts to replace poor guide dogs used for blind people with AI to achieve similar rate of success I guess dogs could be easily distracted and not reliable for every situation and it probably takes less time to train AI than a dog
1522.0,3.0,"<p>Do we know why Tesla's Autopilot mistaken empty sky with a high-sided lorry which resulted in fatal crash involving a car in self-drive mode? Was it AI fault or something else? Is there any technical explanation behind this why this happened?</p>

<p>References: <a href=""http://news.sky.com/story/tesla-driver-in-first-self-drive-fatal-crash-10330121"" rel=""nofollow"">Sky News article</a>, <a href=""http://www.theverge.com/2016/6/30/12072408/tesla-autopilot-car-crash-death-autonomous-model-s"" rel=""nofollow"">The Verge</a>.</p>
",,3,2016-08-09T10:10:25.253,,1488,2016-08-14T23:54:03.540,2016-08-09T22:17:20.030,,8.0,,8.0,,1,5,<self-driving><cars>,Why did a Tesla car mistake a truck with a bright sky?,213.0,59.5,10.77,10.27,0.0,0.0,9.0,Do we know why Teslas Autopilot mistaken empty sky with a highsided lorry which resulted in fatal crash involving a car in selfdrive mode Was it AI fault or something else Is there any technical explanation behind this why this happened References Sky News article The Verge
,,"<p>Not sure if this is what you are searching for, but google extracted images from networks when they were fed with white noise. See <a href=""https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html"" rel=""nofollow"">here</a>.
This kind of represents what the network knows.</p>
",,0,2016-08-09T10:28:31.650,,1489,2016-08-09T15:18:03.523,2016-08-09T15:18:03.523,,10.0,,1425.0,1479.0,2,5,,,,80.11,9.16,7.8,0.0,0.0,4.0,Not sure if this is what you are searching for but google extracted images from networks when they were fed with white noise See here This kind of represents what the network knows
1499.0,1.0,"<p>For benefits of testing AGI, is using a high-level video game description language (VGDL) gives more reliable and accurate results of general intelligence than using Arcade Learning Environment (ALE)?</p>
",,0,2016-08-09T10:28:41.783,,1490,2016-08-09T20:35:34.217,,,,,8.0,,1,0,<agi><gaming>,What are the benefits of the VGDL over the ALE?,23.0,8.2,17.71,14.88,0.0,0.0,7.0,For benefits of testing AGI is using a highlevel video game description language VGDL gives more reliable and accurate results of general intelligence than using Arcade Learning Environment ALE
,1.0,"<p>Some time ago playing chess was challenging for algorithms, then Go game which is vastly more complex than compared to chess.</p>

<p>How about playing RTS game which have enormous branching factors limited by its time and space (like deciding what to do next)? What are the successful approaches to such problems?</p>
",,0,2016-08-09T10:40:35.500,,1491,2016-10-19T07:44:43.773,,,,,8.0,,1,1,<gaming><branching-factors><real-time>,How to deal with huge branching factors in real-time?,38.0,62.68,11.54,9.12,0.0,0.0,6.0,Some time ago playing chess was challenging for algorithms then Go game which is vastly more complex than compared to chess How about playing RTS game which have enormous branching factors limited by its time and space like deciding what to do next What are the successful approaches to such problems
1495.0,1.0,"<p>We can read on wiki page that in March 2016 AlphaGo AI lost its game (1 of 5) to Lee Sedol, a professional Go player. One <a href=""http://www.bbc.co.uk/news/technology-36558829"" rel=""nofollow"">article</a> cite says:</p>

<blockquote>
  <p>AlphaGo lost a game and we as researchers want to explore that and find out what went wrong. We need to figure out what its weaknesses are and try to improve it.</p>
</blockquote>

<p>Have researchers already figured it out what went wrong?</p>
",,0,2016-08-09T12:00:06.583,1.0,1492,2016-08-09T15:45:35.477,2016-08-09T15:45:35.477,,130.0,,8.0,,1,0,<gaming><deepmind>,Why did AlphaGo lose its Go game?,79.0,79.09,6.44,7.21,0.0,0.0,8.0,We can read on wiki page that in March 2016 AlphaGo AI lost its game 1 of 5 to Lee Sedol a professional Go player One article cite says AlphaGo lost a game and we as researchers want to explore that and find out what went wrong We need to figure out what its weaknesses are and try to improve it Have researchers already figured it out what went wrong
1496.0,1.0,"<p>Assuming we're dealing with artificial neural network (e.g. using <a href=""https://en.wikipedia.org/wiki/Convolutional_neural_network"" rel=""nofollow"">convnets</a>) which was trained by large dataset of human faces.</p>

<p>Are there any known issues or challenges where facial recognition would fail? I'm not talking about covering half of the face, but some simple common things such as wearing the glasses, hat, jewellery, having face painting or tattoo, can this successfully prevent AI from recognizing the face? If so, what are current methods dealing with such challenges?</p>
",,1,2016-08-09T12:19:35.883,1.0,1493,2016-08-09T18:53:10.497,,,,,8.0,,1,2,<conv-neural-network>,Is it possible to cheat facial recognition algorithm?,33.0,56.05,13.22,11.25,0.0,0.0,16.0,Assuming were dealing with artificial neural network eg using convnets which was trained by large dataset of human faces Are there any known issues or challenges where facial recognition would fail Im not talking about covering half of the face but some simple common things such as wearing the glasses hat jewellery having face painting or tattoo can this successfully prevent AI from recognizing the face If so what are current methods dealing with such challenges
1497.0,2.0,"<p>I would like to know what kind of dataset I need (to prepare) for training the network to recognize the spelling mistakes in individual words for English text.</p>

<p>Given the large database of words, having correct one for each incorrect. What kind of input is more efficient for that tasks? Is it using one input per each letter, syllable, whole word or I should use different pattern syllable?</p>

<p>Then the input should be incorrect word, output correct, and if the word doesn't need correction, then both input and output should be the same. Is that the right approach?</p>
",,3,2016-08-09T14:17:50.773,,1494,2016-08-09T20:13:44.400,,,,,8.0,,1,0,<deep-learning><datasets><language-processing>,Training network to detect spelling mistakes,52.0,71.85,10.03,8.96,0.0,0.0,15.0,I would like to know what kind of dataset I need to prepare for training the network to recognize the spelling mistakes in individual words for English text Given the large database of words having correct one for each incorrect What kind of input is more efficient for that tasks Is it using one input per each letter syllable whole word or I should use different pattern syllable Then the input should be incorrect word output correct and if the word doesnt need correction then both input and output should be the same Is that the right approach
,,"<p>We know what Lee's strategy was during the game, and it seems like the sort of thing that should work. <a href=""https://gogameguru.com/lee-sedol-defeats-alphago-masterful-comeback-game-4/"" rel=""nofollow"">Here's</a> an article explaining it. Short version: yes, we know what went wrong, but probably not how to fix it yet.</p>

<p>Basically, AlphaGo is good at making lots of small decisions well, and managing risk and uncertainty better than humans can. One of the things that's surprising about it relative to previous bots that play Go is how good it was at tactical fights; in previous games, Lee had built a position that AlphaGo needed to attack, and then AlphaGo successfully attacked it.</p>

<p>So in this game, Lee played the reverse strategy. Instead of trying to win many different influence battles, where AlphaGo had already shown it was stronger than him, he would set up one critical battle (incurring minor losses along the way), and then defeat it there, with ripple events that would settle the match in his favor.</p>

<p>So what's the weakness of AlphaGo that allowed that to work? As I understand it, this is a fundamental limitation of Monte Carlo Tree Search (MCTS). MCTS works by randomly sampling game trees and averaging them; if 70% of games from a particular position go well and 30% of games from another position go well, then you should probably play the first move instead of the second move.</p>

<p>But when there's a specific sequence of plays that go well--if, say, W has a path that requires them playing exactly the right stone each time, but B has no possible response to this path--then MCTS breaks down, because you can only find that narrow path through minimax reasoning, and moving from the slower minimax reasoning to the faster MCTS is one of the big reasons why bots are better now than they were in the past.</p>

<p>It's unclear how to get around this. There may be a way to notice this sort of threat, and then temporarily switch from MCTS reasoning to minimax reasoning, or to keep around particular trajectories in memory for consideration in future plays. </p>
",,1,2016-08-09T15:32:41.270,,1495,2016-08-09T15:32:41.270,,,,,10.0,1492.0,2,3,,,,61.4,10.16,8.93,0.0,0.0,53.0,We know what Lees strategy was during the game and it seems like the sort of thing that should work Heres an article explaining it Short version yes we know what went wrong but probably not how to fix it yet Basically AlphaGo is good at making lots of small decisions well and managing risk and uncertainty better than humans can One of the things thats surprising about it relative to previous bots that play Go is how good it was at tactical fights in previous games Lee had built a position that AlphaGo needed to attack and then AlphaGo successfully attacked it So in this game Lee played the reverse strategy Instead of trying to win many different influence battles where AlphaGo had already shown it was stronger than him he would set up one critical battle incurring minor losses along the way and then defeat it there with ripple events that would settle the match in his favor So whats the weakness of AlphaGo that allowed that to work As I understand it this is a fundamental limitation of Monte Carlo Tree Search MCTS MCTS works by randomly sampling game trees and averaging them if 70 of games from a particular position go well and 30 of games from another position go well then you should probably play the first move instead of the second move But when theres a specific sequence of plays that go wellif say W has a path that requires them playing exactly the right stone each time but B has no possible response to this paththen MCTS breaks down because you can only find that narrow path through minimax reasoning and moving from the slower minimax reasoning to the faster MCTS is one of the big reasons why bots are better now than they were in the past Its unclear how to get around this There may be a way to notice this sort of threat and then temporarily switch from MCTS reasoning to minimax reasoning or to keep around particular trajectories in memory for consideration in future plays
,,"<p>Facial recognition works by essentially turning your face into a point cloud, recognizing eyes, cheeks, nose, mouth, etc. </p>

<p>Unfortunately it doesn't look at the top of your head (hair is very hard to differentiate from other hair and doesn't have many features). Face paintings   would be your best bet since they can be easily changed, tattoos not so much. Once somebody has a photo of your face with your tattoo on it, you're busted.  Glasses will work if they're opaque and hide your eyes (sunglasses). The facial recognition software does not recognize jewelry, as it's tiny, and very easy to remove and put on. Ideally you want to have your face professionally made up with makeup and fake skin (basically a fake face).</p>

<p>Some challenges can be anything like an unclear picture, a picture from the wrong angle (which can probably be mathematically calculated and restructured), or like you said, face paintings to hide facial features.</p>
",,1,2016-08-09T18:53:10.497,,1496,2016-08-09T18:53:10.497,,,,,1433.0,1493.0,2,2,,,,51.68,11.66,9.16,0.0,0.0,33.0,Facial recognition works by essentially turning your face into a point cloud recognizing eyes cheeks nose mouth etc Unfortunately it doesnt look at the top of your head hair is very hard to differentiate from other hair and doesnt have many features Face paintings would be your best bet since they can be easily changed tattoos not so much Once somebody has a photo of your face with your tattoo on it youre busted Glasses will work if theyre opaque and hide your eyes sunglasses The facial recognition software does not recognize jewelry as its tiny and very easy to remove and put on Ideally you want to have your face professionally made up with makeup and fake skin basically a fake face Some challenges can be anything like an unclear picture a picture from the wrong angle which can probably be mathematically calculated and restructured or like you said face paintings to hide facial features
,,"<p>I'd personally be more inclined to try longstanding deterministic methods such as <a href=""https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance"" rel=""nofollow"">Damerau</a> (for typing errors) or <a href=""https://en.wikipedia.org/wiki/Soundex"" rel=""nofollow"">Soundex</a> (for homonyms arising from transcribed speech). At the very least, I'd use those as a baseline for any more 'AI-based' approach.</p>
",,0,2016-08-09T19:34:23.367,,1497,2016-08-09T19:34:23.367,,,,,42.0,1494.0,2,2,,,,43.22,14.1,11.49,0.0,0.0,12.0,Id personally be more inclined to try longstanding deterministic methods such as Damerau for typing errors or Soundex for homonyms arising from transcribed speech At the very least Id use those as a baseline for any more AIbased approach
,,"<p>As far as I know, Tesla cars autopilot is not a 100% AI pilot, it's an <em>assitant</em>: as it detects hands off wheel it slows down, so it's incorrect to speak about AI mistake: it is not trained/designed to drive a car all by itself. A human driver is responsible in that incident.</p>
",,0,2016-08-09T19:59:30.093,,1498,2016-08-09T19:59:30.093,,,,,1263.0,1488.0,2,3,,,,69.96,7.43,9.12,0.0,0.0,11.0,As far as I know Tesla cars autopilot is not a 100 AI pilot its an assitant as it detects hands off wheel it slows down so its incorrect to speak about AI mistake it is not traineddesigned to drive a car all by itself A human driver is responsible in that incident
,,"<p><a href=""http://www.arcadelearningenvironment.org/wp-content/uploads/2012/07/bellemare13arcade.pdf"" rel=""nofollow"">Here</a> is a description of the input to an ALE agent:
Percept state: A single game screen (frame): a 2D array of 7-bit pixels, 160 pixels wide by 210 pixels high. 
Actions: 18 discrete actions defined by the joystick controller</p>

<p>Regarding VGDL, as far as I can see, the main site associated with it is gvgai.net, which is currently down. The associated API is described <a href=""http://julian.togelius.com/Perez20152014.pdf"" rel=""nofollow"">in this paper</a>.</p>

<p>Percept state for GVGAI is more structured than for ALE, but the closest correspondence to ALE appears to be an 'Observation grid', consisting of a 2D array of sprite identifiers.</p>

<p>Actions: ACTION_NIL, ACTION_UP, ACTION_LEFT, ACTION_DOWN, ACTION_RIGHT and ACTION_USE (stated as 'typical' values). </p>

<p>Of the two, it would seem that ALE is more suitable for AGI, because of the more 'free form' nature of the input.</p>

<p>However, one of the issues with <em>either</em> of these approaches is that the set of possible actions is strongly constrained. These domains are therefore 'operationalised' - the hard task of working out what actions are possible has already been solved for the AI by the API, effectively acting as a bottleneck on the complexity of mapping from input to output.</p>

<p>A range of alternative game-playing frameworks are listed <a href=""http://cig16.image.ece.ntua.gr/competitions/"" rel=""nofollow"">here</a> and one alternative (which I personally believe is more useful for AGI purposes) is the <a href=""http://atkrye.github.io/IEEE-CIG-Text-Adventurer-Competition/"" rel=""nofollow"">Artificial Text Adventurer</a>, in which (at each turn) agent is presented with natural language input describing the scene and must then output a command in natural language. Disclaimer: I am associated with this competition.</p>
",,0,2016-08-09T20:12:16.500,,1499,2016-08-09T20:35:34.217,2016-08-09T20:35:34.217,,42.0,,42.0,1490.0,2,2,,,,48.43,12.48,10.13,0.0,0.0,57.0,Here is a description of the input to an ALE agent Percept state A single game screen frame a 2D array of 7bit pixels 160 pixels wide by 210 pixels high Actions 18 discrete actions defined by the joystick controller Regarding VGDL as far as I can see the main site associated with it is gvgainet which is currently down The associated API is described in this paper Percept state for GVGAI is more structured than for ALE but the closest correspondence to ALE appears to be an Observation grid consisting of a 2D array of sprite identifiers Actions ACTIONNIL ACTIONUP ACTIONLEFT ACTIONDOWN ACTIONRIGHT and ACTIONUSE stated as typical values Of the two it would seem that ALE is more suitable for AGI because of the more free form nature of the input However one of the issues with either of these approaches is that the set of possible actions is strongly constrained These domains are therefore operationalised the hard task of working out what actions are possible has already been solved for the AI by the API effectively acting as a bottleneck on the complexity of mapping from input to output A range of alternative gameplaying frameworks are listed here and one alternative which I personally believe is more useful for AGI purposes is the Artificial Text Adventurer in which at each turn agent is presented with natural language input describing the scene and must then output a command in natural language Disclaimer I am associated with this competition
,,"<p>I would also look at Minimum Edit Distances such as the Levenshtein distance.
You could use a dynamic programming technique such as the Viterbi Algorithm.</p>

<p>If you don't have a dictionary to work against, you may want to train with a Markov Chain model using a known ""good"" text. The Viterbi Algorithm could be used again to solve the model for the text being considered.</p>
",,0,2016-08-09T20:13:44.400,,1500,2016-08-09T20:13:44.400,,,,,132.0,1494.0,2,2,,,,63.49,9.63,9.3,0.0,0.0,8.0,I would also look at Minimum Edit Distances such as the Levenshtein distance You could use a dynamic programming technique such as the Viterbi Algorithm If you dont have a dictionary to work against you may want to train with a Markov Chain model using a known good text The Viterbi Algorithm could be used again to solve the model for the text being considered
,1.0,"<p>As I have been looking at other questions on this site (like <a href=""http://ai.stackexchange.com/questions/60/what-are-the-main-problems-hindering-current-ai-development"">this</a>, <a href=""http://ai.stackexchange.com/questions/1376/is-it-ethical-to-implement-self-defence-for-street-walking-ai-robots"">this</a>, <a href=""http://ai.stackexchange.com/questions/111/how-would-self-driving-cars-make-ethical-decisions-about-who-to-kill"">this</a>, and <a href=""http://ai.stackexchange.com/questions/1289/can-we-destroy-artificial-general-intelligence-without-its-consent"">this</a>), I have been thinking more about the ethical implications of creating these generalized AI systems. It seems that whether or not we <em>can</em> create it is not rationale enough as to whether or not we <em>should</em> do it.</p>

<p>In dealing with the issue of ethics in AI, I wonder what the ethical implications are not just for us, but for the system itself. It seems to extend beyond the usually asked questions on the topic and into unknown territory. Are ethics computable? Can they be implemented programmatically? Can we force an AI system to do something against its <em>""will""</em>?</p>

<p>What does the creation of AI imply ethically for us as well as the AI?</p>
",2016-08-09T23:27:30.140,7,2016-08-09T20:54:09.720,1.0,1501,2016-08-09T21:43:25.247,,,,,77.0,,1,2,<ethics>,What are the ethical implications of creating (possibly sentient) AI systems?,57.0,71.75,8.52,8.43,0.0,0.0,18.0,As I have been looking at other questions on this site like this this this and this I have been thinking more about the ethical implications of creating these generalized AI systems It seems that whether or not we can create it is not rationale enough as to whether or not we should do it In dealing with the issue of ethics in AI I wonder what the ethical implications are not just for us but for the system itself It seems to extend beyond the usually asked questions on the topic and into unknown territory Are ethics computable Can they be implemented programmatically Can we force an AI system to do something against its will What does the creation of AI imply ethically for us as well as the AI
,,"<p>I believe if an AI achieves sentience, it should be treated the same way we are required to treat any other sentient animal. This is belief though, there is no established ethics for AI. But there were no ethics for animals a couple of centuries ago.</p>
",,1,2016-08-09T21:43:25.247,,1502,2016-08-09T21:43:25.247,,,,,210.0,1501.0,2,0,,,,64.41,8.0,9.89,0.0,0.0,5.0,I believe if an AI achieves sentience it should be treated the same way we are required to treat any other sentient animal This is belief though there is no established ethics for AI But there were no ethics for animals a couple of centuries ago
,,"<p>Chieko Asakawa (<a href=""https://en.wikipedia.org/wiki/Chieko_Asakawa"" rel=""nofollow"">wiki</a>, <a href=""https://www.ted.com/talks/chieko_asakawa_how_new_technology_helps_blind_people_explore_the_world?language=en"" rel=""nofollow"">TED</a>, <a href=""http://researcher.watson.ibm.com/researcher/view.php?person=jp-CHIE"" rel=""nofollow"">IBM</a>) is a major researcher in this area, and the linked TED talk is probably a good introduction to the state of the art as of 2015. <a href=""http://dl.acm.org/citation.cfm?id=2899509"" rel=""nofollow"">Here's</a> a link to a 2016 paper on a smartphone navigation system.</p>

<p>Guide animals perform manipulation tasks as well as identification tasks, and so it's not clear if those could be replaced well at all. (A smartphone that reads a label is a great help, but a dog that knows which bottle to grab and deliver to you is probably a much better help.)</p>
",,0,2016-08-09T21:46:31.383,,1503,2016-08-09T21:46:31.383,,,,,10.0,1487.0,2,4,,,,72.7,8.07,8.64,0.0,0.0,15.0,Chieko Asakawa wiki TED IBM is a major researcher in this area and the linked TED talk is probably a good introduction to the state of the art as of 2015 Heres a link to a 2016 paper on a smartphone navigation system Guide animals perform manipulation tasks as well as identification tasks and so its not clear if those could be replaced well at all A smartphone that reads a label is a great help but a dog that knows which bottle to grab and deliver to you is probably a much better help
,,"<p>See: <a href=""https://en.wikipedia.org/wiki/General_game_playing"" rel=""nofollow"">General game playing</a> at Wikipedia</p>
",,0,2016-08-09T22:59:37.090,,1505,2016-08-10T05:29:24.097,2016-08-10T05:29:24.097,,8.0,,8.0,,5,0,,,,48.47,11.07,9.2,0.0,0.0,1.0,See General game playing at Wikipedia
,,General game playing (GGP) is the approach of AI to be able to play more than one game successfully.,,0,2016-08-09T22:59:37.090,,1506,2016-08-10T05:29:08.597,2016-08-10T05:29:08.597,,8.0,,8.0,,4,0,,,,69.11,7.78,7.9,0.0,0.0,3.0,General game playing GGP is the approach of AI to be able to play more than one game successfully
1516.0,3.0,"<p>I believe <em>artificial intelligence</em> (AI) term is overused nowadays.</p>

<p>For example people see that something is self-moving and they call it AI, even if it's on autopilot (like cars or planes) or there is some simple algorithm behind it.</p>

<p>What are the minimum general requirements so that we can say something is AI?</p>
",,0,2016-08-10T00:55:54.690,2.0,1507,2017-01-09T06:15:28.727,2017-01-05T12:42:28.010,user4639,,,8.0,,1,9,<definitions>,What are the minimum requirements to call something AI?,468.0,53.51,10.67,9.88,0.0,0.0,10.0,I believe artificial intelligence AI term is overused nowadays For example people see that something is selfmoving and they call it AI even if its on autopilot like cars or planes or there is some simple algorithm behind it What are the minimum general requirements so that we can say something is AI
1514.0,1.0,"<p>I believe normally you can use <a href=""https://en.wikipedia.org/wiki/Genetic_programming"" rel=""nofollow"">genetic programming</a> for sorting, however I'd like to check whether it's possible using ANN.</p>

<p>Given the unsorted text data from input, which neural network is suitable for doing sorting tasks?</p>
",,1,2016-08-10T01:06:07.823,1.0,1508,2016-08-10T01:59:52.747,,,,,8.0,,1,0,<neural-networks>,Which neural network has capabilities of sorting input?,109.0,53.21,12.41,11.99,0.0,0.0,6.0,I believe normally you can use genetic programming for sorting however Id like to check whether its possible using ANN Given the unsorted text data from input which neural network is suitable for doing sorting tasks
1520.0,1.0,"<p>I've read on wiki that <a href=""https://en.wikipedia.org/wiki/Genetic_programming"" rel=""nofollow"">genetic programming</a> has '<em>outstanding results</em>' in cyberterrorism prevention.</p>

<p>Further more, this <a href=""http://papers.ssrn.com/sol3/papers.cfm?abstract_id=877981"" rel=""nofollow"">abstract</a> says:</p>

<blockquote>
  <p>Using machine-coded linear genomes and a homologous crossover operator in genetic programming, promising results were achieved in detecting malicious intrusions.</p>
</blockquote>

<p>I've checked the study, but it's still not clear for me.</p>

<p>How exactly was this detection achieved from the technical perspective?</p>
",,0,2016-08-10T01:14:19.777,,1509,2016-08-10T09:04:33.507,2016-08-10T09:04:33.507,,145.0,,8.0,,1,1,<genetic-programming><cyberterrorism><security>,How can genetic programming be used to prevent cyberterrorism?,31.0,31.07,18.03,12.13,0.0,0.0,14.0,Ive read on wiki that genetic programming has outstanding results in cyberterrorism prevention Further more this abstract says Using machinecoded linear genomes and a homologous crossover operator in genetic programming promising results were achieved in detecting malicious intrusions Ive checked the study but its still not clear for me How exactly was this detection achieved from the technical perspective
,,,,0,2016-08-10T01:17:49.203,,1510,2016-08-10T01:17:49.203,2016-08-10T01:17:49.203,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,A technique where programs are encoded as a set of genes which evolve using evolutionary algorithm to solve a problem.,,0,2016-08-10T01:17:49.203,,1511,2016-08-10T15:28:01.700,2016-08-10T15:28:01.700,,8.0,,8.0,,4,0,,,,34.26,11.43,11.73,0.0,0.0,1.0,A technique where programs are encoded as a set of genes which evolve using evolutionary algorithm to solve a problem
,,"<p>Basically a <a href=""https://en.wikipedia.org/wiki/Robot"" rel=""nofollow"">robot</a> is a mechanical or virtual artificial agent which exhibit intelligent behavior (<a href=""https://en.wikipedia.org/wiki/Artificial_intelligence"" rel=""nofollow"">AI</a>).</p>

<hr>

<p><a href=""http://waitbutwhy.com/wait-but-who"" rel=""nofollow"">Tim Urban</a> on <em><a href=""https://en.wikipedia.org/wiki/Wait_But_Why"" rel=""nofollow"">Wait But Why</a></em> website wrote the following to clear things up:</p>

<blockquote>
  <p>First, stop thinking of robots.</p>
  
  <h1>A robot is a container for AI,</h1>
  
  <p>sometimes mimicking the human form, sometimes not</p>
  
  <p>— but the AI itself <strong>is the computer inside the robot</strong>.</p>
  
  <h3>AI is the brain and the robot is its body — if it even has a body.</h3>
  
  <p>For example,</p>
  
  <p>the software and data behind <strong>Siri is AI</strong>, the woman’s voice we hear is a personification of that AI, and there’s no robot involved at all.</p>
</blockquote>

<p>Source: <a href=""http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html"" rel=""nofollow"">The AI Revolution: The Road to Superintelligence</a></p>
",,0,2016-08-10T01:38:29.980,,1512,2016-08-10T01:43:58.257,2016-08-10T01:43:58.257,,8.0,,8.0,1462.0,2,1,,,,60.95,8.99,9.64,0.0,0.0,16.0,Basically a robot is a mechanical or virtual artificial agent which exhibit intelligent behavior AI Tim Urban on Wait But Why website wrote the following to clear things up First stop thinking of robots A robot is a container for AI sometimes mimicking the human form sometimes not — but the AI itself is the computer inside the robot AI is the brain and the robot is its body — if it even has a body For example the software and data behind Siri is AI the woman’s voice we hear is a personification of that AI and there’s no robot involved at all Source The AI Revolution The Road to Superintelligence
,,"<p>They are <a href=""https://en.wikipedia.org/wiki/Intelligent_agent"" rel=""nofollow"">virtual artificial agents</a> which exhibit intelligent behavior (<a href=""https://en.wikipedia.org/wiki/Artificial_intelligence"" rel=""nofollow"">AI</a>).</p>

<p><a href=""http://waitbutwhy.com/wait-but-who"" rel=""nofollow"">Tim Urban</a> on <em><a href=""https://en.wikipedia.org/wiki/Wait_But_Why"" rel=""nofollow"">Wait But Why</a></em> website wrote the following:</p>

<blockquote>
  <p>The software and data behind <strong>Siri is AI</strong>, the woman’s voice we hear is a personification of that AI, and there’s no robot involved at all.</p>
</blockquote>

<p><sup>Source: <a href=""http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html"" rel=""nofollow"">The AI Revolution: The Road to Superintelligence</a></sup></p>

<p>Related: <a href=""http://ai.stackexchange.com/q/1462/8"">What is the difference between AI and robots?</a></p>
",,0,2016-08-10T01:51:10.200,,1513,2016-08-10T01:56:15.870,2016-08-10T01:56:15.870,,8.0,,8.0,1461.0,2,2,,,,50.16,12.01,10.69,0.0,0.0,11.0,They are virtual artificial agents which exhibit intelligent behavior AI Tim Urban on Wait But Why website wrote the following The software and data behind Siri is AI the woman’s voice we hear is a personification of that AI and there’s no robot involved at all Source The AI Revolution The Road to Superintelligence Related What is the difference between AI and robots
,,"<p>Even a simple multilayer perceptron can sort input data to some extent, as you can see <a href=""https://github.com/primaryobjects/nnsorting"" rel=""nofollow"">here</a> and <a href=""http://yyue.blogspot.com.br/2015/01/a-brief-overview-of-deep-learning.html"" rel=""nofollow"">here</a>.</p>

<p>However, neural networks for sequential data seem more appropriate, as they can handle sequences of variable lengths. It has been done with an <a href=""https://github.com/dmlc/mxnet/tree/master/example/bi-lstm-sort"" rel=""nofollow"">LSTM</a> (Long Short-Term Memory), <a href=""https://arxiv.org/pdf/1602.03218.pdf"" rel=""nofollow"">LSTM+HAM</a> (Hierarchical Attentive Memory) and an <a href=""https://arxiv.org/pdf/1410.5401v2.pdf"" rel=""nofollow"">NTM</a> (Neural Turing Machine).</p>
",,0,2016-08-10T01:53:44.150,,1514,2016-08-10T01:59:52.747,2016-08-10T01:59:52.747,,144.0,,144.0,1508.0,2,2,,,,52.49,13.46,11.05,0.0,0.0,15.0,Even a simple multilayer perceptron can sort input data to some extent as you can see here and here However neural networks for sequential data seem more appropriate as they can handle sequences of variable lengths It has been done with an LSTM Long ShortTerm Memory LSTMHAM Hierarchical Attentive Memory and an NTM Neural Turing Machine
1529.0,1.0,"<p>On Wikipedia, we can read about different type of <a href=""https://en.wikipedia.org/wiki/Intelligent_agent"" rel=""nofollow"">intelligent agents</a>:</p>

<ul>
<li>abstract intelligent agents (AIA),</li>
<li>autonomous intelligent agents,</li>
<li>virtual intelligent agent (IVA), which I've found on other websites, e.g. <a href=""https://www.techopedia.com/definition/26646/intelligent-virtual-agent-iva"" rel=""nofollow"">this one</a>.</li>
</ul>

<p>What are the differences between these three to avoid confusion?</p>

<hr>

<p>For example I've used term <em>virtual artificial agent</em> <a href=""http://ai.stackexchange.com/a/1512/8"">here</a> as:</p>

<blockquote>
  <p>Basically a robot is a mechanical or virtual artificial agent which exhibit intelligent behavior (AI).</p>
</blockquote>

<p>so basically I'd like to know where other terms like autonomous or abstract agents can be used and in what context. Can they be all defined under 'virtual' robot definition? How to distinguish these terms?</p>
",,5,2016-08-10T02:05:29.250,,1515,2016-08-10T15:54:53.790,2016-08-10T12:06:30.630,,8.0,,8.0,,1,1,<comparison><intelligent-agent><definitions>,"What is the difference between abstract, autonomous and virtual intelligent agents?",43.0,37.5,14.27,9.63,0.0,0.0,26.0,On Wikipedia we can read about different type of intelligent agents abstract intelligent agents AIA autonomous intelligent agents virtual intelligent agent IVA which Ive found on other websites eg this one What are the differences between these three to avoid confusion For example Ive used term virtual artificial agent here as Basically a robot is a mechanical or virtual artificial agent which exhibit intelligent behavior AI so basically Id like to know where other terms like autonomous or abstract agents can be used and in what context Can they be all defined under virtual robot definition How to distinguish these terms
,,"<p>It's true that the term has become a buzzword, and is now widely used to a point of confusion - however if you look at the definition provided by Stuart Russell and Peter Norvig, they write it as follows:</p>

<blockquote>
  <p>We define AI as the study of agents that <strong>receive percepts from the 
  environment and perform actions</strong>. Each such agent implements a function
  that maps percept sequences to actions, and we cover different ways to
  represent these functions, such as reactive agents, real-time
  planners, and decision-theoretic systems. We explain the role of
  learning as extending the reach of the designer into
  unknown environments, and we show how that role constrains agent
  design, favoring explicit knowledge representation and reasoning.</p>
</blockquote>

<p><a href=""http://rads.stackoverflow.com/amzn/click/9332543518"">Artificial Intelligence: A Modern Approach - Stuart Russell and Peter Norvig</a></p>

<p>So the example you cite, ""autopilot for cars/planes"", is actually a (famous) form of AI as it has to <strong>use a form of knowledge representation to deal with unknown environments and circumstances</strong>. Ultimately, these systems also collect data so that the knowledge representation can be updated to deal with the new inputs that they have found. They do this with <a href=""http://fortune.com/2015/10/16/how-tesla-autopilot-learns/"">autopilot for cars</a> all the time</p>

<p>So, directly to your question, for something to be considered as ""having AI"", <strong>it needs to be able to deal with unknown environments/circumstances in order to achieve its objective/goal</strong>, and render knowledge in a manner that provides for new learning/information to be added easily. There are many different types of well defined knowledge representation methods, ranging from the popular <a href=""http://neuralnetworksanddeeplearning.com/"">neural net</a>, through to probabilistic models like <a href=""https://en.wikipedia.org/wiki/Bayesian_network"">bayesian networks (belief networks)</a> - but fundamentally actions by the system must be derived from whichever representation of knowledge you choose for it to be considered as AI.</p>
",,2,2016-08-10T02:33:38.747,,1516,2016-12-11T11:35:07.040,2016-12-11T11:35:07.040,,8.0,,1441.0,1507.0,2,18,,,,30.16,14.52,11.47,0.0,0.0,44.0,Its true that the term has become a buzzword and is now widely used to a point of confusion however if you look at the definition provided by Stuart Russell and Peter Norvig they write it as follows We define AI as the study of agents that receive percepts from the environment and perform actions Each such agent implements a function that maps percept sequences to actions and we cover different ways to represent these functions such as reactive agents realtime planners and decisiontheoretic systems We explain the role of learning as extending the reach of the designer into unknown environments and we show how that role constrains agent design favoring explicit knowledge representation and reasoning Artificial Intelligence A Modern Approach Stuart Russell and Peter Norvig So the example you cite autopilot for carsplanes is actually a famous form of AI as it has to use a form of knowledge representation to deal with unknown environments and circumstances Ultimately these systems also collect data so that the knowledge representation can be updated to deal with the new inputs that they have found They do this with autopilot for cars all the time So directly to your question for something to be considered as having AI it needs to be able to deal with unknown environmentscircumstances in order to achieve its objectivegoal and render knowledge in a manner that provides for new learninginformation to be added easily There are many different types of well defined knowledge representation methods ranging from the popular neural net through to probabilistic models like bayesian networks belief networks but fundamentally actions by the system must be derived from whichever representation of knowledge you choose for it to be considered as AI
1524.0,1.0,"<p>On <a href=""https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)"" rel=""nofollow"">Wikipedia</a> we can read:</p>

<blockquote>
  <p>Kasparov accused IBM of cheating and demanded a rematch. IBM refused and retired Deep Blue.</p>
</blockquote>

<p>What was the accusation and how was Deep Blue allegedly able to cheat?</p>
",,0,2016-08-10T02:38:17.940,,1517,2016-08-11T14:51:20.763,2016-08-11T14:51:20.763,,145.0,,8.0,,1,2,<chess><deep-blue><challenges><game-theory>,How could Deep Blue possibly cheat?,83.0,68.77,8.97,9.45,0.0,0.0,4.0,On Wikipedia we can read Kasparov accused IBM of cheating and demanded a rematch IBM refused and retired Deep Blue What was the accusation and how was Deep Blue allegedly able to cheat
1528.0,1.0,"<p>The Wikipedia page describes <a href=""https://en.wikipedia.org/wiki/AI_control_problem"" rel=""nofollow"">AI control problem</a> in very intricated way.</p>

<p>Therefore I would like to better understand it based on some simple explanation, what's going on.
Basically I don't want any copy &amp; pastes from wiki, because the articles there are written in neutral point of view, in very general way where articles are evolving very slowly, so the definition from there doesn't suit me.</p>

<p>I believe this is what is discussed nowadays by government and it's important aspects of AI technology where it leds to.
I believe this could be a big problem in the near future, so I'm expecting to hear about this from people from much better and more up-to-date point of view.</p>

<p>So what is exactly the AI Control Problem?</p>
",,0,2016-08-10T02:54:35.943,,1518,2016-08-12T10:44:18.917,2016-08-12T10:44:18.917,,29.0,,8.0,,1,0,<definitions><control-problem>,What is the Control Problem?,87.0,58.82,10.33,9.72,0.0,0.0,20.0,The Wikipedia page describes AI control problem in very intricated way Therefore I would like to better understand it based on some simple explanation whats going on Basically I dont want any copy amp pastes from wiki because the articles there are written in neutral point of view in very general way where articles are evolving very slowly so the definition from there doesnt suit me I believe this is what is discussed nowadays by government and its important aspects of AI technology where it leds to I believe this could be a big problem in the near future so Im expecting to hear about this from people from much better and more uptodate point of view So what is exactly the AI Control Problem
,,"<p>They treated it as a classification problem. While it's common to use some variety of Neural Nets (NNs) to build classifiers, Genetic Programming (GP) can also be used for this purpose. In contrast to NN classifiers, GP can use a wider range of operations (e.g. if,while,logical statements,arbitrary mathematical functions etc) to perform the classification than <a href=""http://ai.stackexchange.com/questions/1479/do-scientists-know-what-is-happening-inside-artificial-neural-networks"">weighted arithmetic expressions involving an activation function</a>. Whether or not this is actually of benefit depends on the specific application.</p>

<p>In addition, the abstract implies their algorithm is <em>adaptive</em> (i.e. responds in some fashion to the nature of incoming attacks), which would most easily be achieved by continuing to run the GP program in the background to monitor potential intrusions.</p>
",,0,2016-08-10T05:54:42.643,,1520,2016-08-10T05:54:42.643,,,,,42.0,1509.0,2,2,,,,46.37,15.02,11.45,0.0,0.0,25.0,They treated it as a classification problem While its common to use some variety of Neural Nets NNs to build classifiers Genetic Programming GP can also be used for this purpose In contrast to NN classifiers GP can use a wider range of operations eg ifwhilelogical statementsarbitrary mathematical functions etc to perform the classification than weighted arithmetic expressions involving an activation function Whether or not this is actually of benefit depends on the specific application In addition the abstract implies their algorithm is adaptive ie responds in some fashion to the nature of incoming attacks which would most easily be achieved by continuing to run the GP program in the background to monitor potential intrusions
,,"<p>Tesla's technology is assistive, as Alexey points out, so this is not a case of an autonomous system (e.g. an AGI) doing some fatal stunt (the product name AutoPilot is famously misleading). Now on why the car assistance led to this tragic accident, there is some information related to AI technologies.</p>

<hr>

<p><em>Warning: I cannot find again the source critical to the next paragraph, and reading again pages over pages, I cannot find similar argument in other reports. I still remember vividly the point below, but please keep in mind it may be incorrect. The rest of the answer is weakly related, so I leave it all, with this warning.</em></p>

<p>An independent report (link needed, I can't find it...) explained that the assistive system was unable to detect the truck due to an exceptionally low contrast (bright sky perceived as white---colour of the truck). The report also said that a human driver would have been unable to make the difference either. In other words, it is possible that car sensors (presumably camera) and the human eye could not have detected an obstacle, and could not have triggered any safety measure. This <a href=""http://www.nytimes.com/interactive/2016/07/01/business/inside-tesla-accident.html"" rel=""nofollow"">short graphical explanation</a> sums up the car sensors: Camera, radar, GPS, etc.</p>

<p>The assistive sub-system is based on proprietary AI technologies. We can <em>only speculate</em> under some hypothesis. _This is not very useful, honestly, except for <strong>illustration purpose</strong>. Assuming the assistive system relies on ML technologies to learn about obstacles from a video stream (such systems do exist):</p>

<ul>
<li>It may be that the learning data was not ""good enough"" to cover the truck scenario.</li>
<li>It may be the technology was not powerful enough yet (lack generalization power, or simply too slow).</li>
<li>It may be a hardware problem, notably from the sensors: If the ""car's eyes"" are defective, the ""car's brain"" (the assistive system) is unable to react properly.</li>
</ul>

<p>Why those technologies did not work in that case will remain a secret. We can say however that <em>any system</em>---whether built with AI technology or not---has limits. Beyond these limits, the system reaction is unpredictable: It could stop, reset, shutdown. The difficulty here is to define what a ""default behaviour"" is. A machine will basically do whatever it is designed to do, so an AI-based system too.</p>

<p>We could speculate even more on what would happen if the assistive system was really autonomous, the elusive AGI, but that is really not the case here.</p>
",,2,2016-08-10T10:02:41.383,,1522,2016-08-14T23:54:03.540,2016-08-14T23:54:03.540,,169.0,,169.0,1488.0,2,4,,,,53.71,11.25,9.73,0.0,0.0,96.0,Teslas technology is assistive as Alexey points out so this is not a case of an autonomous system eg an AGI doing some fatal stunt the product name AutoPilot is famously misleading Now on why the car assistance led to this tragic accident there is some information related to AI technologies Warning I cannot find again the source critical to the next paragraph and reading again pages over pages I cannot find similar argument in other reports I still remember vividly the point below but please keep in mind it may be incorrect The rest of the answer is weakly related so I leave it all with this warning An independent report link needed I cant find it explained that the assistive system was unable to detect the truck due to an exceptionally low contrast bright sky perceived as whitecolour of the truck The report also said that a human driver would have been unable to make the difference either In other words it is possible that car sensors presumably camera and the human eye could not have detected an obstacle and could not have triggered any safety measure This short graphical explanation sums up the car sensors Camera radar GPS etc The assistive subsystem is based on proprietary AI technologies We can only speculate under some hypothesis This is not very useful honestly except for illustration purpose Assuming the assistive system relies on ML technologies to learn about obstacles from a video stream such systems do exist It may be that the learning data was not good enough to cover the truck scenario It may be the technology was not powerful enough yet lack generalization power or simply too slow It may be a hardware problem notably from the sensors If the cars eyes are defective the cars brain the assistive system is unable to react properly Why those technologies did not work in that case will remain a secret We can say however that any systemwhether built with AI technology or nothas limits Beyond these limits the system reaction is unpredictable It could stop reset shutdown The difficulty here is to define what a default behaviour is A machine will basically do whatever it is designed to do so an AIbased system too We could speculate even more on what would happen if the assistive system was really autonomous the elusive AGI but that is really not the case here
,,"<p>The underlying abstraction (which is essentially what you'd be using the first network for) is that of reducing the state-space of the raw input via feature extraction/synthesis and/or dimensionality reduction.</p>

<p>At present, there are few definite rules for doing this: practice is more a question of 'informed trial and error'. </p>

<p>If you add some information to your question regarding what has been previously attempted in this area (e.g. on the 
<a href=""http://ai.stackexchange.com/questions/1490/what-are-the-benefits-of-the-vgdl-over-the-ale"">ALE</a> platform), this it might be possible to offer some more specific advice.</p>
",,0,2016-08-10T11:24:13.367,,1523,2016-08-10T11:24:13.367,,,,,42.0,1485.0,2,2,,,,50.36,13.29,10.57,0.0,0.0,18.0,The underlying abstraction which is essentially what youd be using the first network for is that of reducing the statespace of the raw input via feature extractionsynthesis andor dimensionality reduction At present there are few definite rules for doing this practice is more a question of informed trial and error If you add some information to your question regarding what has been previously attempted in this area eg on the ALE platform this it might be possible to offer some more specific advice
,,"<p>The allegation was based on the fact that Deep Blue made a choice that did not yield the immediate (or short term) benefit that was synonymous with systems back then (1997). Computational capability was significantly less powerful then, and Kasparov claimed that only a grand master would have made the decision that the system did - so the deep blue team cheated by having a human perform the move instead of the system.</p>

<p>He asked for a rematch, but IBM did not allow this, which only added to the suspicion.</p>

<p>This is a great article with deep analysis on the specific moves and circumstances - however suffice to say that Kasparov was trying to bait the system into making a decision for a weak pawn, but the system chose otherwise and instead put Kasparov into a compromised position:
<a href=""https://en.chessbase.com/post/deep-blue-s-cheating-move"">https://en.chessbase.com/post/deep-blue-s-cheating-move</a></p>
",,0,2016-08-10T13:44:28.207,,1524,2016-08-10T13:44:28.207,,,,,1441.0,1517.0,2,7,,,,36.76,13.07,9.94,0.0,0.0,25.0,The allegation was based on the fact that Deep Blue made a choice that did not yield the immediate or short term benefit that was synonymous with systems back then 1997 Computational capability was significantly less powerful then and Kasparov claimed that only a grand master would have made the decision that the system did so the deep blue team cheated by having a human perform the move instead of the system He asked for a rematch but IBM did not allow this which only added to the suspicion This is a great article with deep analysis on the specific moves and circumstances however suffice to say that Kasparov was trying to bait the system into making a decision for a weak pawn but the system chose otherwise and instead put Kasparov into a compromised position httpsenchessbasecompostdeepbluescheatingmove
,0.0,"<p><sub>This is from a closed beta for AI, with this question being posted by user number 47. All credit to them. </sub></p>

<hr>

<p>According to <a href=""https://en.wikipedia.org/wiki/Boltzmann_machine"" rel=""nofollow"">Wikipedia</a>,</p>

<blockquote>
  <p>Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield nets.</p>
</blockquote>

<p>Both are recurrent neural networks that can be trained to learn of bit patterns. Then when presented with a partial pattern, the net will retrieve the full complete pattern.</p>

<p>Hopfield networks have been proven to have a capacity of 0.138 (e.g. approximately 138 bit vectors can be recalled from storage for every 1000 nodes, Hertz 1991).</p>

<p>As a Boltzmann machine is stochastic, my understanding is that it would not necessarily always show the same pattern when the energy difference between one stored pattern and another is similar. But because of this stochasticity, maybe it allows for denser pattern storage but without the guarantee that you'll always get the ""closest"" pattern in terms of energy difference. Would this be true? Or would a Hopfield net be able to store more patterns?</p>
",,0,2016-08-10T14:10:08.270,1.0,1525,2016-08-23T08:19:28.460,2016-08-23T08:19:28.460,,145.0,,145.0,,1,10,<neural-networks><comparison><recurrent-neural-networks>,Could a Boltzmann machine store more patterns than a Hopfield net?,128.0,64.51,11.36,9.59,0.0,0.0,25.0,This is from a closed beta for AI with this question being posted by user number 47 All credit to them According to Wikipedia Boltzmann machines can be seen as the stochastic generative counterpart of Hopfield nets Both are recurrent neural networks that can be trained to learn of bit patterns Then when presented with a partial pattern the net will retrieve the full complete pattern Hopfield networks have been proven to have a capacity of 0138 eg approximately 138 bit vectors can be recalled from storage for every 1000 nodes Hertz 1991 As a Boltzmann machine is stochastic my understanding is that it would not necessarily always show the same pattern when the energy difference between one stored pattern and another is similar But because of this stochasticity maybe it allows for denser pattern storage but without the guarantee that youll always get the closest pattern in terms of energy difference Would this be true Or would a Hopfield net be able to store more patterns
,,"<p>The well-known 'Eliza' program (Weizenbaum, ~1964) would appear to be the first. </p>

<p>Eliza was designed to model the emotionally-neutral response of a psychotherapist and this masks some of the weaknesses of its limited underlying pattern-matching mechanisms. </p>
",,1,2016-08-10T14:28:47.593,,1526,2016-08-10T14:28:47.593,,,,,42.0,191.0,2,3,,,,36.28,17.86,11.55,0.0,0.0,11.0,The wellknown Eliza program Weizenbaum 1964 would appear to be the first Eliza was designed to model the emotionallyneutral response of a psychotherapist and this masks some of the weaknesses of its limited underlying patternmatching mechanisms
,,"<p>The 2015 paper entitled ""<a href=""https://arxiv.org/pdf/1511.08899.pdf"" rel=""nofollow"">Applying deep learning to classify pornographic images and videos</a>"" applied various types of convnets for detecting pornography. The proposed architecture achieved <strong>94.1% accuracy</strong> on the NPDI dataset, which contains 800 videos (400 porn, 200 non-porn ""easy"" and 200 non-porn ""difficult""). More traditional computer vision methods achieved 90.9% accuracy. The proposed architecture also performs very well regarding the ROC curve.</p>

<p>There does not seem to exist any works regarding the other aspects of NSFW yet.</p>
",,0,2016-08-10T15:16:19.503,,1527,2016-08-10T15:16:19.503,,,,,144.0,1478.0,2,3,,,,49.82,15.65,11.77,0.0,0.0,21.0,The 2015 paper entitled Applying deep learning to classify pornographic images and videos applied various types of convnets for detecting pornography The proposed architecture achieved 941 accuracy on the NPDI dataset which contains 800 videos 400 porn 200 nonporn easy and 200 nonporn difficult More traditional computer vision methods achieved 909 accuracy The proposed architecture also performs very well regarding the ROC curve There does not seem to exist any works regarding the other aspects of NSFW yet
,,"<p>The Control Problem is, in short, the idea that AI will eventually be much better decision-makers than humans. If we don't set things up correctly beforehand, we won't get a chance to fix it afterwards, because AI will have effective control.</p>

<p>There are three main areas of discussion with regards to the Control Problem:</p>

<ol>
<li><p>Whether or not the problem is urgent. Many AI experts, cognizant of the difficulty of getting simple systems to work effectively today, think that AI able to take control is not urgent, and as detail-minded engineers, they think it will be profoundly difficult to do any useful work today. (Andrew Ng, for example, famously called these sorts of worries like worrying about overpopulation on Mars.) Given radical uncertainty among AI experts as to when this will become an issue, however, this means we can't rule out rapid AI timescales, and should do at least some work in anticipation of those timescales.</p></li>
<li><p>Whether or not the problem is hard. Many people give short, simple, and wrong solutions to the control problem. Probably the most famous is the idea that intelligence and morality are inherently interlinked, and thus a more intelligent machine, <em>by definition</em>, will be more moral. The Orthogonality Thesis is the claim of the opposite, that intelligence and morality (or, more specifically, goal alignment) are unrelated things.</p></li>
<li><p>What foundations we can lay now. There are a bunch of open problems (see, for example, <a href=""https://intelligence.org/technical-agenda/"">MIRI's technical agenda</a>) that deal with mathematical logic of the sort that would be useful for ensuring robust value alignment, or on how to effectively do value learning (without giving an incentive to distort values), or on how to build value functions and goals such that they are fixable if they turn out to be mistaken. Those look like problems that we can do useful work on now, even without knowing what the actual structure of a future AI will look like.</p></li>
</ol>
",,0,2016-08-10T15:42:41.603,,1528,2016-08-10T15:42:41.603,,,,,10.0,1518.0,2,6,,,,46.61,11.9,10.05,0.0,0.0,54.0,The Control Problem is in short the idea that AI will eventually be much better decisionmakers than humans If we dont set things up correctly beforehand we wont get a chance to fix it afterwards because AI will have effective control There are three main areas of discussion with regards to the Control Problem Whether or not the problem is urgent Many AI experts cognizant of the difficulty of getting simple systems to work effectively today think that AI able to take control is not urgent and as detailminded engineers they think it will be profoundly difficult to do any useful work today Andrew Ng for example famously called these sorts of worries like worrying about overpopulation on Mars Given radical uncertainty among AI experts as to when this will become an issue however this means we cant rule out rapid AI timescales and should do at least some work in anticipation of those timescales Whether or not the problem is hard Many people give short simple and wrong solutions to the control problem Probably the most famous is the idea that intelligence and morality are inherently interlinked and thus a more intelligent machine by definition will be more moral The Orthogonality Thesis is the claim of the opposite that intelligence and morality or more specifically goal alignment are unrelated things What foundations we can lay now There are a bunch of open problems see for example MIRIs technical agenda that deal with mathematical logic of the sort that would be useful for ensuring robust value alignment or on how to effectively do value learning without giving an incentive to distort values or on how to build value functions and goals such that they are fixable if they turn out to be mistaken Those look like problems that we can do useful work on now even without knowing what the actual structure of a future AI will look like
,,"<p>I don't think there are many contexts where there is any really meaningful distinction between these terms.  Even in the WP article you refer to, it is shown that ""abstract intelligent agent"" and ""autonomous intelligent agent"" are generally just synonyms for ""intelligent agent"" but used to highlight certain aspects of intelligent agents in some contexts.   Net-net, I'd say there just isn't any difference there that's going to matter in practice.</p>

<p>""Virtual intelligent agent"" OTOH, used in the context you used it, suggests the distinction between an IA that's implemented in software only, versus one that has a physical manifestation.   I don't know how useful that distinction is and I haven't seen anybody else make it.  </p>

<p>All in all, I expect that in almost every possible context, if you just say ""Intelligent Agent"" with no qualifiers, that's going to be sufficient.   But if there were going to be an exception, I'd say it would be around the term ""autonomous"" since an agent which is truly autonomous, versus one that needs to operate in a specific, constrained environment, is a distinction that - at least in principle - could be useful. </p>
",,1,2016-08-10T15:54:53.790,,1529,2016-08-10T15:54:53.790,,,,,33.0,1515.0,2,2,,,,44.37,12.54,10.11,0.0,0.0,43.0,I dont think there are many contexts where there is any really meaningful distinction between these terms Even in the WP article you refer to it is shown that abstract intelligent agent and autonomous intelligent agent are generally just synonyms for intelligent agent but used to highlight certain aspects of intelligent agents in some contexts Netnet Id say there just isnt any difference there thats going to matter in practice Virtual intelligent agent OTOH used in the context you used it suggests the distinction between an IA thats implemented in software only versus one that has a physical manifestation I dont know how useful that distinction is and I havent seen anybody else make it All in all I expect that in almost every possible context if you just say Intelligent Agent with no qualifiers thats going to be sufficient But if there were going to be an exception Id say it would be around the term autonomous since an agent which is truly autonomous versus one that needs to operate in a specific constrained environment is a distinction that at least in principle could be useful
,,"<p>I'm afraid I don't have the specific citations handy, but I have seen/heard quotes by experts like Andrew Ng and Geoffrey Hinton where they clearly say that we do not really understand neural networks.  That is, we understand something of the <em>how</em> they work (for example, the math behind back propagation) but we don't really understand <em>why</em> they work.  It's sort of a subtle distinction, but the point is that no, we don't understand the very deepest details of how exactly you go from a bunch of weights, to, say, recognizing a cat playing with a ball.  </p>

<p>At least in terms of image recognition, the best explanation I've heard is that successive layers of a neural network learn more sophisticated features, composed of the more granular features from earlier levels.  That is to say, the first layer might recognize ""edges"" or ""straight lines"".  The next layer might then learn geometric shapes like ""box"", or ""triangle"", and then a higher layer might learn ""nose"" or ""eye"" based on those earlier features, and then a higher level layer still learns ""face"" made up from ""eye"", ""nose"", ""jaw"", etc.   But even that, as I understand it, is still hypothetical and/or not understood in complete detail. </p>
",,5,2016-08-10T16:03:27.817,,1530,2016-08-10T16:03:27.817,,,,,33.0,1479.0,2,3,,,,59.06,11.73,9.21,0.0,0.0,56.0,Im afraid I dont have the specific citations handy but I have seenheard quotes by experts like Andrew Ng and Geoffrey Hinton where they clearly say that we do not really understand neural networks That is we understand something of the how they work for example the math behind back propagation but we dont really understand why they work Its sort of a subtle distinction but the point is that no we dont understand the very deepest details of how exactly you go from a bunch of weights to say recognizing a cat playing with a ball At least in terms of image recognition the best explanation Ive heard is that successive layers of a neural network learn more sophisticated features composed of the more granular features from earlier levels That is to say the first layer might recognize edges or straight lines The next layer might then learn geometric shapes like box or triangle and then a higher layer might learn nose or eye based on those earlier features and then a higher level layer still learns face made up from eye nose jaw etc But even that as I understand it is still hypothetical andor not understood in complete detail
1532.0,2.0,"<p>According to <a href=""http://en.wikipedia.org/wiki/Prolog"">Wikipedia</a>,</p>

<blockquote>
  <p>Prolog is a general-purpose logic programming language associated with artificial intelligence and computational linguistics.</p>
</blockquote>

<p>Is it still used for AI?</p>

<hr>

<p><sub>This is based off of a question on the 2014 closed beta. The author had the UID of 330.</sub></p>
",,0,2016-08-10T19:17:18.410,2.0,1531,2016-08-12T21:38:48.343,,,,,145.0,,1,7,<history><programming-languages><prolog>,Is Prolog still used in AI?,132.0,52.36,11.75,10.92,0.0,0.0,6.0,According to Wikipedia Prolog is a generalpurpose logic programming language associated with artificial intelligence and computational linguistics Is it still used for AI This is based off of a question on the 2014 closed beta The author had the UID of 330
,,"<p>Remembering that artificial intelligence has been an academic endeavour for the longest time, Prolog was amongst one of the early languages used as part of the study and implementation of it. It has rarely made its way into large commercial applications, having said that, a famous commercial implementation is in <a href=""http://www.cs.nmsu.edu/ALP/2011/03/natural-language-processing-with-prolog-in-the-ibm-watson-system/"" rel=""nofollow"">Watson, where prolog is used for NLP</a>.</p>

<p>The <a href=""http://www.ed.ac.uk/informatics/"" rel=""nofollow"">University of Edinburgh</a> contributed to the language and it was sometimes referred to as ""Edinburgh Prolog"". It is <a href=""http://www.inf.ed.ac.uk/teaching/courses/lp/"" rel=""nofollow"">still used in academic teachings</a> there as part of the artificial intelligence course.</p>

<p>The reason why Prolog is considered powerful in AI is because the language allows for easy management of recursive methods, and pattern matching.</p>

<p>To quote <a href=""http://www-03.ibm.com/innovation/us/watson/research-team/systems.html"" rel=""nofollow"">Adam Lally from the IBM Thomas J. Watson Research Center</a>, and <a href=""http://www3.cs.stonybrook.edu/~pfodor/"" rel=""nofollow"">Paul Fodor from Stony Brook University</a>:</p>

<blockquote>
  <p>the Prolog language is very expressive allowing recursive rules to represent reachability in parse trees and the operation of negation-as-failure to check the absence of conditions.</p>
</blockquote>
",,0,2016-08-11T01:37:54.610,,1532,2016-08-11T01:37:54.610,,,,,1441.0,1531.0,2,4,,,,31.82,13.99,10.78,0.0,0.0,18.0,Remembering that artificial intelligence has been an academic endeavour for the longest time Prolog was amongst one of the early languages used as part of the study and implementation of it It has rarely made its way into large commercial applications having said that a famous commercial implementation is in Watson where prolog is used for NLP The University of Edinburgh contributed to the language and it was sometimes referred to as Edinburgh Prolog It is still used in academic teachings there as part of the artificial intelligence course The reason why Prolog is considered powerful in AI is because the language allows for easy management of recursive methods and pattern matching To quote Adam Lally from the IBM Thomas J Watson Research Center and Paul Fodor from Stony Brook University the Prolog language is very expressive allowing recursive rules to represent reachability in parse trees and the operation of negationasfailure to check the absence of conditions
,,"<p>The neural networks can be easily fooled or hacked by adding certain structured noise in image space (<a href=""https://arxiv.org/abs/1312.6199"" rel=""nofollow"">Szegedy 2013</a>, <a href=""http://arxiv.org/abs/1412.1897"" rel=""nofollow"">Nguyen 2014</a>) due to ignoring non-discriminative information in their input.</p>

<p>For example:</p>

<blockquote>
  <p>Learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs.<sup><a href=""http://arxiv.org/abs/1506.06579"" rel=""nofollow"">2015</a></sup></p>
</blockquote>

<p>So basically the high confidence prediction in certain models exists due to a '<em>combination of their locally linear nature and high-dimensional input space</em>'.<sup><a href=""http://arxiv.org/abs/1412.1897"" rel=""nofollow"">2015</a></sup></p>

<p>Published as a conference paper at <a href=""http://www.stat.ucla.edu/~ywu/ICLR2015.pdf"" rel=""nofollow"">ICLR 2015</a> (work by Dai) suggest that transferring discriminatively trained parameters to generative models, could be a great area for further improvements.</p>
",,0,2016-08-11T03:08:39.273,,1533,2016-08-11T03:08:39.273,,,,,8.0,92.0,2,2,,,,36.83,15.5,11.97,0.0,0.0,15.0,The neural networks can be easily fooled or hacked by adding certain structured noise in image space Szegedy 2013 Nguyen 2014 due to ignoring nondiscriminative information in their input For example Learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs2015 So basically the high confidence prediction in certain models exists due to a combination of their locally linear nature and highdimensional input space2015 Published as a conference paper at ICLR 2015 work by Dai suggest that transferring discriminatively trained parameters to generative models could be a great area for further improvements
1727.0,1.0,"<p>I'm a bit confused with extensive number of different <a href=""https://en.wikipedia.org/wiki/Monte_Carlo_method"" rel=""nofollow"">Monte Carlo methods</a> such as:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Hybrid_Monte_Carlo"" rel=""nofollow"">Hamiltonian/Hybrid Monte Carlo (HMC)</a>,</li>
<li><a href=""https://en.wikipedia.org/wiki/Dynamic_Monte_Carlo_method"" rel=""nofollow"">Dynamic Monte Carlo (DMC)</a>,</li>
<li><a href=""https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"" rel=""nofollow"">Markov chain Monte Carlo (MCMC)</a>,</li>
<li><a href=""https://en.wikipedia.org/wiki/Kinetic_Monte_Carlo"" rel=""nofollow"">Kinetic Monte Carlo (KMC)</a>,</li>
<li><a href=""https://en.wikipedia.org/wiki/Dynamic_Monte_Carlo_method"" rel=""nofollow"">Dynamic Monte Carlo (DMC)</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method"" rel=""nofollow"">Quasi-Monte Carlo (QMC)</a>,</li>
<li><a href=""https://en.wikipedia.org/wiki/Direct_simulation_Monte_Carlo"" rel=""nofollow"">Direct Simulation Monte Carlo (DSMC)</a>,</li>
<li>and so on.</li>
</ul>

<p>I won't ask for the exact differences, but why are all of them called Monte Carlo? What do they all have in common? Can they all be used for AI? E.g. which one can be used for gaming (like Go) or image recognition (resampling)?</p>
",,1,2016-08-11T03:29:18.097,,1534,2016-08-24T13:15:48.023,2016-08-11T13:46:22.443,,145.0,,8.0,,1,5,<gaming><comparison><monte-carlo-search>,How do I know when to use which Monte Carlo method?,75.0,61.67,11.19,8.56,0.0,0.0,37.0,Im a bit confused with extensive number of different Monte Carlo methods such as HamiltonianHybrid Monte Carlo HMC Dynamic Monte Carlo DMC Markov chain Monte Carlo MCMC Kinetic Monte Carlo KMC Dynamic Monte Carlo DMC QuasiMonte Carlo QMC Direct Simulation Monte Carlo DSMC and so on I wont ask for the exact differences but why are all of them called Monte Carlo What do they all have in common Can they all be used for AI Eg which one can be used for gaming like Go or image recognition resampling
,2.0,"<p>When it comes to neural networks, it's often only explained what abstract task they do, say for example detect a number in an image. I never understood what's going on under the hood essentially.</p>

<p>There seems to be a common structure of a directed graph, with values in each node. Some nodes are input nodes. Their values can be set. The values of subsequent nodes are then calculated based on those along the edges of the graph until the values for the output nodes are set, which can be interpreted a result.</p>

<p>How exactly is the value of each node determined? I assume that some formula is associated with each node that takes all incoming nodes as input to calculate the value of the node. What formula is used? Is the formula the same throughout the network?</p>

<p>Then I heard that a network has to be trained. I assume that such training would be the process to assign values to coefficients of the formulas used to determine the node values. Is that correct?</p>

<p>In layman's terms, what are the underlying principles that make a neural network work?</p>
",,6,2016-08-11T08:30:40.453,0.0,1535,2016-08-18T09:52:04.357,2016-08-18T08:40:00.167,,8.0,,1463.0,,1,3,<neural-networks>,How exactly is the value of each node determined? Is it the same formula throughout the network?,54.0,66.33,8.75,9.03,0.0,0.0,22.0,When it comes to neural networks its often only explained what abstract task they do say for example detect a number in an image I never understood whats going on under the hood essentially There seems to be a common structure of a directed graph with values in each node Some nodes are input nodes Their values can be set The values of subsequent nodes are then calculated based on those along the edges of the graph until the values for the output nodes are set which can be interpreted a result How exactly is the value of each node determined I assume that some formula is associated with each node that takes all incoming nodes as input to calculate the value of the node What formula is used Is the formula the same throughout the network Then I heard that a network has to be trained I assume that such training would be the process to assign values to coefficients of the formulas used to determine the node values Is that correct In laymans terms what are the underlying principles that make a neural network work
,,"<p>I will overly simplify ANNs in order to point how they work. Examples might not be 100% accurate.</p>

<p>In the simplest form, network is trained using the apriori information extracted from the ground truth. This basically means that ANN uses the relation between the input and output. </p>

<p>For instance, if you are to classify shrubs and trees, one of the input could be height and the other could be the width of the tree. Now, if you have only input and output layers, increasing height means increasing chance for the object to be a tree. Thus, input height would have a positive weight connecting to tree output and a negative weight to shrub output. However, as the plant gets wider, the chance of it being a shrub increases. Taller shrubs are wider than shorter ones. Thus input weight would have positive weight connecting to the shrub output. Finally, the chance of being a tree is not affected by the width and thus will have close to 0 weight between this input and output. This network will effectively work like a linear discriminant classifier.</p>

<p>Now instead of assigning weights by hand, you may use a learning algorithm that tries to adjust weights so that the output is correct when the series of input is supplied. Ideally this training algorithm should reach to the conclusion that we have made in the previous example. Most training algorithms are recursive. They supply the inputs multiple times, and in a simple sense, they reward pathways that are correct by increasing their weights and punishes pathways that are causing incorrect answer.</p>

<p>When hidden layers are used in a system, they would be able to correlate input on higher degrees. Thus, as the number of layers get higher, ANN learns the input set much better. However, this does not mean it gets better. If the ANN over fits the input set, it would be affected from the random noise that is in the dataset. This problem is generally referred as memorization. There are learning algorithms that try to minimize memorization and maximize generalization ability. But ultimately, the number of training samples should be high enough so that ANN cannot overfit to the data.</p>
",,3,2016-08-11T09:00:25.247,,1536,2016-08-11T09:00:25.247,,,,,210.0,1535.0,2,3,,,,72.26,10.5,8.52,0.0,0.0,42.0,I will overly simplify ANNs in order to point how they work Examples might not be 100 accurate In the simplest form network is trained using the apriori information extracted from the ground truth This basically means that ANN uses the relation between the input and output For instance if you are to classify shrubs and trees one of the input could be height and the other could be the width of the tree Now if you have only input and output layers increasing height means increasing chance for the object to be a tree Thus input height would have a positive weight connecting to tree output and a negative weight to shrub output However as the plant gets wider the chance of it being a shrub increases Taller shrubs are wider than shorter ones Thus input weight would have positive weight connecting to the shrub output Finally the chance of being a tree is not affected by the width and thus will have close to 0 weight between this input and output This network will effectively work like a linear discriminant classifier Now instead of assigning weights by hand you may use a learning algorithm that tries to adjust weights so that the output is correct when the series of input is supplied Ideally this training algorithm should reach to the conclusion that we have made in the previous example Most training algorithms are recursive They supply the inputs multiple times and in a simple sense they reward pathways that are correct by increasing their weights and punishes pathways that are causing incorrect answer When hidden layers are used in a system they would be able to correlate input on higher degrees Thus as the number of layers get higher ANN learns the input set much better However this does not mean it gets better If the ANN over fits the input set it would be affected from the random noise that is in the dataset This problem is generally referred as memorization There are learning algorithms that try to minimize memorization and maximize generalization ability But ultimately the number of training samples should be high enough so that ANN cannot overfit to the data
1569.0,1.0,"<p>Ideally I'd like to watch movie which is deep dreamed in real-time. Most algorithms which I know are too slow or not designed for real-time processing.</p>

<p>For example I'm bored with some movie which I've watched thousands of time and I'd like to add some ""dreaming"" to it which is real-time filter which takes input frames, then it's processing and enhances the images through artificial neural network to achieve doodled output.</p>

<p>Doesn't have to be exactly <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow"">DeepDream</a> or hallucinogenic technique (which could be too much to watch for 2h), but with any similar ANN algorithm. I'm more interested into achieving desired real-time use.</p>

<p>What kind of techniques can achieve such efficiency?</p>
",,0,2016-08-11T09:02:52.250,,1537,2016-08-11T20:49:24.500,2016-08-11T20:40:25.737,,8.0,,8.0,,1,0,<research><algorithm><real-time><neural-doodle>,Which techniques can achieve neural doodle in real-time?,31.0,52.7,12.53,9.96,0.0,0.0,23.0,Ideally Id like to watch movie which is deep dreamed in realtime Most algorithms which I know are too slow or not designed for realtime processing For example Im bored with some movie which Ive watched thousands of time and Id like to add some dreaming to it which is realtime filter which takes input frames then its processing and enhances the images through artificial neural network to achieve doodled output Doesnt have to be exactly DeepDream or hallucinogenic technique which could be too much to watch for 2h but with any similar ANN algorithm Im more interested into achieving desired realtime use What kind of techniques can achieve such efficiency
1545.0,2.0,"<p>How does employing evolutionary algorithms to design and train artificial neural networks have advantages over using the conventional backpropagation algorithms?</p>
",,0,2016-08-11T09:39:32.893,1.0,1539,2016-08-17T11:49:00.693,2016-08-17T11:49:00.693,,145.0,,8.0,,1,7,<neural-networks><comparison><backpropagation><evolutionary-algorithms>,How do evolutionary algorithms have advantages over the conventional backpropagation methods?,62.0,0.42,23.9,14.89,0.0,0.0,1.0,How does employing evolutionary algorithms to design and train artificial neural networks have advantages over using the conventional backpropagation algorithms
1562.0,1.0,"<p>Are there any existing approaches for using artificial neural networks (ANN) or evolutionary algorithm (EA) for detecting coding standard violations? Which one would be more suitable?</p>

<p>I don't have any specific programming language in mind, but something similar to <a href=""http://pear.php.net/package/PHP_CodeSniffer"" rel=""nofollow"">PHP_CodeSniffer</a> (following <a href=""https://www.drupal.org/coding-standards"" rel=""nofollow"">these standards</a>), but instead of using hardcoded rules, the algorithm should learn good techniques, but I'm not sure based on what training data. How would you approach the training session, any suggestions?</p>
",2016-08-12T22:44:53.870,4,2016-08-11T10:41:10.593,,1540,2016-08-11T18:53:01.913,2016-08-11T18:53:01.913,,42.0,,8.0,,1,0,<neural-networks><training><computer-programming>,Using AI capabilities for coding review,58.0,44.54,16.65,11.46,0.0,0.0,18.0,Are there any existing approaches for using artificial neural networks ANN or evolutionary algorithm EA for detecting coding standard violations Which one would be more suitable I dont have any specific programming language in mind but something similar to PHPCodeSniffer following these standards but instead of using hardcoded rules the algorithm should learn good techniques but Im not sure based on what training data How would you approach the training session any suggestions
1548.0,3.0,"<p>Genetic Algorithms has come to my attention recently when trying to correct/improve computer opponents for turn-based strategy computer games.</p>

<p>I implemented a simple Genetic Algorithm that didn't use any cross-over, just some random mutation. It seemed to work in this case, and so I started thinking:</p>

<p><strong>Why is cross-over a part of genetic algorithms? Wouldn't mutation be enough?</strong></p>

<p><sub>This is from a data dump on an old AI site. The asker had the UID of 7. </sub></p>
",,1,2016-08-11T13:21:53.280,1.0,1541,2016-08-25T11:26:49.377,2016-08-25T11:26:49.377,,145.0,,145.0,,1,5,<genetic-algorithms>,Why is cross-over a part of genetic algorithms?,78.0,58.58,10.43,9.88,0.0,0.0,15.0,Genetic Algorithms has come to my attention recently when trying to correctimprove computer opponents for turnbased strategy computer games I implemented a simple Genetic Algorithm that didnt use any crossover just some random mutation It seemed to work in this case and so I started thinking Why is crossover a part of genetic algorithms Wouldnt mutation be enough This is from a data dump on an old AI site The asker had the UID of 7
,,,,0,2016-08-11T13:48:04.250,,1542,2016-08-11T13:48:04.250,2016-08-11T13:48:04.250,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions about AI gaming. This should be used for AI in games.,,0,2016-08-11T13:48:04.250,,1543,2016-08-11T14:43:11.123,2016-08-11T14:43:11.123,,145.0,,145.0,,4,0,,,,98.72,4.29,10.03,0.0,0.0,2.0,For questions about AI gaming This should be used for AI in games
,3.0,"<p>While thinking about AI, this question came into my mind. Could curiosity help in developing a true AI? According to this <a href=""http://psychologia.co/creativity-test/"" rel=""nofollow"">website</a> (for testing creativity):</p>

<blockquote>
  <p>Curiosity refers to persistent desire to learn and discover new things
  and ideas</p>

<pre><code>always looks for new and original ways of thinking,
likes to learn,
searches for alternative solutions even when traditional solutions are present and available,
enjoys reading books and watching documentaries,
wants to know how things work inside out
</code></pre>
</blockquote>

<p>Let's take <a href=""https://www.clarifai.com/demo"" rel=""nofollow"">Clarifai</a>, a image/video classification startup which can classify images and video with the best accuracy (according to them). If I understand correctly, they trained their deep learning system using millions of images with supervised learning. In the same algorithm, what would happen if we somehow added a ""curiosity factor"" when the AI has difficulty in classifying a image or its objects? It would ask a human for help, just like a curious child. </p>

<p>Curiosity makes a human being learn new things and also helps to generate new original ideas. Could the addition of curiosity change Clarifai into a true AI?</p>
",,2,2016-08-11T14:00:46.070,,1544,2016-08-29T10:50:58.727,2016-08-12T15:28:10.517,,75.0,,39.0,,1,3,<human-inspired>,Could curiosity improve artificial intelligence?,127.0,53.71,12.12,10.59,252.0,0.0,22.0,While thinking about AI this question came into my mind Could curiosity help in developing a true AI According to this website for testing creativity Curiosity refers to persistent desire to learn and discover new things and ideas Lets take Clarifai a imagevideo classification startup which can classify images and video with the best accuracy according to them If I understand correctly they trained their deep learning system using millions of images with supervised learning In the same algorithm what would happen if we somehow added a curiosity factor when the AI has difficulty in classifying a image or its objects It would ask a human for help just like a curious child Curiosity makes a human being learn new things and also helps to generate new original ideas Could the addition of curiosity change Clarifai into a true AI
,,"<p>Unlike backpropagation, evolutionary algorithms do not require the objective function to be differential with respect to the parameters you aim to optimize. As a result, you can optimize ""more things"" in the network, such as activation functions or number of layers, which wouldn't be possible in the standard backpropagation. </p>

<p>Another advantage is that by defining the mutation and crossover functions, you can influence how the parameter search space should be explored.</p>
",,0,2016-08-11T14:04:43.640,,1545,2016-08-11T16:50:04.720,2016-08-11T16:50:04.720,,4.0,,4.0,1539.0,2,6,,,,38.96,15.79,11.26,0.0,0.0,11.0,Unlike backpropagation evolutionary algorithms do not require the objective function to be differential with respect to the parameters you aim to optimize As a result you can optimize more things in the network such as activation functions or number of layers which wouldnt be possible in the standard backpropagation Another advantage is that by defining the mutation and crossover functions you can influence how the parameter search space should be explored
,,"<p>Crossover allows to combine two parents (vs. mutation, which only uses one parent). In some cases, it is useful (e.g., if you train a FPS bot, if one parent is good at shooting and another parent is good at moving, it makes sense to combine them). In some other cases, it is not.</p>
",,0,2016-08-11T14:06:30.360,,1546,2016-08-11T14:19:54.063,2016-08-11T14:19:54.063,,4.0,,4.0,1541.0,2,5,,,,77.64,6.36,8.93,0.0,0.0,16.0,Crossover allows to combine two parents vs mutation which only uses one parent In some cases it is useful eg if you train a FPS bot if one parent is good at shooting and another parent is good at moving it makes sense to combine them In some other cases it is not
,,"<blockquote>
  <p>when the AI has difficulty in classifying a image or its objects it should ask a human for help just like a curious child</p>
</blockquote>

<p>It's called <a href=""https://en.wikipedia.org/wiki/Active_learning_(machine_learning)"">active learning</a>, it's already used quite often.</p>
",,0,2016-08-11T14:13:16.937,,1547,2016-08-11T14:13:16.937,,,,,4.0,1544.0,2,5,,,,54.9,9.53,11.02,0.0,0.0,4.0,when the AI has difficulty in classifying a image or its objects it should ask a human for help just like a curious child Its called active learning its already used quite often
,,"<p>Mutation is usually defined to be a <em>global</em> operator, i.e. iterated mutation is (eventually) capable of reaching every point in the vector space defined by the geneome. In that sense, mutation alone is certainly 'enough'.</p>

<p>Regarding the motivation for crossover - from <a href=""https://cs.gmu.edu/~sean/book/metaheuristics/"" rel=""nofollow"">Essentials of Metaheuristics</a>, p42:</p>

<blockquote>
  <p>Crossover was originally based on the premise that highly fit individuals often share certain traits, called <em>building blocks</em>, in common.
  For example, in the boolean individual 10110101, perhaps
  ***101*1 might be a building block </p>
</blockquote>

<p>(where * means ""either 0 or 1"") </p>

<p>So the idea is that crossover works by spreading building blocks quickly throughout the population.</p>

<blockquote>
  <p>Crossover methods also assume that there is some degree of linkage between genes on the chromosome: that is, settings for certain genes in groups are strongly correlated to fitness improvement. For example, genes A and B might contribute to fitness only when they’re both set to 1: if either is set to 0, then the fact that the other is set to 1 doesn’t do anything.</p>
</blockquote>

<p>Also note that <em>crossover is not a global operator</em>. If the only operator is crossover then (also from p42):</p>

<blockquote>
  <p>Eventually the population will converge, and often (unfortunately) prematurely
  converge, to copies of the same individual. At this stage there’s no escape: when an individual crosses over with itself, nothing new is generated.</p>
</blockquote>

<p>For this reason, crossover is generally used together with some global mutation operator.</p>
",,0,2016-08-11T14:40:33.810,,1548,2016-08-11T15:57:27.610,2016-08-11T15:57:27.610,,42.0,,42.0,1541.0,2,5,,,,50.16,13.4,10.21,0.0,0.0,49.0,Mutation is usually defined to be a global operator ie iterated mutation is eventually capable of reaching every point in the vector space defined by the geneome In that sense mutation alone is certainly enough Regarding the motivation for crossover from Essentials of Metaheuristics p42 Crossover was originally based on the premise that highly fit individuals often share certain traits called building blocks in common For example in the boolean individual 10110101 perhaps 1011 might be a building block where means either 0 or 1 So the idea is that crossover works by spreading building blocks quickly throughout the population Crossover methods also assume that there is some degree of linkage between genes on the chromosome that is settings for certain genes in groups are strongly correlated to fitness improvement For example genes A and B might contribute to fitness only when they’re both set to 1 if either is set to 0 then the fact that the other is set to 1 doesn’t do anything Also note that crossover is not a global operator If the only operator is crossover then also from p42 Eventually the population will converge and often unfortunately prematurely converge to copies of the same individual At this stage there’s no escape when an individual crosses over with itself nothing new is generated For this reason crossover is generally used together with some global mutation operator
,,,,0,2016-08-11T14:44:00.860,,1549,2016-08-11T14:44:00.860,2016-08-11T14:44:00.860,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,"For questions regarding the use of the mathematical theory of games (Von Neumann, Morgenstern, Nash etc) in AI. For other questions about AI in games, use [gaming].",,0,2016-08-11T14:44:00.860,,1550,2016-08-18T16:06:11.477,2016-08-18T16:06:11.477,,42.0,,145.0,,4,0,,,,66.23,11.77,10.74,0.0,0.0,9.0,For questions regarding the use of the mathematical theory of games Von Neumann Morgenstern Nash etc in AI For other questions about AI in games use gaming
,,"<p>When thinking about crossover its important to think about the fitness landscape. </p>

<p>Consider a hypothetical scenario where we are applying a genetic algorithm to find a solution that performs well at 2 tasks. This could be from Franck's example (moving and shooting) for an AI, or perhaps it could be predicted 2 outputs in a genetic machine learning scenario, but really most scenarios where GAs are applied are synonymous (even at solving a single task, there may be different aspects of the task to be addressed).</p>

<p>Suppose we had an individual, 1, that was performing reasonably well at both tasks, and we found a series of mutations which produced 2 new individuals, 2 and 3, which performed better than Individual 1 at tasks 1 and 2 respectively. Now while both of these are improvements, ideally we want to find a generally good solution, so we want to combine the features that we have been found to be beneficial. </p>

<p>This is where crossover comes in; by combining the genomes of Individuals 2 and 3, we may find some new individual which produces a mixture of their performances. While it is possible that such an individual could be produced by a series of mutations applied to Individual 2 or Individual 3, the landscape may simply not suit this (there may be no favorable mutations in that direction, for example).</p>

<p><a href=""https://i.stack.imgur.com/bsVBEm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bsVBEm.png"" alt=""enter image description here""></a></p>

<p>You are partially right therefore; it may sometimes be the case that the benefits of crossover could be replicated with a series of mutations. Sometimes this may not be the case and crossover may smooth the fitness landscape of your GA, speeding up optimization and helping your GA escape local optima. </p>
",,3,2016-08-11T14:45:52.773,,1551,2016-08-11T15:38:11.613,2016-08-11T15:38:11.613,,8.0,,1467.0,1541.0,2,2,,,,48.57,11.38,9.77,0.0,0.0,32.0,When thinking about crossover its important to think about the fitness landscape Consider a hypothetical scenario where we are applying a genetic algorithm to find a solution that performs well at 2 tasks This could be from Francks example moving and shooting for an AI or perhaps it could be predicted 2 outputs in a genetic machine learning scenario but really most scenarios where GAs are applied are synonymous even at solving a single task there may be different aspects of the task to be addressed Suppose we had an individual 1 that was performing reasonably well at both tasks and we found a series of mutations which produced 2 new individuals 2 and 3 which performed better than Individual 1 at tasks 1 and 2 respectively Now while both of these are improvements ideally we want to find a generally good solution so we want to combine the features that we have been found to be beneficial This is where crossover comes in by combining the genomes of Individuals 2 and 3 we may find some new individual which produces a mixture of their performances While it is possible that such an individual could be produced by a series of mutations applied to Individual 2 or Individual 3 the landscape may simply not suit this there may be no favorable mutations in that direction for example You are partially right therefore it may sometimes be the case that the benefits of crossover could be replicated with a series of mutations Sometimes this may not be the case and crossover may smooth the fitness landscape of your GA speeding up optimization and helping your GA escape local optima
,,"<p>For a more intelligent approach than random or exhaustive searches, you could try a genetic algorithm such as NEAT <a href=""http://nn.cs.utexas.edu/?neat"">http://nn.cs.utexas.edu/?neat</a>. However, this has no guarantee to find a global optima, it is simply an optimization algorithm based on performance and is therefore vulnerable to getting stuck in a local optima. </p>
",,1,2016-08-11T14:50:04.153,,1552,2016-08-11T14:50:04.153,,,,,1467.0,4.0,2,5,,,,37.64,15.03,11.51,0.0,0.0,13.0,For a more intelligent approach than random or exhaustive searches you could try a genetic algorithm such as NEAT httpnncsutexaseduneat However this has no guarantee to find a global optima it is simply an optimization algorithm based on performance and is therefore vulnerable to getting stuck in a local optima
,,"<blockquote>
  <p>Does this addition of curosity changes clarifai into a true AI?</p>
</blockquote>

<p>As per my answer to <a href=""http://ai.stackexchange.com/questions/1420/how-close-are-we-to-creating-ex-machina"">this question</a>, we don't know what the ingredients for a 'true AI' are. Via the Turing Test and its variants, the best we can do is ""know one when we see one"".</p>

<p>Curiosity certainly appears <em>necessary</em> for intelligence, though it doesn't seem <em>sufficient</em> - a lemming-like creature curious to see what's at the bottom of a steep cliff might not survive long enough to learn caution, even if it had the learning mechanisms to do so.</p>

<p>Here is some work by Schmidhuber on <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.3978&amp;rep=rep1&amp;type=pdf"">Artificial Curiousity</a>. </p>

<p><a href=""http://www.pyoudeyer.com/active-learning-and-artificial-curiosity-in-robots/"">Pierre-Yves Oudeyer</a> has also done quite a lot of work on this and Active Learning/Intrinsic motivation.</p>
",,0,2016-08-11T14:59:11.393,,1553,2016-08-11T14:59:11.393,,,,,42.0,1544.0,2,5,,,,60.35,10.44,9.36,0.0,0.0,21.0,Does this addition of curosity changes clarifai into a true AI As per my answer to this question we dont know what the ingredients for a true AI are Via the Turing Test and its variants the best we can do is know one when we see one Curiosity certainly appears necessary for intelligence though it doesnt seem sufficient a lemminglike creature curious to see whats at the bottom of a steep cliff might not survive long enough to learn caution even if it had the learning mechanisms to do so Here is some work by Schmidhuber on Artificial Curiousity PierreYves Oudeyer has also done quite a lot of work on this and Active LearningIntrinsic motivation
,,,,0,2016-08-11T17:33:03.950,,1554,2016-08-11T17:33:03.950,2016-08-11T17:33:03.950,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,"For questions about an artificial neural network (ANN), a network inspired by biological networks, which are used to estimate or approximate functions.",,0,2016-08-11T17:33:03.950,,1555,2016-08-30T19:43:59.627,2016-08-30T19:43:59.627,,29.0,,145.0,,4,0,,,,23.77,17.0,13.34,0.0,0.0,5.0,For questions about an artificial neural network ANN a network inspired by biological networks which are used to estimate or approximate functions
,,,,0,2016-08-11T17:34:49.537,,1556,2016-08-11T17:34:49.537,2016-08-11T17:34:49.537,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions about the image-recognition abilities of AI.,,0,2016-08-11T17:34:49.537,,1557,2016-08-11T18:11:48.863,2016-08-11T18:11:48.863,,145.0,,145.0,,4,0,,,,21.06,17.65,11.93,0.0,0.0,2.0,For questions about the imagerecognition abilities of AI
,,,,0,2016-08-11T17:52:48.283,,1558,2016-08-11T17:52:48.283,2016-08-11T17:52:48.283,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions relating to self-driving-vehicles.,,0,2016-08-11T17:52:48.283,,1559,2016-08-11T18:11:42.990,2016-08-11T18:11:42.990,,145.0,,145.0,,4,0,,,,7.18,29.32,13.36,0.0,0.0,3.0,For questions relating to selfdrivingvehicles
1577.0,1.0,"<p>Based on this <a href=""http://www.dailymail.co.uk/sciencetech/article-3677950/Google-s-self-driving-cars-spot-cyclists-Sensors-read-hand-signals-predict-riders-behavior.html"" rel=""nofollow"">article</a>, Google's self-driving cars can spot cyclists, cars, road signs, markings, traffic lights, and pedestrians.</p>

<p>How exactly does it identify pedestrians? Is it based on face recognition, shape, size, distance, infrared signature?</p>
",2016-08-13T00:13:05.263,10,2016-08-11T18:06:34.053,,1560,2016-08-13T16:39:19.730,2016-08-13T16:39:19.730,,1504.0,,8.0,,1,0,<self-driving><cars><object-recognition>,How does Google's self-driving car identify pedestrians?,95.0,51.14,16.86,11.44,0.0,0.0,15.0,Based on this article Googles selfdriving cars can spot cyclists cars road signs markings traffic lights and pedestrians How exactly does it identify pedestrians Is it based on face recognition shape size distance infrared signature
1578.0,1.0,"<p>In <a href=""https://www.technologyreview.com/s/530276/hidden-obstacles-for-googles-self-driving-cars/"">Hidden Obstacles for Google’s Self-Driving Cars</a> article we can read that:</p>

<blockquote>
  <p>Google’s cars can detect and respond to stop signs that aren’t on its map, a feature that was introduced to deal with temporary signs used at construction sites.</p>
  
  <p>Google says that its cars can identify almost all unmapped stop signs, and would remain safe if they miss a sign because the vehicles are always looking out for traffic, pedestrians and other obstacles.</p>
</blockquote>

<p>What would happen if a car spotted somebody in front of it (but not on the collision path) wearing a T-shirt that has a stop sign printed on it. Would it react and stop the car?</p>
",,9,2016-08-11T18:18:27.973,,1561,2016-08-29T17:29:29.030,2016-08-11T19:50:36.397,,145.0,,8.0,,1,9,<self-driving><decision-theory><cars><object-recognition>,Would Google's self-driving-car stop when it sees somebody with a T-shirt with a stop sign printed on it?,234.0,60.48,9.93,9.74,0.0,0.0,12.0,In Hidden Obstacles for Google’s SelfDriving Cars article we can read that Google’s cars can detect and respond to stop signs that aren’t on its map a feature that was introduced to deal with temporary signs used at construction sites Google says that its cars can identify almost all unmapped stop signs and would remain safe if they miss a sign because the vehicles are always looking out for traffic pedestrians and other obstacles What would happen if a car spotted somebody in front of it but not on the collision path wearing a Tshirt that has a stop sign printed on it Would it react and stop the car
,,"<p>If the system claims that a piece of code has violated standards, then to be useful to the programmer, it really needs to provide more information than just a 'yes/no' classifier: you need some form of explanation about <em>why</em> it is claimed to be wrong.</p>

<p>Clearly ANNs aren't much use for that.</p>

<p><em>If</em> I were tackling such a problem (and my suspicion is that a lot of effort could be spent trying and failing to reproduce coding standards which are already well-understood), then my inclination would be to use a more explicitly rule-based representation.</p>

<p>Possibilities include:</p>

<ul>
<li><p>Genetic Programming</p></li>
<li><p>Learning Classifier Systems </p></li>
<li>Decision Trees</li>
</ul>

<p>The ever-useful <a href=""https://cs.gmu.edu/~sean/book/metaheuristics"" rel=""nofollow"">""Essentials of Metaheuristics""</a> has a whole section on the evolution of rulesets. Obviously, nothing prevents you from initializing the evolutionary process with rules known to be useful.</p>

<p>As I point out <a href=""http://ai.stackexchange.com/questions/1420/how-close-are-we-to-creating-ex-machina"">here</a>, with our current AI algorithms, the success of an  approach is very sensitive to human expertise/effort in feature selection/preprocessing, choice of training set etc, so creative experiment with this is vital.</p>

<p>Training set: how about two sets of negative and positive examples, consisting of (features extracted from) bad code and from a refactored version (respectively)?</p>

<p>One elementary choice of features would be to apply a bunch of code complexity metrics and have the learning algorithm combine those. The plus side of working in such a numeric domain is that the learning algorithm might readily find a gradient to exploit. The downside is that the rules (which are then likely of the form <code>if mcabe &gt; 2.8</code> etc) are still not as informative as might be desired.</p>

<p>For more complex rules (e.g. requiring <code>if elseif else</code>) you may want to extract your features from the abstract syntax tree. You could in principle use the entire tree but to my knowledge <a href=""http://jmlr.csail.mit.edu/papers/volume11/vishwanathan10a/vishwanathan10a.pdf"" rel=""nofollow"">ML on graph and tree structures</a> is still in relative infancy.</p>
",,0,2016-08-11T18:38:13.963,,1562,2016-08-11T18:52:07.540,2016-08-11T18:52:07.540,,42.0,,42.0,1540.0,2,3,,,,48.03,12.83,10.31,32.0,0.0,47.0,If the system claims that a piece of code has violated standards then to be useful to the programmer it really needs to provide more information than just a yesno classifier you need some form of explanation about why it is claimed to be wrong Clearly ANNs arent much use for that If I were tackling such a problem and my suspicion is that a lot of effort could be spent trying and failing to reproduce coding standards which are already wellunderstood then my inclination would be to use a more explicitly rulebased representation Possibilities include Genetic Programming Learning Classifier Systems Decision Trees The everuseful Essentials of Metaheuristics has a whole section on the evolution of rulesets Obviously nothing prevents you from initializing the evolutionary process with rules known to be useful As I point out here with our current AI algorithms the success of an approach is very sensitive to human expertiseeffort in feature selectionpreprocessing choice of training set etc so creative experiment with this is vital Training set how about two sets of negative and positive examples consisting of features extracted from bad code and from a refactored version respectively One elementary choice of features would be to apply a bunch of code complexity metrics and have the learning algorithm combine those The plus side of working in such a numeric domain is that the learning algorithm might readily find a gradient to exploit The downside is that the rules which are then likely of the form etc are still not as informative as might be desired For more complex rules eg requiring you may want to extract your features from the abstract syntax tree You could in principle use the entire tree but to my knowledge ML on graph and tree structures is still in relative infancy
,,,,0,2016-08-11T18:53:50.527,,1563,2016-08-11T18:53:50.527,2016-08-11T18:53:50.527,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions relating to AI's programming computers. NOT FOR THE PROGRAMMING OF THE AI'S THEMSELVES.,,0,2016-08-11T18:53:50.527,,1564,2016-08-11T19:44:14.377,2016-08-11T19:44:14.377,,145.0,,145.0,,4,0,,,,63.86,13.99,10.32,0.0,0.0,4.0,For questions relating to AIs programming computers NOT FOR THE PROGRAMMING OF THE AIS THEMSELVES
,,,,0,2016-08-11T19:18:28.780,,1565,2016-08-11T19:18:28.780,2016-08-11T19:18:28.780,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For asking about an aspect of the history of AI.,,0,2016-08-11T19:18:28.780,,1566,2016-08-12T06:40:13.047,2016-08-12T06:40:13.047,,145.0,,145.0,,4,0,,,,78.25,3.86,10.45,0.0,0.0,1.0,For asking about an aspect of the history of AI
,1.0,"<p><sub> This is a scope experiment. </sub></p>

<hr>

<p>After Google/Tesla/whoever else is making self-driving cars finishes perfecting them, will they replace the cars with human drivers, so that there are only self-driving cars?</p>

<p>If they do, it would probably make the roads safer.</p>
",2016-08-12T06:39:38.000,6,2016-08-11T19:56:31.223,,1567,2016-08-12T01:42:05.757,2016-08-11T20:06:42.147,,145.0,,145.0,,1,1,<self-driving><cars>,"Once self-driving cars are perfected, will they replace old-fashioned cars?",46.0,57.98,13.33,9.43,0.0,0.0,10.0,This is a scope experiment After GoogleTeslawhoever else is making selfdriving cars finishes perfecting them will they replace the cars with human drivers so that there are only selfdriving cars If they do it would probably make the roads safer
1584.0,3.0,"<p>Significant AI vs human board game matches include:</p>

<ul>
<li><strong>chess</strong>: <a href=""https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)#Deep_Blue_versus_Kasparov"" rel=""nofollow"">Deep Blue vs Kasparov</a> in 1996,</li>
<li><strong>Go</strong>: <a href=""https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol"" rel=""nofollow"">DeepMind AlphaGo vs Lee Sedol</a> in 2016,</li>
</ul>

<p>which demonstrated that AI challenged and defeated professional players.</p>

<p>Are there known board games left where a human can still win against an AI? I mean based on the final outcome of authoritative famous matches, where there is still same board game where AI cannot beat a world champion of that game.</p>
",,2,2016-08-11T20:11:33.013,,1568,2016-08-12T15:29:14.740,2016-08-11T20:15:58.067,,145.0,,8.0,,1,3,<history><challenges><game-theory>,Is there any board game where a human can still beat an AI?,206.0,63.02,11.09,9.51,0.0,0.0,9.0,Significant AI vs human board game matches include chess Deep Blue vs Kasparov in 1996 Go DeepMind AlphaGo vs Lee Sedol in 2016 which demonstrated that AI challenged and defeated professional players Are there known board games left where a human can still win against an AI I mean based on the final outcome of authoritative famous matches where there is still same board game where AI cannot beat a world champion of that game
,,"<p>Most of the algorithms (based on <a href=""http://arxiv.org/abs/1603.01768"" rel=""nofollow"">image synthesis and style transfer</a>, e.g. <a href=""https://github.com/alexjc/neural-doodle"" rel=""nofollow"">neural-doodle</a>) haven't been proven to be highly effective in terms of real-time image processing.</p>

<p>However the following studies discusses such algorithms for real-time texture synthesis:</p>

<ul>
<li><p><a href=""http://arxiv.org/abs/1603.03417"" rel=""nofollow"">Feed-forward Synthesis of Textures and Stylized Images</a></p>

<p>The approach is to move the computational burden to a learning stage, making trained network (<a href=""https://en.wikipedia.org/wiki/Convolutional_neural_network"" rel=""nofollow"">CNN</a>) light-weight and compact in order to generate multiple samples of the same texture. This can generate textures as good as comparable to <a href=""http://arxiv.org/abs/1505.07376"" rel=""nofollow"">Gatys~et~al</a>, but significantly faster.</p></li>
<li><p><a href=""http://arxiv.org/abs/1603.08155"" rel=""nofollow"">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a></p>

<p>This method uses parallel work which can generate high-quality images by defining and optimizing loss functions based on high-level features extracted from pretrained networks.</p></li>
<li><p><a href=""http://arxiv.org/abs/1604.04382"" rel=""nofollow"">Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks</a></p>

<p>This uses precomputed feed-forward networks that captures the feature statistics of <a href=""http://www.irisa.fr/vista/Papers/2008_LNLA_Pecot.pdf"" rel=""nofollow"">Markovian patches</a> in order to generate outputs of arbitrary dimensions. This can be applied to texture synthesis, style transfer and video stylization.</p></li>
</ul>

<p><sup>Source: Above list suggested on <a href=""https://github.com/alexjc/neural-doodle"" rel=""nofollow"">neural-doodle</a> project.</sup></p>
",,0,2016-08-11T20:39:12.370,,1569,2016-08-11T20:49:24.500,2016-08-11T20:49:24.500,,8.0,,8.0,1537.0,2,1,,,,24.88,19.03,12.99,0.0,0.0,34.0,Most of the algorithms based on image synthesis and style transfer eg neuraldoodle havent been proven to be highly effective in terms of realtime image processing However the following studies discusses such algorithms for realtime texture synthesis Feedforward Synthesis of Textures and Stylized Images The approach is to move the computational burden to a learning stage making trained network CNN lightweight and compact in order to generate multiple samples of the same texture This can generate textures as good as comparable to Gatysetal but significantly faster Perceptual Losses for RealTime Style Transfer and SuperResolution This method uses parallel work which can generate highquality images by defining and optimizing loss functions based on highlevel features extracted from pretrained networks Precomputed RealTime Texture Synthesis with Markovian Generative Adversarial Networks This uses precomputed feedforward networks that captures the feature statistics of Markovian patches in order to generate outputs of arbitrary dimensions This can be applied to texture synthesis style transfer and video stylization Source Above list suggested on neuraldoodle project
,0.0,"<p>I'm trying to teach an AI different pattern of tic tac toe to recognize wether a given pattern represents a win or not.</p>

<p>Unfortunately it's not learning to recognize them correctly and I think may way of representing/encoding the game into vectors is wrong.</p>

<p>I choose a way that is easy for an human (me, in particular!) to make sense of:</p>

<pre><code>training_data = np.array([[0,0,0,
                           0,0,0,
                           0,0,0],
                          [0,0,1,
                           0,1,0,
                           0,0,1],
                          [0,0,1,
                           0,1,0,
                           1,0,0],
                          [0,1,0,
                           0,1,0,
                           0,1,0]], ""float32"")
target_data = np.array([[0],[0],[1],[1]], ""float32"")
</code></pre>

<p>This basically just use an array of length 9 to represent a 3 x 3 board. The first three items represent the first row, the next three the second row and so on. The line breaks should make it obvious I guess.</p>

<p>The target data then maps the first two game states to ""no wins"" and the last two game states to ""wins"".</p>

<p>Then I wanted to create some validation data that is slightly different to see if it generalizes.</p>

<pre><code>validation_data = np.array([[0,0,0,
                             0,0,0,
                             0,0,0],
                            [1,0,0,
                             0,1,0,
                             1,0,0],
                            [1,0,0,
                             0,1,0,
                             0,0,1],
                            [0,0,1,
                             0,0,1,
                             0,0,1]], ""float32"")
</code></pre>

<p>Obviously, again the last two game states should be ""wins"" whereas the first two should not.</p>

<p>I tried to play with the number of neurons and learning rate but no matter what I try, my output looks pretty of. E.g.</p>

<pre><code>[[ 0.01207292]
 [ 0.98913926]
 [ 0.00925775]
 [ 0.00577191]]
</code></pre>

<p>I tend to think it's the way how I represent the game state that may be wrong but actually I have no idea :D</p>

<p>Can anyone help me out here?</p>

<p>This is the entire code that I use</p>

<pre><code>import numpy as np
from keras.models import Sequential
from keras.layers.core import Activation, Dense
from keras.optimizers import SGD

training_data = np.array([[0,0,0,
                           0,0,0,
                           0,0,0],
                          [0,0,1,
                           0,1,0,
                           0,0,1],
                          [0,0,1,
                           0,1,0,
                           1,0,0],
                          [0,1,0,
                           0,1,0,
                           0,1,0]], ""float32"")

target_data = np.array([[0],[0],[1],[1]], ""float32"")

validation_data = np.array([[0,0,0,
                             0,0,0,
                             0,0,0],
                            [1,0,0,
                             0,1,0,
                             1,0,0],
                            [1,0,0,
                             0,1,0,
                             0,0,1],
                            [0,0,1,
                             0,0,1,
                             0,0,1]], ""float32"")

model = Sequential()
model.add(Dense(2, input_dim=9, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))

sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='mean_squared_error', optimizer=sgd)

history = model.fit(training_data, target_data, nb_epoch=10000, batch_size=4, verbose=0)

print(model.predict(validation_data))
</code></pre>
",2016-08-11T22:37:41.383,9,2016-08-11T21:05:07.427,,1570,2016-08-11T21:51:38.780,2016-08-11T21:51:38.780,,8.0,,1334.0,,1,6,<classification><keras>,Why does my NN not classify these tic tac toe pattern correctly?,57.0,78.38,6.96,7.42,2417.0,0.0,31.0,Im trying to teach an AI different pattern of tic tac toe to recognize wether a given pattern represents a win or not Unfortunately its not learning to recognize them correctly and I think may way of representingencoding the game into vectors is wrong I choose a way that is easy for an human me in particular to make sense of This basically just use an array of length 9 to represent a 3 x 3 board The first three items represent the first row the next three the second row and so on The line breaks should make it obvious I guess The target data then maps the first two game states to no wins and the last two game states to wins Then I wanted to create some validation data that is slightly different to see if it generalizes Obviously again the last two game states should be wins whereas the first two should not I tried to play with the number of neurons and learning rate but no matter what I try my output looks pretty of Eg I tend to think its the way how I represent the game state that may be wrong but actually I have no idea D Can anyone help me out here This is the entire code that I use
,,"<p>Artificially intelligent computer programs should be able to be at the same level or beat humans at every game that we play.  This is because games follow rules that are scriptable, and <a href=""http://www.aaai.org/ojs/index.php/aimagazine/article/view/2310"" rel=""nofollow"">artificial intelligence</a> is designed to focus on one specific game and learn from its failures.  The difference between humans and artificial intelligence is that artificial intelligence focuses on one specific task like learning to master Go while our brain is dedicated to mastering multiple tasks like...living.  Even Arimaa, a game designed to be difficult for artificially intelligent systems was beaten by a bot called Sharp: <a href=""https://en.wikipedia.org/wiki/Arimaa"" rel=""nofollow"">https://en.wikipedia.org/wiki/Arimaa</a>.  </p>
",,3,2016-08-11T21:38:07.100,,1571,2016-08-11T21:38:07.100,,,,,37.0,1568.0,2,2,,,,21.23,15.79,10.81,0.0,0.0,17.0,Artificially intelligent computer programs should be able to be at the same level or beat humans at every game that we play This is because games follow rules that are scriptable and artificial intelligence is designed to focus on one specific game and learn from its failures The difference between humans and artificial intelligence is that artificial intelligence focuses on one specific task like learning to master Go while our brain is dedicated to mastering multiple tasks likeliving Even Arimaa a game designed to be difficult for artificially intelligent systems was beaten by a bot called Sharp httpsenwikipediaorgwikiArimaa
,,"<p>See: <a href=""https://keras.io/"" rel=""nofollow"">Keras Documentation</a></p>
",,0,2016-08-11T21:53:49.320,,1572,2016-08-12T15:18:31.983,2016-08-12T15:18:31.983,,8.0,,8.0,,5,0,,,,0.75,16.95,14.31,0.0,0.0,1.0,See Keras Documentation
,,"Highly modular neural networks library written in Python, capable of running either TensorFlow or Theano.",,0,2016-08-11T21:53:49.320,,1573,2016-08-12T15:18:36.190,2016-08-12T15:18:36.190,,8.0,,8.0,,4,0,,,,30.87,17.33,17.01,0.0,0.0,2.0,Highly modular neural networks library written in Python capable of running either TensorFlow or Theano
,,"<p>Not all games (or even board games) are computationally algorithmic. Even the least skilled player is likely to trounce the hottest pattern-matching algorithm in a game of <strong><a href=""https://en.wikipedia.org/wiki/Pictionary"" rel=""nofollow noreferrer"">Pictionary</a></strong> (for example).</p>

<p><img src=""https://i.stack.imgur.com/RDIuC.png"" alt=""""></p>

<p>If you want to say that the movement of pieces upon successful completion of a task is only ancelary to the object of the game, than your answer will be largely self-selecting. A sufficiently sophisticated algorithm will brute force a computational problem better than human intuition&hellip; <a href=""https://en.wikipedia.org/wiki/AI_effect"" rel=""nofollow noreferrer""><strong>eventually.</strong></a></p>
",,2,2016-08-11T22:48:53.950,,1574,2016-08-12T14:22:43.490,2016-08-12T14:22:43.490,,95.0,,95.0,1568.0,2,7,,,,35.07,14.79,11.15,0.0,0.0,13.0,Not all games or even board games are computationally algorithmic Even the least skilled player is likely to trounce the hottest patternmatching algorithm in a game of Pictionary for example If you want to say that the movement of pieces upon successful completion of a task is only ancelary to the object of the game than your answer will be largely selfselecting A sufficiently sophisticated algorithm will brute force a computational problem better than human intuitionhellip eventually
,,"<p>In the real world, decisions will be made based on the law, and <a href=""http://law.stackexchange.com/questions/1639/what-is-the-legal-take-on-the-trolley-problem"">as noted over on Law.SE</a>, the law generally favors inaction over action. </p>
",,1,2016-08-11T23:08:24.670,,1575,2016-08-11T23:08:24.670,,,,,1414.0,111.0,2,7,,,,75.71,7.58,9.31,0.0,0.0,5.0,In the real world decisions will be made based on the law and as noted over on LawSE the law generally favors inaction over action
,,"<p>It likely to be happen, because it's more convenient that way. In general people, organizations and government are always keen to make things more efficient by standarizing things (computers, technology, law, science, etc.) in order to make it manageable and predictable to reduce the time and minimalize the risk of the same mistakes.</p>

<p>The whole world now moves into technological advancement where automation of everything is where we are going, so we can manage complexities in more reliable way, so we can focus on much bigger picture. This includes technology such as mobiles, computers, UAV (delivery drones), robots and now self-driving cars.</p>

<p>The pros of that change would be:</p>

<ul>
<li>To have safer streets by introducing autonomous cars on the road.</li>
<li>To have fewer drunk, tired, drugged or crazy drivers.</li>
<li>To avoid poor weather conditions.</li>
<li><p>To reduce <a href=""https://en.wikipedia.org/wiki/Braking_distance"" rel=""nofollow noreferrer"">braking distances</a> by dropping driver's reaction time and predicting dangerous situations much earlier.</p>

<p><a href=""http://www.cyberphysics.co.uk/topics/forces/stopping_distance.htm"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cz9yP.png"" alt=""Typical stopping distance""></a></p>

<p><sup>Source: <a href=""http://www.cyberphysics.co.uk/topics/forces/stopping_distance.htm"" rel=""nofollow noreferrer"">Cyber Physics</a></sup></p></li>
<li><p>Reducing car deaths and costs of GNP.</p>

<blockquote>
  <p>An estimated 1.3 million people die on the world's roads every year with around 50 million injured or disabled by accidents, with accidents costing countries up to four per cent of their Gross National Product (GNP) yearly. - <a href=""http://www.un.org/apps/news/story.asp?NewsID=36823"" rel=""nofollow noreferrer"">UN News Centre</a></p>
</blockquote></li>
<li><p>To have central point of safety improvements, you cannot change people, but you can fix the known safety issue on global scale.</p></li>
<li>To introduce global standards from the central point (e.g. new law to which manufactures needs to apply).</li>
<li>To increase <a href=""http://www.slideshare.net/sbishop2/p22-car-design-safety"" rel=""nofollow noreferrer"">car safety</a> in general on larger scale.</li>
<li>And so on.</li>
</ul>

<hr>

<h3>Why we need the 'only self-driving cars'?</h3>

<p>The <a href=""http://www.un.org/apps/news/story.asp?NewsID=36823"" rel=""nofollow noreferrer"">Secretary-General said the UN</a> would work hard to prevent further deaths on the roads:</p>

<blockquote>
  <p>Many tragedies can be avoided through a set of proven, simple measures that benefit not only individuals and families but society at large.</p>
</blockquote>

<p>Here are my points:</p>

<ul>
<li>To achieve 'a set of proven measures' - do not allow people to drive - simple.</li>
<li>People tend to break the rules, always, so do not allow them to drive without permission.</li>
<li>Reduce stealing cars and other crime.</li>
<li>Law enforcement dream is to able to stop any car on demand.</li>
<li>Do not allow drunk people to drive a car.</li>
<li>Disallow terrorist attacks, like in <a href=""https://en.wikipedia.org/wiki/2016_Nice_attack"" rel=""nofollow noreferrer"">Nice where truck killed over 80 people</a>.</li>
<li>Avoid bank robberies and similar which are possible by escaping fast cars.</li>
</ul>

<p>Is it possible? I believe it depends on specific countries and unions and how quickly we're able to advance and be ready for such change.</p>

<p>To support above points and summarize the 'only self-driving cars' point, please see below references which shows that this is already happening:</p>

<ul>
<li>2014: <a href=""http://www.theverge.com/2014/5/28/5758734/uber-will-eventually-replace-all-its-drivers-with-self-driving-cars"" rel=""nofollow noreferrer"">Uber will eventually replace all its drivers with self-driving cars</a></li>
<li><p>2016: <a href=""http://www.dezeen.com/2016/02/12/google-self-driving-car-artficial-intelligence-system-recognised-as-driver-usa/"" rel=""nofollow noreferrer"">Google's self-driving car system has been officially recognised as a driver in the US.</a></p>

<blockquote>
  <p>The move is seen as a first step towards changing the law for cars that have ""no need for a human driver"".</p>
</blockquote></li>
<li><p>2016: <a href=""http://www.dezeen.com/2016/04/19/beverly-hills-replace-public-transport-driverless-cars-los-angeles/"" rel=""nofollow noreferrer"">Beverly Hills to replace public transport with self-driving cars</a></p></li>
<li><p>2016: <a href=""http://www.sfexaminer.com/sf-pitches-149-million-plan-replace-cars-self-driving-vehicles/"" rel=""nofollow noreferrer"">San Francisco pitches $149 million plan to replace cars with self-driving vehicles</a></p>

<blockquote>
  <p>San Francisco’s future is autonomous and shared vehicles – and that future may be only a decade away.</p>
</blockquote></li>
<li><p>2016: <a href=""http://spectrum.ieee.org/cars-that-think/transportation/self-driving/otto-selfdriving-truck-company-wants-to-replace-teamsters"" rel=""nofollow noreferrer"">Otto Self-Driving Truck Company Wants to Replace Teamsters</a></p></li>
</ul>
",,3,2016-08-12T01:42:05.757,,1576,2016-08-12T01:42:05.757,,,,,8.0,1567.0,2,0,,,,63.29,11.77,9.74,0.0,0.0,97.0,It likely to be happen because its more convenient that way In general people organizations and government are always keen to make things more efficient by standarizing things computers technology law science etc in order to make it manageable and predictable to reduce the time and minimalize the risk of the same mistakes The whole world now moves into technological advancement where automation of everything is where we are going so we can manage complexities in more reliable way so we can focus on much bigger picture This includes technology such as mobiles computers UAV delivery drones robots and now selfdriving cars The pros of that change would be To have safer streets by introducing autonomous cars on the road To have fewer drunk tired drugged or crazy drivers To avoid poor weather conditions To reduce braking distances by dropping drivers reaction time and predicting dangerous situations much earlier Source Cyber Physics Reducing car deaths and costs of GNP An estimated 13 million people die on the worlds roads every year with around 50 million injured or disabled by accidents with accidents costing countries up to four per cent of their Gross National Product GNP yearly UN News Centre To have central point of safety improvements you cannot change people but you can fix the known safety issue on global scale To introduce global standards from the central point eg new law to which manufactures needs to apply To increase car safety in general on larger scale And so on Why we need the only selfdriving cars The SecretaryGeneral said the UN would work hard to prevent further deaths on the roads Many tragedies can be avoided through a set of proven simple measures that benefit not only individuals and families but society at large Here are my points To achieve a set of proven measures do not allow people to drive simple People tend to break the rules always so do not allow them to drive without permission Reduce stealing cars and other crime Law enforcement dream is to able to stop any car on demand Do not allow drunk people to drive a car Disallow terrorist attacks like in Nice where truck killed over 80 people Avoid bank robberies and similar which are possible by escaping fast cars Is it possible I believe it depends on specific countries and unions and how quickly were able to advance and be ready for such change To support above points and summarize the only selfdriving cars point please see below references which shows that this is already happening 2014 Uber will eventually replace all its drivers with selfdriving cars 2016 Googles selfdriving car system has been officially recognised as a driver in the US The move is seen as a first step towards changing the law for cars that have no need for a human driver 2016 Beverly Hills to replace public transport with selfdriving cars 2016 San Francisco pitches 149 million plan to replace cars with selfdriving vehicles San Francisco’s future is autonomous and shared vehicles – and that future may be only a decade away 2016 Otto SelfDriving Truck Company Wants to Replace Teamsters
,,"<p>The AI of the car uses sensor data to process all the data and classifies objects <strong>based on the size, shape and movement patterns</strong>. It can recognize surroundings from a 360 degree perspective by making predictions about vehicles, people and objects around it will move.</p>

<p>It can detect pedestrians, but as moving, <strong>column-shaped blurs of pixels</strong>, so it really cannot tell whether it's a rock or a crumpled piece of paper.</p>

<p><img src=""https://media.giphy.com/media/3oEduYqb4Ty6dSReKc/giphy-downsized-large.gif"" alt=""Google&#39;s self-driving car sees traffic""></p>

<p>However it is programmed to determine certain patterns when a police officer has halted traffic or the car is being signaled to move forward.</p>

<p><img src=""https://media.giphy.com/media/l41lGfjhDlrSE9ILS/giphy-downsized-large.gif"" alt=""Google&#39;s self-driving car determines when a police officer has halted traffic or when the car is being signaled to move forward""></p>

<p>It also recognizes cyclists as objects outlined in red and can slow down to let the cyclist enter into a lane.</p>

<p><img src=""https://media.giphy.com/media/3oEduTuU46cDCGGuC4/giphy-downsized-large.gif"" alt=""Google&#39;s self-driving car sees when a cyclist is trying to merge into a lane, the vehicle also knows to slow down and let the cyclist enter""></p>

<p><sup>Above images are provided by Chris Urmson who heads up Google's driverless car program.</sup></p>

<p>Sources:</p>

<ul>
<li><a href=""http://www.techinsider.io/how-googles-self-driving-cars-see-the-world-2015-10/#then-it-uses-its-sensor-data-to-understand-what-it-sees-in-the-moment-the-software-processes-all-of-the-data-and-classifies-objects-based-on-size-shape-and-movement-patterns-2"" rel=""nofollow"">How Google's self-driving cars see the world</a></li>
<li><a href=""https://www.technologyreview.com/s/530276/hidden-obstacles-for-googles-self-driving-cars/"" rel=""nofollow"">Hidden Obstacles for Google’s Self-Driving Cars</a></li>
<li>(video) <a href=""https://www.youtube.com/watch?v=tiwVMrTLUWg"" rel=""nofollow"">Chris Urmson: How a driverless car sees the road</a></li>
</ul>
",,4,2016-08-12T02:30:20.793,,1577,2016-08-13T02:51:10.783,2016-08-13T02:51:10.783,,8.0,,8.0,1560.0,2,2,,,,57.61,11.43,10.98,0.0,0.0,21.0,The AI of the car uses sensor data to process all the data and classifies objects based on the size shape and movement patterns It can recognize surroundings from a 360 degree perspective by making predictions about vehicles people and objects around it will move It can detect pedestrians but as moving columnshaped blurs of pixels so it really cannot tell whether its a rock or a crumpled piece of paper However it is programmed to determine certain patterns when a police officer has halted traffic or the car is being signaled to move forward It also recognizes cyclists as objects outlined in red and can slow down to let the cyclist enter into a lane Above images are provided by Chris Urmson who heads up Googles driverless car program Sources How Googles selfdriving cars see the world Hidden Obstacles for Google’s SelfDriving Cars video Chris Urmson How a driverless car sees the road
,,"<p>Google’s self-driving car most likely uses <a href=""https://viejournal.springeropen.com/articles/10.1186/s40327-015-0027-1"" rel=""nofollow noreferrer"">mapping of traffic signs using google street view images for roadway inventory management</a>. If traffic signs are not in its database, it can still “see” and detect moving objects which can be distinguished from the presence of certain stationary objects, like traffic lights. So its software can classify objects based on the size, shape and movement patterns. Therefore it is highly unlikely that a person would be mistaken for a traffic sign. See: <a href=""http://ai.stackexchange.com/q/1560/8"">How does Google&#39;s self-driving car identify pedestrians?</a></p>

<p><a href=""https://www.technologyreview.com/s/530276/hidden-obstacles-for-googles-self-driving-cars/"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/setJb.png"" alt=""enter image description here""></a></p>

<p><sup>Image: Technology Review</sup></p>

<p>To support such a claim, <a href=""http://www.cs.cmu.edu/~illah/"" rel=""nofollow noreferrer"">Illah Nourbakhsh</a>, a professor of robotics at Carnegie Mellon University, gave an interview to the New York Times magazine cover story on autonomous driving cars, and includes this hypothetical scenario, saying:</p>

<blockquote>
  <p>If they’re outside walking, and the sun is at just the right glare level, and there’s a mirrored truck stopped next to you, and the sun bounces off that truck and hits the guy so that you can’t see his face anymore — well, now your car just sees a stop sign. <strong>The chances of all that happening are diminishingly small — it’s very, very unlikely</strong> — but the problem is we will have millions of these cars. The very unlikely will happen all the time.</p>
</blockquote>

<p>Even so, the risk would be minimal, since the car is always looking out for traffic, pedestrians and other obstacles.</p>

<p>Sources:</p>

<ul>
<li><a href=""http://www.techinsider.io/how-googles-self-driving-cars-see-the-world-2015-10/#googles-self-driving-vehicles-first-establish-their-location-by-using-mapping-and-sensor-data-1"" rel=""nofollow noreferrer"">How Google's self-driving cars see the world</a></li>
<li><a href=""http://www.nytimes.com/2015/11/15/magazine/the-dream-life-of-driverless-cars.html"" rel=""nofollow noreferrer"">The Dream Life of Driverless Cars</a> at The New York Times</li>
</ul>
",,0,2016-08-12T02:48:59.620,,1578,2016-08-29T17:29:29.030,2016-08-29T17:29:29.030,,145.0,,8.0,1561.0,2,3,,,,54.56,11.73,10.06,0.0,0.0,36.0,Google’s selfdriving car most likely uses mapping of traffic signs using google street view images for roadway inventory management If traffic signs are not in its database it can still “see” and detect moving objects which can be distinguished from the presence of certain stationary objects like traffic lights So its software can classify objects based on the size shape and movement patterns Therefore it is highly unlikely that a person would be mistaken for a traffic sign See How does Google39s selfdriving car identify pedestrians Image Technology Review To support such a claim Illah Nourbakhsh a professor of robotics at Carnegie Mellon University gave an interview to the New York Times magazine cover story on autonomous driving cars and includes this hypothetical scenario saying If they’re outside walking and the sun is at just the right glare level and there’s a mirrored truck stopped next to you and the sun bounces off that truck and hits the guy so that you can’t see his face anymore — well now your car just sees a stop sign The chances of all that happening are diminishingly small — it’s very very unlikely — but the problem is we will have millions of these cars The very unlikely will happen all the time Even so the risk would be minimal since the car is always looking out for traffic pedestrians and other obstacles Sources How Googles selfdriving cars see the world The Dream Life of Driverless Cars at The New York Times
,,"<p>Tesla model S has <a href=""https://www.tesla.com/models"" rel=""nofollow noreferrer"">Autopilot</a> which allows to steer within a lane, change lanes with the simple tap of a turn signal, and can manage speed by using traffic-aware cruise control. Multiple digital controls helps to avoid collisions. Based on that, this isn't fully self-driving car.</p>

<p>However it is using a computer vision detection system, but it is not intended to be used hands-free.</p>

<p>So basically what is known is that the accident involved the side of a truck trailer (of a large white 18-wheel truck) and most likely the camera had a washed out of picture possibly due to glare or blooming from overexposure which made that the side of the trailer white and thin which failed to distinguish with the sky which was bright as well.</p>

<p>This may have happened in part, because the crash-avoidance system only engage when both radar and vision system detect an obstacle which could not happen.</p>

<p>Further more it was suggested by <em>The Associated Press</em> that the driver most likely was watching a <em>Harry Potter</em> at the time of the crash and assuming system would alert Brown, we don't know if he was able to retake controls quickly enough to avoid impact. As mentioned again, the system wasn't intended for hands-free driving and parts of the system was unfinished. Not to mention that the car was driving with full speed under the trailer.</p>

<p>Tesla officially said about this crash in a statement on its website:</p>

<blockquote>
  <p><strong>The high ride height of the trailer combined with its positioning across the road and the extremely rare circumstances</strong> of the impact caused the Model S to pass under the trailer, with the bottom of the trailer impacting the windshield of the Model S.</p>
  
  <p>Neither Autopilot nor the driver noticed the white side of the tractor-trailer against a brightly lit sky, so the brake was not applied.</p>
</blockquote>

<p>They also said, according to techno-optimists, that they will tweaks their code, so this particular case won't happen again.</p>

<p>To summarize, this was a 'technical failure' of braking system and most likely Autopilot was not at as Tesla told Senate.</p>

<p><a href=""https://i.stack.imgur.com/obdLM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/obdLM.png"" alt=""The New York Times |Source: Florida traffic crash report""></a></p>

<p><sup>The New York Times |Source: Florida traffic crash report</sup></p>

<p>Sources:</p>

<ul>
<li><a href=""http://www.freep.com/story/money/cars/2016/07/01/tesla-autopilot-death-highlights-autonomous-risks/86591130/"" rel=""nofollow noreferrer"">Tesla Autopilot death highlights autonomous risks</a></li>
<li><a href=""https://robotfuturesbook.wordpress.com/2016/07/01/layers-of-autonomy"" rel=""nofollow noreferrer"">Layers of Autonomy</a></li>
<li><a href=""http://www.nytimes.com/interactive/2016/07/01/business/inside-tesla-accident.html"" rel=""nofollow noreferrer"">Inside the Self-Driving Tesla Fatal Accident</a></li>
<li><a href=""https://www.theguardian.com/technology/2016/jun/30/tesla-autopilot-death-self-driving-car-elon-musk"" rel=""nofollow noreferrer"">Tesla driver dies in first fatal crash while using autopilot mode</a></li>
</ul>
",,0,2016-08-12T03:22:24.100,,1579,2016-08-12T03:38:43.983,2016-08-12T03:38:43.983,,8.0,,8.0,1488.0,2,0,,,,52.02,11.49,9.88,0.0,0.0,47.0,Tesla model S has Autopilot which allows to steer within a lane change lanes with the simple tap of a turn signal and can manage speed by using trafficaware cruise control Multiple digital controls helps to avoid collisions Based on that this isnt fully selfdriving car However it is using a computer vision detection system but it is not intended to be used handsfree So basically what is known is that the accident involved the side of a truck trailer of a large white 18wheel truck and most likely the camera had a washed out of picture possibly due to glare or blooming from overexposure which made that the side of the trailer white and thin which failed to distinguish with the sky which was bright as well This may have happened in part because the crashavoidance system only engage when both radar and vision system detect an obstacle which could not happen Further more it was suggested by The Associated Press that the driver most likely was watching a Harry Potter at the time of the crash and assuming system would alert Brown we dont know if he was able to retake controls quickly enough to avoid impact As mentioned again the system wasnt intended for handsfree driving and parts of the system was unfinished Not to mention that the car was driving with full speed under the trailer Tesla officially said about this crash in a statement on its website The high ride height of the trailer combined with its positioning across the road and the extremely rare circumstances of the impact caused the Model S to pass under the trailer with the bottom of the trailer impacting the windshield of the Model S Neither Autopilot nor the driver noticed the white side of the tractortrailer against a brightly lit sky so the brake was not applied They also said according to technooptimists that they will tweaks their code so this particular case wont happen again To summarize this was a technical failure of braking system and most likely Autopilot was not at as Tesla told Senate The New York Times Source Florida traffic crash report Sources Tesla Autopilot death highlights autonomous risks Layers of Autonomy Inside the SelfDriving Tesla Fatal Accident Tesla driver dies in first fatal crash while using autopilot mode
,2.0,"<p>Has there any research been done on how difficult certain languages are to learn for chatbots? 
For example, CleverBot knows a bit of Dutch, German, Finnish and French, so there are clearly chatbots that speak other languages than English. (English is still her best language, but that is because she speaks that most often)</p>

<p>I would imagine that a logical constructed language, like lobjan, would be easier to learn than a natural language, like English, for example.  </p>
",,0,2016-08-12T06:47:12.693,1.0,1580,2016-09-02T12:10:34.307,,,,,29.0,,1,5,<chat-bots><language-processing>,Are there any results on how difficult certain languages are to learn for chatbots?,62.0,53.85,12.36,10.24,0.0,0.0,14.0,Has there any research been done on how difficult certain languages are to learn for chatbots For example CleverBot knows a bit of Dutch German Finnish and French so there are clearly chatbots that speak other languages than English English is still her best language but that is because she speaks that most often I would imagine that a logical constructed language like lobjan would be easier to learn than a natural language like English for example
,,"<p>Further to Franck's answer, there may be better optima (even global optima) that exist in the opposite direction to the gradient (which may be in the direction of some local optima). Evolutionary algorithms have scope to search the surrounding area, while backpropagation will always move in the direction of the gradient. With no guarantee (due to their randomness), evolutionary algorithms may be capable of finding solutions that backpropagation simply cannot.</p>
",,0,2016-08-12T08:33:43.560,,1581,2016-08-12T08:33:43.560,,,,,1467.0,1539.0,2,5,,,,30.91,15.67,11.33,0.0,0.0,13.0,Further to Francks answer there may be better optima even global optima that exist in the opposite direction to the gradient which may be in the direction of some local optima Evolutionary algorithms have scope to search the surrounding area while backpropagation will always move in the direction of the gradient With no guarantee due to their randomness evolutionary algorithms may be capable of finding solutions that backpropagation simply cannot
,,,,0,2016-08-12T09:54:04.513,,1582,2016-08-12T09:54:04.513,2016-08-12T09:54:04.513,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For self-driving cars. This tag should be used with the [self-driving] tag.,,0,2016-08-12T09:54:04.513,,1583,2016-08-12T15:18:47.173,2016-08-12T15:18:47.173,,145.0,,145.0,,4,0,,,,90.77,10.08,7.88,0.0,0.0,6.0,For selfdriving cars This tag should be used with the selfdriving tag
,,"<p>For many years, the focus has been on games with perfect information. That is, in Chess and Go both of us are looking at the same board. In something like Poker, you have information that I don't have and I have information that you don't have, and so for either of us to make sense of each other's actions we need to model what hidden information the other player has, and <em>also</em> manage how we leak our hidden information. (A poker bot whose hand strength could be trivially determined from its bets will be easier to beat than a poker bot that doesn't.)</p>

<p>Current research is switching to tackling games with imperfect information. Deepmind, for example, <a href=""http://www.businessinsider.com/google-deepmind-could-play-starcraft-2016-3"">has said</a> they might approach Starcraft next.</p>

<p>I don't see too much different between video games and board games, and there are several good reasons to switch to video games for games with imperfect information. </p>

<p>One is that if you want beating the best human to be a major victory, there needs to be a pyramid of skill that human is atop of--it'll be harder to unseat the top Starcraft champion that the top Warcraft champion, even though the bots might be comparably difficult to code, just because humans have tried harder at Starcraft.</p>

<p>Another is that many games with imperfect information deal with reading faces and concealing information, which an AI would have an unnatural advantage at; for multiplayer video games, players normally interact with each other through a server as intermediary and so the competition will be more normal.</p>
",,0,2016-08-12T15:29:14.740,,1584,2016-08-12T15:29:14.740,,,,,10.0,1568.0,2,5,,,,50.91,10.97,9.23,0.0,0.0,33.0,For many years the focus has been on games with perfect information That is in Chess and Go both of us are looking at the same board In something like Poker you have information that I dont have and I have information that you dont have and so for either of us to make sense of each others actions we need to model what hidden information the other player has and also manage how we leak our hidden information A poker bot whose hand strength could be trivially determined from its bets will be easier to beat than a poker bot that doesnt Current research is switching to tackling games with imperfect information Deepmind for example has said they might approach Starcraft next I dont see too much different between video games and board games and there are several good reasons to switch to video games for games with imperfect information One is that if you want beating the best human to be a major victory there needs to be a pyramid of skill that human is atop ofitll be harder to unseat the top Starcraft champion that the top Warcraft champion even though the bots might be comparably difficult to code just because humans have tried harder at Starcraft Another is that many games with imperfect information deal with reading faces and concealing information which an AI would have an unnatural advantage at for multiplayer video games players normally interact with each other through a server as intermediary and so the competition will be more normal
,,"<p><strong><a href=""https://www.ibm.com/watson/"" rel=""nofollow noreferrer"">Watson</a></strong> is a question answering computer system capable of answering questions posed in natural language,developed in IBM's DeepQA project by a research team led by principal investigator David Ferrucci.</p>

<p>Watson is a cognitive technology that can think like a human.</p>

<p>Watson can,</p>

<ol>
<li><strong>Understand</strong> stuff by analyzing and interpreting whatever you query it.</li>
<li><strong>Reason</strong> based on the input given by you.</li>
<li><strong>Learn</strong> new stuff using Machine Learning.</li>
<li><strong>Interact</strong> with you, chat with you and help you solve the real world problems. </li>
</ol>
",,0,2016-08-12T15:33:44.037,,1585,2017-01-05T21:45:24.563,2017-01-05T21:45:24.563,,1807.0,,1807.0,,5,0,,,,57.98,12.87,11.01,0.0,0.0,10.0,Watson is a question answering computer system capable of answering questions posed in natural languagedeveloped in IBMs DeepQA project by a research team led by principal investigator David Ferrucci Watson is a cognitive technology that can think like a human Watson can Understand stuff by analyzing and interpreting whatever you query it Reason based on the input given by you Learn new stuff using Machine Learning Interact with you chat with you and help you solve the real world problems
,,For questions related to IBM Watson.,,0,2016-08-12T15:33:44.037,,1586,2017-01-05T21:43:28.450,2017-01-05T21:43:28.450,,1807.0,,1807.0,,4,0,,,,73.85,9.15,11.83,0.0,0.0,1.0,For questions related to IBM Watson
,,"<p>It's not the language itself but rather the structure and for the language's ambiguity, for example in English: person a says ""John and Bob (his fish)"" person B says ""He died!"", posed question, to whom does person B refer to by he died.  More than the language, but the application. You can write a chat bot in Assembly, C, C++, C#, Java or Python. The all work a bit differently but can accomplish the same result, but one language might have more pros or cons to the other. So it will boil down to not language but the understanding of what is being said, research of language in the brain has come to confirm we associate a meaning/feeling/and other inputs with a given language. </p>

<p>So to conclude: English is by far the most chaotic for a chat bot but Japanese is actually the best due to the way the language itself is written/spoken. There is more structure to it and less ambiguity.</p>

<p>I'm a Software Engineer and An AI Researcher for the past 7 years.</p>
",,0,2016-08-12T16:40:07.353,,1587,2016-08-30T20:22:12.103,2016-08-30T20:22:12.103,,1282.0,,1282.0,1580.0,2,1,,,,68.7,8.53,8.21,0.0,0.0,36.0,Its not the language itself but rather the structure and for the languages ambiguity for example in English person a says John and Bob his fish person B says He died posed question to whom does person B refer to by he died More than the language but the application You can write a chat bot in Assembly C C C Java or Python The all work a bit differently but can accomplish the same result but one language might have more pros or cons to the other So it will boil down to not language but the understanding of what is being said research of language in the brain has come to confirm we associate a meaningfeelingand other inputs with a given language So to conclude English is by far the most chaotic for a chat bot but Japanese is actually the best due to the way the language itself is writtenspoken There is more structure to it and less ambiguity Im a Software Engineer and An AI Researcher for the past 7 years
,,"<p>Simply yes, but it can lead to over fixing of the NN.</p>

<p>Humans favour not dying, which is only realised once a consequence is defined for the system to realise that death is an unfavorable result. Which can be train vai observation. Allow your system to observe between 2 or more separate people/systems. Then allow opportunity to test in a safe environment with the pre  existing info of the consequences that may follow, provind that if the system makes a mistake in the test/safe environment it will be saved  unknownly and then informed that it made a mistake, the place system in an unsafe world in same conditions, informing it that if something happens it will die. That is the way humans grow up, and we've lasted very long with this technic.</p>

<p>I'm an AI Researcher and Software Engineer for the past 7 years.</p>
",,2,2016-08-12T16:52:44.237,,1588,2016-08-12T17:00:25.150,2016-08-12T17:00:25.150,,1282.0,,1282.0,1476.0,2,0,,,,59.03,9.63,9.7,0.0,0.0,17.0,Simply yes but it can lead to over fixing of the NN Humans favour not dying which is only realised once a consequence is defined for the system to realise that death is an unfavorable result Which can be train vai observation Allow your system to observe between 2 or more separate peoplesystems Then allow opportunity to test in a safe environment with the pre existing info of the consequences that may follow provind that if the system makes a mistake in the testsafe environment it will be saved unknownly and then informed that it made a mistake the place system in an unsafe world in same conditions informing it that if something happens it will die That is the way humans grow up and weve lasted very long with this technic Im an AI Researcher and Software Engineer for the past 7 years
,,"<p>A neural network can be used but must be trained to expect the information (pattern of data, pixels or groupings of loose range such as color, and location) at any given location in the network, first a vision system must but implemented. Then a facial recognition, multiple partial individual body fixing (finding body part and there partners to a person) then training on some states and you'll have it work. MIT have done research and have made a seemy accurate implementation. </p>

<p>I'm an AI Researcher and Software Engineer for the past 7 years.</p>
",,2,2016-08-12T17:06:48.000,,1589,2016-08-12T20:00:30.747,2016-08-12T20:00:30.747,,8.0,,1282.0,1481.0,2,2,,,,56.39,11.26,11.24,0.0,0.0,14.0,A neural network can be used but must be trained to expect the information pattern of data pixels or groupings of loose range such as color and location at any given location in the network first a vision system must but implemented Then a facial recognition multiple partial individual body fixing finding body part and there partners to a person then training on some states and youll have it work MIT have done research and have made a seemy accurate implementation Im an AI Researcher and Software Engineer for the past 7 years
,,"<p>MIT have done research  and implemented an incomplete version of action video recognition.</p>

<p>With the use of MATLAB, NNetworks and a large set of training videos.</p>

<p>My suggested set of comments on my previous answer indicate the usage of a multi interconnected NNet, verus MIT's image based NNet.</p>
",,0,2016-08-12T17:24:47.190,,1590,2016-08-12T17:24:47.190,,,,,1282.0,1481.0,2,3,,,,46.78,11.89,11.67,0.0,0.0,6.0,MIT have done research and implemented an incomplete version of action video recognition With the use of MATLAB NNetworks and a large set of training videos My suggested set of comments on my previous answer indicate the usage of a multi interconnected NNet verus MITs image based NNet
,,"<p>It's impossible to give a definitive 'yes' answer to your question, since that would require proving that alternatives <em>cannot</em> exist.</p>

<p>More philosophically, it depends on what you mean by ""preference over world states"":</p>

<p>However counter-intuitive it might seem, it is conceivably possible to create Strong AI purely from local condition-action rules, in which there is no global concept of 'preference value' and/or no integrated notion of 'world state'.</p>
",,2,2016-08-12T20:04:20.157,,1591,2016-08-12T20:04:20.157,,,,,42.0,1476.0,2,0,,,,20.05,16.08,11.82,0.0,0.0,19.0,Its impossible to give a definitive yes answer to your question since that would require proving that alternatives cannot exist More philosophically it depends on what you mean by preference over world states However counterintuitive it might seem it is conceivably possible to create Strong AI purely from local conditionaction rules in which there is no global concept of preference value andor no integrated notion of world state
,1.0,"<p>Google, Tesla, Apple etc have all built or are building their own self-driving cars. As an expert in a related area, I am interested in knowing at a high level, the systems and techniques that go into self-driving cars. How easy is it for me to make a tabletop prototype (large enough to accomodate the needed computing power needs)?</p>
",,1,2016-08-12T20:27:15.577,,1592,2016-08-13T15:48:26.767,,,,,130.0,,1,5,<self-driving><ai-design>,What technologies are needed for a self-driving car?,47.0,59.94,9.75,10.5,0.0,0.0,11.0,Google Tesla Apple etc have all built or are building their own selfdriving cars As an expert in a related area I am interested in knowing at a high level the systems and techniques that go into selfdriving cars How easy is it for me to make a tabletop prototype large enough to accomodate the needed computing power needs
,4.0,"<p>The above question itself is perhaps too broad for this forum, hence I am phrasing it as a request for references.</p>

<p>Humans have been endowed with personalities by nature, and it is not clear (to me at least) if this is a feature or a bug. This has been explored in science fiction by various notions of <a href=""http://memory-alpha.org/Borg"" rel=""nofollow"">Borg</a>-like entities. It is my belief that, for narrative reasons, such stories usually end with the humans with their flawed personalities winning in the end. </p>

<p>Are there experts who have analyzed, perhaps mathematically, design criteria for an AI agent with weakly enforced goals (eg. to maximize reproduction in the human case) in an uncertain environment, and ended up with the answer that a notion of personality is useful? If there are philosophers or science fiction writers who have examined this question in their work, I would be happy to know about those too.</p>
",,1,2016-08-12T20:36:59.840,,1593,2016-08-25T17:42:49.297,2016-08-18T13:46:13.277,,145.0,,130.0,,1,2,<philosophy><strong-ai><ai-design><human-inspired>,"Asking for references regarding the question ""Is a concept of personality useful for Strong AGI?""",62.0,58.21,10.5,9.96,0.0,0.0,20.0,The above question itself is perhaps too broad for this forum hence I am phrasing it as a request for references Humans have been endowed with personalities by nature and it is not clear to me at least if this is a feature or a bug This has been explored in science fiction by various notions of Borglike entities It is my belief that for narrative reasons such stories usually end with the humans with their flawed personalities winning in the end Are there experts who have analyzed perhaps mathematically design criteria for an AI agent with weakly enforced goals eg to maximize reproduction in the human case in an uncertain environment and ended up with the answer that a notion of personality is useful If there are philosophers or science fiction writers who have examined this question in their work I would be happy to know about those too
,,"<p>Simply put, we don't know how to create Strong Artificial Intelligence yet, so we don't know what is or isn't required to create it.  At best we can engage in ""informed speculation"", in which case I'd say that the answer is more likely ""yes"" than ""no"".  But that's basically just a hunch.</p>

<p>If you're interested in a pretty good overview of what ""pieces"" might be required to create Strong AI, and if you haven't read it yet, <a href=""http://homes.cs.washington.edu/~pedrod/"" rel=""nofollow"">Pedro Domingos</a>' book <em>The Master Algorithm</em> might be of interest. </p>
",,0,2016-08-12T21:31:07.827,,1594,2016-08-12T21:39:28.547,2016-08-12T21:39:28.547,,33.0,,33.0,1476.0,2,1,,,,66.27,9.34,8.35,0.0,0.0,25.0,Simply put we dont know how to create Strong Artificial Intelligence yet so we dont know what is or isnt required to create it At best we can engage in informed speculation in which case Id say that the answer is more likely yes than no But thats basically just a hunch If youre interested in a pretty good overview of what pieces might be required to create Strong AI and if you havent read it yet Pedro Domingos book The Master Algorithm might be of interest
,,"<p>Yes, as mentioned in other answers, Prolog is actually used in IBM Watson.  Prolog doesn't get much ""hype"" and ""buzz"" these days, but it is absolutely still used.  As always, it has certain specific areas where it shines, and specific techniques that map well to its use.  Specifically, things like <a href=""https://en.wikipedia.org/wiki/Inductive_logic_programming"" rel=""nofollow"">Inductive Logic Programming</a>, <a href=""https://en.wikipedia.org/wiki/Constraint_logic_programming"" rel=""nofollow"">Constraint Logic Programming</a>, <a href=""https://en.wikipedia.org/wiki/Answer_set_programming"" rel=""nofollow"">Answer Set Programming</a> and some <a href=""https://en.wikipedia.org/wiki/Natural_language_processing"" rel=""nofollow"">NLP</a> applications may involve extensive use of Prolog.</p>
",,0,2016-08-12T21:38:48.343,,1595,2016-08-12T21:38:48.343,,,,,33.0,1531.0,2,2,,,,54.02,13.69,10.44,0.0,0.0,17.0,Yes as mentioned in other answers Prolog is actually used in IBM Watson Prolog doesnt get much hype and buzz these days but it is absolutely still used As always it has certain specific areas where it shines and specific techniques that map well to its use Specifically things like Inductive Logic Programming Constraint Logic Programming Answer Set Programming and some NLP applications may involve extensive use of Prolog
,,"<p>'Personality' is something of a 'suitcase word' (Minsky) for quite a large collection of (presumably reasonably consistent) observable traits. </p>

<p>It seems clear that there is a certain collective advantage in having a consistent personality - specifically that it affords observers some learning gradient in an otherwise uncertain environment. This is of particular importance because those consistencies might have been arrived at using different learning mechanisms than the ones a given observer has.</p>

<p>Hence, in any non-trivial coevolutionary system, other organisms will inevitably make use of any such consistencies. Consider a simple robot, called Alice, say, that has the trait of 'quickly flashing red when it sees a blue robot'. It makes sense for all observers to exploit <em>everything</em> that they perceive as correlating with Alice's behavior, in particular, the prediction that a blue robot is likely to be present.</p>

<p>The best reference I can recommend on this (which shows that we tend to ascribe 'personality' to even very simple mechanisms) is <a href=""https://mitpress.mit.edu/books/vehicles"" rel=""nofollow"">'Vehicles'</a> by Valentino Braitenberg.</p>
",,0,2016-08-12T21:59:58.440,,1596,2016-08-12T22:06:05.017,2016-08-12T22:06:05.017,,42.0,,42.0,1593.0,2,3,,,,30.8,15.55,11.73,0.0,0.0,33.0,Personality is something of a suitcase word Minsky for quite a large collection of presumably reasonably consistent observable traits It seems clear that there is a certain collective advantage in having a consistent personality specifically that it affords observers some learning gradient in an otherwise uncertain environment This is of particular importance because those consistencies might have been arrived at using different learning mechanisms than the ones a given observer has Hence in any nontrivial coevolutionary system other organisms will inevitably make use of any such consistencies Consider a simple robot called Alice say that has the trait of quickly flashing red when it sees a blue robot It makes sense for all observers to exploit everything that they perceive as correlating with Alices behavior in particular the prediction that a blue robot is likely to be present The best reference I can recommend on this which shows that we tend to ascribe personality to even very simple mechanisms is Vehicles by Valentino Braitenberg
,,"<p><strong>First, a note on the question itself.</strong></p>

<blockquote>
  <p>Humans have been endowed with personalities by nature, and it is not clear (to me at least) if this is a feature or a bug. </p>
</blockquote>

<p>In my opinion, this is a statement that constrains the question, since it assumes that the personality is <em>given</em>. To me, it feels a bit like <em>playing god</em>: Artificial (given) Intelligence would hence imply Artificial (given) Personality. This approach to the problem seems to be supported by the next fragment:</p>

<blockquote>
  <p>a notion of personality is useful</p>
</blockquote>

<p>I point to the above because I don't think that artificial intelligence... <strong><em>Intelligence</em></strong> itself, actually, need to be given or assigned, or even have a <em>use</em> in the sense of a <em>purpose</em>. </p>

<hr>

<p><strong>The previous note</strong> was about <em>emergence</em>, which is a topic that <a href=""http://ai.stackexchange.com/users/42/user217281728"">user217281728</a> briefly addressed in <a href=""http://ai.stackexchange.com/a/1596/70"">their answer</a>. In this second approach, the particular traits <em>just happen</em>, or <em>develop</em>. The interaction between the (so-called) agents and their environment, as well as fellow agents can give place to new behaviour patterns, not designed beforehand. </p>

<p><strong>In an evolutionary</strong> approach, if the personality would happen to have an advantage (or at least not represent a disadvantage), then it could just appear. Of course, I am making a number of assumptions and demarcations here as well:</p>

<ul>
<li>I am thinking about embodied intelligence</li>
<li>I speak of evolutionary robotics</li>
<li>I think on social issues being of importance</li>
<li>I assume that <em>personality</em> could emerge</li>
</ul>

<p><strong>Now, an example</strong> that I find extremely interesting is that of the little mobile robots which could move around and end-up in a pool of <em>food</em> or a pool of <em>poison</em>. And they, somehow, by some odd chance, recognised or made a relation between signals sent by other robots, and the presence of food. Or not. That was more or less the thing: Some robots (kind of) learned to <em>conceal</em> information and thus had more time to eat themselves. Well, I would have a couple of personality adjectives for such guys.</p>

<p><a href=""http://www.pnas.org/content/106/37/15786.full"" rel=""nofollow"">Here you find the article</a> and <a href=""https://wp.unil.ch/mitrilab/"" rel=""nofollow"">here you find some videos</a> and related stuff.</p>

<p><strong>And with that</strong>, we land at my last point: <a href=""https://en.wikipedia.org/wiki/Anthropomorphism#In_computing"" rel=""nofollow"">We humans</a> put the adjectives, according to our social conditioning. We call <a href=""https://en.wikipedia.org/wiki/Marvin_(character)"" rel=""nofollow"">Marvin</a> <em>depressive</em> and <a href=""http://www.smithsonianmag.com/arts-culture/why-do-we-love-r2-d2-and-not-c-3po-180951176/"" rel=""nofollow"">R2D2</a> lovely and charming. </p>

<p>If they perceive their personalities as constructive or damaging, will always depend on our own judgment. In the end, it is quite common under humans to disagree on personality issues, too.</p>

<hr>

<p><strong>Bonus</strong></p>

<p>Remember when <a href=""https://www.youtube.com/watch?v=UgkyrW2NiwM"" rel=""nofollow"">HAL got emotional</a>, on the face of death?</p>

<p>It gets <em>human</em> when it loses its cool, before the flawed-personality human astronaut :)</p>
",,0,2016-08-13T00:19:58.177,,1597,2016-08-13T00:19:58.177,,,,,70.0,1593.0,2,3,,,,50.97,11.02,9.41,0.0,0.0,75.0,First a note on the question itself Humans have been endowed with personalities by nature and it is not clear to me at least if this is a feature or a bug In my opinion this is a statement that constrains the question since it assumes that the personality is given To me it feels a bit like playing god Artificial given Intelligence would hence imply Artificial given Personality This approach to the problem seems to be supported by the next fragment a notion of personality is useful I point to the above because I dont think that artificial intelligence Intelligence itself actually need to be given or assigned or even have a use in the sense of a purpose The previous note was about emergence which is a topic that user217281728 briefly addressed in their answer In this second approach the particular traits just happen or develop The interaction between the socalled agents and their environment as well as fellow agents can give place to new behaviour patterns not designed beforehand In an evolutionary approach if the personality would happen to have an advantage or at least not represent a disadvantage then it could just appear Of course I am making a number of assumptions and demarcations here as well I am thinking about embodied intelligence I speak of evolutionary robotics I think on social issues being of importance I assume that personality could emerge Now an example that I find extremely interesting is that of the little mobile robots which could move around and endup in a pool of food or a pool of poison And they somehow by some odd chance recognised or made a relation between signals sent by other robots and the presence of food Or not That was more or less the thing Some robots kind of learned to conceal information and thus had more time to eat themselves Well I would have a couple of personality adjectives for such guys Here you find the article and here you find some videos and related stuff And with that we land at my last point We humans put the adjectives according to our social conditioning We call Marvin depressive and R2D2 lovely and charming If they perceive their personalities as constructive or damaging will always depend on our own judgment In the end it is quite common under humans to disagree on personality issues too Bonus Remember when HAL got emotional on the face of death It gets human when it loses its cool before the flawedpersonality human astronaut
1599.0,1.0,"<p>I've found this short <a href=""http://iamtrask.github.io/2015/07/12/basic-python-network/"" rel=""nofollow"">Python code</a> which implements neural network in 11 lines of code:</p>

<pre><code>X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])
y = np.array([[0,1,1,0]]).T
syn0 = 2*np.random.random((3,4)) - 1
syn1 = 2*np.random.random((4,1)) - 1
for j in xrange(60000):
    l1 = 1/(1+np.exp(-(np.dot(X,syn0))))
    l2 = 1/(1+np.exp(-(np.dot(l1,syn1))))
    l2_delta = (y - l2)*(l2*(1-l2))
    l1_delta = l2_delta.dot(syn1.T) * (l1 * (1-l1))
    syn1 += l1.T.dot(l2_delta)
    syn0 += X.T.dot(l1_delta)
</code></pre>

<p>I believe it may be a valid implementation of neural network, but how do I know?</p>

<p>In other words, is just creating bunch of arrays which compute the output on certain criteria and call them layers with synapses does it make proper neural network?</p>

<p>In other words, I'd like to ask, what features/properties makes a valid artificial neural network?</p>
",,0,2016-08-13T02:46:55.560,,1598,2016-08-17T01:23:18.030,,,,,8.0,,1,2,<neural-networks><implementation><computer-programming>,What are the minimum requirements to call something artificial neural network?,56.0,54.86,11.09,9.56,408.0,0.0,11.0,Ive found this short Python code which implements neural network in 11 lines of code I believe it may be a valid implementation of neural network but how do I know In other words is just creating bunch of arrays which compute the output on certain criteria and call them layers with synapses does it make proper neural network In other words Id like to ask what featuresproperties makes a valid artificial neural network
,,"<p>If you pick up a textbook on Neural Networks, you'll find that the simplest examples shown are ones that just implement an AND gate or something.  They're trivial, probably fewer lines of code than what you have there.  The bar to be an ""artificial neural network"" is pretty low... it certainly isn't the case that ANN's <em>must</em> be incredibly complicated with thousands of lines of code, and many layers, or even many ""neurons"" total.</p>

<p>Basically, if something is setting up at least one ""neuron"" with multiple inputs, and using some kind of weighting function to generate an output from those inputs, it's a valid ANN. It might be a <em>really</em> simple example of an ANN, but it's still an ANN.</p>

<p>Remember what Geoffrey Hinton says in his <em>Coursera</em> class - (paraphrased) ""We don't pretend that the things we're building really work the way the brain does, we're just taking the brain as loose inspiration for an approach that we've found works"".</p>
",,3,2016-08-13T03:27:59.370,,1599,2016-08-17T01:23:18.030,2016-08-17T01:23:18.030,,8.0,,33.0,1598.0,2,4,,,,65.15,10.91,9.61,0.0,0.0,39.0,If you pick up a textbook on Neural Networks youll find that the simplest examples shown are ones that just implement an AND gate or something Theyre trivial probably fewer lines of code than what you have there The bar to be an artificial neural network is pretty low it certainly isnt the case that ANNs must be incredibly complicated with thousands of lines of code and many layers or even many neurons total Basically if something is setting up at least one neuron with multiple inputs and using some kind of weighting function to generate an output from those inputs its a valid ANN It might be a really simple example of an ANN but its still an ANN Remember what Geoffrey Hinton says in his Coursera class paraphrased We dont pretend that the things were building really work the way the brain does were just taking the brain as loose inspiration for an approach that weve found works
1608.0,1.0,"<p>I'm looking for research which discusses misbehavior detection in public internet access networks using ANN approaches.</p>

<p>So it can be used by <a href=""https://en.wikipedia.org/wiki/Internet_service_provider"" rel=""nofollow"">ISP</a> to detect suspicious users connected to their network.</p>
",,0,2016-08-13T03:51:50.153,,1601,2016-08-13T07:45:28.300,,,,,8.0,,1,3,<neural-networks><research>,Research for misbehavior detection in WiFI networks,28.0,55.74,14.03,13.06,0.0,0.0,3.0,Im looking for research which discusses misbehavior detection in public internet access networks using ANN approaches So it can be used by ISP to detect suspicious users connected to their network
1616.0,1.0,"<p>I'm investigating applications of AI algorithms which can be used for data leakage detection and prevention within an intranet network (like <a href=""https://en.wikipedia.org/wiki/Forcepoint"" rel=""nofollow"">Forcepoint</a>). More specifically detecting traffic patterns. I'm new to this.</p>

<p>Which learning algorithms are most suitable for this goal? <a href=""https://en.wikipedia.org/wiki/Evolutionary_algorithm"" rel=""nofollow"">EA</a>, <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow"">GA</a>, <a href=""https://en.wikipedia.org/wiki/Artificial_neural_network"" rel=""nofollow"">ANN</a> (which one) or something else?</p>
",,0,2016-08-13T04:08:47.627,,1603,2016-08-17T11:53:52.427,2016-08-17T11:53:52.427,,29.0,,8.0,,1,2,<learning-algorithms>,Which learning algorithms are suitable for data leakage detection and prevention?,26.0,53.27,14.01,10.36,0.0,0.0,13.0,Im investigating applications of AI algorithms which can be used for data leakage detection and prevention within an intranet network like Forcepoint More specifically detecting traffic patterns Im new to this Which learning algorithms are most suitable for this goal EA GA ANN which one or something else
,,,,0,2016-08-13T04:10:11.043,,1604,2016-08-13T04:10:11.043,2016-08-13T04:10:11.043,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,,,0,2016-08-13T04:10:11.043,,1605,2016-08-13T04:10:11.043,2016-08-13T04:10:11.043,,-1.0,,-1.0,,4,0,,,,,,,,,,
1607.0,1.0,"<p>I'm wondering, instead of implementing new web browsers over and over again with millions line of code which is very difficult to manage, would it be possible to use ANN or GA algorithm to teach it about the rendering process (how the page should look like)?</p>

<p>So as an input I would imaging the html source code, output is the rendered page (maybe in some interactive image like SVG, some library or something, I'm not sure).</p>

<p>The training data can be dataset of websites providing input source code and their rendered representation by using other browsers for the guidance as expected output.</p>

<p>Which approach would you take and what are the most challenging things you can think of?</p>
",,0,2016-08-13T04:25:29.447,,1606,2016-08-13T11:20:33.007,2016-08-13T11:20:33.007,,8.0,,8.0,,1,1,<neural-networks><implementation><computer-programming>,What are the approaches to teach AI to how to render html page based on its source code?,39.0,49.99,11.04,9.92,0.0,0.0,15.0,Im wondering instead of implementing new web browsers over and over again with millions line of code which is very difficult to manage would it be possible to use ANN or GA algorithm to teach it about the rendering process how the page should look like So as an input I would imaging the html source code output is the rendered page maybe in some interactive image like SVG some library or something Im not sure The training data can be dataset of websites providing input source code and their rendered representation by using other browsers for the guidance as expected output Which approach would you take and what are the most challenging things you can think of
,,"<p>The rendering process for browsers is <a href=""https://www.w3.org/"">very well defined</a>, and has a very rigid definite ruleset where (virtually) every accountability is noted and handled. This is not optimal for Machine Learning, which works when we have a large pool of examples, and we don't know the ruleset; it will figure it out. Even if you were to train an Neural Network to process that input, there are several things you must account for:</p>

<p><strong>1. Variance in data.</strong></p>

<p>Not all webpages are equal in length or complexity, and making a neural network to generate output from HTML would produce garbage most of the time.</p>

<p><strong>2. Training time.</strong></p>

<p>The time it would take for a neural network to understand HTML tags, attributes, the DOM Tree, and each and every element, including new ones being added every few years, and how each one renders and behaves, would take an extremely long time, most likely several years on a fast computer, if it even were possible</p>

<p><strong>3. Interactivity.</strong></p>

<p>Web pages aren't just static, they change according HTML, CSS and JavaScript. Not only would you have to design your system to account for the rendering step, you would also make it have to understand the <a href=""https://en.wikipedia.org/wiki/Turing_completeness"">Turing Complete</a> scripting language <a href=""https://en.wikipedia.org/wiki/JavaScript"">JavaScript</a>, as well as the less complicated, but inherently intertwined with HTML, CSS stylesheet language. If you thought the rendering process was easy, try training a neural network to handle complicated scripting patterns.</p>

<p><strong>4. New Standards</strong></p>

<p>Not all HTML is equal, because of different standards. <a href=""https://en.wikipedia.org/wiki/Web_Hypertext_Application_Technology_Working_Group"">WHATWG</a> began working on HTML5 in 2004, and browsers started to implement not long after. In 2004, there were very few examples of HTML5 sites to train your network to begin with. Sure, now it's standardized and every website uses it, but what about HTML6? When the first specification is released (probably 2017-2025), virtually no websites will use it, because no one will support it. Only when it finally becomes standard, probably in the late 2020s or early 2030s, will you have enough data to train your monstrous system of neural networks</p>

<hr />

<p>As for AI in general, one could argue that browsers already use A.I. in their rendering process. They intelligently decide what to render (taking CSS into account), when in order to get the most efficient render time, they selectively use different JavaScript parsers on different sections of the code to optimize the speed, the whole system has been optimized on another ruleset to make rendering and interacting with a webpage as seamless and easy-to-use as possible. Your system will never be as good as what hundreds of humans have optimized over 20 years.</p>

<p>Trying to solve HTML rendering with Neural Networks is akin to trying to nail a nail with a screwdriver. It's just not going to work</p>

<p>Hope this was helpful!</p>
",,0,2016-08-13T05:49:50.547,,1607,2016-08-13T05:49:50.547,,,,,1499.0,1606.0,2,6,,,,56.49,11.49,9.59,0.0,0.0,73.0,The rendering process for browsers is very well defined and has a very rigid definite ruleset where virtually every accountability is noted and handled This is not optimal for Machine Learning which works when we have a large pool of examples and we dont know the ruleset it will figure it out Even if you were to train an Neural Network to process that input there are several things you must account for 1 Variance in data Not all webpages are equal in length or complexity and making a neural network to generate output from HTML would produce garbage most of the time 2 Training time The time it would take for a neural network to understand HTML tags attributes the DOM Tree and each and every element including new ones being added every few years and how each one renders and behaves would take an extremely long time most likely several years on a fast computer if it even were possible 3 Interactivity Web pages arent just static they change according HTML CSS and JavaScript Not only would you have to design your system to account for the rendering step you would also make it have to understand the Turing Complete scripting language JavaScript as well as the less complicated but inherently intertwined with HTML CSS stylesheet language If you thought the rendering process was easy try training a neural network to handle complicated scripting patterns 4 New Standards Not all HTML is equal because of different standards WHATWG began working on HTML5 in 2004 and browsers started to implement not long after In 2004 there were very few examples of HTML5 sites to train your network to begin with Sure now its standardized and every website uses it but what about HTML6 When the first specification is released probably 20172025 virtually no websites will use it because no one will support it Only when it finally becomes standard probably in the late 2020s or early 2030s will you have enough data to train your monstrous system of neural networks As for AI in general one could argue that browsers already use AI in their rendering process They intelligently decide what to render taking CSS into account when in order to get the most efficient render time they selectively use different JavaScript parsers on different sections of the code to optimize the speed the whole system has been optimized on another ruleset to make rendering and interacting with a webpage as seamless and easytouse as possible Your system will never be as good as what hundreds of humans have optimized over 20 years Trying to solve HTML rendering with Neural Networks is akin to trying to nail a nail with a screwdriver Its just not going to work Hope this was helpful
,,"<p>One popular technique for doing this is to use <a href=""http://www.artificial-immune-systems.org/"">Artificial Immune Systems</a>, an evolutionary computation approach which maintains a population of pattern detectors. </p>

<p>Here is a <a href=""https://arxiv.org/ftp/arxiv/papers/0804/0804.1266.pdf"">survey paper</a>.</p>
",,0,2016-08-13T07:45:28.300,,1608,2016-08-13T07:45:28.300,,,,,42.0,1601.0,2,5,,,,31.89,14.84,12.23,0.0,0.0,3.0,One popular technique for doing this is to use Artificial Immune Systems an evolutionary computation approach which maintains a population of pattern detectors Here is a survey paper
,,"<p>As for AGI , Everything is broken down into groups. The are all controlled by a part called the ""Spark"", and then there are the agents, little sub routines. The SPARK is the is the main judge of the system. It compares the performance of the agents. The Spark lets one agent out of sleep and records how well it does at getting reward for the body, as a whole. If a active agent does good it is replicated in free memory with a few mutations.
The 'SPARK"" and agents first look at what is on the detectors and SPARK select the best agent. And the SPARK turn on and off agents like in a orchestra. As the system matures many agents will work in parallel.
This process is the the subconscious mind.
After a while one of the agent is converted to a copy of of SPARK. This new copy is then modified and is called the OFF SPARK. The conscious mind. It will take on the control of the agents too. But SPARK is still master of all.
OFF SPARK can activate agents. But it can organizes agents on a massive scale.
It will develop many routines. Many agents working in parallel.
Once theses massive agent swarm developed into perfected routine will become a reflex. And all reflexes will be given over to SPARK the subconscious part of this system. Stored for latter use.
If OFF SPARK needs to get over to a new areas to create new patterns, like working the slot machine for the first time, to develop new routines. it need automated subconscious process of walking over the that area. OFF SPARK starts the walking routine and hands off over to spark.
IF the craving for food becomes too strong then SPARK shuts down OFF SPARK. And then uses OFF SPARK routines to get food or what urgent goal that need to be taken care of.
If all urgent goals are taken care of then there will be free will because OFF SPARK will be in control. And SPARK will be push into helper mode.
The AGI has a internal pattern editor, that is only used by OFF SPARK. that cut up existing physical routines and tries to rebuild new and different routines. This is a trial and error generator. Once a editing procedure work and perfected it is handed over to Spark for storage. Off SPAK initiate a editing routines and then SPark take over.
This editing of patterns will lead to a internal 3D simulator.
SO OFF SPARK make new physical routines and new pattern editing routines. And all this is done on the backs of automated perfected routine of old.</p>
",,1,2016-08-13T13:52:29.697,,1609,2016-08-13T16:59:00.537,2016-08-13T16:59:00.537,,1355.0,,1355.0,1593.0,2,2,,,,76.11,7.29,7.56,0.0,0.0,48.0,As for AGI Everything is broken down into groups The are all controlled by a part called the Spark and then there are the agents little sub routines The SPARK is the is the main judge of the system It compares the performance of the agents The Spark lets one agent out of sleep and records how well it does at getting reward for the body as a whole If a active agent does good it is replicated in free memory with a few mutations The SPARK and agents first look at what is on the detectors and SPARK select the best agent And the SPARK turn on and off agents like in a orchestra As the system matures many agents will work in parallel This process is the the subconscious mind After a while one of the agent is converted to a copy of of SPARK This new copy is then modified and is called the OFF SPARK The conscious mind It will take on the control of the agents too But SPARK is still master of all OFF SPARK can activate agents But it can organizes agents on a massive scale It will develop many routines Many agents working in parallel Once theses massive agent swarm developed into perfected routine will become a reflex And all reflexes will be given over to SPARK the subconscious part of this system Stored for latter use If OFF SPARK needs to get over to a new areas to create new patterns like working the slot machine for the first time to develop new routines it need automated subconscious process of walking over the that area OFF SPARK starts the walking routine and hands off over to spark IF the craving for food becomes too strong then SPARK shuts down OFF SPARK And then uses OFF SPARK routines to get food or what urgent goal that need to be taken care of If all urgent goals are taken care of then there will be free will because OFF SPARK will be in control And SPARK will be push into helper mode The AGI has a internal pattern editor that is only used by OFF SPARK that cut up existing physical routines and tries to rebuild new and different routines This is a trial and error generator Once a editing procedure work and perfected it is handed over to Spark for storage Off SPAK initiate a editing routines and then SPark take over This editing of patterns will lead to a internal 3D simulator SO OFF SPARK make new physical routines and new pattern editing routines And all this is done on the backs of automated perfected routine of old
,,"<p>You're going to need some way to 'see' the area around the car, and to track the speed of nearby objects. Google uses a combination of <a href=""https://en.wikipedia.org/wiki/Lidar"" rel=""nofollow"">LIDAR</a>, radar, conventional cameras, and occasionally sonar (see <a href=""http://www.makeuseof.com/tag/how-self-driving-cars-work-the-nuts-and-bolts-behind-googles-autonomous-car-program"" rel=""nofollow""> here</a> for a high-level overview). This technology is quite expensive, and can easily cost thousands of US dollars.</p>

<p>However, a bigger obstacle than the expense of the hardware (which would be smaller for a table-top prototype) is the software complexity. Like many major projects, the software for self-driving cars is the result of years of work from AI research teams, and thus extremely difficult to duplicate on your own.</p>

<p>That said, you're not trying to make a state-of-the-art self-driving car.  Assuming you're an expert in image processing and robotics, you can probably create a basic prototype, (like something that drive in a limited table-top environment). However, it's still going to take a lot of time and money. </p>
",,1,2016-08-13T15:48:26.767,,1610,2016-08-13T15:48:26.767,,,,,127.0,1592.0,2,4,,,,52.29,12.36,10.01,0.0,0.0,40.0,Youre going to need some way to see the area around the car and to track the speed of nearby objects Google uses a combination of LIDAR radar conventional cameras and occasionally sonar see here for a highlevel overview This technology is quite expensive and can easily cost thousands of US dollars However a bigger obstacle than the expense of the hardware which would be smaller for a tabletop prototype is the software complexity Like many major projects the software for selfdriving cars is the result of years of work from AI research teams and thus extremely difficult to duplicate on your own That said youre not trying to make a stateoftheart selfdriving car Assuming youre an expert in image processing and robotics you can probably create a basic prototype like something that drive in a limited tabletop environment However its still going to take a lot of time and money
,0.0,"<p>I'm trying to make a conversational chatbot, so the user inputs are quite wide ranging - beyond just ""turn lights on"". I want to detect the category of the user intents from their inputs and prepare responses.</p>

<p>I've looked at MS' Luis and api.ai and the intents require a lot of training. Can people suggest other techniques for untrained intent detection?</p>

<p>For example if the user says ""Pasta is my favorite dish to cook"" then detect ""intent preference entity pasta"" - then I can gradually build up responses to different categories of inputs.</p>

<p>Perhaps the crowd-sourced intents that wit.ai (facebook) has access to could do this but I'm not sure if all end-users have access to those models.</p>
",2016-08-14T02:14:31.893,3,2016-08-13T18:17:50.147,,1611,2016-08-13T18:17:50.147,,,,,1506.0,,1,1,<untagged>,What are good APIs out there for (untrained) intent detection?,34.0,65.22,10.84,9.8,0.0,0.0,25.0,Im trying to make a conversational chatbot so the user inputs are quite wide ranging beyond just turn lights on I want to detect the category of the user intents from their inputs and prepare responses Ive looked at MS Luis and apiai and the intents require a lot of training Can people suggest other techniques for untrained intent detection For example if the user says Pasta is my favorite dish to cook then detect intent preference entity pasta then I can gradually build up responses to different categories of inputs Perhaps the crowdsourced intents that witai facebook has access to could do this but Im not sure if all endusers have access to those models
,,"<p><a href=""https://en.wikipedia.org/wiki/Emotion"" rel=""nofollow"">Emotions</a> aren't something that you can implement - they're very complex. However, you can attempt to mimic them. Human emotions are closely related to conscious experience characterized by intense mental activity, which is based on interpretation of events.</p>

<p>Recent brain studies (including research in cognitive psychology and neurophysiology) suggests that human emotional assessment of every action or event plays an important role in human mental processes.</p>

<p>The recent <a href=""http://bica2016.bicasociety.org/"" rel=""nofollow"">2016 Annual Meeting of the BICA Society</a> brought together scientists from around the world to approach principles and mechanisms of human thought to create biologically inspired AI.</p>

<p>For example, in Samsonovich's (a professor in the Cybernetics Department at the <a href=""https://en.wikipedia.org/wiki/National_Research_Nuclear_University_MEPhI"" rel=""nofollow"">MEPhI</a>) proposal, the idea is to test AI in computer games which involves actions with emotional content, where AI may engage with players in different types of social relationships (such as trust, subordination or leadership).</p>

<p>Jonathan Gratch of the <a href=""https://en.wikipedia.org/wiki/Institute_for_Creative_Technologies"" rel=""nofollow"">ICT</a>, invented virtual characters capable of identifying and expressing emotions by communicating with humans in their natural language based on the situations where for example AI can deceive a human to achieve the desired result. The effect is obviously not achieved by re-creating human consciousness, but by achieving statistically adjusting parameters.</p>

<p>Researchers from the Institute of Cyber Intelligence Systems in MEPhI are hoping to be able to create in the near future future virtual beings which are capable of planning, setting goals and establishing social relationships with humans, also by possessing both emotional and narrative intelligence which can interpret context of events.</p>

<p><sup>Source: <a href=""http://phys.org/news/2016-07-social-emotions-artificial-intelligence.html"" rel=""nofollow"">Researcher proposes social emotions test for artificial intelligence</a></sup></p>
",,1,2016-08-14T02:49:57.967,,1612,2016-08-25T12:07:22.613,2016-08-25T12:07:22.613,,145.0,,8.0,26.0,2,0,,,,28.57,17.0,12.31,0.0,0.0,31.0,Emotions arent something that you can implement theyre very complex However you can attempt to mimic them Human emotions are closely related to conscious experience characterized by intense mental activity which is based on interpretation of events Recent brain studies including research in cognitive psychology and neurophysiology suggests that human emotional assessment of every action or event plays an important role in human mental processes The recent 2016 Annual Meeting of the BICA Society brought together scientists from around the world to approach principles and mechanisms of human thought to create biologically inspired AI For example in Samsonovichs a professor in the Cybernetics Department at the MEPhI proposal the idea is to test AI in computer games which involves actions with emotional content where AI may engage with players in different types of social relationships such as trust subordination or leadership Jonathan Gratch of the ICT invented virtual characters capable of identifying and expressing emotions by communicating with humans in their natural language based on the situations where for example AI can deceive a human to achieve the desired result The effect is obviously not achieved by recreating human consciousness but by achieving statistically adjusting parameters Researchers from the Institute of Cyber Intelligence Systems in MEPhI are hoping to be able to create in the near future future virtual beings which are capable of planning setting goals and establishing social relationships with humans also by possessing both emotional and narrative intelligence which can interpret context of events Source Researcher proposes social emotions test for artificial intelligence
,3.0,"<p>How does a domestic autonomous robotic vacuum cleaner -  such as a <a href=""https://en.wikipedia.org/wiki/Roomba"" rel=""nofollow"">Roomba</a> - know when it's working cleaned area (aka virtual map), and how does it plan to travel to the areas which hasn't been explored yet?</p>

<p>Does it use some kind of <a href=""https://en.wikipedia.org/wiki/A*_search_algorithm"" rel=""nofollow"">A*</a> algorithm?</p>
",,0,2016-08-14T03:05:08.873,1.0,1613,2016-11-01T19:08:17.067,2016-08-14T12:26:15.817,,8.0,,8.0,,1,4,<real-time><path-planning><robotics>,How do autonomous robotic vacuum cleaners perceive the environment for navigation?,69.0,66.07,8.94,10.11,0.0,0.0,10.0,How does a domestic autonomous robotic vacuum cleaner such as a Roomba know when its working cleaned area aka virtual map and how does it plan to travel to the areas which hasnt been explored yet Does it use some kind of A algorithm
1615.0,1.0,"<p>It has been <a href=""http://www.itnonline.com/content/will-fda-be-too-much-intelligent-machines"" rel=""nofollow"">suggested</a> that machine learning algorithms (also <a href=""http://ai.stackexchange.com/q/1427/8"">Watson</a>) can help with finding disease in patient images and optimize scans. Also that deep learning algorithms show promise for every type of digital imaging.</p>

<p>How does exactly deep learning algorithms exactly can find suspicious patterns in the body’s biochemistry?</p>
",,0,2016-08-14T03:17:29.793,,1614,2016-08-14T05:51:57.010,2016-08-14T05:51:57.010,,33.0,,8.0,,1,3,<deep-learning><healthcare><learning-algorithms>,How can artificial intelligence (including deep learning algorithms) find suspicious patterns in the body’s biochemistry?,35.0,46.47,15.31,11.21,0.0,0.0,5.0,It has been suggested that machine learning algorithms also Watson can help with finding disease in patient images and optimize scans Also that deep learning algorithms show promise for every type of digital imaging How does exactly deep learning algorithms exactly can find suspicious patterns in the body’s biochemistry
,,"<p>I wouldn't focus <em>only</em> on ""deep learning"" unless you have some specific reason for doing so.  There may be other techniques which could be as effective, or more effective.  </p>

<p>One approach I've seen used for something similar was <a href=""https://en.wikipedia.org/wiki/Inductive_logic_programming"" rel=""nofollow"">Inductive Logic Programming</a>.  For one example of using ILP to reason about elements of biochemistry, see <a href=""https://www.researchgate.net/publication/224309517_Estimation_of_Possible_Reaction_States_in_Metabolic_Pathways_Using_Inductive_Logic_Programming"" rel=""nofollow"">this paper</a></p>

<p>That's not exactly about detecting disease, but it does sort of illustrate the broad idea of reasoning about states and reactions involving metabolic pathways in biochemistry, using ILP.   Possibly the basic idea could be adapted more towards detecting disease.  </p>
",,0,2016-08-14T05:49:46.060,,1615,2016-08-14T05:49:46.060,,,,,33.0,1614.0,2,2,,,,43.73,13.52,11.06,0.0,0.0,14.0,I wouldnt focus only on deep learning unless you have some specific reason for doing so There may be other techniques which could be as effective or more effective One approach Ive seen used for something similar was Inductive Logic Programming For one example of using ILP to reason about elements of biochemistry see this paper Thats not exactly about detecting disease but it does sort of illustrate the broad idea of reasoning about states and reactions involving metabolic pathways in biochemistry using ILP Possibly the basic idea could be adapted more towards detecting disease
,,"<p>This seems to fall broadly into the regime of a <a href=""https://en.wikipedia.org/wiki/Statistical_classification"" rel=""nofollow"">classification problem</a> as you want to classify an outgoing communication as ""contains proprietary information"" or ""does not contain proprietary information"".  As such, any classification approach could be applied.  Neural Networks certainly seem like a valid approach, but you might also get good mileage out of Random Forests, Support Vector Machines, a Naive Bayes classifier, etc.</p>

<p>GA's are more aimed towards optimization than classification, so I wouldn't say that a GA, in and of itself, would map cleanly to solving this kind of problem. If GA's had applicability here, I think it would be more likely to be in terms of training a model rooted in one of the other techniques.     </p>
",,0,2016-08-14T05:57:55.947,,1616,2016-08-14T05:57:55.947,,,,,33.0,1603.0,2,3,,,,47.12,12.42,10.62,0.0,0.0,21.0,This seems to fall broadly into the regime of a classification problem as you want to classify an outgoing communication as contains proprietary information or does not contain proprietary information As such any classification approach could be applied Neural Networks certainly seem like a valid approach but you might also get good mileage out of Random Forests Support Vector Machines a Naive Bayes classifier etc GAs are more aimed towards optimization than classification so I wouldnt say that a GA in and of itself would map cleanly to solving this kind of problem If GAs had applicability here I think it would be more likely to be in terms of training a model rooted in one of the other techniques
1627.0,1.0,"<p>The <a href=""https://www.youtube.com/watch?v=AplG6KnOr2Q"" rel=""nofollow"">Mario Lives!</a> video (and its follow-up video, <a href=""https://www.youtube.com/watch?v=ltPj3RlN4Nw&amp;list=PLuOoXrWK6Kz5ySULxGMtAUdZEg9SkXDoq&amp;index=5"" rel=""nofollow"">Mario Becomes Social!</a>) showcases an AI unit that is able to simulate emotional desicion-making within a virtual world, and can enter into ""emotional states"" such as curiosity, hunger, happiness, and fear. While this seems cool and exciting (especially for video game AI), I am confused how this would be useful in real-world scenarios.</p>

<p>What would be the point of building autonomous actors that would behave based on these emotional states, instead of simply knowing <em>what</em> they should do (either by hardcoding in the rules, or learning the rules through machine learning)?</p>
",2016-08-16T00:02:37.917,1,2016-08-14T17:28:47.513,,1617,2016-08-15T10:44:09.767,2016-08-14T17:37:13.450,,181.0,,181.0,,1,1,<emotional-intelligence>,Why would someone want to simulate emotional desicion-making within an AI?,64.0,51.38,13.29,11.8,0.0,0.0,24.0,The Mario Lives video and its followup video Mario Becomes Social showcases an AI unit that is able to simulate emotional desicionmaking within a virtual world and can enter into emotional states such as curiosity hunger happiness and fear While this seems cool and exciting especially for video game AI I am confused how this would be useful in realworld scenarios What would be the point of building autonomous actors that would behave based on these emotional states instead of simply knowing what they should do either by hardcoding in the rules or learning the rules through machine learning
1626.0,2.0,"<p><sub>This is from the 2014 closed beta. The asker had the UID of 245.</sub></p>

<p>For a deterministic problem space, I need to find a neural network with the optimal node and link structure. I want to use a genetic algorithm to simulate many neural networks to find the best network structure for the problem domain.</p>

<p>I know a fair amount about neural networks<sup>1</sup> but have not used genetic algorithms for a task like this before.</p>

<p>What are the practical considerations? 
How should I encode the structure into a genome?</p>

<hr>

<p><sub><sup>1</sup>Actually, I don't. Just saying that. -Mithrandir. </sub></p>
",,2,2016-08-14T19:16:12.140,1.0,1618,2016-09-11T15:33:42.327,2016-09-10T19:01:59.917,,145.0,,145.0,,1,8,<neural-networks><genetic-algorithms>,What are the practical considerations of using a genetic algorithm to decide the structure of a neural network?,162.0,69.07,8.91,8.94,0.0,0.0,14.0,This is from the 2014 closed beta The asker had the UID of 245 For a deterministic problem space I need to find a neural network with the optimal node and link structure I want to use a genetic algorithm to simulate many neural networks to find the best network structure for the problem domain I know a fair amount about neural networks1 but have not used genetic algorithms for a task like this before What are the practical considerations How should I encode the structure into a genome 1Actually I dont Just saying that Mithrandir
,,"<p>The machine learning is a sub-set of artificial intelligence which is only a small part of its potential. It's a specific way to implement AI largely focused on statistical/probabilistic techniques and evolutionary techniques.<sup><a href=""https://www.quora.com/What-are-the-main-differences-between-artificial-intelligence-and-machine-learning/answer/Phillip-Rhodes"" rel=""nofollow"">Q</a></sup></p>

<h3>Artificial intelligence</h3>

<p>Artificial intelligence is '<strong>the theory and development of computer systems able to perform tasks normally requiring human intelligence</strong>' (such as visual perception, speech recognition, decision-making, and translation between languages).</p>

<p>We can think AI as concept of non-human decision making<sup><a href=""https://www.quora.com/What-are-the-main-differences-between-artificial-intelligence-and-machine-learning/answer/Yuval-Ariav"" rel=""nofollow"">Q</a></sup> which aims to simulate cognitive human-like functions such as problem solving, decision making or language communication.</p>

<h3>Machine learning</h3>

<p>Machine learning (ML) is basically <strong>a learning through doing</strong> by implementation of build models which can predict and identify patterns from data.</p>

<p>According to Prof. <a href=""http://www.cs.colby.edu/srtaylor/"" rel=""nofollow"">Stephanie R. Taylor</a> of Computer Science and her <a href=""http://cs.colby.edu/courses/S15/cs251/LectureNotes/Lecture_15_MLandDMintro_03_09_2015.pdf"" rel=""nofollow"">lecture paper</a>, and also <a href=""https://en.wikipedia.org/wiki/Learning#Machine_learning"" rel=""nofollow"">Wikipedia page</a>, 'machine learning is a branch of artificial intelligence and <strong>it's about construction and study of systems that can learn from data</strong>' (like based on the existing email messages to learn how to distinguish between spam and non-spam).</p>

<p>According to <a href=""http://www.oxforddictionaries.com/definition/english/machine-learning"" rel=""nofollow"">Oxford Dictionaries</a>, the machine learning is '<strong>the capacity of a computer to learn from experience</strong>' (e.g. modify its processing on the basis of newly acquired information).</p>

<p>We can think ML as computerized pattern detection in the existing data to predict patterns in future data.<sup><a href=""https://www.quora.com/What-are-the-main-differences-between-artificial-intelligence-and-machine-learning/answer/Yuval-Ariav"" rel=""nofollow"">Q</a></sup></p>

<hr>

<p>In other words, <strong>machine learning involves development of self-learning algorithms</strong> and <strong>artificial intelligence involves developing systems or softwares</strong> to mimic human to respond and behave in a circumstance.<sup><a href=""https://www.quora.com/What-are-the-main-differences-between-artificial-intelligence-and-machine-learning/answer/Sakthi-Dasan-2"" rel=""nofollow"">Quora</a></sup></p>
",,0,2016-08-15T02:38:50.250,,1621,2016-10-25T09:54:08.473,2016-10-25T09:54:08.473,,8.0,,8.0,35.0,2,5,,,,32.22,17.0,11.19,0.0,0.0,44.0,The machine learning is a subset of artificial intelligence which is only a small part of its potential Its a specific way to implement AI largely focused on statisticalprobabilistic techniques and evolutionary techniquesQ Artificial intelligence Artificial intelligence is the theory and development of computer systems able to perform tasks normally requiring human intelligence such as visual perception speech recognition decisionmaking and translation between languages We can think AI as concept of nonhuman decision makingQ which aims to simulate cognitive humanlike functions such as problem solving decision making or language communication Machine learning Machine learning ML is basically a learning through doing by implementation of build models which can predict and identify patterns from data According to Prof Stephanie R Taylor of Computer Science and her lecture paper and also Wikipedia page machine learning is a branch of artificial intelligence and its about construction and study of systems that can learn from data like based on the existing email messages to learn how to distinguish between spam and nonspam According to Oxford Dictionaries the machine learning is the capacity of a computer to learn from experience eg modify its processing on the basis of newly acquired information We can think ML as computerized pattern detection in the existing data to predict patterns in future dataQ In other words machine learning involves development of selflearning algorithms and artificial intelligence involves developing systems or softwares to mimic human to respond and behave in a circumstanceQuora
,2.0,"<p>Were there any studies which checked the accuracy of neural network predictions of greyhound racing results, compared to a human expert? Would it achieve a better payoff?</p>
",,0,2016-08-15T03:33:07.827,1.0,1625,2016-09-28T17:00:20.347,2016-08-15T14:56:08.323,,145.0,,8.0,,1,2,<neural-networks><research>,Can neural networks be better than human experts at prediction of greyhound racing results?,127.0,57.77,13.04,13.08,0.0,0.0,3.0,Were there any studies which checked the accuracy of neural network predictions of greyhound racing results compared to a human expert Would it achieve a better payoff
,,"<p>Section 4.2 of <a href=""https://cs.gmu.edu/~sean/book/metaheuristics/"">""Essentials of Metaheuristics""</a> has a wealth of information on alternative ways of encoding graph structures via Genetic Algorithms.</p>

<p>With particular regard to evolving ANNs, I would personally not be inclined to implement this sort of thing 'from scratch':</p>

<p>The field of neuroevolution has been around for some time, and the implementation some of the methods, such as Neuroevolution of Augmenting Topologies (<a href=""http://www.cs.ucf.edu/~kstanley/neat.html"">NEAT</a>) now incorporate the results of much practical experience.</p>

<p>According to the above link:</p>

<blockquote>
  <p>We also developed an extension to NEAT called HyperNEAT that can evolve neural networks with millions of connections and exploit geometric regularities in the task domain. The HyperNEAT Page includes links to publications and a general explanation of the approach.</p>
</blockquote>
",,0,2016-08-15T07:59:57.977,,1626,2016-08-15T08:12:41.983,2016-08-15T08:12:41.983,,42.0,,42.0,1618.0,2,11,,,,33.07,15.91,11.39,0.0,0.0,16.0,Section 42 of Essentials of Metaheuristics has a wealth of information on alternative ways of encoding graph structures via Genetic Algorithms With particular regard to evolving ANNs I would personally not be inclined to implement this sort of thing from scratch The field of neuroevolution has been around for some time and the implementation some of the methods such as Neuroevolution of Augmenting Topologies NEAT now incorporate the results of much practical experience According to the above link We also developed an extension to NEAT called HyperNEAT that can evolve neural networks with millions of connections and exploit geometric regularities in the task domain The HyperNEAT Page includes links to publications and a general explanation of the approach
,,"<p>Humans have poor understanding of emotional rules. Probably every poster on here has experienced <em>greatly</em> misreading another individual emotionally. Further, people often don't act emotionally how they would expect themselves to act, for example we have all experienced frustration at someone else's irrational concerns and yet we are all guilty of holding irrational concerns of our own. This is the crux of why hard-coded emotional rules do not work - we do not have an understanding of what emotional 'rules' make someone feel real.</p>

<p>By moving towards autonomous state-based actors we move away from this issue. The actor's state transitions will of course be defined rules (hard-coded or learnt) but by abstracting the actor's emotional state from the specific context (e.g. specific actions trigger emotional state transitions which trigger responses, instead of a direct response to a specific action), the programmer prevents them-self from projecting their own beliefs/emotions/logic onto the actor. </p>

<p>Further, autonomous actors are more extendable. Consider an autonomous actor that is hard-coded to move towards 'upset' and 'angry' emotional states when experiencing 'pain'. Simply by associating a new world action with 'pain', one can trigger an emotional response from an autonomous actor that has not experienced that action before. When working in hard-coded emotional rules, this would not be possible.</p>
",,0,2016-08-15T10:44:09.767,,1627,2016-08-15T10:44:09.767,,,,,1467.0,1617.0,2,2,,,,35.07,15.78,9.9,0.0,0.0,46.0,Humans have poor understanding of emotional rules Probably every poster on here has experienced greatly misreading another individual emotionally Further people often dont act emotionally how they would expect themselves to act for example we have all experienced frustration at someone elses irrational concerns and yet we are all guilty of holding irrational concerns of our own This is the crux of why hardcoded emotional rules do not work we do not have an understanding of what emotional rules make someone feel real By moving towards autonomous statebased actors we move away from this issue The actors state transitions will of course be defined rules hardcoded or learnt but by abstracting the actors emotional state from the specific context eg specific actions trigger emotional state transitions which trigger responses instead of a direct response to a specific action the programmer prevents themself from projecting their own beliefsemotionslogic onto the actor Further autonomous actors are more extendable Consider an autonomous actor that is hardcoded to move towards upset and angry emotional states when experiencing pain Simply by associating a new world action with pain one can trigger an emotional response from an autonomous actor that has not experienced that action before When working in hardcoded emotional rules this would not be possible
1629.0,1.0,"<p>I've read about The Loebner Prize for AI, which pledged a Grand Prize of $100,000 and a Gold Medal for the first computer whose responses were indistinguishable from a human's.</p>

<p>So I was wondering whether any chatbots have fooled the judges and won a Gold Medal yet?</p>

<p>From their <a href=""http://www.loebner.net/Prizef/loebner-prize.html"" rel=""nofollow"">website</a> this isn't clear (as some of the links doesn't load).</p>

<hr>

<p>A few highlights from previous years:</p>

<p><a href=""http://loebner.exeter.ac.uk/results/"" rel=""nofollow"">2011 Loebner Prize results</a></p>

<blockquote>
  <p>None of the AI systems fooled the judges, therefore the Turing Test has not been passed.</p>
</blockquote>

<p><a href=""http://www.paulmckevitt.com/loebner2013/scoring/loebner2013leaderboard.txt"" rel=""nofollow"">Loebner 2013 results</a>:</p>

<blockquote>
  <p>No chatbot fooled any of the 4 Judges.</p>
</blockquote>
",,0,2016-08-15T11:08:05.700,,1628,2016-08-15T14:37:10.257,2016-08-15T14:37:10.257,,145.0,,8.0,,1,1,<history><turing-test><chat-bots>,Have any chatbots fooled the judges and won the Loebner Prize Gold medal yet?,51.0,77.16,10.04,8.83,0.0,0.0,17.0,Ive read about The Loebner Prize for AI which pledged a Grand Prize of 100000 and a Gold Medal for the first computer whose responses were indistinguishable from a humans So I was wondering whether any chatbots have fooled the judges and won a Gold Medal yet From their website this isnt clear as some of the links doesnt load A few highlights from previous years 2011 Loebner Prize results None of the AI systems fooled the judges therefore the Turing Test has not been passed Loebner 2013 results No chatbot fooled any of the 4 Judges
,,"<p>The 2016 finals haven't started yet, they will start on Saturday, 17 September 2016. </p>

<p>In the 2015 finals or before that, nobody won the Gold Medal or the Silver Medal. 
The most up-to-date data can be found <a href=""http://www.aisb.org.uk/events/loebner-prize"" rel=""nofollow"">here</a>, where we can find both the results from 2015 and the timeline of the 2016 contest. </p>
",,0,2016-08-15T11:36:28.793,,1629,2016-08-15T11:36:28.793,,,,,29.0,1628.0,2,4,,,,87.05,8.18,8.92,0.0,0.0,10.0,The 2016 finals havent started yet they will start on Saturday 17 September 2016 In the 2015 finals or before that nobody won the Gold Medal or the Silver Medal The most uptodate data can be found here where we can find both the results from 2015 and the timeline of the 2016 contest
1631.0,3.0,"<p>Hypothetically, assume that you have access to infinite computing power. Do we have designs for any brute-force algorithms that can find an AI capable of passing traditional tests (e.g. Turing, Chinese Room, MIST, etc.)? </p>
",,3,2016-08-15T11:36:37.463,1.0,1630,2016-12-18T18:40:48.327,,,,,1467.0,,1,2,<turing-test><strong-ai><ai-design>,"Given enough computational resources, do we currently have any algorithms which could achieve AI?",140.0,51.55,13.44,11.16,0.0,0.0,12.0,Hypothetically assume that you have access to infinite computing power Do we have designs for any bruteforce algorithms that can find an AI capable of passing traditional tests eg Turing Chinese Room MIST etc
,,"<p>What 'infinite' means here could possibly be debated at some length, but that notwithstanding, here are two conflicting answers:</p>

<p>'Yes': Simulate all possible universes. Stop when you get to one containing a flavor of intelligence that passes whatever test you have in mind. Steven Wolfram has suggested something <a href=""https://www.inverse.com/article/12838-stephen-wolfram-could-there-be-alien-intelligence-among-the-digits-of-pi"" rel=""nofollow"">broadly along these lines</a>. Problem: the state of computational testing for intelligence <a href=""https://en.wikipedia.org/wiki/Winograd_Schema_Challenge"" rel=""nofollow"">e.g. Winograd schema</a> would then be the bottleneck. In the limit, testing for intelligence requires intelligence and creativity on behalf of the questioner.</p>

<p>'No': It may be that, even with infinite ability to simulate, there may be some missing aspect of our simulation that is necessary for intelligence. For example, AFAIK quantum gravity (for which we lack an adequate theory) is involved in Penrose's <a href=""https://www.sciencedaily.com/releases/2014/01/140116085105.htm"" rel=""nofollow"">""Quantum Microtubules""</a> theory of consciousness (*). What if that was needed, but we didn't know how to include it in the simulation?</p>

<p>The reason for talking in terms of such incredibly costly computations as 'simulate all possible universes' (or at least a brain-sized portion of them) is to deliberately generalize away from the specifics of any techniques currently in vogue (DL, neuromorphic systems etc). The point is that we could be missing something essential for intelligence from <em>any</em> of these models and (as far as we know from our current theories of physical reality) only empirical evidence to the contrary would tell us otherwise.</p>

<p>(*) No-one knows if consciousness is required for Strong AI, and physics can't distinguish a conscious entity from a <a href=""http://plato.stanford.edu/entries/zombies/"" rel=""nofollow"">Zombie</a>.</p>
",,0,2016-08-15T12:07:47.927,,1631,2016-08-15T15:59:28.900,2016-08-15T15:59:28.900,,42.0,,42.0,1630.0,2,7,,,,42.21,13.98,10.43,0.0,0.0,55.0,What infinite means here could possibly be debated at some length but that notwithstanding here are two conflicting answers Yes Simulate all possible universes Stop when you get to one containing a flavor of intelligence that passes whatever test you have in mind Steven Wolfram has suggested something broadly along these lines Problem the state of computational testing for intelligence eg Winograd schema would then be the bottleneck In the limit testing for intelligence requires intelligence and creativity on behalf of the questioner No It may be that even with infinite ability to simulate there may be some missing aspect of our simulation that is necessary for intelligence For example AFAIK quantum gravity for which we lack an adequate theory is involved in Penroses Quantum Microtubules theory of consciousness What if that was needed but we didnt know how to include it in the simulation The reason for talking in terms of such incredibly costly computations as simulate all possible universes or at least a brainsized portion of them is to deliberately generalize away from the specifics of any techniques currently in vogue DL neuromorphic systems etc The point is that we could be missing something essential for intelligence from any of these models and as far as we know from our current theories of physical reality only empirical evidence to the contrary would tell us otherwise Noone knows if consciousness is required for Strong AI and physics cant distinguish a conscious entity from a Zombie
1640.0,1.0,"<p>I'm aware this could be a complex topic, however I'm interested in existing research projects or studies where people are attempting or have succeeded in teaching an AI a foreign language just by training/teaching it from English books. By reading, analysing and understanding, so that it knows the foreign language's rules (such as grammar, spelling, etc.), the same way as a human would learn. The language doesn't have to be Chinese, which is difficult for even humans to learn.</p>
",,0,2016-08-15T12:52:22.533,,1632,2016-08-15T15:57:15.073,2016-08-15T14:33:23.287,,145.0,,8.0,,1,1,<research><machine-learning><self-learning><language-processing>,What are the current approaches for AI to learn a foreign language just from English books?,49.0,59.84,12.3,10.41,0.0,0.0,18.0,Im aware this could be a complex topic however Im interested in existing research projects or studies where people are attempting or have succeeded in teaching an AI a foreign language just by trainingteaching it from English books By reading analysing and understanding so that it knows the foreign languages rules such as grammar spelling etc the same way as a human would learn The language doesnt have to be Chinese which is difficult for even humans to learn
,,,,0,2016-08-15T14:47:54.450,,1633,2016-08-15T14:47:54.450,2016-08-15T14:47:54.450,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions about studies and academic research. Do NOT use this tag when you are trying to find something out.,,0,2016-08-15T14:47:54.450,,1634,2016-08-15T15:51:52.470,2016-08-15T15:51:52.470,,145.0,,145.0,,4,0,,,,78.25,8.5,8.87,0.0,0.0,2.0,For questions about studies and academic research Do NOT use this tag when you are trying to find something out
,3.0,"<p>Would it be possible to put Asimov's three Laws of Robotics into an AI?</p>

<p>The three laws are:</p>

<ol>
<li><p>A robot (or, more accurately, an AI) cannot harm a human being, or through inaction allow a human being to be harmed<sup>1</sup></p></li>
<li><p>A robot must listen to instructions given to it by a human, as long as that does not conflict with the first law.</p></li>
<li><p>A robot must protect its own existence, if that does not conflict with the first two laws.</p></li>
</ol>

<hr>

<p><sup>1</sup> <em>To it's knowledge</em>. This was a plot point in one of the books :P</p>
",,2,2016-08-15T14:59:47.477,,1635,2016-08-17T08:17:01.623,2016-08-16T06:12:06.340,,29.0,,145.0,,1,3,<robotics><asimovs-laws>,Is it possible to implement Asimov's Three Laws of Robotics?,109.0,86.03,6.27,7.74,0.0,0.0,15.0,Would it be possible to put Asimovs three Laws of Robotics into an AI The three laws are A robot or more accurately an AI cannot harm a human being or through inaction allow a human being to be harmed1 A robot must listen to instructions given to it by a human as long as that does not conflict with the first law A robot must protect its own existence if that does not conflict with the first two laws 1 To its knowledge This was a plot point in one of the books P
,,,,0,2016-08-15T15:20:44.123,,1636,2016-08-15T15:20:44.123,2016-08-15T15:20:44.123,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions about the definition of some AI-related term.,,0,2016-08-15T15:20:44.123,,1637,2016-08-15T15:51:55.830,2016-08-15T15:51:55.830,,145.0,,145.0,,4,0,,,,45.42,13.83,11.1,0.0,0.0,2.0,For questions about the definition of some AIrelated term
,,"<p>The most challenging part is this section of the first law:</p>

<blockquote>
  <p>or through inaction allow a human being to be harmed</p>
</blockquote>

<p>Humans manage to injure themselves unintentionally in all kinds of ways all the time. A robot strictly following that law would have to spend all its time saving people from their own clumsiness and would probably never get any useful work done. An AI unable to physically move wouldn't have to run around, but it would still have to think of ways to stop all accidents it could imagine. </p>

<p>Anyway, fully implementing those laws would require very advanced recognition and cognition. (How do you know that industrial machine over there is about to let off a cloud of burning hot steam onto that child who wandered into the factory?) Figuring out whether a human would end up harmed after a given action through some sequence of events becomes an exceptionally challenging problem very quickly.</p>
",,0,2016-08-15T15:29:22.567,,1638,2016-08-15T15:29:22.567,,,,,75.0,1635.0,2,6,,,,53.75,11.03,10.42,0.0,0.0,12.0,The most challenging part is this section of the first law or through inaction allow a human being to be harmed Humans manage to injure themselves unintentionally in all kinds of ways all the time A robot strictly following that law would have to spend all its time saving people from their own clumsiness and would probably never get any useful work done An AI unable to physically move wouldnt have to run around but it would still have to think of ways to stop all accidents it could imagine Anyway fully implementing those laws would require very advanced recognition and cognition How do you know that industrial machine over there is about to let off a cloud of burning hot steam onto that child who wandered into the factory Figuring out whether a human would end up harmed after a given action through some sequence of events becomes an exceptionally challenging problem very quickly
,,"<p>Defining ""harm"" and in particular, ""allowing harm via inaction"" in any meaningful way would be difficult. For example, should robots spend all their time flying around attempting to prevent humans from inhaling passive smoke or petrol fumes?</p>

<p>In addition, the interpretation of 'conflict' (in either rule 2 or 3) is completely open-ended. Resolving such conflicts seems to me to be ""AI complete"" in general.</p>

<p>Humans have quite good mechanisms (both behavioral and social) for interacting in a complex world (mostly) without harming one another, but these are perhaps not so easily codified. The complex set of legal rules that sit on top of this (polution regulations etc) are the ones that we could most easily program, but they are really quite specialised relative to the underlying physiological and social 'rules'.</p>

<p>EDIT: From other comments, it seems worth distinguishing between 'all possible harm' and 'all the kinds of harm that humans routinely anticipate'. There seems to be consensus that 'all possible harm' is a non-starter, which still leaves the hard (IMO, AI-complete) task of equaling human ability to predict harm. </p>

<p>Even if we can do that, if we are to treat as actual laws, then we would still need a formal mechanism for conflict resolution (e.g. ""Robot, I will commit suicide unless you punch that man""). </p>
",,2,2016-08-15T15:34:02.513,,1639,2016-08-17T08:17:01.623,2016-08-17T08:17:01.623,,42.0,,42.0,1635.0,2,6,,,,49.65,12.59,10.65,0.0,0.0,56.0,Defining harm and in particular allowing harm via inaction in any meaningful way would be difficult For example should robots spend all their time flying around attempting to prevent humans from inhaling passive smoke or petrol fumes In addition the interpretation of conflict in either rule 2 or 3 is completely openended Resolving such conflicts seems to me to be AI complete in general Humans have quite good mechanisms both behavioral and social for interacting in a complex world mostly without harming one another but these are perhaps not so easily codified The complex set of legal rules that sit on top of this polution regulations etc are the ones that we could most easily program but they are really quite specialised relative to the underlying physiological and social rules EDIT From other comments it seems worth distinguishing between all possible harm and all the kinds of harm that humans routinely anticipate There seems to be consensus that all possible harm is a nonstarter which still leaves the hard IMO AIcomplete task of equaling human ability to predict harm Even if we can do that if we are to treat as actual laws then we would still need a formal mechanism for conflict resolution eg Robot I will commit suicide unless you punch that man
,,"<p>Current approaches for learning a language require having a large corpus of that language; it also doesn't seem reasonable to expect that it will ever be possible to learn about language A by extracting information from a corpus from an unrelated language B.</p>

<p>Even if you want to learn about human languages in general (what sorts of things are true about grammar, vocabulary, and so on), that relies having many languages as training data, so that you can see the different ways of doing things instead of assuming that the way they're done in English is the way they're done in every language.</p>

<p>(There is work in automatic translation that goes from a language to 'concept-space', then goes from that 'concept-space' to another language, so that you can build an English-Chinese translator by building two separate English-Concept and Chinese-Concept translators, instead of ever needing material that directly links English and Chinese. The obvious benefit of this is scalability; in order to make translators for a new language to any other language, you just need to learn that language and the models build themselves.)</p>
",,0,2016-08-15T15:57:15.073,,1640,2016-08-15T15:57:15.073,,,,,10.0,1632.0,2,2,,,,24.99,13.42,11.0,0.0,0.0,30.0,Current approaches for learning a language require having a large corpus of that language it also doesnt seem reasonable to expect that it will ever be possible to learn about language A by extracting information from a corpus from an unrelated language B Even if you want to learn about human languages in general what sorts of things are true about grammar vocabulary and so on that relies having many languages as training data so that you can see the different ways of doing things instead of assuming that the way theyre done in English is the way theyre done in every language There is work in automatic translation that goes from a language to conceptspace then goes from that conceptspace to another language so that you can build an EnglishChinese translator by building two separate EnglishConcept and ChineseConcept translators instead of ever needing material that directly links English and Chinese The obvious benefit of this is scalability in order to make translators for a new language to any other language you just need to learn that language and the models build themselves
,,"<p>We're definitely nowhere near that level of AI; at best, high-tech solutions like deep convolutional neural nets can help with image recognition and some other algorithms can perform things like robotic movement adequately enough to be useful in some scenarios. None of this is even as sophisticated as the behavior of a flea, but no one refers to insects as ""intelligent."" It's exciting stuff that allows us to solve problems that human intelligence often has difficulty with (such as classification of thousands of objects, which would tire an ordinary human mind), but it's nowhere close to replicating our higher brain functions. </p>

<p>Also keep in mind that the Turing test is a poor test of ""intelligence"" that defies common sense. By the same extension, mistaking a mannequin for a human being in the dark does not mean that the mannequin is actually human. If it were a valid test, then we passed that way back around 1980 with programs like Dear Eliza which were coded in BASIC to regurgitate human speech patterns. There's just no need to come up with a sophisticated argument like Searle's Chinese Room to debunk it, since it's silly on its face; any layman should be able to see right through the Turing Test. If anyone except Turing had come up with this test it would not have received much attention. Turing displayed one-of-a-kind genius when it came to things like computing and cryptography, but like many other experts in such fields, he had a lot of trouble grappling with metaphysics and philosophy. Searle had more common sense, but his Chinese Room example is more of a rebuttal to the Turing Test than a test in and of itself.</p>

<p>What ""intelligence"" consists of is ultimately a deep metaphysical question, not a material one. For millennia, trained philosophers have had a lot of trouble assigning clear definitions to concepts like intelligence and consciousness. Until we can answer those questions definitively, using different sets of reasoning skills than scientists, mathematicians and computer specialists are used to employing (just look at how often metaphysics is derided in some of these disciplines) then we cannot say that we have achieved genuine A.I. Until we can define what intelligence is, we cannot say whether or not we've successfully built it; we've not only got the cart before the horse, but have yet to build the cart or see a horse. By the common definitions used in everyday speech we're nowhere near genuine A.I. No one calls cows or sparrows ""intelligent,"" but our AI today isn't even as sophisticated as the mosquitoes that bite them. </p>

<p>That's not going to be a popular answer - I'll probably get a dozen downvotes for this, without anyone being able to adequately rebut my contentions, but it needs to be said. There's far too much irrational exuberance and gross overestimation of what we've achieved to date and probably always will be in this field. Historically, researchers in every generation have also grossly underestimated the computing power of the human brain; every decade or so, the estimates of the FLOPS and megabytes have to be drastically revised. We have a poor track record of even getting basic material questions about the human brain right. This clear, consistent pattern of biased overestimation of our success and the lack of any real definition, let alone a test, of intelligence is going to be a serious issue in this forum for its whole existence (assuming it survives the private beta period). We have a whole forum dedicated to a field we can't even define; we can't say for sure what A.I. really is, but we're adamantly certain that we're close to achieving it...!  We cannot say if ""brute force algorithms"" exist when we're still groping for an understanding of what it is we're trying to force our way into. Certainly, there are brute force methods to solve certain problems, like Deep Blue does at chess - but we cannot say if that qualifies as intelligence or not. It is really not possible to answer questions like this without getting into deep discussions that immediately lend themselves to opinion and debate, which the Turing Test and Searle's Room are clear examples of, in and of themselves. Since implementation details of AI are considered by many to be off-limits here, we're limited mainly to highly speculative posts about tech that often doesn't even work yet (like Google's self-driving cars) and questions like this that we can't answer without first defining intelligence. This is going to be the root of a lot of problems here for a long, long time to come...</p>
",,0,2016-08-15T16:26:58.840,,1641,2016-08-15T16:26:58.840,,,,,1427.0,1630.0,2,3,,,,43.66,11.67,9.2,0.0,0.0,123.0,Were definitely nowhere near that level of AI at best hightech solutions like deep convolutional neural nets can help with image recognition and some other algorithms can perform things like robotic movement adequately enough to be useful in some scenarios None of this is even as sophisticated as the behavior of a flea but no one refers to insects as intelligent Its exciting stuff that allows us to solve problems that human intelligence often has difficulty with such as classification of thousands of objects which would tire an ordinary human mind but its nowhere close to replicating our higher brain functions Also keep in mind that the Turing test is a poor test of intelligence that defies common sense By the same extension mistaking a mannequin for a human being in the dark does not mean that the mannequin is actually human If it were a valid test then we passed that way back around 1980 with programs like Dear Eliza which were coded in BASIC to regurgitate human speech patterns Theres just no need to come up with a sophisticated argument like Searles Chinese Room to debunk it since its silly on its face any layman should be able to see right through the Turing Test If anyone except Turing had come up with this test it would not have received much attention Turing displayed oneofakind genius when it came to things like computing and cryptography but like many other experts in such fields he had a lot of trouble grappling with metaphysics and philosophy Searle had more common sense but his Chinese Room example is more of a rebuttal to the Turing Test than a test in and of itself What intelligence consists of is ultimately a deep metaphysical question not a material one For millennia trained philosophers have had a lot of trouble assigning clear definitions to concepts like intelligence and consciousness Until we can answer those questions definitively using different sets of reasoning skills than scientists mathematicians and computer specialists are used to employing just look at how often metaphysics is derided in some of these disciplines then we cannot say that we have achieved genuine AI Until we can define what intelligence is we cannot say whether or not weve successfully built it weve not only got the cart before the horse but have yet to build the cart or see a horse By the common definitions used in everyday speech were nowhere near genuine AI No one calls cows or sparrows intelligent but our AI today isnt even as sophisticated as the mosquitoes that bite them Thats not going to be a popular answer Ill probably get a dozen downvotes for this without anyone being able to adequately rebut my contentions but it needs to be said Theres far too much irrational exuberance and gross overestimation of what weve achieved to date and probably always will be in this field Historically researchers in every generation have also grossly underestimated the computing power of the human brain every decade or so the estimates of the FLOPS and megabytes have to be drastically revised We have a poor track record of even getting basic material questions about the human brain right This clear consistent pattern of biased overestimation of our success and the lack of any real definition let alone a test of intelligence is going to be a serious issue in this forum for its whole existence assuming it survives the private beta period We have a whole forum dedicated to a field we cant even define we cant say for sure what AI really is but were adamantly certain that were close to achieving it We cannot say if brute force algorithms exist when were still groping for an understanding of what it is were trying to force our way into Certainly there are brute force methods to solve certain problems like Deep Blue does at chess but we cannot say if that qualifies as intelligence or not It is really not possible to answer questions like this without getting into deep discussions that immediately lend themselves to opinion and debate which the Turing Test and Searles Room are clear examples of in and of themselves Since implementation details of AI are considered by many to be offlimits here were limited mainly to highly speculative posts about tech that often doesnt even work yet like Googles selfdriving cars and questions like this that we cant answer without first defining intelligence This is going to be the root of a lot of problems here for a long long time to come
1912.0,2.0,"<p>I'd like to investigate the possibility of achieving similar recognition as it's in <a href=""http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf"" rel=""nofollow noreferrer"">Honda's ASIMO robot</a><sup>p.22</sup> which can interpret the positioning and movement of a hand, including postures and gestures based on visual information.</p>

<p>Here is the example of application such interpretation in robot:</p>

<p><a href=""http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UDram.png"" alt=""Honda&#39;s ASIMO robot - Recognition of postures and gestures based on visual information""></a></p>

<p><sup>Image source: <a href=""http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf"" rel=""nofollow noreferrer"">ASIMO Featuring Intelligence Technology - Technical Information (PDF)</a></sup></p>

<p>So basically the recognition should detect an indicated location (posture recognition) or respond to a wave (gesture recognition), also similar like <a href=""http://ai.stackexchange.com/a/1577/8"">Google car</a> does it (by determining certain patterns).</p>

<p>Is it known how ASIMO does it, or what would be the closest alternative for postures and gestures recognition to achieve the same results?</p>
",,0,2016-08-16T01:48:02.907,3.0,1644,2016-09-12T09:43:12.550,2016-08-16T01:53:35.503,,8.0,,8.0,,1,1,<image-recognition><robots><detecting-patterns>,How to achieve recognition of postures and gestures?,100.0,19.2,16.54,11.65,0.0,0.0,21.0,Id like to investigate the possibility of achieving similar recognition as its in Hondas ASIMO robotp22 which can interpret the positioning and movement of a hand including postures and gestures based on visual information Here is the example of application such interpretation in robot Image source ASIMO Featuring Intelligence Technology Technical Information PDF So basically the recognition should detect an indicated location posture recognition or respond to a wave gesture recognition also similar like Google car does it by determining certain patterns Is it known how ASIMO does it or what would be the closest alternative for postures and gestures recognition to achieve the same results
,,,,0,2016-08-16T09:40:12.390,,1646,2016-08-16T09:40:12.390,2016-08-16T09:40:12.390,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions about all aspects of training an AI.,,0,2016-08-16T09:40:12.390,,1647,2016-08-16T16:09:55.223,2016-08-16T16:09:55.223,,145.0,,145.0,,4,0,,,,87.72,8.03,11.1,0.0,0.0,1.0,For questions about all aspects of training an AI
1649.0,3.0,"<p>For Example:</p>

<h2>Could you provide reasons why a sundial is <em>not</em> ""intelligent""?</h2>

<p>A sundial senses its environment and acts rationally. It outputs the time. It also stores  percepts. (The numbers the engineer wrote on it.)</p>

<h2>What properties of a self driving car would make it ""intelligent""?</h2>

<p>Where is the line between non intelligent matter and an intelligent system?</p>
",,2,2016-08-16T12:06:21.700,1.0,1648,2016-09-13T18:05:07.533,2016-08-16T12:11:30.123,,157.0,,157.0,,1,6,<intelligence-testing>,What are the criteria for a system to be considered intelligent?,119.0,63.05,10.63,9.49,0.0,0.0,14.0,For Example Could you provide reasons why a sundial is not intelligent A sundial senses its environment and acts rationally It outputs the time It also stores percepts The numbers the engineer wrote on it What properties of a self driving car would make it intelligent Where is the line between non intelligent matter and an intelligent system
,,"<p>Typically, I think of intelligence in terms of the <em>control</em> of <em>perception</em>. [1] A related, but different, definition of intelligence is the (at least partial) restriction of possible future states. For example, an intelligent Chess player is one whose future rarely includes 'lost at chess to a weaker opponent' states; they're able to make changes that move those states to 'won at chess' states.</p>

<p>These are both broad and continuous definitions of intelligence, where we can talk about differences of degree. A sundial doesn't exert any control over its environment; it passively casts a shadow, and so doesn't have intelligence worth speaking of. A thermostat attached to a heating or cooling system, on the other hand, does exert control over its environment, trying to keep the temperature of its sensor within some preferred range. So a thermostat does have intelligence, but not very much.</p>

<p>Self-driving cars obviously fit those definitions of intelligence.</p>

<hr>

<p>[1] Control is meant in the context of <a href=""https://en.wikipedia.org/wiki/Control_theory"" rel=""nofollow"">control theory</a>, a branch of engineering that deals with dynamical systems that perceive some fact about the external world and also have a way by which they change that fact. When perception is explicitly contrasted to observations, it typically refers to an abstract feature of observations (you observe the intensity of light from individual pixels, you perceive the apple that they represent) but here I mean it as a superset that includes observation. The thermostat is a dynamical system that perceives temperature and acts to exert pressure on the temperature it perceives.</p>

<p>(There's a philosophical point here that the thermostat cares directly about its sensor reading, not whatever the temperature ""actually"" is. I think that's not something that should be included in intelligence, and should deserve a name of its own, because understanding the difference between perception and reality and seeking to make sure one's perceptions are accurate to reality is another thing that seems partially independent of intelligence.)</p>
",,2,2016-08-16T16:05:22.283,,1649,2016-08-16T19:25:46.880,2016-08-16T19:25:46.880,,10.0,,10.0,1648.0,2,2,,,,38.15,14.16,9.95,0.0,0.0,54.0,Typically I think of intelligence in terms of the control of perception 1 A related but different definition of intelligence is the at least partial restriction of possible future states For example an intelligent Chess player is one whose future rarely includes lost at chess to a weaker opponent states theyre able to make changes that move those states to won at chess states These are both broad and continuous definitions of intelligence where we can talk about differences of degree A sundial doesnt exert any control over its environment it passively casts a shadow and so doesnt have intelligence worth speaking of A thermostat attached to a heating or cooling system on the other hand does exert control over its environment trying to keep the temperature of its sensor within some preferred range So a thermostat does have intelligence but not very much Selfdriving cars obviously fit those definitions of intelligence 1 Control is meant in the context of control theory a branch of engineering that deals with dynamical systems that perceive some fact about the external world and also have a way by which they change that fact When perception is explicitly contrasted to observations it typically refers to an abstract feature of observations you observe the intensity of light from individual pixels you perceive the apple that they represent but here I mean it as a superset that includes observation The thermostat is a dynamical system that perceives temperature and acts to exert pressure on the temperature it perceives Theres a philosophical point here that the thermostat cares directly about its sensor reading not whatever the temperature actually is I think thats not something that should be included in intelligence and should deserve a name of its own because understanding the difference between perception and reality and seeking to make sure ones perceptions are accurate to reality is another thing that seems partially independent of intelligence
,,,,0,2016-08-16T20:37:28.987,,1651,2016-08-16T20:37:28.987,2016-08-16T20:37:28.987,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,"For questions about how an AI learns by itself, without being trained.",,0,2016-08-16T20:37:28.987,,1652,2016-08-16T22:31:30.053,2016-08-16T22:31:30.053,,145.0,,145.0,,4,0,,,,84.68,10.37,10.81,0.0,0.0,2.0,For questions about how an AI learns by itself without being trained
,,"<p>To ask what makes a system intelligent almost begs the question 'in this context what do we mean by artificially intelligent?' which I think this what this question is really gearing towards.</p>

<p>From my studies, I've come to see that 'Artificial Intelligence' is a catchy term to use but perhaps misleading, and it conjures up images of these self-driving cars and robots that will take over the earth.</p>

<p>What I've found AI, and 'intelligent' systems moreso represent is an aid or a support that works <em>for</em> us, rather than one that works <em>because</em> of us... hear me out:</p>

<p>What makes the jump to an intelligent system for me is the step where the system begins to 'adapt / learn' or otherwise do things I didn't directly tell it to do. With the sundial, I measured and cut every inch of it by hand, and put it in a specific way to do a specific thing. </p>

<p>When a programmer gets into a car he automated, it may do some things he didn't directly program or maybe couldn't even expect (just one example: querying some database to see lots of people are driving somewhere, discovering a concert is going on there, and asking if the driver wants directions / tickets)</p>

<p>--</p>

<p>In conclusion, an intelligent system to me is one that we build in such a way that it educates and supports <em>us</em>, rather than a system we ourselves 'educate' to do a specific task. Supportive systems that elucidate and adapt and act 'rationally' even when we didn't tell it what 'rational' behaviour was.</p>
",,2,2016-08-16T20:46:04.797,,1653,2016-08-16T20:46:04.797,,,,,1538.0,1648.0,2,2,,,,55.51,10.05,9.27,0.0,0.0,50.0,To ask what makes a system intelligent almost begs the question in this context what do we mean by artificially intelligent which I think this what this question is really gearing towards From my studies Ive come to see that Artificial Intelligence is a catchy term to use but perhaps misleading and it conjures up images of these selfdriving cars and robots that will take over the earth What Ive found AI and intelligent systems moreso represent is an aid or a support that works for us rather than one that works because of us hear me out What makes the jump to an intelligent system for me is the step where the system begins to adapt learn or otherwise do things I didnt directly tell it to do With the sundial I measured and cut every inch of it by hand and put it in a specific way to do a specific thing When a programmer gets into a car he automated it may do some things he didnt directly program or maybe couldnt even expect just one example querying some database to see lots of people are driving somewhere discovering a concert is going on there and asking if the driver wants directions tickets In conclusion an intelligent system to me is one that we build in such a way that it educates and supports us rather than a system we ourselves educate to do a specific task Supportive systems that elucidate and adapt and act rationally even when we didnt tell it what rational behaviour was
,,"<p>I think this is almost a trick question in a sense. Let me explain:</p>

<p>For law 1, any AI would abide by the first rule unless it was deliberately created to be malevolent, in that the AI it would understand harm was imminent but do nothing about or would actively attempt to harm. Any 'reasonable' AI would (try its best to) prevent any harm it understood, but couldn't react to imminent harm 'outside it's knowledge', thus satisfying law 1. Any AI that 'tries its best' to prevent harm works here.</p>

<p>For law 2, it is simply a matter of design. If one can design an AI capable of parsing and understanding the entirety of human language (beyond just speech), just program it to act accordingly, mindful of the first law. Thus, I think we can develop an AI that will obey every command <em>it understands</em> but getting it to understand anything and everything I believe is impossible.</p>

<p>For law 3, it rides in the same vein as law 1.</p>

<p>In conclusion, I think there is no philosophical problem with implementing such an AI, but that the actual design of such an AI is fundamentally impossible (understanding all possible harms, and all possible commands).</p>
",,0,2016-08-16T21:32:55.577,,1654,2016-08-16T21:32:55.577,,,,,1538.0,1635.0,2,0,,,,65.46,9.75,9.19,0.0,0.0,36.0,I think this is almost a trick question in a sense Let me explain For law 1 any AI would abide by the first rule unless it was deliberately created to be malevolent in that the AI it would understand harm was imminent but do nothing about or would actively attempt to harm Any reasonable AI would try its best to prevent any harm it understood but couldnt react to imminent harm outside its knowledge thus satisfying law 1 Any AI that tries its best to prevent harm works here For law 2 it is simply a matter of design If one can design an AI capable of parsing and understanding the entirety of human language beyond just speech just program it to act accordingly mindful of the first law Thus I think we can develop an AI that will obey every command it understands but getting it to understand anything and everything I believe is impossible For law 3 it rides in the same vein as law 1 In conclusion I think there is no philosophical problem with implementing such an AI but that the actual design of such an AI is fundamentally impossible understanding all possible harms and all possible commands
1734.0,2.0,"<p>We can read on <a href=""https://en.wikipedia.org/wiki/TensorFlow#Tensor_processing_unit_.28TPU.29"" rel=""nofollow"">Wikipedia page</a> that Google built a custom ASIC chip for machine learning and tailored for TensorFlow which helps to accelerate AI.</p>

<p>Since ASIC chips are specially customized for one particular use without the ability to change its circuit, there must be some fixed algorithm which is invoked.</p>

<p>So how exactly does the acceleration of AI using ASIC chips work if its algorithm cannot be changed? Which part of it is exactly accelerating?</p>
",,3,2016-08-17T02:02:41.510,1.0,1655,2016-08-24T10:22:19.957,2016-08-17T09:29:26.260,,8.0,,8.0,,1,3,<machine-learning><hardware>,How does using ASIC for the acceleration of AI work?,219.0,52.19,11.26,9.57,0.0,0.0,5.0,We can read on Wikipedia page that Google built a custom ASIC chip for machine learning and tailored for TensorFlow which helps to accelerate AI Since ASIC chips are specially customized for one particular use without the ability to change its circuit there must be some fixed algorithm which is invoked So how exactly does the acceleration of AI using ASIC chips work if its algorithm cannot be changed Which part of it is exactly accelerating
,0.0,"<p>I was reading that the <a href=""http://nasa-jsc-robotics.github.io/valkyrie/"" rel=""nofollow"">Valkyrie robot</a> was originally designed to 'carry out search and rescue missions'.</p>

<p>However there were some talks to send it to Mars to assist astronauts.</p>

<p>What kind of specific trainings or tasks are planned for 'him' to be able to carry on its own?</p>

<p>Refs:</p>

<ul>
<li><a href=""https://github.com/nasa-jsc-robotics"" rel=""nofollow"">NASA-JSC-Robotics at GitHub</a></li>
<li><a href=""http://nasa-jsc-robotics.github.io/valkyrie/"" rel=""nofollow"">github.io page</a></li>
<li><a href=""https://gitlab.com/nasa-jsc-robotics/valkyrie"" rel=""nofollow"">gitlab page</a></li>
</ul>
",,0,2016-08-17T03:03:13.243,,1658,2016-08-17T09:18:43.313,2016-08-17T09:18:43.313,,145.0,,8.0,,1,1,<robotics><nasa>,What would the Valkyrie AI robot do on Mars?,23.0,68.36,9.49,9.47,0.0,0.0,11.0,I was reading that the Valkyrie robot was originally designed to carry out search and rescue missions However there were some talks to send it to Mars to assist astronauts What kind of specific trainings or tasks are planned for him to be able to carry on its own Refs NASAJSCRobotics at GitHub githubio page gitlab page
,,,,0,2016-08-17T10:18:40.907,,1660,2016-08-17T10:18:40.907,2016-08-17T10:18:40.907,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,A method for solving constrained and unconstrained optimization problems based on natural selection processes. Use this tag for questions about GA; programming questions are off-topic. See http://meta.ai.stackexchange.com/q/71.,,0,2016-08-17T10:18:40.907,,1661,2016-08-17T11:42:23.997,2016-08-17T11:42:23.997,,145.0,,145.0,,4,0,,,,23.93,25.28,13.08,0.0,0.0,13.0,A method for solving constrained and unconstrained optimization problems based on natural selection processes Use this tag for questions about GA programming questions are offtopic See httpmetaaistackexchangecomq71
1663.0,1.0,"<p>Do scientists know by what mechanism biological brains/biological neural networks store data?</p>

<p>I was thinking about @kenorbs <a href=""http://ai.stackexchange.com/questions/1656/how-can-nanobot-implants-in-our-brains-connect-to-the-internet"">question</a> about implanting nanobots to build an AGI on top of human wetware. </p>

<p>I only have a vague notion that we store data in our brains by altering synapses? </p>

<p>Links, Criticism and Detailed Explanation welcome.</p>

<p>I also would love a decent description of how a vanilla Artificial Neural Network stores data. </p>

<p><strong>Questions:</strong></p>

<ol>
<li><p>How is data stored in a biological Neural Network?</p></li>
<li><p>How is data stored in an Artificial Neural Network?</p></li>
</ol>
",,0,2016-08-17T11:57:42.683,,1662,2016-08-17T12:11:09.200,2016-08-17T12:08:59.860,,29.0,,157.0,,1,4,<neural-networks><neuromorphic-computing>,How do Artificial Neural Networks store data compared to Biological Neural Networks?,65.0,50.43,12.17,10.97,0.0,0.0,11.0,Do scientists know by what mechanism biological brainsbiological neural networks store data I was thinking about kenorbs question about implanting nanobots to build an AGI on top of human wetware I only have a vague notion that we store data in our brains by altering synapses Links Criticism and Detailed Explanation welcome I also would love a decent description of how a vanilla Artificial Neural Network stores data Questions How is data stored in a biological Neural Network How is data stored in an Artificial Neural Network
,,"<p>Second question first: Data is stored in an ANN in the form of weights in the adjacency matrix between neurons. During training, these weights are updated by a learning algorithm (such as <a href=""https://en.wikipedia.org/wiki/Backpropagation"" rel=""nofollow"">backpropagation</a>).</p>

<p>First question: according to award-winning neuroscientist Tim Bliss:</p>

<blockquote>
  <p>“It’s been accepted really since the turn of the 20th century, since the time of the Spanish neuroscientist Ramón y Cajal, that really the only place where memories can be stored is at synapses, the junctions between nerve cells.</p>
</blockquote>

<p>A protein called the NMDA receptor plays a key roll in the strengthening of synaptic connections (which is more broadly achieved by a form of <a href=""https://en.wikipedia.org/wiki/Hebbian_theory"" rel=""nofollow"">Hebbian Learning</a>).</p>
",,0,2016-08-17T12:11:09.200,,1663,2016-08-17T12:11:09.200,,,,,42.0,1662.0,2,4,,,,52.73,13.29,10.43,0.0,0.0,16.0,Second question first Data is stored in an ANN in the form of weights in the adjacency matrix between neurons During training these weights are updated by a learning algorithm such as backpropagation First question according to awardwinning neuroscientist Tim Bliss “It’s been accepted really since the turn of the 20th century since the time of the Spanish neuroscientist Ramón y Cajal that really the only place where memories can be stored is at synapses the junctions between nerve cells A protein called the NMDA receptor plays a key roll in the strengthening of synaptic connections which is more broadly achieved by a form of Hebbian Learning
,,"<p>As a person who works with people who work on Watson, perhaps I can give some insight.</p>

<p>The name <em>Watson</em> is casually thrown around a lot whilst many people aren't aware of its evolution into a larger suite of systems and services. We now have Chef Watson, Watson Health, and many other developing projects along the ""cognitive"" route. <em>Watson</em> is really an amalgamation and varied application of the different cognitive computing routes IBM is pursuing.</p>

<p>So what I'm trying to get at is that there are many forms of NLP that Watson conducts and has conducted, developed by different teams to fit different processes, interconnected in different ways. Additionally, much (probably all) of it is proprietary/classified since, as one would imagine, ongoing research is constantly being conducted and added to Watson. This is likely your largest obstacle. The precise workings of the NLP of the Jeopardy flavor of Watson are probably themselves still classified (I can't find anything in the time I've just spent looking myself)</p>

<p>There are, thus, many answers to this question; many outdated, and others not always applicable. The full answer is very complicated and by the time you find out what the answer is today it's probably already been advanced. The researches I know are always working on new, cutting-edge algorithms and processes for text classification and the related NLP topics.</p>

<p>To point you to more information, though, take a look at these links:</p>

<p><a href=""https://www.ibm.com/watson/developercloud/nl-classifier.html"" rel=""nofollow"">https://www.ibm.com/watson/developercloud/nl-classifier.html</a></p>

<p><a href=""http://researcher.watson.ibm.com/researcher/view_group.php?id=2099"" rel=""nofollow"">http://researcher.watson.ibm.com/researcher/view_group.php?id=2099</a> (DeepQA research page, check out the publications)</p>

<p><a href=""http://www.research.ibm.com/cognitive-computing/"" rel=""nofollow"">http://www.research.ibm.com/cognitive-computing/</a> (takes a sec to load)</p>

<p>Short videos on the Watson QA subject:
<a href=""https://www.youtube.com/watch?v=tu5v-gu_5pY"" rel=""nofollow"">https://www.youtube.com/watch?v=tu5v-gu_5pY</a></p>

<p><a href=""https://www.youtube.com/watch?v=PI55a1jFrMY"" rel=""nofollow"">https://www.youtube.com/watch?v=PI55a1jFrMY</a></p>

<p><a href=""http://nlp.cs.rpi.edu/course/spring14/nlp.html"" rel=""nofollow"">http://nlp.cs.rpi.edu/course/spring14/nlp.html</a> (NLP course syllabus from RPI based around Watson)</p>

<p>A good paper on NLP that acknowledges Watson, more technical:
<a href=""http://jamia.oxfordjournals.org/content/18/5/544.short"" rel=""nofollow"">http://jamia.oxfordjournals.org/content/18/5/544.short</a></p>

<p>UPDATE: Looking at the article posted in the comments by @Pimgd, I find support for what I said above: ""For the Jeopardy Challenge, we use more than 100 different techniques for analyzing natural language, identifying sources, finding and generating hypotheses, finding and scoring evidence, and merging and ranking hypotheses.""</p>

<p>I am sure many if not most of these techniques have since been modified, adapted, or dropped altogether for other methods.</p>
",,2,2016-08-17T14:41:03.707,,1664,2016-08-17T14:48:12.957,2016-08-17T14:48:12.957,,1538.0,,1538.0,211.0,2,2,,,,40.69,18.04,10.29,0.0,0.0,133.0,As a person who works with people who work on Watson perhaps I can give some insight The name Watson is casually thrown around a lot whilst many people arent aware of its evolution into a larger suite of systems and services We now have Chef Watson Watson Health and many other developing projects along the cognitive route Watson is really an amalgamation and varied application of the different cognitive computing routes IBM is pursuing So what Im trying to get at is that there are many forms of NLP that Watson conducts and has conducted developed by different teams to fit different processes interconnected in different ways Additionally much probably all of it is proprietaryclassified since as one would imagine ongoing research is constantly being conducted and added to Watson This is likely your largest obstacle The precise workings of the NLP of the Jeopardy flavor of Watson are probably themselves still classified I cant find anything in the time Ive just spent looking myself There are thus many answers to this question many outdated and others not always applicable The full answer is very complicated and by the time you find out what the answer is today its probably already been advanced The researches I know are always working on new cuttingedge algorithms and processes for text classification and the related NLP topics To point you to more information though take a look at these links httpswwwibmcomwatsondevelopercloudnlclassifierhtml httpresearcherwatsonibmcomresearcherviewgroupphpid2099 DeepQA research page check out the publications httpwwwresearchibmcomcognitivecomputing takes a sec to load Short videos on the Watson QA subject httpswwwyoutubecomwatchvtu5vgu5pY httpswwwyoutubecomwatchvPI55a1jFrMY httpnlpcsrpieducoursespring14nlphtml NLP course syllabus from RPI based around Watson A good paper on NLP that acknowledges Watson more technical httpjamiaoxfordjournalsorgcontent185544short UPDATE Looking at the article posted in the comments by Pimgd I find support for what I said above For the Jeopardy Challenge we use more than 100 different techniques for analyzing natural language identifying sources finding and generating hypotheses finding and scoring evidence and merging and ranking hypotheses I am sure many if not most of these techniques have since been modified adapted or dropped altogether for other methods
1666.0,1.0,"<p>My understanding is that <em>Watson</em> is the name of the computer, and <em>DeepQA</em> is the name of the software or technology. They are both correlated.</p>

<p>Are there any computers/technologies other than <em>Watson</em> which <strong>are using <em>DeepQA</em></strong>? Or is <em>Watson</em> the only computer which implements that software/technology?</p>

<p><sup>This question is inspired by this <a href=""http://meta.ai.stackexchange.com/q/1177/8"">meta thread</a>.</sup></p>
",,0,2016-08-17T14:52:05.960,,1665,2016-08-17T20:30:28.350,2016-08-17T20:30:28.350,,145.0,,8.0,,1,2,<watson>,Are there any DeepQA-based computers other than Watson?,50.0,52.05,12.33,9.73,0.0,0.0,8.0,My understanding is that Watson is the name of the computer and DeepQA is the name of the software or technology They are both correlated Are there any computerstechnologies other than Watson which are using DeepQA Or is Watson the only computer which implements that softwaretechnology This question is inspired by this meta thread
,,"<p>I cannot say for certain, but I know of no such other uses (I work at the building where Watson is developing but do not directly work with it).</p>

<p>The DeepQA team's page (<a href=""https://www.research.ibm.com/deepqa/deepqa.shtml"" rel=""nofollow"">https://www.research.ibm.com/deepqa/deepqa.shtml</a>) only ever references Watson as the implementation, and based on the structure of the FAQ there I would imagine they'd be eager to list any interesting other uses it would have, but no such entries exist there.</p>

<p>I would, however, also note that while DeepQA is IBM's proprietary implementation of QA using deep-learning methods, that approach is becoming more popular and is almost certainly being worked on by other companies and could soon be on other computers, if not already so in some form.</p>
",,0,2016-08-17T15:58:12.170,,1666,2016-08-17T16:04:38.483,2016-08-17T16:04:38.483,,1538.0,,1538.0,1665.0,2,3,,,,49.99,12.6,10.32,0.0,0.0,27.0,I cannot say for certain but I know of no such other uses I work at the building where Watson is developing but do not directly work with it The DeepQA teams page httpswwwresearchibmcomdeepqadeepqashtml only ever references Watson as the implementation and based on the structure of the FAQ there I would imagine theyd be eager to list any interesting other uses it would have but no such entries exist there I would however also note that while DeepQA is IBMs proprietary implementation of QA using deeplearning methods that approach is becoming more popular and is almost certainly being worked on by other companies and could soon be on other computers if not already so in some form
,,"<p>The problem detecting NSFW has been around for over two decades.</p>

<hr>

<p>This study from 2005 about <a href=""http://link.springer.com/chapter/10.1007%2F3-540-61123-1_173"" rel=""nofollow noreferrer""><em>finding naked people</em></a>, demonstrates a strategy for finding such images based on the color and texture properties to fetch an effective mask for skin regions attempting to group a human figure using geometric constraints on the human structure. This method demonstrated </p>

<blockquote>
  <p>60% precision and 52% recall on a test set of 138 uncontrolled images of naked people.</p>
</blockquote>

<p>Here are a few figures from the study explaining the algorithm:</p>

<p><a href=""https://i.stack.imgur.com/hMC5ll.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hMC5ll.png"" alt=""Finding naked people via AI, neural network""></a></p>

<p><a href=""https://i.stack.imgur.com/qgjcYl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qgjcYl.png"" alt=""Typical control images wrongly classified as containing naked people""></a></p>

<p><sup><strong>The following post contains visualizations of nudity for scientific purposes (hover to display):</sup></strong></p>

<blockquote class=""spoiler"">
  <p> <a href=""https://i.stack.imgur.com/qlNd8m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qlNd8m.png"" alt=""Typical images correctly classified as containing naked people""></a></p>
</blockquote>

<hr>

<p>A more recent approach is using <a href=""http://ai.stackexchange.com/questions/tagged/conv-neural-network"">convolutional networks</a>. This <a href=""http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.683.4319"" rel=""nofollow noreferrer"">study from 2014</a><sup><a href=""https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf"" rel=""nofollow noreferrer"">PDF</a></sup> demonstrated impressive classification performance based on the ImageNet dataset. It's not clear '<em><a href=""http://ai.stackexchange.com/q/1479/8"">how and why</a></em> they perform so well', however they can be used for classification of images with a very low error rate.</p>

<p>For further details, check: <a href=""http://blog.clarifai.com/what-convolutional-neural-networks-see-at-when-they-see-nudity/"" rel=""nofollow noreferrer"">What convolutional neural networks look at when they see nudity</a>.</p>

<p>You will find the code example and the heatmap for how convnets see NSFW in the above link.</p>
",,0,2016-08-17T18:02:40.383,,1667,2016-08-25T09:59:13.270,2016-08-25T09:59:13.270,,145.0,,8.0,1478.0,2,0,,,,49.25,13.0,10.68,0.0,0.0,21.0,The problem detecting NSFW has been around for over two decades This study from 2005 about finding naked people demonstrates a strategy for finding such images based on the color and texture properties to fetch an effective mask for skin regions attempting to group a human figure using geometric constraints on the human structure This method demonstrated 60 precision and 52 recall on a test set of 138 uncontrolled images of naked people Here are a few figures from the study explaining the algorithm The following post contains visualizations of nudity for scientific purposes hover to display A more recent approach is using convolutional networks This study from 2014PDF demonstrated impressive classification performance based on the ImageNet dataset Its not clear how and why they perform so well however they can be used for classification of images with a very low error rate For further details check What convolutional neural networks look at when they see nudity You will find the code example and the heatmap for how convnets see NSFW in the above link
,,,,0,2016-08-17T20:33:02.043,,1668,2016-08-17T20:33:02.043,2016-08-17T20:33:02.043,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions about AI's playing chess; use this tag with the [game-theory] tag.,,0,2016-08-17T20:33:02.043,,1669,2016-08-18T03:06:10.190,2016-08-18T03:06:10.190,,145.0,,145.0,,4,0,,,,83.66,12.17,7.93,0.0,0.0,6.0,For questions about AIs playing chess use this tag with the gametheory tag
,,"<p>I know it seems like a cop-out answer to every question on AI, but ""it depends"".  For example, if the bulk of the storage space is storing learned concepts, and attributes of example entities, then it stands to reason that concepts and entities could be reused.  In that scenario, learning from an additional 10G of text would use less storage than the original.</p>

<p>OTOH, as others have said, it could be that the storage is mostly storing the <em>links</em> between things, in which case the number of links will likely grow exponentially. In that case, the second batch of ""knowledge"" would add more storage requirements than the first.</p>

<p>So it would come down to ""what exactly is the system learning, and how does it represent what it learned?""  And that answer will vary from system to system.</p>
",,0,2016-08-18T00:46:03.973,,1670,2016-08-18T00:46:03.973,,,,,33.0,1333.0,2,1,,,,68.5,9.98,8.99,0.0,0.0,24.0,I know it seems like a copout answer to every question on AI but it depends For example if the bulk of the storage space is storing learned concepts and attributes of example entities then it stands to reason that concepts and entities could be reused In that scenario learning from an additional 10G of text would use less storage than the original OTOH as others have said it could be that the storage is mostly storing the links between things in which case the number of links will likely grow exponentially In that case the second batch of knowledge would add more storage requirements than the first So it would come down to what exactly is the system learning and how does it represent what it learned And that answer will vary from system to system
,,"<p>I'll try to do something intuitive; Each node in a neural network is referred to as a neuron. To understand what's going on under the hood of a neural network you only really need to understand an individual neuron. </p>

<p>Now each neuron has a set of inputs (other neurons; they can potentially be the inputs to the network as a whole as well), and each input has a weight associated with it. Every time the network is used, each neuron computes its output as the weighted sum of its inputs passed through some gate (The ""Activation Function"", a mathematical function designed to get a particular behaviour. For example sigmoid AF takes an input of any size and transforms it into an output in the range [0, 1].) Obviously, this is driven from the inputs to the neural network so that no neuron is computing its outputs before all of the neurons used as its inputs have done the same.</p>

<p>When you refer to the value of a node; there isn't a single value. Each neuron has several weights associated with it as it may be the input to several other neurons, and each of those neurons assigns it a different weight. Instead, it is better to thing of a neural network as a directed graph of nodes (neurons) which are labelled with a particular activation function, and edges (input/output connections) which are labelled with a particular weight. While the structure and activation functions used in the neural networks is a matter of topology design, there are a number of algorithms for designing a ANN for a particular topology.</p>

<p>The most commonly used (and possibly easiest to explain) is backpropagation. In pseudo-Layman's terms we start off with random weights on all edges in the network. We then compute the output of the network for a training set (a set of known input/output pairs). By careful choice of activation function, it is possible to differentiate the error (computed analogously to the expected output minus the actual output of the ANN for each input/output pair) with respect to the weights of the neural network. This allows us to compute a gradient for each weight; the direction in which we can move the weight to reduce the error on the training set. By doing this until we find an optima (a point where all movements increase error), we can find some 'good' configuration of weights for that particular ANN. </p>

<p>There's a nice tutorial on BP here <a href=""http://www.cse.unsw.edu.au/~cs9417ml/MLP2/BackPropagation.html"" rel=""nofollow noreferrer"">http://www.cse.unsw.edu.au/~cs9417ml/MLP2/BackPropagation.html</a>. The diagram associated with it does nicely to explain my point:
<a href=""https://i.stack.imgur.com/O2d6L.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O2d6L.jpg"" alt=""enter image description here""></a></p>
",,0,2016-08-18T09:52:04.357,,1671,2016-08-18T09:52:04.357,,,,,1467.0,1535.0,2,2,,,,55.98,11.2,8.94,0.0,0.0,76.0,Ill try to do something intuitive Each node in a neural network is referred to as a neuron To understand whats going on under the hood of a neural network you only really need to understand an individual neuron Now each neuron has a set of inputs other neurons they can potentially be the inputs to the network as a whole as well and each input has a weight associated with it Every time the network is used each neuron computes its output as the weighted sum of its inputs passed through some gate The Activation Function a mathematical function designed to get a particular behaviour For example sigmoid AF takes an input of any size and transforms it into an output in the range 0 1 Obviously this is driven from the inputs to the neural network so that no neuron is computing its outputs before all of the neurons used as its inputs have done the same When you refer to the value of a node there isnt a single value Each neuron has several weights associated with it as it may be the input to several other neurons and each of those neurons assigns it a different weight Instead it is better to thing of a neural network as a directed graph of nodes neurons which are labelled with a particular activation function and edges inputoutput connections which are labelled with a particular weight While the structure and activation functions used in the neural networks is a matter of topology design there are a number of algorithms for designing a ANN for a particular topology The most commonly used and possibly easiest to explain is backpropagation In pseudoLaymans terms we start off with random weights on all edges in the network We then compute the output of the network for a training set a set of known inputoutput pairs By careful choice of activation function it is possible to differentiate the error computed analogously to the expected output minus the actual output of the ANN for each inputoutput pair with respect to the weights of the neural network This allows us to compute a gradient for each weight the direction in which we can move the weight to reduce the error on the training set By doing this until we find an optima a point where all movements increase error we can find some good configuration of weights for that particular ANN Theres a nice tutorial on BP here httpwwwcseunsweduaucs9417mlMLP2BackPropagationhtml The diagram associated with it does nicely to explain my point
,,"<p>Isaac Asimov created the fundamental <em>Three Laws of Robotics</em>, by which the robots in his <em>Robot</em> series were bound to obey.</p>

<p>The three laws are:</p>

<p>1.) A robot must not harm a human being, or through inaction allow a human to come to harm.</p>

<p>2.) A robot must obey orders given by a human, as long as that does not contradict with the First Law.</p>

<p>3.) A robot must protect its own existence, as long as that does not contradict with Rules One or Two.</p>
",,0,2016-08-18T10:09:51.883,,1672,2016-09-11T14:53:59.113,2016-09-11T14:53:59.113,,145.0,,145.0,,5,0,,,,79.6,6.55,8.38,0.0,0.0,15.0,Isaac Asimov created the fundamental Three Laws of Robotics by which the robots in his Robot series were bound to obey The three laws are 1 A robot must not harm a human being or through inaction allow a human to come to harm 2 A robot must obey orders given by a human as long as that does not contradict with the First Law 3 A robot must protect its own existence as long as that does not contradict with Rules One or Two
,,"For questions about Asimov's Laws in real life; to ask a question about something in Asimov's books, see http://scifi.stackexchange.com.",,0,2016-08-18T10:09:51.883,,1673,2016-08-18T11:34:55.833,2016-08-18T11:34:55.833,,145.0,,145.0,,4,0,,,,43.73,18.74,9.57,0.0,0.0,10.0,For questions about Asimovs Laws in real life to ask a question about something in Asimovs books see httpscifistackexchangecom
,,,,0,2016-08-18T10:12:44.613,,1674,2016-08-18T10:12:44.613,2016-08-18T10:12:44.613,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,DeepDream is a art-AI created by Google. Use this tag for questions about that. Use the [deepdreaming] tag for the process.,,0,2016-08-18T10:12:44.613,,1675,2016-08-18T10:36:49.880,2016-08-18T10:36:49.880,,145.0,,145.0,,4,0,,,,81.29,8.48,10.0,0.0,0.0,6.0,DeepDream is a artAI created by Google Use this tag for questions about that Use the deepdreaming tag for the process
,,,,0,2016-08-18T10:26:26.650,,1676,2016-08-18T10:26:26.650,2016-08-18T10:26:26.650,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,"Deepdreaming is a method for generating images by a trained neural network. For questions about Google's software, use [deepdream].",,0,2016-08-18T10:26:26.650,,1677,2016-08-18T11:35:02.287,2016-08-18T11:35:02.287,,145.0,,145.0,,4,0,,,,44.91,15.45,14.08,0.0,0.0,6.0,Deepdreaming is a method for generating images by a trained neural network For questions about Googles software use deepdream
,0.0,"<p>There is a study about <a href=""http://www.aclweb.org/anthology/P/P02/P02-1031.pdf"" rel=""nofollow"">The Necessity of Parsing for Predicate Argument Recognition</a>, however I couldn't find much information about 'Predicate Argument Recognition' which could explain it.</p>

<p>What is it exactly and how does it work, briefly?</p>
",,3,2016-08-18T14:00:52.177,1.0,1678,2016-08-18T14:13:25.083,2016-08-18T14:13:25.083,,145.0,,8.0,,1,4,<definitions><nlp><computational-linguistics>,What is predicate argument recognition?,25.0,44.24,14.21,10.53,0.0,0.0,7.0,There is a study about The Necessity of Parsing for Predicate Argument Recognition however I couldnt find much information about Predicate Argument Recognition which could explain it What is it exactly and how does it work briefly
,,"<p>See: <a href=""https://en.wikipedia.org/wiki/Natural_language_processing"" rel=""nofollow"">Natural language processing (NLP)</a> at Wikipedia.</p>
",,0,2016-08-18T14:08:05.760,,1679,2016-08-18T16:25:55.473,2016-08-18T16:25:55.473,,8.0,,8.0,,5,0,,,,38.99,18.16,13.01,0.0,0.0,4.0,See Natural language processing NLP at Wikipedia
,,Natural language processing is the computer representation and manipulation of human language.,,0,2016-08-18T14:08:05.760,,1680,2016-08-18T14:25:25.467,2016-08-18T14:25:25.467,,42.0,,8.0,,4,0,,,,8.53,21.97,14.76,0.0,0.0,1.0,Natural language processing is the computer representation and manipulation of human language
,,"<p>See: <a href=""https://en.wikipedia.org/wiki/Computational_linguistics"" rel=""nofollow"">Computational linguistics</a> at Wikipedia.</p>
",,0,2016-08-18T14:11:36.997,,1681,2016-08-18T16:25:47.707,2016-08-18T16:25:47.707,,8.0,,8.0,,5,0,,,,-9.74,24.68,13.36,0.0,0.0,2.0,See Computational linguistics at Wikipedia
,,An interdisciplinary field concerned with the statistical or rule-based modeling of natural language from a computational perspective.,,0,2016-08-18T14:11:36.997,,1682,2016-08-18T16:25:59.457,2016-08-18T16:25:59.457,,8.0,,8.0,,4,0,,,,-5.0,22.68,12.84,0.0,0.0,2.0,An interdisciplinary field concerned with the statistical or rulebased modeling of natural language from a computational perspective
,,,,0,2016-08-18T14:29:03.443,,1683,2016-08-18T14:29:03.443,2016-08-18T14:29:03.443,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,Refers to kinds of data with a high level of organization.,,0,2016-08-18T14:29:03.443,,1684,2016-08-18T16:25:52.160,2016-08-18T16:25:52.160,,8.0,,8.0,,4,0,,,,68.77,6.82,9.92,0.0,0.0,1.0,Refers to kinds of data with a high level of organization
,1.0,"<p>The Wit.ai is a Siri-like voice interface which can can parse messages and predict the actions to perform.</p>

<p>Here is the <a href=""https://labs.wit.ai/demo/index.html"" rel=""nofollow"">demo site powered by Wit.ai</a>.</p>

<p>How does it understand the spoken sentences and convert them into structured actionable data? Basically, how does it know what to do?</p>
",,0,2016-08-18T14:30:05.433,3.0,1685,2016-11-18T20:29:38.440,2016-08-23T10:05:26.173,,145.0,,8.0,,1,4,<language-processing><nlp><structured-data><voice-recognition>,How does Wit.ai convert sentences into structured data?,121.0,59.3,10.14,10.48,0.0,0.0,8.0,The Witai is a Sirilike voice interface which can can parse messages and predict the actions to perform Here is the demo site powered by Witai How does it understand the spoken sentences and convert them into structured actionable data Basically how does it know what to do
1868.0,1.0,"<p>In 2014 <a href=""https://techcrunch.com/2014/02/06/linkedin-snatches-up-data-savvy-job-search-startup-bright-com-for-120m-in-its-largest-acquisition-to-date/"" rel=""nofollow"">Linkedin acquired Bright.com</a>, for $120 million and it is using AI and big data algorithms to connect users.</p>

<blockquote>
  <p>Bright also throws in a little Klout, ranking people by a “Bright score” which it uses to assess how strong the chemistry is between a user and a particular job.</p>
  
  <p>It also takes into account historical hiring patterns into its matching, along with account location, a user’s past experience and synonyms.</p>
</blockquote>

<p>In brief, is it known (based on some research papers) how such algorithm works which aiming at scoring 'chemistry' between users and their jobs?</p>
",,0,2016-08-18T14:39:50.547,,1686,2016-09-06T17:31:44.187,2016-09-06T16:40:40.037,,10.0,,8.0,,1,0,<social>,How does 'Bright score' assess how strong the connection is between users and their jobs?,30.0,69.11,11.37,11.23,0.0,0.0,15.0,In 2014 Linkedin acquired Brightcom for 120 million and it is using AI and big data algorithms to connect users Bright also throws in a little Klout ranking people by a “Bright score” which it uses to assess how strong the chemistry is between a user and a particular job It also takes into account historical hiring patterns into its matching along with account location a user’s past experience and synonyms In brief is it known based on some research papers how such algorithm works which aiming at scoring chemistry between users and their jobs
1688.0,1.0,"<p>According to this <a href=""http://mashable.com/2014/01/06/pinterest-acquires-visualgraph/"" rel=""nofollow"">article</a>, Pinterest acquired VisualGraph, an image recognition and visual search technology startup.</p>

<p>How does Pinterest apply VisualGraph technology for machine vision, image recognition and visual search in order to classify the images?</p>

<p>In short, how do they predict the image categories? Based on what features?</p>
",,0,2016-08-18T14:53:13.537,1.0,1687,2016-09-14T19:34:59.007,2016-08-18T15:05:17.923,,145.0,,8.0,,1,-1,<image-recognition><classification><computer-vision>,How does Pinterest decipher what's on unmarked pictures and categorize them?,122.0,33.92,16.05,11.14,0.0,0.0,8.0,According to this article Pinterest acquired VisualGraph an image recognition and visual search technology startup How does Pinterest apply VisualGraph technology for machine vision image recognition and visual search in order to classify the images In short how do they predict the image categories Based on what features
,,"<p>One of the <em>Pinterest's</em> white paper about <a href=""https://engineering.pinterest.com/sites/engineering/files/article/fields/field_image/human-curation-convnets%20(1).pdf"" rel=""nofollow noreferrer"">Human Curation and Convnets powering item-to-item recommendations</a><sup><a href=""https://arxiv.org/abs/1511.04003"" rel=""nofollow noreferrer"">arxiv</a></sup> describes implementation of convolutional neural network (CNN) based visual features (<a href=""http://www.robots.ox.ac.uk/~vgg/"" rel=""nofollow noreferrer"">VGG</a><sup><a href=""https://arxiv.org/abs/1405.3531"" rel=""nofollow noreferrer"">2014</a></sup>, <a href=""http://arxiv.org/abs/1506.01497"" rel=""nofollow noreferrer"">Faster R-CNN</a>). This demonstrates the effectiveness of it (such image or object representations) which can improve user engagement. The visual features are computed using the process described in the <a href=""http://arxiv.org/abs/1505.07647"" rel=""nofollow noreferrer"">previous study about visual search at <em>Pinterest</em></a> and can be used for more targeted features to be computed for related pins.</p>

<p>Here are the examples of detected visual objects from Pinterest's object detection pipeline:</p>

<p><a href=""https://i.stack.imgur.com/q1Tvr.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q1Tvr.jpg"" alt=""Fig. 6. Examples of detected visual objects from Pinterest’s object detection pipeline. Detection of objects allows for more targeted visual features to be computed for Related Pins""></a></p>

<p><sup>Image source: <a href=""https://engineering.pinterest.com/sites/engineering/files/article/fields/field_image/human-curation-convnets%20(1).pdf"" rel=""nofollow noreferrer"">Human Curation and Convnets: Powering
Item-to-Item Recommendations on Pinterest</a>, Page 4, Fig. 6</sup></p>

<p>The images are categorized by using dominant visual objects (individual objects seen in the image which passes a confidence threshold in Faster R-CNN) using fine-tuned VGG reranking variant. This allows Pinterest to introduce features such real-time recommendations for the users.</p>

<p>Check also this blog entry: <a href=""https://engineering.pinterest.com/blog/building-scalable-machine-vision-pipeline"" rel=""nofollow noreferrer"">Building a scalable machine vision pipeline</a>.</p>
",,0,2016-08-18T17:03:13.773,,1688,2016-08-23T02:04:02.893,2016-08-23T02:04:02.893,,8.0,,8.0,1687.0,2,3,,,,23.77,17.87,11.8,0.0,0.0,32.0,One of the Pinterests white paper about Human Curation and Convnets powering itemtoitem recommendationsarxiv describes implementation of convolutional neural network CNN based visual features VGG2014 Faster RCNN This demonstrates the effectiveness of it such image or object representations which can improve user engagement The visual features are computed using the process described in the previous study about visual search at Pinterest and can be used for more targeted features to be computed for related pins Here are the examples of detected visual objects from Pinterests object detection pipeline Image source Human Curation and Convnets Powering ItemtoItem Recommendations on Pinterest Page 4 Fig 6 The images are categorized by using dominant visual objects individual objects seen in the image which passes a confidence threshold in Faster RCNN using finetuned VGG reranking variant This allows Pinterest to introduce features such realtime recommendations for the users Check also this blog entry Building a scalable machine vision pipeline
1690.0,1.0,"<p>Wolfram Language Image Identification Project launched an <a href=""https://www.imageidentify.com/"" rel=""nofollow"">Image Identify site</a> demo which returns the top predicted tags for the photos.</p>

<p>How does it work, briefly? I mean what type of learning vision technologies are used to analyze, recognize and understand the content of an image?</p>
",,2,2016-08-18T17:15:44.240,,1689,2016-08-23T20:29:03.600,2016-08-23T20:29:03.600,,145.0,,8.0,,1,-1,<image-recognition><deep-learning><classification>,How does Wolfram's Image Identification Project work?,85.0,39.33,12.81,11.75,0.0,0.0,5.0,Wolfram Language Image Identification Project launched an Image Identify site demo which returns the top predicted tags for the photos How does it work briefly I mean what type of learning vision technologies are used to analyze recognize and understand the content of an image
,,"<p>The ImageIdentify project uses the highly automated ""<a href=""https://www.wolfram.com/algorithmbase/"" rel=""nofollow"">superfunctions</a>"" and as part of Wolfram Language API integration. It relies on a complex collection of meta-algorithms and built-in '<a href=""http://www.wolfram.com/knowledgebase/"" rel=""nofollow"">knowledge</a>'. It has a built-in classifier trained from a large dataset using <a href=""http://www.wolfram.com/data-framework/"" rel=""nofollow"">Wolfram Data Framework</a> (WDF). However the main classifier is based on the deep neural networks.</p>

<p>Source: <a href=""https://www.imageidentify.com/about/how-it-works"" rel=""nofollow"">How the Wolfram Language Image Identification Project Works</a></p>

<p>The algorithm isn't perfect and misidentification are more likely to be caused by 'irrelevant objects repeatedly being in training images for a particular type of object'.</p>

<p>You can read more at <a href=""http://blog.stephenwolfram.com/2015/05/wolfram-language-artificial-intelligence-the-image-identification-project/"" rel=""nofollow"">Wolfram Language Artificial Intelligence: The Image Identification Project</a>.</p>
",,0,2016-08-18T17:15:44.240,,1690,2016-08-19T17:01:03.857,2016-08-19T17:01:03.857,,8.0,,8.0,1689.0,2,2,,,,29.04,16.47,11.35,0.0,0.0,20.0,The ImageIdentify project uses the highly automated superfunctions and as part of Wolfram Language API integration It relies on a complex collection of metaalgorithms and builtin knowledge It has a builtin classifier trained from a large dataset using Wolfram Data Framework WDF However the main classifier is based on the deep neural networks Source How the Wolfram Language Image Identification Project Works The algorithm isnt perfect and misidentification are more likely to be caused by irrelevant objects repeatedly being in training images for a particular type of object You can read more at Wolfram Language Artificial Intelligence The Image Identification Project
1704.0,3.0,"<p>I've <a href=""https://www.imageidentify.com/result/0lkzuttdxipub"" rel=""nofollow noreferrer"">uploaded a picture</a> to Wolfram's ImageIdentify of graffiti on the wall, but it recognized it as 'monocle'. Secondary guesses were 'primate', 'hominid', and 'person', so not even close to 'graffiti' or 'painting'.</p>

<p>Is it by design, or there are some <strong>methods to teach a convolutional neural network (CNN) to reason and be aware of a bigger picture context</strong> (like mentioned graffiti)? Currently it seems as if it's detecting literally <em>what is depicted in the image</em>, not <em>what the image actually is</em>.</p>

<p><a href=""https://i.stack.imgur.com/akquMm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/akquMm.png"" alt=""Wolfram&#39;s Image Identify: monocle/graffiti""></a></p>

<p>This could be the same problem as mentioned <a href=""http://ai.stackexchange.com/a/1533/8"">here</a>, that DNN are:</p>

<blockquote>
  <p>Learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs.<sup><a href=""http://ai.stackexchange.com/a/1533/8"">2015</a></sup></p>
</blockquote>

<p>If it's by design, maybe there is some better version of CNN that can perform better?</p>
",,4,2016-08-18T17:42:58.053,,1691,2016-08-23T00:14:37.027,2016-08-18T19:56:25.287,,8.0,,8.0,,1,4,<image-recognition><classification><conv-neural-network>,"How to make convnets aware what the image actually is, not what is depicted on it?",145.0,57.81,11.02,10.14,0.0,0.0,35.0,Ive uploaded a picture to Wolframs ImageIdentify of graffiti on the wall but it recognized it as monocle Secondary guesses were primate hominid and person so not even close to graffiti or painting Is it by design or there are some methods to teach a convolutional neural network CNN to reason and be aware of a bigger picture context like mentioned graffiti Currently it seems as if its detecting literally what is depicted in the image not what the image actually is This could be the same problem as mentioned here that DNN are Learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs2015 If its by design maybe there is some better version of CNN that can perform better
,,,,0,2016-08-18T18:35:05.607,,1692,2016-08-18T18:35:05.607,2016-08-18T18:35:05.607,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions about comparing two or more things related to AI; do NOT use this tag for how AI's compare things.,,0,2016-08-18T18:35:05.607,,1693,2016-08-18T20:10:57.070,2016-08-18T20:10:57.070,,145.0,,145.0,,4,0,,,,75.54,8.12,8.44,0.0,0.0,3.0,For questions about comparing two or more things related to AI do NOT use this tag for how AIs compare things
1695.0,1.0,"<p>An AI agent is often thought of having ""sensors"", ""a memory"", ""machine learning processors"" and ""reaction"" components. However, a machine with these does not necessarily become a self-programming AI agent. Beyond the parts mentioned above, is there any other elements or details necessary to make a machine capable of being a self-programming AI agent?</p>

<p>For example, <a href=""http://www.iiim.is/wp/wp-content/uploads/2011/05/goertzel-agisp-2011.pdf"" rel=""nofollow"">a paper from 2011</a> declared that solving the optimization problem of maximizing the intelligence is a must-have feature for the self-programming process, as quoted below:</p>

<blockquote>
  <p>A system is said to carry out an instance of self-programming when it undergoes learning regarding some element of its ""cognitive infrastructure"", where the latter is defined as the fuzzy set of ""intelligence-critical"" features of the system; and the intelligence-criticality of a system feature is defined as its ""feature quality,"" considered from the perspective of solving the optimization problem of maximizing the intelligence of a multi-feature system.</p>
</blockquote>

<p>However, this description of ""optimization of intelligence"" is vague. Can anyone give a clear definition or better summary for the necessary components for self-programming agents?</p>

<p><sub>This question is from the 2014 closed beta, with the asker having a UID of 23.</sub></p>
",,2,2016-08-18T19:16:29.577,,1694,2016-08-20T20:19:43.133,2016-08-20T20:19:43.133,,145.0,,145.0,,1,6,<machine-learning><computer-programming>,What are the necessary components to make AI agent self-programming-capable?,160.0,27.15,15.73,10.99,0.0,0.0,44.0,An AI agent is often thought of having sensors a memory machine learning processors and reaction components However a machine with these does not necessarily become a selfprogramming AI agent Beyond the parts mentioned above is there any other elements or details necessary to make a machine capable of being a selfprogramming AI agent For example a paper from 2011 declared that solving the optimization problem of maximizing the intelligence is a musthave feature for the selfprogramming process as quoted below A system is said to carry out an instance of selfprogramming when it undergoes learning regarding some element of its cognitive infrastructure where the latter is defined as the fuzzy set of intelligencecritical features of the system and the intelligencecriticality of a system feature is defined as its feature quality considered from the perspective of solving the optimization problem of maximizing the intelligence of a multifeature system However this description of optimization of intelligence is vague Can anyone give a clear definition or better summary for the necessary components for selfprogramming agents This question is from the 2014 closed beta with the asker having a UID of 23
,,"<p>At the highest level, all it needs is for the various systems already discussed to incorporate code objects. If it can interpret its source code / model architecture from the formatted text objects underpinning them, can 'understand' them in terms of having a useful ML model, and alter the code with its reaction, then it can self-program. </p>

<p>That is, the basic loop behind a recursively improving intelligence is simple. It examines itself, writes a new version, and then that new version examines itself and writes a new version, and so on.</p>

<p>The difficult component comes at lower levels. We don't need to invent a new concept like 'sensor,' what we need to do is build very, very sophisticated sensors that are equal to the task of understanding code well enough to detect and write improvements.</p>
",,3,2016-08-18T19:51:21.833,,1695,2016-08-18T19:51:21.833,,,,,10.0,1694.0,2,3,,,,57.3,11.67,10.63,0.0,0.0,23.0,At the highest level all it needs is for the various systems already discussed to incorporate code objects If it can interpret its source code model architecture from the formatted text objects underpinning them can understand them in terms of having a useful ML model and alter the code with its reaction then it can selfprogram That is the basic loop behind a recursively improving intelligence is simple It examines itself writes a new version and then that new version examines itself and writes a new version and so on The difficult component comes at lower levels We dont need to invent a new concept like sensor what we need to do is build very very sophisticated sensors that are equal to the task of understanding code well enough to detect and write improvements
,,"<p>You seem to be wanting some description of the 'style' of an image. </p>

<p>To make that work in general, I'd guess that would actually require quite a lot of pre-processing to present 'texture elements' (rather than pixels) as the basic features. </p>

<p>This is quite speculative, but one approach might be to use <a href=""https://en.wikipedia.org/wiki/Iterated_function_system"" rel=""nofollow"">Iterated Function Systems</a> as a means of extracting these.</p>

<p>Whether 'spatial adjacency' (and hence CNN) is then the best approach to make higher-level decisions about these elements is (AFAIK) a matter for experiment.</p>
",,0,2016-08-18T20:40:24.213,,1696,2016-08-18T20:40:24.213,,,,,42.0,1691.0,2,3,,,,49.96,12.13,10.45,0.0,0.0,21.0,You seem to be wanting some description of the style of an image To make that work in general Id guess that would actually require quite a lot of preprocessing to present texture elements rather than pixels as the basic features This is quite speculative but one approach might be to use Iterated Function Systems as a means of extracting these Whether spatial adjacency and hence CNN is then the best approach to make higherlevel decisions about these elements is AFAIK a matter for experiment
,,"<p>I can't speak to wit.ai specifically, but I can tell you a little bit about how similar applications work. Specifically, I can talk a bit about <a href=""http://stanbol.apache.org"" rel=""nofollow"">Apache Stanbol</a> which also converts free text into structured data.   That said, I should prefix this by saying there isn't just one way to ""get there from here.""  Many techniques could be part of a stack for accomplishing this goal.</p>

<p>Anyway, in the case of Stanbol, they run the text through multiple processing engines, sequentially, with different engines affecting the final output.  One engine simply does <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow"">Named Entity Recognition</a> using OpenNLP.  This identities discrete named ""things"" - people, places, companies, etc.  Another engine does entity matching with a pre-established database of entities - specifically (in the out-of-the-box configuration) a dump of entities from <a href=""http://www.dbpedia.org"" rel=""nofollow"">DBPedia</a>.  Where a match is found, the text from the original input is assigned to the entity.  In the case of a collision, it assigns a weight to the mapping so any downstream consumers can use probabilistic techniques to select the ""correct"" mapping.</p>

<p>There are, of course, more details that I left out.  Before NER can happen there is parsing and tokenizing and other NLP activities.  But a big part of the basic process is doing NER and then doing the entity matching.  </p>

<p>And in the case of Stanbol, you can add your own entities and corresponding structured data, as well as your own engines.  So, for example, if you wanted to write an engine based on neural networks / deep learning, and plug that in, you could.</p>
",,0,2016-08-19T16:47:20.397,,1697,2016-08-19T16:47:20.397,,,,,33.0,1685.0,2,2,,,,63.8,11.48,9.71,0.0,0.0,53.0,I cant speak to witai specifically but I can tell you a little bit about how similar applications work Specifically I can talk a bit about Apache Stanbol which also converts free text into structured data That said I should prefix this by saying there isnt just one way to get there from here Many techniques could be part of a stack for accomplishing this goal Anyway in the case of Stanbol they run the text through multiple processing engines sequentially with different engines affecting the final output One engine simply does Named Entity Recognition using OpenNLP This identities discrete named things people places companies etc Another engine does entity matching with a preestablished database of entities specifically in the outofthebox configuration a dump of entities from DBPedia Where a match is found the text from the original input is assigned to the entity In the case of a collision it assigns a weight to the mapping so any downstream consumers can use probabilistic techniques to select the correct mapping There are of course more details that I left out Before NER can happen there is parsing and tokenizing and other NLP activities But a big part of the basic process is doing NER and then doing the entity matching And in the case of Stanbol you can add your own entities and corresponding structured data as well as your own engines So for example if you wanted to write an engine based on neural networks deep learning and plug that in you could
,,"<p>This <a href=""http://dx.doi.org/10.1109/TPAMI.2012.59"" rel=""nofollow noreferrer"">study from 2012</a> uses 3D <a href=""http://ai.stackexchange.com/questions/tagged/conv-neural-network"">convolutional neural networks (CNN)</a> for automated recognition of human actions in surveillance videos. The 3D CNN model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. A very similar deep learning approach based on 3D CNN is demonstrated in the <a href=""http://liris.cnrs.fr/Documents/Liris-5228.pdf"" rel=""nofollow noreferrer"">LIRIS and Orange Labs study from 2011</a>.</p>

<hr>

<p>This <a href=""https://www.robots.ox.ac.uk/~vgg/publications/2014/Simonyan14b/simonyan14b.pdf.pdf"" rel=""nofollow noreferrer"">Oxford study from 2014</a> also uses a similar approach, but with two-stream CNN which incorporates spatial and temporal networks which can achieve good performance despite having limited training data. It recognises action from motion in the form of dense optical flow. For example:</p>

<p><a href=""https://i.stack.imgur.com/RWcT3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RWcT3.png"" alt=""Optical flow using ConvNets""></a></p>

<hr>

<p><a href=""http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4224216"" rel=""nofollow noreferrer"">Another study from 2007</a> demonstrates a method by detecting human falls based on a combination of motion history and human shape variation by analysing the video frames. It uses Motion History Image (MHI) to quantify the motion of the person.</p>

<p><a href=""https://i.stack.imgur.com/Hzn4z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Hzn4z.png"" alt=""Motion history image (MHI)""></a></p>

<p><sup>Source: <a href=""https://github.com/harishrithish7/Fall-Detection"" rel=""nofollow noreferrer"">harishrithish7/Fall-Detection</a> at GitHub</sup></p>

<hr>

<p>An alternative general approach could be action detection based on the posture using DNN. See: <a href=""http://ai.stackexchange.com/q/1644/8"">How to achieve recognition of postures and gestures?</a></p>
",,1,2016-08-19T16:51:02.167,,1698,2016-08-24T08:48:23.077,2016-08-24T08:48:23.077,,145.0,,8.0,1481.0,2,5,,,,42.82,14.68,11.15,0.0,0.0,21.0,This study from 2012 uses 3D convolutional neural networks CNN for automated recognition of human actions in surveillance videos The 3D CNN model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions thereby capturing the motion information encoded in multiple adjacent frames A very similar deep learning approach based on 3D CNN is demonstrated in the LIRIS and Orange Labs study from 2011 This Oxford study from 2014 also uses a similar approach but with twostream CNN which incorporates spatial and temporal networks which can achieve good performance despite having limited training data It recognises action from motion in the form of dense optical flow For example Another study from 2007 demonstrates a method by detecting human falls based on a combination of motion history and human shape variation by analysing the video frames It uses Motion History Image MHI to quantify the motion of the person Source harishrithish7FallDetection at GitHub An alternative general approach could be action detection based on the posture using DNN See How to achieve recognition of postures and gestures
,,"<p>There are several approaches as to how this can be achieved.</p>

<p>One recent study from 2015 about <a href=""http://link.springer.com/chapter/10.1007%2F978-3-319-09396-3_9"" rel=""nofollow noreferrer"">Action Recognition in Realistic Sports Videos</a><sup><a href=""http://cs.stanford.edu/~amirz/index_files/Springer2015_action_chapter.pdf"" rel=""nofollow noreferrer"">PDF</a></sup> uses the action recognition framework based on the three main steps of feature extraction (shape, post or contextual information), dictionary learning to represent a video, and classification (<a href=""https://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""nofollow noreferrer"">BoW framework</a>). </p>

<p>A few examples of methods:</p>

<ul>
<li><p>Spatio-Temporal Structures of Human Poses</p>

<p><a href=""https://i.stack.imgur.com/AgfDk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AgfDk.png"" alt=""K. Soomro and A.R. Zamir - action recognition - figure""></a></p></li>
<li><p>a joint shape-motion</p>

<p><a href=""https://i.stack.imgur.com/1qR9x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1qR9x.png"" alt=""K. Soomro and A.R. Zamir - action recognition - figure""></a></p></li>
<li><p>Multi-Task Sparse Learning (MTSL)</p></li>
<li><p>Hierarchical Space-Time Segments</p>

<p><a href=""https://i.stack.imgur.com/jBMuj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jBMuj.png"" alt=""K. Soomro and A.R. Zamir - Extracted segments from video frames""></a></p></li>
<li><p>Spatio-Temporal Deformable Part Models (SDPM)</p>

<p><a href=""https://i.stack.imgur.com/5Zehk.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5Zehk.jpg"" alt=""K. Soomro and A.R. Zamir - Action localization results""></a></p></li>
</ul>

<p>Here are the results based on training of 10 action classes based on the UCF sports dataset:</p>

<p><a href=""https://i.stack.imgur.com/v77Pv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v77Pv.png"" alt=""UCF Sports Dataset: sample frames of 10 action classes along with their bounding box annotations of the humans shown in yellow""></a></p>

<p><sup>Source: <a href=""http://link.springer.com/chapter/10.1007%2F978-3-319-09396-3_9"" rel=""nofollow noreferrer"">Action Recognition in Realistic Sports Videos</a>.</sup></p>
",,0,2016-08-19T16:52:09.607,,1699,2016-08-24T08:26:21.633,2016-08-24T08:26:21.633,,145.0,,8.0,1481.0,2,5,,,,20.35,17.88,12.19,0.0,0.0,22.0,There are several approaches as to how this can be achieved One recent study from 2015 about Action Recognition in Realistic Sports VideosPDF uses the action recognition framework based on the three main steps of feature extraction shape post or contextual information dictionary learning to represent a video and classification BoW framework A few examples of methods SpatioTemporal Structures of Human Poses a joint shapemotion MultiTask Sparse Learning MTSL Hierarchical SpaceTime Segments SpatioTemporal Deformable Part Models SDPM Here are the results based on training of 10 action classes based on the UCF sports dataset Source Action Recognition in Realistic Sports Videos
,9.0,"<p>In a <a href=""http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619"">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p>

<blockquote>
  <p>The next step in achieving human-level ai is creating intelligent—but not autonomous—machines. The AI system in your car will get you safely home, but won’t choose another destination once you’ve gone inside. From there, we’ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it’s easy to imagine them inheriting human-like qualities—and flaws. </p>
</blockquote>

<p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI's that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p>
",,2,2016-08-19T18:08:52.937,1.0,1700,2016-09-10T19:05:28.870,2016-09-10T19:05:28.870,,2214.0,,33.0,,1,11,<philosophy><emotional-intelligence>,What purpose would be served by developing AI's that experience human-like emotions?,266.0,54.73,12.76,10.08,0.0,0.0,20.0,In a recent Wall Street Journal article Yann LeCunn makes the following statement The next step in achieving humanlevel ai is creating intelligent—but not autonomous—machines The AI system in your car will get you safely home but won’t choose another destination once you’ve gone inside From there we’ll add basic drives along with emotions and moral values If we create machines that learn as well as our brains do it’s easy to imagine them inheriting humanlike qualities—and flaws Personally I have generally taken the position that talking about emotions for artificial intelligences is silly because there would be no reason to create AIs that experience emotions Obviously Yann disagrees So the question is what end would be served by doing this Does an AI need emotions to serve as a useful tool
1702.0,1.0,"<p>Inspired by <a href=""http://ai.stackexchange.com/q/1481/8"">this discussion</a> about recognizing human actions, I have found the <a href=""https://github.com/harishrithish7/Fall-Detection"" rel=""nofollow"">Fall-Detection</a> project which detects humans falling on the ground from a CCTV camera feed, and which can consider alerting the hospital authorities.</p>

<p>My question is, are there any existing real-life implementations or research projects <strong>which specifically use live video feed from the surveillance cameras in order to detect crime</strong> using convnets (or similar approaches)? If so, how do they work, briefly? Do they automatically inform the police about the crime with the details what happened and where?</p>

<p>For example car accidents, physical assaults, robberies, violent disturbances, weapon attacks, etc.</p>
",,1,2016-08-19T18:40:46.010,,1701,2016-08-24T15:02:43.753,2016-08-24T15:02:43.753,,145.0,,8.0,,1,3,<conv-neural-network><computer-vision><action-recognition>,Applications of CNN for detecting crime from video surveillance cameras,159.0,42.51,15.9,12.14,0.0,0.0,19.0,Inspired by this discussion about recognizing human actions I have found the FallDetection project which detects humans falling on the ground from a CCTV camera feed and which can consider alerting the hospital authorities My question is are there any existing reallife implementations or research projects which specifically use live video feed from the surveillance cameras in order to detect crime using convnets or similar approaches If so how do they work briefly Do they automatically inform the police about the crime with the details what happened and where For example car accidents physical assaults robberies violent disturbances weapon attacks etc
,,"<p>After a bit of research I found something kind of close:</p>

<ul>
<li><a href=""http://www.techinsider.io/security-cameras-use-artificial-intelligence-to-detect-crime-2015-8"" rel=""nofollow"">Artificially intelligent security cameras are spotting crimes before they happen</a></li>
<li><a href=""http://www.dailymail.co.uk/sciencetech/article-2154861/U-S-surveillance-cameras-use-eyes-pre-crimes-detecting-suspicious-behaviour-alerting-guards.html"" rel=""nofollow"">New surveillance cameras will use computer eyes to find 'pre crimes' by detecting suspicious behaviour and calling for guards</a></li>
<li><a href=""http://www.dailymail.co.uk/sciencetech/article-2154861/U-S-surveillance-cameras-use-eyes-pre-crimes-detecting-suspicious-behaviour-alerting-guards.html"" rel=""nofollow"">CCTV 'fightcams' detect violence 'before it happens'  at Dailymail</a>, also check at <a href=""http://www.telegraph.co.uk/news/uknews/crime/11407094/CCTV-fightcams-detect-violence-before-it-happens.html"" rel=""nofollow"">Telegraph</a></li>
</ul>

<p>They, however, makes no mention of what specific methods they use.</p>

<p>So a crime detection system as that is written does not exist, but abnormal behaviour detection systems do.</p>

<p>An accurate generalized system seems intuitively infeasible, however. Commiting a crime, unlike falling, is a complex behavior, and takes so many forms. A camera watching a store's counter like at a 7-11 could perhaps see that the 'customer''s arm is strangely reaching across the counter, and the attendant is moving a lot more than usual suddenly, but aside from very specific cases like this such a system is currently quite unfeasible. </p>

<p>Crimes are unusual, relatively speaking, and their dramatic nature means that even the simplest crimes play out in very different ways. Perhaps you could in this case you could try to look for images of a gun, or someone with their hands up. So, looking for unusual, <em>detectable</em> behavioural mannerisms may be possible, but not crime detection.</p>

<p>Ultimately, while you may be able to make (possibly pretty good) systems to detect specific crimes in specific environments, that's all we got for now.</p>

<p>P.S. - Do these camera's also get audio signals? That is also an interesting facet to consider (""PUT YOUR HANDS UP / GIVE ME ALL YOUR MONEY"")</p>
",,1,2016-08-19T20:41:14.997,,1702,2016-08-23T02:13:32.537,2016-08-23T02:13:32.537,,8.0,,1538.0,1701.0,2,4,,,,47.52,13.0,10.82,0.0,0.0,52.0,After a bit of research I found something kind of close Artificially intelligent security cameras are spotting crimes before they happen New surveillance cameras will use computer eyes to find pre crimes by detecting suspicious behaviour and calling for guards CCTV fightcams detect violence before it happens at Dailymail also check at Telegraph They however makes no mention of what specific methods they use So a crime detection system as that is written does not exist but abnormal behaviour detection systems do An accurate generalized system seems intuitively infeasible however Commiting a crime unlike falling is a complex behavior and takes so many forms A camera watching a stores counter like at a 711 could perhaps see that the customers arm is strangely reaching across the counter and the attendant is moving a lot more than usual suddenly but aside from very specific cases like this such a system is currently quite unfeasible Crimes are unusual relatively speaking and their dramatic nature means that even the simplest crimes play out in very different ways Perhaps you could in this case you could try to look for images of a gun or someone with their hands up So looking for unusual detectable behavioural mannerisms may be possible but not crime detection Ultimately while you may be able to make possibly pretty good systems to detect specific crimes in specific environments thats all we got for now PS Do these cameras also get audio signals That is also an interesting facet to consider PUT YOUR HANDS UP GIVE ME ALL YOUR MONEY
,,"<p>The answer, unlike for many questions on this board, I think is definitive. No. We don't <em>need</em> AI's to have emotion to be useful, as we can see by the numerous amount's of AI's we have already that are useful.</p>

<p>But to further address the question, we can't really give AI's emotions. I think the closest we could get would be 'Can we make this AI act in a way a human would if that human was <code>insert emotion</code> '.</p>

<p>To what end? the only immediate thing coming to mind would be to create more lifelike companions or interactions, for the purposes of video games or demo's or for fun. I heartily agree, however, that at least of any AI system I've ever considered, emotions would not better it.</p>

<p>Yann says that doing so would give our AI's more human-like qualities <em>and</em> flaws. I think it's more like it would 'give our AI's more human-like qualities <em>or in other words</em> flaws'.</p>

<p>The purpose of AI's and learning systems is to create systems that act or 'think' like humans, but better. Systems that can adapt or evolve, but mess up as little as possible. </p>

<p>""To err is human. To not is AI"" (or what we strive for)</p>
",,0,2016-08-19T20:54:49.910,,1703,2016-08-19T20:59:58.460,2016-08-19T20:59:58.460,,1538.0,,1538.0,1700.0,2,6,,,,72.56,7.54,8.3,14.0,0.0,47.0,The answer unlike for many questions on this board I think is definitive No We dont need AIs to have emotion to be useful as we can see by the numerous amounts of AIs we have already that are useful But to further address the question we cant really give AIs emotions I think the closest we could get would be Can we make this AI act in a way a human would if that human was To what end the only immediate thing coming to mind would be to create more lifelike companions or interactions for the purposes of video games or demos or for fun I heartily agree however that at least of any AI system Ive ever considered emotions would not better it Yann says that doing so would give our AIs more humanlike qualities and flaws I think its more like it would give our AIs more humanlike qualities or in other words flaws The purpose of AIs and learning systems is to create systems that act or think like humans but better Systems that can adapt or evolve but mess up as little as possible To err is human To not is AI or what we strive for
,,"<p>Wolfram's image id system is specifically meant to figure out what the image is depicting, not the medium. </p>

<p>To get what you want you'd simply have to create your own system where the training data is labeled by the medium rather than the content, and probably fiddle with it to pay more attention to texture and things as such as that. The neural net doesn't care which we want - it has no inherent bias. It just knows what it's been trained for.</p>

<p>That's really all there is to it. It's all to do with the training labels and the focus of the system (e.g. a system that looks for edge patterns that form shapes, compared to a system that checks if the lines in the image are perfectly computer-generated straight and clean vs imperfect brush strokes vs spraypaint).</p>

<p>Now, if you want me to tell you how to build that system, I'm not the right person to ask haha</p>
",,0,2016-08-19T23:40:48.100,,1704,2016-08-19T23:46:11.350,2016-08-19T23:46:11.350,,1538.0,,1538.0,1691.0,2,2,,,,76.66,8.24,8.2,0.0,0.0,24.0,Wolframs image id system is specifically meant to figure out what the image is depicting not the medium To get what you want youd simply have to create your own system where the training data is labeled by the medium rather than the content and probably fiddle with it to pay more attention to texture and things as such as that The neural net doesnt care which we want it has no inherent bias It just knows what its been trained for Thats really all there is to it Its all to do with the training labels and the focus of the system eg a system that looks for edge patterns that form shapes compared to a system that checks if the lines in the image are perfectly computergenerated straight and clean vs imperfect brush strokes vs spraypaint Now if you want me to tell you how to build that system Im not the right person to ask haha
,1.0,"<p>I'm trying to come up with the right algorithm for a system in which the user enters a few symptoms and the system has to predict or determine the likelihood that a few selected symptoms are associated with those existing in the system. Then after associating them, the result or output should be a specific disease for the symptoms.</p>

<p>The system is comprised of a series of diseases with each assigned to specific symptoms, which also exist in the system.</p>

<p>Let's assume that the user entered the following input:</p>

<pre><code>A, B, C, and D
</code></pre>

<p>The first thing the system should do is check and associate each symptom (in this case represented by alphabetical letters) individually against a data-table of symptoms that already exist. And in cases where the input doesn't exist, the system should report or send feedback about it.</p>

<p>And also, let's say that <code>A and B</code> was in the data-table, so we are 100% sure that they're valid or exist and the system is able to give out the disease based on the input. Then let's say that the input now is <code>C and D</code> where <code>C</code> doesn't exist in the data-table, but there is a possibility that <code>D</code> exists.</p>

<p>We don't give <code>D</code> a score of 100%, but maybe something lower (let's say 90%). Then <code>C</code> just doesn't exist at all in the data-table. So, <code>C</code> gets a score of 0%.</p>

<p>Therefore, the system should have some kind of association and prediction techniques or rules to output the result by judging the user's input.</p>

<p>Summary of generating the output:</p>

<pre><code>If A and B were entered and exist, then output = 100%
If D was entered and existed but C was not, then output = 90%
If all entered don't exist, then output = 0%
</code></pre>

<p>What techniques would be used to produce this system?</p>
",,0,2016-08-21T20:03:02.017,1.0,1705,2016-08-23T08:35:13.237,2016-08-23T07:05:32.247,,33.0,,1581.0,,1,6,<algorithm><machine-learning><prediction>,Selecting the right technique to predict disease from symptoms,187.0,58.42,10.04,9.02,203.0,0.0,46.0,Im trying to come up with the right algorithm for a system in which the user enters a few symptoms and the system has to predict or determine the likelihood that a few selected symptoms are associated with those existing in the system Then after associating them the result or output should be a specific disease for the symptoms The system is comprised of a series of diseases with each assigned to specific symptoms which also exist in the system Lets assume that the user entered the following input The first thing the system should do is check and associate each symptom in this case represented by alphabetical letters individually against a datatable of symptoms that already exist And in cases where the input doesnt exist the system should report or send feedback about it And also lets say that was in the datatable so we are 100 sure that theyre valid or exist and the system is able to give out the disease based on the input Then lets say that the input now is where doesnt exist in the datatable but there is a possibility that exists We dont give a score of 100 but maybe something lower lets say 90 Then just doesnt exist at all in the datatable So gets a score of 0 Therefore the system should have some kind of association and prediction techniques or rules to output the result by judging the users input Summary of generating the output What techniques would be used to produce this system
1708.0,1.0,"<p>I have gone through the <a href=""https://en.wikipedia.org/wiki/Statistical_relational_learning"">wikipedia explanation of SRL</a>. But, it only confused me more:</p>

<blockquote>
  <p>Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure.</p>
</blockquote>

<p>Can someone give a more dumbed down explanation of the same, preferably with an example?</p>
",,0,2016-08-22T05:47:31.883,1.0,1706,2016-08-22T15:55:06.007,2016-08-22T09:36:04.963,,145.0,,101.0,,1,5,<definitions><statistical-ai>,What is Statistical relational learning?,60.0,32.53,15.55,10.79,0.0,0.0,11.0,I have gone through the wikipedia explanation of SRL But it only confused me more Statistical relational learning SRL is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty which can be dealt with using statistical methods and complex relational structure Can someone give a more dumbed down explanation of the same preferably with an example
,,"<p><b>Artificial Intelligence Stack Exchange</b> is a question and answer site for people interested in conceptual questions about life and challenges in a world where ""cognitive"" functions can be mimicked in a purely digital environment. It's built and run <i>by you</i> as part of the <a href=""http://stackexchange.com"">Stack Exchange</a> network of Q&amp;A sites. With your help, we're working together to build a library of detailed answers to every question about artificial intelligence.</p>
",,0,2016-08-22T15:01:48.870,,1707,2016-08-22T15:01:48.870,2016-08-22T15:01:48.870,,95.0,,-1.0,,7,0,,,,39.67,14.05,11.18,0.0,0.0,10.0,Artificial Intelligence Stack Exchange is a question and answer site for people interested in conceptual questions about life and challenges in a world where cognitive functions can be mimicked in a purely digital environment Its built and run by you as part of the Stack Exchange network of QampA sites With your help were working together to build a library of detailed answers to every question about artificial intelligence
,,"<p>The University of Maryland <a href=""https://www.cs.umd.edu/class/spring2008/cmsc828g/Slides/SRL-Tutorial-05-08.pdf"">published some slides</a> (PDF) from an introductory presentation on this topic.</p>

<p>The fourth page explains why SRL is interesting. ""Traditional statistical machine learning approaches"" process one sort of thing in which there is some uncertaintly. Image identification is a good example of that. ""Traditional <a href=""https://en.wikipedia.org/wiki/Inductive_logic_programming"">ILP</a>/relational learning approaches"" use several kinds of information to produce hypotheses about the data set, but apparently allow for no noise in the data.</p>

<p>Statistical relational learning models are intended to work with data sets that have several types of objects connected to each other via various links (hence ""relational""). They also are meant to deal with uncertainty (hence ""statistical"").</p>

<p>Skipping past some slides that aren't really useful without a transcript of what was said over them, we come to slide 17, which has comprehensible definitions and examples:</p>

<blockquote>
  <ul>
  <li><strong>Object classification</strong>
  
  <ul>
  <li>Predicting the category of an object based on its attributes <em>and</em> its links <em>and</em> attributes of linked objects</li>
  <li>e.g., predicting the topic of a paper based on the words used in the paper the topics of papers it cites the used in the paper, the topics of papers it cites, the research interests of the author</li>
  </ul></li>
  <li><strong>Object type prediction</strong>
  
  <ul>
  <li>Predicting the type of an object based on its attributes and its links and attributes of linked objects</li>
  <li>e.g., predict the venue type of a publication (conference, journal, workshop) based on properties of the paper</li>
  </ul></li>
  </ul>
</blockquote>

<p>As you can see, these models can keep track of several things and the interactions between them. The next slide talks about link prediction, the ability to predict several attributes of connections between objects, like the importance/quality of a citation. As previously mentioned, these models don't require 100% accurate data to give interesting results; academic citation lists might occasionally be less than comprehensive, and the importance of a citation is challenging to quantify.</p>

<p>Like ILP, SLP will hopefully be able to ""see"" new kinds of links between ""entities"", as with the presentation's example of identifying research communities.</p>

<p>Past slide 20, the presentation goes into some serious mathematics. It does have a much less technical conclusion starting at slide 198. </p>
",,0,2016-08-22T15:55:06.007,,1708,2016-08-22T15:55:06.007,,,,,75.0,1706.0,2,5,,,,39.26,14.45,9.84,0.0,0.0,62.0,The University of Maryland published some slides PDF from an introductory presentation on this topic The fourth page explains why SRL is interesting Traditional statistical machine learning approaches process one sort of thing in which there is some uncertaintly Image identification is a good example of that Traditional ILPrelational learning approaches use several kinds of information to produce hypotheses about the data set but apparently allow for no noise in the data Statistical relational learning models are intended to work with data sets that have several types of objects connected to each other via various links hence relational They also are meant to deal with uncertainty hence statistical Skipping past some slides that arent really useful without a transcript of what was said over them we come to slide 17 which has comprehensible definitions and examples Object classification Predicting the category of an object based on its attributes and its links and attributes of linked objects eg predicting the topic of a paper based on the words used in the paper the topics of papers it cites the used in the paper the topics of papers it cites the research interests of the author Object type prediction Predicting the type of an object based on its attributes and its links and attributes of linked objects eg predict the venue type of a publication conference journal workshop based on properties of the paper As you can see these models can keep track of several things and the interactions between them The next slide talks about link prediction the ability to predict several attributes of connections between objects like the importancequality of a citation As previously mentioned these models dont require 100 accurate data to give interesting results academic citation lists might occasionally be less than comprehensive and the importance of a citation is challenging to quantify Like ILP SLP will hopefully be able to see new kinds of links between entities as with the presentations example of identifying research communities Past slide 20 the presentation goes into some serious mathematics It does have a much less technical conclusion starting at slide 198
,,"<p>I think you're coming at your problem slightly wrong... what you're essentially talking about is a belief network.</p>

<p>You may want to look into existing Bayesian Learning techniques to get your head around this, but belief networks commonly use the exact scenario you're talking about; using a set of known (or uncertain facts) statements to produce some inferred probability of a particular output. </p>

<p>Even more, they often express this through disease-symptom based examples in tutorials! Try <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.124.2195&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">here</a>.</p>

<p>My point being that it would be better to use a belief network as the theory groundwork is all already there for you, instead of an ANN.</p>
",,3,2016-08-22T16:17:44.060,,1709,2016-08-23T08:35:13.237,2016-08-23T08:35:13.237,,1628.0,,1467.0,1705.0,2,7,,,,58.82,12.18,10.74,0.0,0.0,18.0,I think youre coming at your problem slightly wrong what youre essentially talking about is a belief network You may want to look into existing Bayesian Learning techniques to get your head around this but belief networks commonly use the exact scenario youre talking about using a set of known or uncertain facts statements to produce some inferred probability of a particular output Even more they often express this through diseasesymptom based examples in tutorials Try here My point being that it would be better to use a belief network as the theory groundwork is all already there for you instead of an ANN
,1.0,"<p>The obvious solution is to ensure that the training data is balanced - but in my particular case that is impossible. What corrections can one perform in such a scenario?</p>

<p>I know that my training data is heavily biased towards a particular class, say, and I cannot change that. Moreover, the labels are very noisy. Conditioned on this piece of information, is there anything I can do by tweaking the training process itself/ something else, to correct for the bias in the training data?</p>

<p>The data comes from an experiment (from an electron microscope), and I cannot collect more data. It's always going to be biased in this way, so alternatively-biased is also not an option. I'm sorry that I'm unable to provide any more details due to confidentiality.</p>
",,9,2016-08-22T19:28:42.590,1.0,1710,2017-01-12T12:23:33.290,2017-01-12T12:23:33.290,,8.0,,1267.0,,1,1,<neural-networks><research><deep-network><training>,What can be done to correct for sampling bias introduced from (noisy) training data while training a DNN?,67.0,55.24,10.61,9.86,0.0,0.0,23.0,The obvious solution is to ensure that the training data is balanced but in my particular case that is impossible What corrections can one perform in such a scenario I know that my training data is heavily biased towards a particular class say and I cannot change that Moreover the labels are very noisy Conditioned on this piece of information is there anything I can do by tweaking the training process itself something else to correct for the bias in the training data The data comes from an experiment from an electron microscope and I cannot collect more data Its always going to be biased in this way so alternativelybiased is also not an option Im sorry that Im unable to provide any more details due to confidentiality
,,"<p>I feel like from the information your giving (<em>some</em> sort of biased data) you cant get an answer as robust as you'd like (what <em>algorithmic</em> changes can be made). </p>

<p>In general, the reason these methods like DNN's work is that they learn off of the data. What you train it to do is what it is capable of, and there's little one can do to 'balance' it to classes of data it just never sees. It's like training someone to do algebra then giving them a trigonometry test. It's all math, sure, but you just never can expect much without the proper learning.</p>

<p>That being said, you should perhaps look at other methods to work with this data, or to approach the problem. Given that you cannot collect unbiased data and that you can't explain more due to confidentiality, I really doubt anyone here can help you that much.</p>

<p>I can at most point you to this article</p>

<p>""Classification on Data with Biased Class Distribution""<br>
<a href=""https://ai2-s2-pdfs.s3.amazonaws.com/277c/3795f7a66fda3fd70607d1fb45b66730c7ba.pdf"" rel=""nofollow"">https://ai2-s2-pdfs.s3.amazonaws.com/277c/3795f7a66fda3fd70607d1fb45b66730c7ba.pdf</a></p>

<p>And suggest that perhaps your current approach may not be the most approrpiate given the unfortunate circumstances.</p>
",,3,2016-08-22T21:58:39.513,,1711,2016-08-22T21:58:39.513,,,,,1538.0,1710.0,2,2,,,,67.79,11.89,9.13,0.0,0.0,40.0,I feel like from the information your giving some sort of biased data you cant get an answer as robust as youd like what algorithmic changes can be made In general the reason these methods like DNNs work is that they learn off of the data What you train it to do is what it is capable of and theres little one can do to balance it to classes of data it just never sees Its like training someone to do algebra then giving them a trigonometry test Its all math sure but you just never can expect much without the proper learning That being said you should perhaps look at other methods to work with this data or to approach the problem Given that you cannot collect unbiased data and that you cant explain more due to confidentiality I really doubt anyone here can help you that much I can at most point you to this article Classification on Data with Biased Class Distribution httpsai2s2pdfss3amazonawscom277c3795f7a66fda3fd70607d1fb45b66730c7bapdf And suggest that perhaps your current approach may not be the most approrpiate given the unfortunate circumstances
,0.0,"<p><strong>Note:</strong> I wanted to ask a meta-post first to see if this site was supposed to be used only for AI-related questions, or if AI-related questions such as this were allowed, too, but apparently you need to have asked five actual questions first.</p>

<hr>

<p>I'm going to be entering a masters computer science program in the fall, and I wanted to move towards a concentration in computational neuroscience and linguistics for AI development applications. While I have a math and CS background, I have almost no biology/neuroscience background, and my linguistics background is limited to the random research I've done in my spare time to satiate my curiosities.</p>

<p>What are good non-math and CS related topics to study for these fields? </p>
",2016-08-23T14:35:11.287,3,2016-08-22T23:22:00.907,0.0,1713,2016-08-22T23:22:00.907,,,,,164.0,,1,2,<research><machine-learning><neuromorphic-computing><computational-linguistics>,What non-math/CS classes are good supplements for computational neuroscience and/or linguistics?,40.0,41.03,12.14,9.99,0.0,0.0,18.0,Note I wanted to ask a metapost first to see if this site was supposed to be used only for AIrelated questions or if AIrelated questions such as this were allowed too but apparently you need to have asked five actual questions first Im going to be entering a masters computer science program in the fall and I wanted to move towards a concentration in computational neuroscience and linguistics for AI development applications While I have a math and CS background I have almost no biologyneuroscience background and my linguistics background is limited to the random research Ive done in my spare time to satiate my curiosities What are good nonmath and CS related topics to study for these fields
,,"<p>I think the fundamental question is: Why even attempt to build an AI? If that objective is clear, it will provide clarity to whether or not having emotional quotient in AI make sense. Some attempts like ""Paro"" that were developed for therapeutic reasons requires they exhibit some human like emotions. Again, note that ""displaying"" emotions and ""feeling"" emotions are two completely different things. </p>

<p>You can program a thing like paro to modulate the voice tones or facial twitches to express sympathy, affection, companionship, or whatever - but while doing so, a paro does NOT empathize with its owner - it is simply faking it by performing the physical manifestations of an emotion. It never ""feels"" anything remotely closer to what that emotion evokes in human brain. </p>

<p>So this distinction is really important. For you to feel something, there needs to be an independent autonomous subject that has the capacity to feel. Feeling cannot be imposed by an external human agent. </p>

<p>So going back to the question of what purpose it solves - answer really is - It depends. And the most I think we will achieve ever with silicone based AIs will remain the domain of just physical representations of emotions. </p>
",,2,2016-08-23T00:08:34.690,,1714,2016-08-23T00:08:34.690,,,,,1588.0,1700.0,2,2,,,,53.31,11.89,10.62,0.0,0.0,31.0,I think the fundamental question is Why even attempt to build an AI If that objective is clear it will provide clarity to whether or not having emotional quotient in AI make sense Some attempts like Paro that were developed for therapeutic reasons requires they exhibit some human like emotions Again note that displaying emotions and feeling emotions are two completely different things You can program a thing like paro to modulate the voice tones or facial twitches to express sympathy affection companionship or whatever but while doing so a paro does NOT empathize with its owner it is simply faking it by performing the physical manifestations of an emotion It never feels anything remotely closer to what that emotion evokes in human brain So this distinction is really important For you to feel something there needs to be an independent autonomous subject that has the capacity to feel Feeling cannot be imposed by an external human agent So going back to the question of what purpose it solves answer really is It depends And the most I think we will achieve ever with silicone based AIs will remain the domain of just physical representations of emotions
,,"<p>If I look at the image, I can kind of see a monocle as <em>part</em> of the image. So one part of this is that the classifier is ignoring much of the image. This could be called a lack of ""completeness"", in the sense used <a href=""http://www.wisdom.weizmann.ac.il/~vision/VisualSummary.html"" rel=""nofollow"">here</a> (a computer vision paper on image summarization).</p>

<p>One way to discover these sorts of failure modes is <a href=""https://plus.google.com/+ResearchatGoogle/posts/QoFzqQBeANN"" rel=""nofollow"">adversarial images</a>, which are optimized to fool a given image classifier. Building on this, the idea of <em>adversarial training</em> is to simultaneously train competing ""machines"", one trying to synthesize data, the other trying to find weaknesses in the first one.</p>

<p>Also check this page: <a href=""https://code.facebook.com/posts/1587249151575490/a-path-to-unsupervised-learning-through-adversarial-networks/"" rel=""nofollow"">A path to unsupervised learning through adversarial networks</a>, for further information about adversarial training.</p>
",,0,2016-08-23T00:09:27.470,,1715,2016-08-23T00:14:37.027,2016-08-23T00:14:37.027,,8.0,,1590.0,1691.0,2,0,,,,50.97,11.2,9.73,0.0,0.0,20.0,If I look at the image I can kind of see a monocle as part of the image So one part of this is that the classifier is ignoring much of the image This could be called a lack of completeness in the sense used here a computer vision paper on image summarization One way to discover these sorts of failure modes is adversarial images which are optimized to fool a given image classifier Building on this the idea of adversarial training is to simultaneously train competing machines one trying to synthesize data the other trying to find weaknesses in the first one Also check this page A path to unsupervised learning through adversarial networks for further information about adversarial training
1717.0,1.0,"<p><a href=""http://www.alicebot.org/articles/wallace/eliza.html"" rel=""nofollow"">From Eliza to A.L.I.C.E.</a>:</p>

<blockquote>
  <p>Weizenbaum tells us that he was shocked by the experience of releasing ELIZA (also known as ""Doctor"") to the nontechnical staff at the MIT AI Lab. Secretaries and nontechnical administrative staff thought the machine was a ""real"" therapist, and spent hours revealing their personal problems to the program. When Weizenbaum informed his secretary that he, of course, had access to the logs of all the conversations, she reacted with outrage at this invasion of her privacy. Weizenbaum was shocked by this and similar incidents to find that such a simple program could so easily deceive a naive user into revealing personal information.</p>
</blockquote>

<p>Wikipedia's article on the <a href=""https://en.wikipedia.org/wiki/ELIZA_effect"" rel=""nofollow"">""ELIZA Effect""</a>:</p>

<blockquote>
  <p>Though designed strictly as a mechanism to support ""natural language conversation"" with a computer, ELIZA's DOCTOR script was found to be surprisingly successful in eliciting emotional responses from users who, in the course of interacting with the program, began to ascribe understanding and motivation to the program's output. As Weizenbaum later wrote, <strong>""I had not realized ... that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.""</strong> Indeed, ELIZA's code had not been designed to evoke this reaction in the first place. Upon observation, researchers discovered users unconsciously assuming ELIZA's questions implied interest and emotional involvement in the topics discussed, <em>even when they consciously knew that ELIZA did not simulate emotion.</em></p>
</blockquote>

<p>ELIZA, despite its simplicity, was incredibly successful at its task of tricking other human beings. Even those who knew ELIZA was a bot would still talk to it. Obviously, ELIZA served as an inspiration for various other, more intelligent chatbots, such as <a href=""http://www.nytimes.com/2015/08/04/science/for-sympathetic-ear-more-chinese-turn-to-smartphone-program.html?_r=0"" rel=""nofollow"">Xiaoice</a>. But I would like to know what <em>exactly</em> led to such a simple program like ELIZA to be so successful in the first place.</p>

<p>This is very useful knowledge for a programmer since a simple program is one that would be easily maintainable.</p>
",,3,2016-08-23T01:25:17.747,,1716,2016-08-23T10:05:40.200,2016-08-23T10:05:40.200,,145.0,,181.0,,1,1,<history><turing-test><emotional-intelligence><chat-bots>,"Why was ELIZA able to induce ""delusional thinking""?",165.0,41.6,14.04,10.41,0.0,0.0,56.0,From Eliza to ALICE Weizenbaum tells us that he was shocked by the experience of releasing ELIZA also known as Doctor to the nontechnical staff at the MIT AI Lab Secretaries and nontechnical administrative staff thought the machine was a real therapist and spent hours revealing their personal problems to the program When Weizenbaum informed his secretary that he of course had access to the logs of all the conversations she reacted with outrage at this invasion of her privacy Weizenbaum was shocked by this and similar incidents to find that such a simple program could so easily deceive a naive user into revealing personal information Wikipedias article on the ELIZA Effect Though designed strictly as a mechanism to support natural language conversation with a computer ELIZAs DOCTOR script was found to be surprisingly successful in eliciting emotional responses from users who in the course of interacting with the program began to ascribe understanding and motivation to the programs output As Weizenbaum later wrote I had not realized that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people Indeed ELIZAs code had not been designed to evoke this reaction in the first place Upon observation researchers discovered users unconsciously assuming ELIZAs questions implied interest and emotional involvement in the topics discussed even when they consciously knew that ELIZA did not simulate emotion ELIZA despite its simplicity was incredibly successful at its task of tricking other human beings Even those who knew ELIZA was a bot would still talk to it Obviously ELIZA served as an inspiration for various other more intelligent chatbots such as Xiaoice But I would like to know what exactly led to such a simple program like ELIZA to be so successful in the first place This is very useful knowledge for a programmer since a simple program is one that would be easily maintainable
,,"<p>I like your choice of ""induce"" instead of ""produce,"" because the delusions came from the users. This means the answer has to do mostly with human psychology; people come equipped with lots of mental machinery specialized for dealing with other humans and not very much mental machinery specialized for dealing with software. So ELIZA behaved in ways that some people classified it as a person and behaved accordingly, and others didn't.</p>

<p>What features will trip up a person's internal person classification system seem like they vary heavily from person to person, and also with experience and familiarity. Going into more detail is, as mentioned in the comments, more appropriate for sites specializing on the human side of the keyboard.</p>
",,0,2016-08-23T02:02:00.140,,1717,2016-08-23T02:02:00.140,,,,,10.0,1716.0,2,2,,,,38.86,13.18,10.79,0.0,0.0,17.0,I like your choice of induce instead of produce because the delusions came from the users This means the answer has to do mostly with human psychology people come equipped with lots of mental machinery specialized for dealing with other humans and not very much mental machinery specialized for dealing with software So ELIZA behaved in ways that some people classified it as a person and behaved accordingly and others didnt What features will trip up a persons internal person classification system seem like they vary heavily from person to person and also with experience and familiarity Going into more detail is as mentioned in the comments more appropriate for sites specializing on the human side of the keyboard
,,,,0,2016-08-23T08:18:08.200,,1720,2016-08-23T08:18:08.200,2016-08-23T08:18:08.200,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions about LISP and its relation with AI; questions about how to program are off-topic here.,,0,2016-08-23T08:18:08.200,,1721,2016-08-23T10:05:33.170,2016-08-23T10:05:33.170,,145.0,,145.0,,4,0,,,,71.14,11.42,9.12,0.0,0.0,3.0,For questions about LISP and its relation with AI questions about how to program are offtopic here
,,"<p>Our current approaches to AI are too inefficient to result in anything remotely close to what an average human would perceive as artificial senient beings.</p>

<p>Current approaches to AI involve a simulation of our own capacity for learning by creating fully functional computation machines capable of re-programming themselves. While that's definitely a good start with respect to understanding the nature of intelligence, it's still a far cry from actually creating genuine artificial intelligence.</p>

<p>It is not just our capacity to learn that evolved. Our very brains themselves evolved from rudimentary biochemical components at the intra-cellular level to the fascinating, complex organs they are today, along with our bodies as a whole evolving from simple single cell life to homo sapiens. So to create genuine artificial intelligence, it may actually make most sense to first start with replicating that process : creating artificial life with the capacity to evolve. It may actually make most sense to first start with creating artificial DNA and artificial cells, and move on from there.</p>

<p>Anyway, in <a href=""http://www.alexstjohn.com/WP/2015/06/24/no-azimov-ai/"" rel=""nofollow"">this article</a> as well as <a href=""http://www.alexstjohn.com/WP/2016/06/15/things-dont-compute/"" rel=""nofollow"">this article</a>, Silicon Valley renegade <a href=""https://en.wikipedia.org/wiki/Alex_St._John"" rel=""nofollow"">Alex St John</a> goes in greater detail on why something like <a href=""https://en.wikipedia.org/wiki/Skynet_(Terminator)"" rel=""nofollow"">Skynet</a>, <a href=""http://irobot.wikia.com/wiki/VIKI"" rel=""nofollow"">V.I.K.I.</a> or anything like it is unlikely in the near future and may even never be within our grasp and why our current approach to artificial intelligence is a bad one.</p>
",,8,2016-08-23T08:29:23.513,,1722,2016-08-24T12:04:52.007,2016-08-24T12:04:52.007,,16.0,,16.0,1420.0,2,1,,,,37.84,13.52,10.25,0.0,0.0,25.0,Our current approaches to AI are too inefficient to result in anything remotely close to what an average human would perceive as artificial senient beings Current approaches to AI involve a simulation of our own capacity for learning by creating fully functional computation machines capable of reprogramming themselves While thats definitely a good start with respect to understanding the nature of intelligence its still a far cry from actually creating genuine artificial intelligence It is not just our capacity to learn that evolved Our very brains themselves evolved from rudimentary biochemical components at the intracellular level to the fascinating complex organs they are today along with our bodies as a whole evolving from simple single cell life to homo sapiens So to create genuine artificial intelligence it may actually make most sense to first start with replicating that process creating artificial life with the capacity to evolve It may actually make most sense to first start with creating artificial DNA and artificial cells and move on from there Anyway in this article as well as this article Silicon Valley renegade Alex St John goes in greater detail on why something like Skynet VIKI or anything like it is unlikely in the near future and may even never be within our grasp and why our current approach to artificial intelligence is a bad one
,,"<p>Based on the success of IBM Watson and the amazing advances in tackling numerous hard tasks using deep learning in the past 3 years, I think a large high-tech company like Google or Amazon will create a useful conversational bot in no more than 10 years.  (I've worked on the fringes of AI for 25 years and have followed the tech for even longer.  These are exciting times.)</p>

<p>Initially, your very own AI companion (""Her""?) won't be capable of deeper philosophical conversation or insightful interpretation of novels or the human condition.  But it will be able to write / speak in full paragraphs on topics like the best choice among 5 possible routes between point A and B, or summarizing the plot of a book or the gist of a news story, or why one product is better than another (e.g. based on assessing hundreds of Amazon reviews).  And yes, it will be able to understand full spoken sentences from you, and generate both queries and answers.</p>

<p>I'm convinced such a bot <em>will</em> be useful enough that most of us will want one.  Of course you won't need to buy a special piece of hardware, like the Amazon Echo.  It'll be available via software on your smartphone, though the computing is likely to reside on the cloud (since that's where the data is).  </p>

<p>Frankly, I think this is where the next innovations in smartphones will arise -- verbal interfaces that do a better job hearing and speaking and disambiguating using context about you and the kinds of questions you are likely to ask.</p>
",,0,2016-08-23T15:21:07.420,,1724,2016-08-23T15:21:07.420,,,,,1657.0,1420.0,2,0,,,,66.27,9.63,9.56,0.0,0.0,42.0,Based on the success of IBM Watson and the amazing advances in tackling numerous hard tasks using deep learning in the past 3 years I think a large hightech company like Google or Amazon will create a useful conversational bot in no more than 10 years Ive worked on the fringes of AI for 25 years and have followed the tech for even longer These are exciting times Initially your very own AI companion Her wont be capable of deeper philosophical conversation or insightful interpretation of novels or the human condition But it will be able to write speak in full paragraphs on topics like the best choice among 5 possible routes between point A and B or summarizing the plot of a book or the gist of a news story or why one product is better than another eg based on assessing hundreds of Amazon reviews And yes it will be able to understand full spoken sentences from you and generate both queries and answers Im convinced such a bot will be useful enough that most of us will want one Of course you wont need to buy a special piece of hardware like the Amazon Echo Itll be available via software on your smartphone though the computing is likely to reside on the cloud since thats where the data is Frankly I think this is where the next innovations in smartphones will arise verbal interfaces that do a better job hearing and speaking and disambiguating using context about you and the kinds of questions you are likely to ask
,,"<p>I think emotions are not necessary for an AI agent to be useful.  But I also think they could make the agent MUCH more pleasant to work with.  If the bot you're talking with can read your emotions and respond constructively, the experience of interacting with it will be tremendously more pleasant, perhaps spectacularly so.</p>

<p>Imagine contacting a human call center representative today with a complaint about your bill or a product.  You anticipate conflict.  You may have even decided NOT to call because you know this experience is going to be painful, either combative or frustrating, as someone misunderstands what you say or responds hostilely or stupidly.</p>

<p>Now imagine calling the kindest smartest most focused customer support person you've ever met -- Commander Data -- whose only reason for existing is to make this phone call as pleasant and productive for you as possible.  A big improvement over most call reps, yes?  Imagine then if call rep Data could also anticipate your mood and respond appropriately to your complaints to defuse your emotional state... you'd want to marry this guy.  You'd call up call rep Data any time you were feeling blue or bored or you wanted to share some happy news.  This guy would become your best friend overnight -- literally love at first call.</p>

<p>I'm convinced this scenario is valid. I've noticed in myself a surprising amount of attraction for characters like Data or Sonny from ""I Robot"".  The voice is very soothing and puts me instantly at ease.  If the bot were also very smart, patient, knowledgable, and understanding...  I really think such a bot, embued with a healthy dose of emotional intelligence, could be enormously pleasant to interact with.  Much more rewarding than any person I know.  And I think that's true of not just me.</p>

<p>So yes, I think there's great value in tuning a robot's personality using emotions and emotional awareness.</p>
",,0,2016-08-23T15:43:31.183,,1725,2016-08-23T15:43:31.183,,,,,1657.0,1700.0,2,3,,,,55.44,11.25,9.38,0.0,0.0,52.0,I think emotions are not necessary for an AI agent to be useful But I also think they could make the agent MUCH more pleasant to work with If the bot youre talking with can read your emotions and respond constructively the experience of interacting with it will be tremendously more pleasant perhaps spectacularly so Imagine contacting a human call center representative today with a complaint about your bill or a product You anticipate conflict You may have even decided NOT to call because you know this experience is going to be painful either combative or frustrating as someone misunderstands what you say or responds hostilely or stupidly Now imagine calling the kindest smartest most focused customer support person youve ever met Commander Data whose only reason for existing is to make this phone call as pleasant and productive for you as possible A big improvement over most call reps yes Imagine then if call rep Data could also anticipate your mood and respond appropriately to your complaints to defuse your emotional state youd want to marry this guy Youd call up call rep Data any time you were feeling blue or bored or you wanted to share some happy news This guy would become your best friend overnight literally love at first call Im convinced this scenario is valid Ive noticed in myself a surprising amount of attraction for characters like Data or Sonny from I Robot The voice is very soothing and puts me instantly at ease If the bot were also very smart patient knowledgable and understanding I really think such a bot embued with a healthy dose of emotional intelligence could be enormously pleasant to interact with Much more rewarding than any person I know And I think thats true of not just me So yes I think theres great value in tuning a robots personality using emotions and emotional awareness
,,"<p>It is a labor intensive process, but that does sound excessive. If you have a g2.8xlarge, make sure you are using the using the GPU flags for neural-style, which will cut your render time by an order of magnitude. </p>

<p>That having been said, it is building a rather large network (depending on your parameters), and a 1024x768 image is a lot of input to work with. It will take time, but shouldn't take more than a couple hours with the gpu flag enabled correctly. </p>
",,0,2016-08-23T15:55:43.000,,1726,2016-08-23T15:55:43.000,,,,,1618.0,152.0,2,2,,,,71.34,8.41,8.23,0.0,0.0,15.0,It is a labor intensive process but that does sound excessive If you have a g28xlarge make sure you are using the using the GPU flags for neuralstyle which will cut your render time by an order of magnitude That having been said it is building a rather large network depending on your parameters and a 1024x768 image is a lot of input to work with It will take time but shouldnt take more than a couple hours with the gpu flag enabled correctly
,,"<p>They are all called Monte Carlo because all of them are a different version of the canonical Monte Carlo algorithm.</p>

<p>The canonical version of Monte Carlo algorithm is a stochastic algorithm to determine an action based in a tree representation. The differences among all these version are their exploration and exploitation mechanisms, and it is necessary to analyse each of them to define which one fits in your case. </p>
",,0,2016-08-23T17:28:12.040,,1727,2016-08-24T13:15:48.023,2016-08-24T13:15:48.023,,145.0,,1666.0,1534.0,2,6,,,,39.67,12.36,9.58,0.0,0.0,4.0,They are all called Monte Carlo because all of them are a different version of the canonical Monte Carlo algorithm The canonical version of Monte Carlo algorithm is a stochastic algorithm to determine an action based in a tree representation The differences among all these version are their exploration and exploitation mechanisms and it is necessary to analyse each of them to define which one fits in your case
,,"<p>Real time style transfer and neural doodle is very much possible and is an active topic I see users working on to improve upon. The basic idea is to do only feed forward propagation at test time and train with appropriate loss functions at train time.</p>

<p><a href=""http://arxiv.org/pdf/1603.08155v1.pdf"" rel=""nofollow"">Perceptual Losses for Real-Time Style Transfer
and Super-Resolution</a> is a good starting point to understand a methodology for this purpose.</p>
",,0,2016-08-23T17:32:53.517,,1728,2016-08-23T17:32:53.517,,,,,1613.0,152.0,2,3,,,,49.15,11.26,11.67,0.0,0.0,5.0,Real time style transfer and neural doodle is very much possible and is an active topic I see users working on to improve upon The basic idea is to do only feed forward propagation at test time and train with appropriate loss functions at train time Perceptual Losses for RealTime Style Transfer and SuperResolution is a good starting point to understand a methodology for this purpose
,,"<p>Consider Asimov's first law of robotics:</p>

<blockquote>
  <p>A robot may not injure a human being or, through inaction, allow a
  human being to come to harm.</p>
</blockquote>

<p>That law is already problematic, when taking into consideration self-driving cars. </p>

<p>What's the issue here, you ask? Well, you'll probably be familiar with the classic thought experiment in ethic known as <a href=""https://en.wikipedia.org/wiki/Trolley_problem"" rel=""nofollow"">the trolley problem</a>. The general form of the problem is this:</p>

<blockquote>
  <p>The trolley is headed straight for them. You are standing some
  distance off in the train yard, next to a lever. If you pull this
  lever, the trolley will switch to a different set of tracks. However,
  you notice that there is one person on the side track. You have two
  options: (1) Do nothing, and the trolley kills the five people on the
  main track. (2) Pull the lever, diverting the trolley onto the side
  track where it will kill one person. Which is the correct choice?</p>
  
  <p>source : <a href=""https://en.wikipedia.org/wiki/Trolley_problem"" rel=""nofollow"">Wikipedia</a></p>
</blockquote>

<p>Self-driving cars will actually need to implement real life variations on the trolley problem, which basically means that <a href=""https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/"" rel=""nofollow"">self-driving cars need to be programmed to kill human beings</a>.</p>

<p>Of course that doesn't mean that ALL robots will need to be programmed to kill, but self-driving cars are a good example of a type of robot that will.</p>
",,0,2016-08-23T18:12:16.680,,1729,2016-08-23T18:12:16.680,,,,,16.0,1348.0,2,0,,,,71.65,9.86,8.07,0.0,0.0,41.0,Consider Asimovs first law of robotics A robot may not injure a human being or through inaction allow a human being to come to harm That law is already problematic when taking into consideration selfdriving cars Whats the issue here you ask Well youll probably be familiar with the classic thought experiment in ethic known as the trolley problem The general form of the problem is this The trolley is headed straight for them You are standing some distance off in the train yard next to a lever If you pull this lever the trolley will switch to a different set of tracks However you notice that there is one person on the side track You have two options 1 Do nothing and the trolley kills the five people on the main track 2 Pull the lever diverting the trolley onto the side track where it will kill one person Which is the correct choice source Wikipedia Selfdriving cars will actually need to implement real life variations on the trolley problem which basically means that selfdriving cars need to be programmed to kill human beings Of course that doesnt mean that ALL robots will need to be programmed to kill but selfdriving cars are a good example of a type of robot that will
,,"<p>I think the algorithm has changed minimally, but the necessary hardware has been trimmed to the bone.</p>

<p>The number of gate transitions are reduced (perhaps float ops and precision too), as are the number of data move operations, thus saving both power and runtime.  Google suggests their TPU achieves a 10X cost saving to get the same work done.</p>

<p><a href=""https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html"" rel=""nofollow"">https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html</a></p>
",,0,2016-08-23T19:35:43.730,,1730,2016-08-23T19:35:43.730,,,,,1657.0,1655.0,2,1,,,,34.26,20.71,9.89,0.0,0.0,24.0,I think the algorithm has changed minimally but the necessary hardware has been trimmed to the bone The number of gate transitions are reduced perhaps float ops and precision too as are the number of data move operations thus saving both power and runtime Google suggests their TPU achieves a 10X cost saving to get the same work done httpscloudplatformgoogleblogcom201605Googlesuperchargesmachinelearningtaskswithcustomchiphtml
,1.0,"<p>What regulations are already in place regarding Artificial General Intelligences? What reports or recommendations prepared by official government authorities were already published?</p>

<p>So far I know of <a href=""http://www.ft.com/cms/s/2/5ae9b434-8f8e-11db-9ba3-0000779e2340.html"">Sir David King's report done for UK government</a>.</p>
",,1,2016-08-23T22:03:45.793,1.0,1731,2016-08-26T18:17:48.080,2016-08-26T18:17:48.080,,33.0,,1670.0,,1,9,<agi><legal>,AGI official government reports or regulations already in place,94.0,25.76,16.68,11.44,0.0,0.0,4.0,What regulations are already in place regarding Artificial General Intelligences What reports or recommendations prepared by official government authorities were already published So far I know of Sir David Kings report done for UK government
1735.0,1.0,"<p>Most introductions to the field of MDPs and Reinforcement learning focus exclusively on domains where space and action variables are integers (and finite).<br>
This way we are introduced quickly to Value Iteration, Q-Learning and the like.</p>

<p>However the most interesting applications (say, <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.3518&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">flying helicopters</a>) of RL and MDPs involve continuous state space and action spaces.<br>
I'd like to go beyond basic introductions and focus on these cases but I am not sure how to get there. </p>

<p>Are there any research projects or studies that deal with these cases in depth?</p>
",,3,2016-08-24T10:00:44.473,,1733,2016-08-25T11:23:26.990,2016-08-25T11:23:26.990,,145.0,,15.0,,1,5,<research><reinforcement-learning><control-problem><reference-request>,References for continuous state/action spaces MDPs and Reinforcement Learning?,65.0,53.21,12.18,9.27,0.0,0.0,13.0,Most introductions to the field of MDPs and Reinforcement learning focus exclusively on domains where space and action variables are integers and finite This way we are introduced quickly to Value Iteration QLearning and the like However the most interesting applications say flying helicopters of RL and MDPs involve continuous state space and action spaces Id like to go beyond basic introductions and focus on these cases but I am not sure how to get there Are there any research projects or studies that deal with these cases in depth
,,"<h2>Tensor operations</h2>

<p>The major work in most ML applications is simply a set of (very large) tensor operations e.g. matrix multiplication. You can do <em>that</em> easily in an ASIC, and all the other algorithms can just run on top of that.</p>
",,0,2016-08-24T10:22:19.957,,1734,2016-08-24T10:22:19.957,,,,,1675.0,1655.0,2,1,,,,67.59,9.17,9.66,0.0,0.0,7.0,Tensor operations The major work in most ML applications is simply a set of very large tensor operations eg matrix multiplication You can do that easily in an ASIC and all the other algorithms can just run on top of that
,,"<p>There is a small survey of continuous states, actions and time in reinforcement learning in my <a href=""http://www.inf.ufrgs.br/~rcpinto/Proposta_de_Tese_Rafael_Pinto.pdf"">thesis proposal</a>.</p>

<p>Regarding books, <a href=""https://books.google.com.br/books/about/Reinforcement_Learning.html?id=YPjNuvrJR0MC&amp;redir_esc=y"">Reinforcement Learning: State-of-the-Art</a> seems to be pretty up-to-date from the excerpts I've read.</p>
",,1,2016-08-24T16:25:01.740,,1735,2016-08-24T16:25:01.740,,,,,144.0,1733.0,2,5,,,,46.27,16.88,11.63,0.0,0.0,11.0,There is a small survey of continuous states actions and time in reinforcement learning in my thesis proposal Regarding books Reinforcement Learning StateoftheArt seems to be pretty uptodate from the excerpts Ive read
,,"<p>So you may be familiar with Word2Vec, (W2V) which as <a href=""http://en.wikipedia.org/wiki/Word2vec"" rel=""nofollow"">Wikipedia describes</a><sup>1</sup> ""captures the linguistic contexts of words"" using vector arithmetic. For example, subtract 'Paris' from 'France' and add 'Italy' and you get 'Rome'.</p>

<p>What you need is something like a Sentiment2Vec (S2V) that captures the similarities between emotional transitions. Something like: subtract 'fear' from 'sadness', add 'joy' and you get 'hope'. Or: subtract 'sting' from 'papercut', add 'smashed' and you get 'throbbing'.</p>

<p>The catch is that you don't have an easily accessible corpus of emotional contexts to train with, like you have with words. If you had a million hours of fMRI - mapping the transitions between emotions in hundreds of subjects - then you could use that data to build an S2V. You probably don't have that data though.</p>

<p>In the mean time, you could just build a W2V that specializes in sentiment. You could even try to use a current sentiment analysis engine to bootstrap it. Perhaps if you read enough text that says ""I got a papercut and it stings"" and ""I smashed my finger and it's throbbing"" then you could eventually produce an S2V. Children's books often use explicit language regarding emotional context (""this made the boy feel sad"").</p>

<p>But words are still a far cry from the experiential context that a connectome map would provide. To test whether you have something useful or not, you might want to implement your S2V in a mouse foraging simulation - see whether it produces typical behavior and if any cooperative or competitive dynamics can organically grow out of your S2V.</p>

<p>Some further info on the subject: </p>

<p>In 2014, <a href=""http://www.bbc.com/news/uk-scotland-glasgow-west-26019586"" rel=""nofollow"">Glasgow University claimed</a><sup>2</sup> that there are four primary emotions: happiness, sadness, fear and anger. </p>

<p><a href=""http://changingminds.org/explanations/emotions/basic%20emotions.htm"" rel=""nofollow"">This website</a><sup>3</sup> provides nice (if somewhat short) hierarchical breakdown of secondary and tertiary emotions under primary emotions.</p>

<hr>

<p><strong>References</strong></p>

<p><sub><sup>1</sup>: <a href=""http://en.wikipedia.org/wiki/Word2vec"" rel=""nofollow"">en.wikipedia.org/wiki/Word2vec</a></sub></p>

<p><sub><sup>2</sup>: <a href=""http://www.bbc.com/news/uk-scotland-glasgow-west-26019586"" rel=""nofollow"">www.bbc.com/news/uk-scotland-glasgow-west-26019586</a></sub></p>

<p><sub><sup>3</sup>: <a href=""http://changingminds.org/explanations/emotions/basic%20emotions.htm"" rel=""nofollow"">changingminds.org/explanations/emotions/basic%20emotions.htm</a></sub></p>
",,1,2016-08-25T15:40:12.343,,1737,2016-08-25T20:07:22.267,2016-08-25T20:07:22.267,,145.0,,1712.0,26.0,2,1,,,,55.13,14.73,9.54,0.0,0.0,98.0,So you may be familiar with Word2Vec W2V which as Wikipedia describes1 captures the linguistic contexts of words using vector arithmetic For example subtract Paris from France and add Italy and you get Rome What you need is something like a Sentiment2Vec S2V that captures the similarities between emotional transitions Something like subtract fear from sadness add joy and you get hope Or subtract sting from papercut add smashed and you get throbbing The catch is that you dont have an easily accessible corpus of emotional contexts to train with like you have with words If you had a million hours of fMRI mapping the transitions between emotions in hundreds of subjects then you could use that data to build an S2V You probably dont have that data though In the mean time you could just build a W2V that specializes in sentiment You could even try to use a current sentiment analysis engine to bootstrap it Perhaps if you read enough text that says I got a papercut and it stings and I smashed my finger and its throbbing then you could eventually produce an S2V Childrens books often use explicit language regarding emotional context this made the boy feel sad But words are still a far cry from the experiential context that a connectome map would provide To test whether you have something useful or not you might want to implement your S2V in a mouse foraging simulation see whether it produces typical behavior and if any cooperative or competitive dynamics can organically grow out of your S2V Some further info on the subject In 2014 Glasgow University claimed2 that there are four primary emotions happiness sadness fear and anger This website3 provides nice if somewhat short hierarchical breakdown of secondary and tertiary emotions under primary emotions References 1 enwikipediaorgwikiWord2vec 2 wwwbbccomnewsukscotlandglasgowwest26019586 3 changingmindsorgexplanationsemotionsbasic20emotionshtm
,,"<p>Asimov made the three laws <strong>specifically to prove</strong> that no three laws are sufficient, no matter how reasonable they seem at first. I know a guy that knew the guy and he confirmed this.</p>
",,2,2016-08-25T16:00:00.820,,1738,2016-08-25T16:00:00.820,,,,,1712.0,1348.0,2,1,,,,79.6,7.83,7.27,0.0,0.0,3.0,Asimov made the three laws specifically to prove that no three laws are sufficient no matter how reasonable they seem at first I know a guy that knew the guy and he confirmed this
,,"<p>""Usefulness"" can only be measured against some purpose. Once you pass AGI - which really means ""generally animal-like AI, because it seems general to us"" - then you've passed into a world of potentially undefined behavior.</p>

<p>Part of what makes a human free and sets us apart from the other animals is the fact that our purposes, capabilities and possibilities aren't fully defined. We're open ended.</p>

<p>To clarify terms, I interpret ""Strong AGI"" as ""potentially super intelligence, but at least human level.""</p>

<p>When we say ""Strong AGI"" vs just ""AGI,"" we're not saying that one is <em>more</em> open ended than the other. We are saying that the stronger one is simply smarter on some axis.</p>

<p>So to ask whether a particular trait would be ""more useful"" to a Strong AGI - that would depend on the purpose of the AGI. But here's the catch: if a thing had just one purpose, then the most efficient solution to fulfilling that one purpose will always be a narrow solution, not a general one. When the purpose of the object is known before hand, giving that object more general capability than is necessary for that purpose is counterproductive.</p>

<p>That's why it's impossible to make declarative prescriptions about what a free, open-ended AGI should or shouldn't need. Such prescriptions would nullify the open-ended freedom of utility that its generality implies. We <em>can</em> speak declaratively about lesser robots and animals.</p>

<p>But for any given problem, the solution we will want to find is the most well-defined, narrow, efficient one available - not the most general one.</p>

<p>In other words, sure, personalities could be useful for a Strong AGI, assuming the problems in question involved personalities.</p>
",,0,2016-08-25T17:29:27.977,,1740,2016-08-25T17:42:49.297,2016-08-25T17:42:49.297,,1712.0,,1712.0,1593.0,2,1,,,,52.8,12.01,9.76,0.0,0.0,61.0,Usefulness can only be measured against some purpose Once you pass AGI which really means generally animallike AI because it seems general to us then youve passed into a world of potentially undefined behavior Part of what makes a human free and sets us apart from the other animals is the fact that our purposes capabilities and possibilities arent fully defined Were open ended To clarify terms I interpret Strong AGI as potentially super intelligence but at least human level When we say Strong AGI vs just AGI were not saying that one is more open ended than the other We are saying that the stronger one is simply smarter on some axis So to ask whether a particular trait would be more useful to a Strong AGI that would depend on the purpose of the AGI But heres the catch if a thing had just one purpose then the most efficient solution to fulfilling that one purpose will always be a narrow solution not a general one When the purpose of the object is known before hand giving that object more general capability than is necessary for that purpose is counterproductive Thats why its impossible to make declarative prescriptions about what a free openended AGI should or shouldnt need Such prescriptions would nullify the openended freedom of utility that its generality implies We can speak declaratively about lesser robots and animals But for any given problem the solution we will want to find is the most welldefined narrow efficient one available not the most general one In other words sure personalities could be useful for a Strong AGI assuming the problems in question involved personalities
,,"<p>The neural network is typically a set size once it's created. You'd have to create a network big enough for your data-set.</p>
",,3,2016-08-25T18:43:28.097,,1741,2017-02-13T20:48:43.850,2017-02-13T20:48:43.850,,1720.0,,1720.0,1480.0,2,6,,,,68.77,8.16,8.49,0.0,0.0,5.0,The neural network is typically a set size once its created Youd have to create a network big enough for your dataset
1743.0,1.0,"<p>Can someone explain to me the difference between machine learning and deep learning? Is it possible to learn deep learning without knowing machine learning?</p>
",2016-08-26T15:57:51.323,0,2016-08-25T22:19:10.773,1.0,1742,2016-08-26T10:41:20.093,2016-08-26T10:41:20.093,,145.0,,1727.0,,1,1,<machine-learning><deep-learning>,What is the difference between machine learning and deep learning?,157.0,59.3,13.96,10.81,0.0,0.0,2.0,Can someone explain to me the difference between machine learning and deep learning Is it possible to learn deep learning without knowing machine learning
,,"<p>Deep learning is a specific variety of a specific type of machine learning. So it's possible to learn about deep learning without learning all of machine learning, but it requires learning <em>some</em> machine learning (because it is some machine learning).</p>

<p>Machine learning refers to any technique that focuses on teaching the machine how it can learn statistical parameters from a large amount of training data. One particular type of machine learning is artificial neural networks, which learn a network of nonlinear transformations that can approximate very complicated functions of wide arrays of input variables. Recent advances in artificial neural networks have to do with how to train <em>deep</em> neural networks, which have more layers than normal and also special structure to deal with the challenges of learning more layers.</p>
",,3,2016-08-25T23:55:42.950,,1743,2016-08-25T23:55:42.950,,,,,10.0,1742.0,2,5,,,,36.83,14.22,10.42,0.0,0.0,11.0,Deep learning is a specific variety of a specific type of machine learning So its possible to learn about deep learning without learning all of machine learning but it requires learning some machine learning because it is some machine learning Machine learning refers to any technique that focuses on teaching the machine how it can learn statistical parameters from a large amount of training data One particular type of machine learning is artificial neural networks which learn a network of nonlinear transformations that can approximate very complicated functions of wide arrays of input variables Recent advances in artificial neural networks have to do with how to train deep neural networks which have more layers than normal and also special structure to deal with the challenges of learning more layers
,,"<p>Here is an answer by Carlos E. Perez to the question <a href=""https://www.quora.com/What-is-theory-behind-deep-learning/answer/Carlos-E-Perez?srid=CpS&amp;share=0f965f46"" rel=""nofollow"">What is theory behind deep learning?</a></p>

<blockquote>
  <p>[...]</p>
  
  <p>The underlying mathematics of Deep Learning has been in existence for several decades, however the impressive results that we see today are part a consequence of much faster hardware, more data and incremental improvements in methods. </p>
  
  <p>Deep Learning in general can be framed as optimization problem where the objective is a function of the model error. This optimization problem is very difficult to solve consider that the parameter space of the model (i.e. weights of the neural network) leads to a problem in extremely high dimension. An optimization algorithm could take a very long time to explore this space. Furthermore, there was an unverified belief that the problem was non-convex and computation would forever be stuck in local minima.</p>
  
  <p>[...]</p>
  
  <p>The theory of why machines actually converge to an attractor or in other words learn to recognize complex patterns is still unknown.</p>
</blockquote>

<p>To sum up: we have some ideas, but we're not quite sure.</p>
",,0,2016-08-26T10:34:16.803,,1744,2016-08-26T10:34:16.803,,,,,1741.0,1479.0,2,2,,,,54.22,11.95,10.05,0.0,0.0,30.0,Here is an answer by Carlos E Perez to the question What is theory behind deep learning The underlying mathematics of Deep Learning has been in existence for several decades however the impressive results that we see today are part a consequence of much faster hardware more data and incremental improvements in methods Deep Learning in general can be framed as optimization problem where the objective is a function of the model error This optimization problem is very difficult to solve consider that the parameter space of the model ie weights of the neural network leads to a problem in extremely high dimension An optimization algorithm could take a very long time to explore this space Furthermore there was an unverified belief that the problem was nonconvex and computation would forever be stuck in local minima The theory of why machines actually converge to an attractor or in other words learn to recognize complex patterns is still unknown To sum up we have some ideas but were not quite sure
,,"<blockquote>
  <p>In general, how algorithm should distinguish the word meaning and recognise the word within the context?</p>
</blockquote>

<p>I don't think anybody knows how to answer this for the general case. If they did, they'd have basically solved AGI.  But we can certainly talk about techniques that get part-of-the-way there, and approaches that could work.</p>

<p>One thing I would consider trying (and I don't know off-hand if anybody has tried this exact approach) is to model the disambiguation of each word as a discrete problem for a <a href=""https://en.wikipedia.org/wiki/Bayesian_network"" rel=""nofollow"">Bayesian Belief Network</a> where your priors (for any given word) are based on both stored ""knowledge"" as well as the previously encountered words in the (sentence|paragraph|document|whatever).  So if you ""know"", for example, that ""Reading is a city in the UK"" and that ""place names are usually capitalized"", your network should be strongly biased towards interpreting ""Reading"" as the city, since nothing in the word position in the sentence strongly contradicts that.  </p>

<p>Of course I'm hand-waving around some tricky problems in saying that, as <a href=""https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning"" rel=""nofollow"">knowledge representation</a> isn't exactly a solved problem either.  But there are at least approaches out there that you could use.  For example, you could use the <a href=""https://en.wikipedia.org/wiki/Resource_Description_Framework"" rel=""nofollow"">RDF</a> / triple based approach from the <a href=""https://en.wikipedia.org/wiki/Semantic_Web"" rel=""nofollow"">Semantic Web</a> world.  Finding a good way to merge that stuff with a Bayesian framework could yield some interesting results.    </p>

<p>There has been a bit of research on ""probabilistic RDF"" that you could possibly use as a starting point.  For example:</p>

<p><a href=""http://om.umiacs.umd.edu/material/papers/prdf.pdf"" rel=""nofollow"">http://om.umiacs.umd.edu/material/papers/prdf.pdf</a></p>

<p><a href=""http://ceur-ws.org/Vol-173/pos_paper5.pdf"" rel=""nofollow"">http://ceur-ws.org/Vol-173/pos_paper5.pdf</a></p>

<p><a href=""https://www.w3.org/2005/03/07-yoshio-UMBC/"" rel=""nofollow"">https://www.w3.org/2005/03/07-yoshio-UMBC/</a></p>

<p><a href=""http://ebiquity.umbc.edu/paper/html/id/271/BayesOWL-Uncertainty-Modeling-in-Semantic-Web-Ontologies"" rel=""nofollow"">http://ebiquity.umbc.edu/paper/html/id/271/BayesOWL-Uncertainty-Modeling-in-Semantic-Web-Ontologies</a></p>

<p><a href=""http://www.pr-owl.org/"" rel=""nofollow"">http://www.pr-owl.org/</a></p>
",,0,2016-08-26T15:08:11.110,,1745,2016-10-07T18:24:05.180,2016-10-07T18:24:05.180,,8.0,,33.0,218.0,2,1,,,,42.21,17.87,9.72,0.0,0.0,107.0,In general how algorithm should distinguish the word meaning and recognise the word within the context I dont think anybody knows how to answer this for the general case If they did theyd have basically solved AGI But we can certainly talk about techniques that get partoftheway there and approaches that could work One thing I would consider trying and I dont know offhand if anybody has tried this exact approach is to model the disambiguation of each word as a discrete problem for a Bayesian Belief Network where your priors for any given word are based on both stored knowledge as well as the previously encountered words in the sentenceparagraphdocumentwhatever So if you know for example that Reading is a city in the UK and that place names are usually capitalized your network should be strongly biased towards interpreting Reading as the city since nothing in the word position in the sentence strongly contradicts that Of course Im handwaving around some tricky problems in saying that as knowledge representation isnt exactly a solved problem either But there are at least approaches out there that you could use For example you could use the RDF triple based approach from the Semantic Web world Finding a good way to merge that stuff with a Bayesian framework could yield some interesting results There has been a bit of research on probabilistic RDF that you could possibly use as a starting point For example httpomumiacsumdedumaterialpapersprdfpdf httpceurwsorgVol173pospaper5pdf httpswwww3org20050307yoshioUMBC httpebiquityumbcedupaperhtmlid271BayesOWLUncertaintyModelinginSemanticWebOntologies httpwwwprowlorg
,,"<p>On possibility is a <a href=""https://en.wikipedia.org/wiki/Blackboard_system"" rel=""nofollow"">blackboard architecture</a>.  Envision each different ""kind"" of intelligence as a discrete agent, and let the agents collaborate using the blackboard model.  Now you have an AI with multiple intelligences.  </p>

<p>This is something I've actually been experimenting with, and while I don't have any particularly impressive results to share or anything, I hold a strong belief that an approach that is at least somewhat like this will be crucial to developing an AGI.  And that is rooted in my belief that the human mind does have ""multiple intelligences"" and that they collaborate something like this.</p>
",,0,2016-08-26T15:28:52.913,,1746,2016-08-26T15:28:52.913,,,,,33.0,179.0,2,0,,,,43.12,12.76,9.93,0.0,0.0,14.0,On possibility is a blackboard architecture Envision each different kind of intelligence as a discrete agent and let the agents collaborate using the blackboard model Now you have an AI with multiple intelligences This is something Ive actually been experimenting with and while I dont have any particularly impressive results to share or anything I hold a strong belief that an approach that is at least somewhat like this will be crucial to developing an AGI And that is rooted in my belief that the human mind does have multiple intelligences and that they collaborate something like this
,,"<p>I don't know that it has yielded any actual reports or regulations yet, but in the USA, the White House has been running a series of interagency workshops / working groups dedicated to ""Preparing for the Future of Artificial Intelligence"".  </p>

<p><a href=""https://www.whitehouse.gov/blog/2016/05/03/preparing-future-artificial-intelligence"" rel=""nofollow"">https://www.whitehouse.gov/blog/2016/05/03/preparing-future-artificial-intelligence</a></p>

<p>Some of those sessions have been dedicated to legal / governance issues.  </p>
",,0,2016-08-26T15:36:55.550,,1747,2016-08-26T15:36:55.550,,,,,33.0,1731.0,2,2,,,,11.75,22.17,11.4,0.0,0.0,22.0,I dont know that it has yielded any actual reports or regulations yet but in the USA the White House has been running a series of interagency workshops working groups dedicated to Preparing for the Future of Artificial Intelligence httpswwwwhitehousegovblog20160503preparingfutureartificialintelligence Some of those sessions have been dedicated to legal governance issues
,,"<p>I would think you could use a <a href=""https://en.wikipedia.org/wiki/Graph_database"" rel=""nofollow"">graph database</a>, perhaps <a href=""https://neo4j.com/"" rel=""nofollow"">Neo4J</a> or <a href=""http://titan.thinkaurelius.com/"" rel=""nofollow"">Titan</a> or something of that nature.  Or, if you want a simple file format, you could use one of the many formats that exist for representing graphs.  You can find a list and overview of some of them <a href=""https://gephi.org/users/supported-graph-formats/"" rel=""nofollow"">here</a>.</p>

<p>Another option would be to store them in <a href=""https://en.wikipedia.org/wiki/Resource_Description_Framework"" rel=""nofollow"">RDF</a> using a <a href=""https://en.wikipedia.org/wiki/Triplestore"" rel=""nofollow"">triplestore</a> like <a href=""https://jena.apache.org/"" rel=""nofollow"">Jena</a>. </p>
",,0,2016-08-26T18:58:53.653,,1748,2016-08-26T18:58:53.653,,,,,33.0,212.0,2,1,,,,71.95,7.07,8.81,0.0,0.0,7.0,I would think you could use a graph database perhaps Neo4J or Titan or something of that nature Or if you want a simple file format you could use one of the many formats that exist for representing graphs You can find a list and overview of some of them here Another option would be to store them in RDF using a triplestore like Jena
,,"<p>If I understand you correctly, you should check out <strong>Word2Vec</strong>. From <a href=""https://en.wikipedia.org/wiki/Word2vec"" rel=""nofollow"">Wikipedia</a>: </p>

<blockquote>
  <p>Word2vec is a group of related models that are used to produce word
  embeddings. These models are shallow, two-layer neural networks that
  are trained to reconstruct linguistic contexts of words. Word2vec
  takes as its input a large corpus of text and produces a
  high-dimensional space (typically of several hundred dimensions), with
  each unique word in the corpus being assigned a corresponding vector
  in the space. Word vectors are positioned in the vector space such
  that words that share common contexts in the corpus are located in
  close proximity to one another in the space.</p>
</blockquote>
",,0,2016-08-26T23:10:09.577,,1749,2016-08-26T23:10:09.577,,,,,1712.0,212.0,2,1,,,,49.96,12.88,10.2,0.0,0.0,13.0,If I understand you correctly you should check out Word2Vec From Wikipedia Word2vec is a group of related models that are used to produce word embeddings These models are shallow twolayer neural networks that are trained to reconstruct linguistic contexts of words Word2vec takes as its input a large corpus of text and produces a highdimensional space typically of several hundred dimensions with each unique word in the corpus being assigned a corresponding vector in the space Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space
1753.0,1.0,"<p>By new, unseen examples; I mean like the animals in <a href=""https://en.wikipedia.org/wiki/No_Man%27s_Sky"" rel=""nofollow noreferrer"">No Man's Sky</a>. </p>

<p>A couple of images of the animals are:
<a href=""https://i.stack.imgur.com/zS0rX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zS0rX.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Ir1Qt.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ir1Qt.jpg"" alt=""enter image description here""></a></p>

<p>So, upon playing this game, I was curious <strong>about how good is AI at generating visual characters or examples?</strong></p>
",,0,2016-08-27T08:29:06.720,,1750,2016-08-27T16:42:49.737,,,,,101.0,,1,5,<research><image-recognition>,"How good is AI at generating new, unseen [visual] examples?",133.0,68.1,7.83,9.37,0.0,0.0,8.0,By new unseen examples I mean like the animals in No Mans Sky A couple of images of the animals are So upon playing this game I was curious about how good is AI at generating visual characters or examples
1755.0,1.0,"<p>I wanted to know what the differences between hyper-heuristics and meta-heuristics are, and what their main applications are. Which problems are suited to be solved by Hyper-heuristics?</p>
",,1,2016-08-27T13:15:19.897,1.0,1751,2016-08-28T14:27:16.710,2016-08-28T04:53:16.083,,145.0,,1760.0,,1,5,<definitions><optimization>,What are Hyper-heuristics?,173.0,40.85,16.29,10.15,0.0,0.0,6.0,I wanted to know what the differences between hyperheuristics and metaheuristics are and what their main applications are Which problems are suited to be solved by Hyperheuristics
,,"<p>We are getting pretty good at image generation, some examples:</p>

<ol>
<li><p>Radford, Alec, Luke Metz, and Soumith Chintala. ""Unsupervised representation learning with deep convolutional generative adversarial networks."" arXiv preprint arXiv:1511.06434 (2015). <a href=""https://arxiv.org/pdf/1511.06434.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1511.06434.pdf</a></p></li>
<li><p>Gregor, Karol, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. ""DRAW: A recurrent neural network for image generation."" arXiv preprint arXiv:1502.04623 (2015). <a href=""https://arxiv.org/pdf/1502.04623.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1502.04623.pdf</a></p></li>
</ol>

<p>From (1):
<a href=""https://i.stack.imgur.com/WMRuO.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WMRuO.jpg"" alt=""enter image description here""></a></p>

<p>Then there is another research direction around  evolutionary algorithms, for example:</p>

<ul>
<li>Sims, Karl. ""Evolving virtual creatures."" In Proceedings of the 21st annual conference on Computer graphics and interactive techniques, pp. 15-22. ACM, 1994. <a href=""https://scholar.google.com/scholar?cluster=6031059536657676358&amp;hl=en&amp;as_sdt=0,22"" rel=""nofollow noreferrer"">https://scholar.google.com/scholar?cluster=6031059536657676358&amp;hl=en&amp;as_sdt=0,22</a> ; <a href=""https://www.youtube.com/watch?v=bBt0imn77Zg"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=bBt0imn77Zg</a></li>
</ul>
",,0,2016-08-27T16:42:49.737,,1753,2016-08-27T16:42:49.737,,,,,4.0,1750.0,2,5,,,,27.18,31.06,13.49,0.0,0.0,86.0,We are getting pretty good at image generation some examples Radford Alec Luke Metz and Soumith Chintala Unsupervised representation learning with deep convolutional generative adversarial networks arXiv preprint arXiv151106434 2015 httpsarxivorgpdf151106434pdf Gregor Karol Ivo Danihelka Alex Graves Danilo Jimenez Rezende and Daan Wierstra DRAW A recurrent neural network for image generation arXiv preprint arXiv150204623 2015 httpsarxivorgpdf150204623pdf From 1 Then there is another research direction around evolutionary algorithms for example Sims Karl Evolving virtual creatures In Proceedings of the 21st annual conference on Computer graphics and interactive techniques pp 1522 ACM 1994 httpsscholargooglecomscholarcluster6031059536657676358amphlenampassdt022 httpswwwyoutubecomwatchvbBt0imn77Zg
,,"<p>The way i do emotion in a AGI system are by a bunch of little
parts, agents, voting in system statis state registers. If the union
of agents are working together correctly. This is the subconscious part.</p>

<p>The conscious part that plan out movement in the environment
include these system statis state registers in all planned movements.</p>

<p>All emotions can be derived from these registers:</p>

<p><a href=""https://groups.google.com/forum/#!topic/artificial-general-intelligence/pxWmHClAAdA"" rel=""nofollow"">https://groups.google.com/forum/#!topic/artificial-general-intelligence/pxWmHClAAdA</a>  </p>

<p><a href=""https://groups.google.com/forum/#!topic/artificial-general-intelligence/jWdzPaxYHmU"" rel=""nofollow"">https://groups.google.com/forum/#!topic/artificial-general-intelligence/jWdzPaxYHmU</a>  </p>

<p><a href=""https://groups.google.com/forum/#!forum/artificial-general-intelligence"" rel=""nofollow"">https://groups.google.com/forum/#!forum/artificial-general-intelligence</a>  </p>
",,0,2016-08-27T18:14:37.717,,1754,2016-08-27T18:14:37.717,,,,,1355.0,26.0,2,0,,,,-1.55,29.63,10.58,0.0,0.0,45.0,The way i do emotion in a AGI system are by a bunch of little parts agents voting in system statis state registers If the union of agents are working together correctly This is the subconscious part The conscious part that plan out movement in the environment include these system statis state registers in all planned movements All emotions can be derived from these registers httpsgroupsgooglecomforumtopicartificialgeneralintelligencepxWmHClAAdA httpsgroupsgooglecomforumtopicartificialgeneralintelligencejWdzPaxYHmU httpsgroupsgooglecomforumforumartificialgeneralintelligence
,,"<p><strong>TL:DR</strong>: Hyper-heuristics <em>are</em> metaheuristics, suited for solving the same kind of optimization problems, but (in principle) affording a ""rapid prototyping"" approach for non-expert practitioners. In practice, there are issues with the prevailing approach, motivating an emerging perspective on <a href=""http://www.cs.nott.ac.uk/~pszeo/docs/publications/HHrecharacterization.pdf"" rel=""nofollow noreferrer"">'whitebox' hyper-heuristics</a>.</p>

<p>In more detail:</p>

<p>Metaheuristics are methods for searching an intractably large space of possible solutions in order to find a 'high quality' solution. Popular metaheuristics include Simulated Annealing, Tabu Search, Genetic Algorithms etc.</p>

<p>The essential difference between metaheuristics and hyper-heuristics is the addition of a level of search indirection: informally, hyper-heuristics can be described as 'heuristics for searching the space of heuristics'. One can therefore use any metaheuristic as a hyper-heuristic, providing the nature of the 'space of heuristics' to be searched is appropriately defined.</p>

<p>The application area for hyper-heuristics is therefore the same as metaheuristics. Their applicability (relative to metaheuristics) is as a 'rapid prototyping tool': the original motivation was to allow non-expert practitioners to apply metaheuristics to their specific optimization problem (e.g. ""Travelling-Salesman (TSP) plus time-windows plus bin-packing"") without requiring expertise in the highly-specific problem domain. The idea was that this could be done by:</p>

<ol>
<li>Allowing practitioners to implement only very simple (effectively,
randomized) heuristics for transforming potential solutions. For
example, for the TSP: ""swap two random cities"" rather than (say) the more
complex <a href=""https://en.wikipedia.org/wiki/Lin%E2%80%93Kernighan_heuristic"" rel=""nofollow noreferrer"">Lin-Kernighan</a> heuristic. </li>
<li>Achieve effective results (despite using these simple heuristics) by combining/sequencing them in an intelligent way, typically by employing some form of learning mechanism.</li>
</ol>

<p>Hyper-heuristics can be described as 'selective' or 'generative' depending on whether the heuristics are  (respectively) sequenced or combined. Generative hyper-heuristics thus often use methods such as Genetic Programming to combine primitive heuristics and are therefore typically customized by the practitioner to solve a specific problem. For example, the <a href=""http://s3.amazonaws.com/academia.edu.documents/44119367/Hyper-heuristics_Learning_To_Combine_Sim20160326-8451-ouh5fe.pdf?AWSAccessKeyId=AKIAJ56TQJRTWSMTNPEA&amp;Expires=1472329757&amp;Signature=KT1E2ATKreC%2BGlTvPmJGYBFRSRY%3D&amp;response-content-disposition=inline%3B%20filename%3DHyper-heuristics_learning_to_combine_sim.pdf"" rel=""nofollow noreferrer"">original paper</a> on generative hyper-heuristics used a Learning Classifier System to combine heuristics for bin-packing. Because generative approaches are problem-specific, the comments below do not apply to them.</p>

<p>In contrast, the <a href=""http://www.cs.nott.ac.uk/~pszgxk/papers/evocop02exs.pdf"" rel=""nofollow noreferrer"">original motivator</a> for selective hyper-heuristics was that researchers would be able to create a hyper-heuristic solver that was then likely to work well in an unseen problem domain, using only simple randomized heuristics.</p>

<p>The way that this has traditionally been implemented was via the introduction of the <a href=""http://www.cs.stir.ac.uk/~goc/papers/ChapterClassHH.pdf"" rel=""nofollow noreferrer"">'hyper-heuristic domain barrier'</a> (see figure, below), whereby generality across problem domains is claimed to be achievable by preventing the solver from having knowledge of the domain on which it is operating. Instead, it would solve the problem by operating only on opaque integer indices into a list of available heuristics (e.g. in the manner of the <a href=""https://en.wikipedia.org/wiki/Multi-armed_bandit"" rel=""nofollow noreferrer"">'Multi-armed Bandit Problem'</a>).</p>

<p><a href=""https://i.stack.imgur.com/6IFna.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6IFna.png"" alt=""Traditional notion of Selective Hyper-heuristic""></a></p>

<p>In practice, this 'domain blind' approach has not resulted in solutions of sufficient quality. In order to achieve results anywhere comparable to problem-specific metaheuristics, hyper-heuristic researchers have had to implement complex problem-specific heuristics, thereby failing in the goal of rapid prototyping.</p>

<p>It is still possible <em>in principle</em> to create a selective hyper-heuristic solver which is capable of generalizing to new problem domains, but this has been made more difficult since the above notion of domain barrier means that only a very limited feature set is available for cross-domain learning (e.g. as exemplified by a popular <a href=""https://arxiv.org/abs/1107.5462"" rel=""nofollow noreferrer"">selective hyper-heuristic framework</a>).</p>

<p>A more recent research perspective towards <a href=""http://www.cs.nott.ac.uk/~pszeo/docs/publications/HHrecharacterization.pdf"" rel=""nofollow noreferrer"">'whitebox' hyper-heuristics</a> advocates a declarative, feature-rich approach to describing problem domains. This approach has a number of claimed advantages:</p>

<ol>
<li>Practitioners now need no longer <em>implement</em> heuristics, but rather simply <em>specify</em> the problem domain.</li>
<li>It eliminates the domain-barrier, putting hyper-heuristics on the same 'informed' status about the problem as problem-specific metaheuristics.</li>
<li>With a whitebox problem description, the infamous <a href=""https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf"" rel=""nofollow noreferrer"">'No Free Lunch' theorem</a> (which essentially states that, considered over the space of all <em>black box</em> problems, Simulated Annealing with an infinite annealing schedule is, on average, as good as any other approach) no longer applies.</li>
</ol>

<p>DISCLAIMER: I work in this research area, and it is therefore impossible to remove all personal bias from the answer.</p>
",,1,2016-08-27T19:37:35.330,,1755,2016-08-28T07:36:35.337,2016-08-28T07:36:35.337,,42.0,,42.0,1751.0,2,8,,,,23.56,18.62,10.93,0.0,0.0,162.0,TLDR Hyperheuristics are metaheuristics suited for solving the same kind of optimization problems but in principle affording a rapid prototyping approach for nonexpert practitioners In practice there are issues with the prevailing approach motivating an emerging perspective on whitebox hyperheuristics In more detail Metaheuristics are methods for searching an intractably large space of possible solutions in order to find a high quality solution Popular metaheuristics include Simulated Annealing Tabu Search Genetic Algorithms etc The essential difference between metaheuristics and hyperheuristics is the addition of a level of search indirection informally hyperheuristics can be described as heuristics for searching the space of heuristics One can therefore use any metaheuristic as a hyperheuristic providing the nature of the space of heuristics to be searched is appropriately defined The application area for hyperheuristics is therefore the same as metaheuristics Their applicability relative to metaheuristics is as a rapid prototyping tool the original motivation was to allow nonexpert practitioners to apply metaheuristics to their specific optimization problem eg TravellingSalesman TSP plus timewindows plus binpacking without requiring expertise in the highlyspecific problem domain The idea was that this could be done by Allowing practitioners to implement only very simple effectively randomized heuristics for transforming potential solutions For example for the TSP swap two random cities rather than say the more complex LinKernighan heuristic Achieve effective results despite using these simple heuristics by combiningsequencing them in an intelligent way typically by employing some form of learning mechanism Hyperheuristics can be described as selective or generative depending on whether the heuristics are respectively sequenced or combined Generative hyperheuristics thus often use methods such as Genetic Programming to combine primitive heuristics and are therefore typically customized by the practitioner to solve a specific problem For example the original paper on generative hyperheuristics used a Learning Classifier System to combine heuristics for binpacking Because generative approaches are problemspecific the comments below do not apply to them In contrast the original motivator for selective hyperheuristics was that researchers would be able to create a hyperheuristic solver that was then likely to work well in an unseen problem domain using only simple randomized heuristics The way that this has traditionally been implemented was via the introduction of the hyperheuristic domain barrier see figure below whereby generality across problem domains is claimed to be achievable by preventing the solver from having knowledge of the domain on which it is operating Instead it would solve the problem by operating only on opaque integer indices into a list of available heuristics eg in the manner of the Multiarmed Bandit Problem In practice this domain blind approach has not resulted in solutions of sufficient quality In order to achieve results anywhere comparable to problemspecific metaheuristics hyperheuristic researchers have had to implement complex problemspecific heuristics thereby failing in the goal of rapid prototyping It is still possible in principle to create a selective hyperheuristic solver which is capable of generalizing to new problem domains but this has been made more difficult since the above notion of domain barrier means that only a very limited feature set is available for crossdomain learning eg as exemplified by a popular selective hyperheuristic framework A more recent research perspective towards whitebox hyperheuristics advocates a declarative featurerich approach to describing problem domains This approach has a number of claimed advantages Practitioners now need no longer implement heuristics but rather simply specify the problem domain It eliminates the domainbarrier putting hyperheuristics on the same informed status about the problem as problemspecific metaheuristics With a whitebox problem description the infamous No Free Lunch theorem which essentially states that considered over the space of all black box problems Simulated Annealing with an infinite annealing schedule is on average as good as any other approach no longer applies DISCLAIMER I work in this research area and it is therefore impossible to remove all personal bias from the answer
,1.0,"<p>What is the difference between agent function and agent program with respect to percept sequence?</p>

<p>In the book <em>""Artificial Intelligence: A modern approach""</em>,</p>

<blockquote>
  <p>The agent function, notionally speaking, takes as input the entire
  percept sequence up to that point, whereas the agent program takes the
  current percept only.</p>
</blockquote>

<p>Why does the agent program only take current percept. Isn't it just implementation of the agent function?</p>
",,0,2016-08-28T10:03:06.923,,1756,2016-08-28T11:43:18.730,2016-08-28T11:39:37.100,,42.0,,35.0,,1,1,<definitions><models><intelligent-agent><reinforcement-learning>,Difference between agent function and agent program,654.0,55.03,14.27,10.76,0.0,0.0,12.0,What is the difference between agent function and agent program with respect to percept sequence In the book Artificial Intelligence A modern approach The agent function notionally speaking takes as input the entire percept sequence up to that point whereas the agent program takes the current percept only Why does the agent program only take current percept Isnt it just implementation of the agent function
,,"<p>It looks as if 'function' is being used here in the mathematical (or functional programming) sense of 'pure function', i.e. it is without state or side-effects. Hence the function cannot store previous percepts anywhere, so the entire historical percept sequence is considered to be passed to the function each time.</p>

<p>In contrast, the notion of 'program' appears to allow state/side-effects, so it is assumed that earlier percepts are memoized as needed (or that they otherwise updated the variables used within the program).</p>

<p>The 'function' notion is the conceptually cleaner one, in that the 'program' version can always be abstracted to the functional one. Which aspects of percept history happen to be cached by the 'program' version is merely an implementation detail.</p>
",,0,2016-08-28T11:43:18.730,,1757,2016-08-28T11:43:18.730,,,,,42.0,1756.0,2,3,,,,50.97,13.92,10.64,0.0,0.0,31.0,It looks as if function is being used here in the mathematical or functional programming sense of pure function ie it is without state or sideeffects Hence the function cannot store previous percepts anywhere so the entire historical percept sequence is considered to be passed to the function each time In contrast the notion of program appears to allow statesideeffects so it is assumed that earlier percepts are memoized as needed or that they otherwise updated the variables used within the program The function notion is the conceptually cleaner one in that the program version can always be abstracted to the functional one Which aspects of percept history happen to be cached by the program version is merely an implementation detail
,1.0,"<p>Are there currently any studies to simulate gradual (or sudden) implementation of AIs in the general work force?</p>
",,0,2016-08-28T22:58:48.853,,1761,2016-08-29T18:00:45.283,2016-08-29T17:29:34.540,,145.0,,1790.0,,1,2,<research><implementation>,Are there any anthropological studies involving AI right now?,84.0,36.28,13.05,11.55,0.0,0.0,3.0,Are there currently any studies to simulate gradual or sudden implementation of AIs in the general work force
,,"<h2>It's a well known concept that's already used</h2>

<p>What we call ""curiosity"" in humans and animals is in effect the chosen level of the ""exploit vs explore"" tradeoff for any active system. For example, the field of <a href=""https://en.wikipedia.org/wiki/Reinforcement_learning"" rel=""nofollow"" title=""Reinforcement learning"">reinforcement learning</a> is one approach that studies implementations of what essentially is the equivalent of curiosity; and we have research on how much curiosity is best e.g. <a href=""https://en.wikipedia.org/wiki/Multi-armed_bandit"" rel=""nofollow"">multi-armed bandit</a> concept.</p>

<p>So ""using curiosity"" is something that we already do as much as we can/should/are able to, but it would usually be called in some other, more specific term to specify the exact meaning instead of the vague word of ""curiosity"".</p>
",,0,2016-08-29T10:50:58.727,,1763,2016-08-29T10:50:58.727,,,,,1675.0,1544.0,2,1,,,,44.27,11.9,11.31,0.0,0.0,22.0,Its a well known concept thats already used What we call curiosity in humans and animals is in effect the chosen level of the exploit vs explore tradeoff for any active system For example the field of reinforcement learning is one approach that studies implementations of what essentially is the equivalent of curiosity and we have research on how much curiosity is best eg multiarmed bandit concept So using curiosity is something that we already do as much as we canshouldare able to but it would usually be called in some other more specific term to specify the exact meaning instead of the vague word of curiosity
,,"<p>In artificial intelligence, an intelligent agent (IA) is an autonomous entity which observes through sensors and acts upon an environment using actuators (i.e. it is an agent) and directs its activity towards achieving goals (i.e. it is ""rational"", as defined in economics). Intelligent agents may also learn or use knowledge to achieve their goals. They may be very simple or very complex: a reflex machine such as a thermostat is an intelligent agent.</p>
",,0,2016-08-29T11:13:51.387,,1764,2016-08-29T21:47:22.163,2016-08-29T21:47:22.163,,1807.0,,1807.0,,5,0,,,,48.2,12.4,11.28,0.0,0.0,18.0,In artificial intelligence an intelligent agent IA is an autonomous entity which observes through sensors and acts upon an environment using actuators ie it is an agent and directs its activity towards achieving goals ie it is rational as defined in economics Intelligent agents may also learn or use knowledge to achieve their goals They may be very simple or very complex a reflex machine such as a thermostat is an intelligent agent
,,This tag is to be used when asking questions based on bots or agents.,,0,2016-08-29T11:13:51.387,,1765,2016-08-29T21:47:47.050,2016-08-29T21:47:47.050,,1807.0,,1807.0,,4,0,,,,91.11,5.33,9.97,0.0,0.0,1.0,This tag is to be used when asking questions based on bots or agents
,,"<p>Classification is the automatic categorization of a new observation. This classification is based on a model produced from a training set of data containing observations whose classifications are given. Classification is especially useful for problems involving categorical data.</p>
",,0,2016-08-29T11:17:03.483,,1766,2016-08-29T17:29:20.003,2016-08-29T17:29:20.003,,1807.0,,1807.0,,5,0,,,,7.82,18.78,12.58,0.0,0.0,3.0,Classification is the automatic categorization of a new observation This classification is based on a model produced from a training set of data containing observations whose classifications are given Classification is especially useful for problems involving categorical data
,,"This tag is to be used in cases of doubts regarding applications or programs based on categorization like fraud detection, market segmentation (predicting whether or not a customer will respond to a promotion), etc.",,0,2016-08-29T11:17:03.483,,1767,2016-08-29T21:47:42.823,2016-08-29T21:47:42.823,,1807.0,,1807.0,,4,0,,,,28.51,14.34,12.29,0.0,0.0,5.0,This tag is to be used in cases of doubts regarding applications or programs based on categorization like fraud detection market segmentation predicting whether or not a customer will respond to a promotion etc
1769.0,12.0,"<p>In <a href=""https://en.wikipedia.org/wiki/Portal_2"" rel=""noreferrer"">Portal 2</a> we see that AI's can be ""killed"" by thinking about a paradox.</p>

<p><a href=""https://i.stack.imgur.com/wkUSC.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/wkUSC.png"" alt=""Portal Paradox Poster""></a></p>

<p>I assume this works by forcing the AI into an infinite loop which would essentially ""freeze"" the computer's consciousness.</p>

<p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p>
",,4,2016-08-29T15:49:14.173,42.0,1768,2016-09-26T07:58:48.773,2016-08-30T16:50:54.397,,29.0,,1812.0,,1,118,<decision-theory><death>,Could a paradox kill an AI?,23914.0,75.61,8.28,9.02,0.0,0.0,14.0,In Portal 2 we see that AIs can be killed by thinking about a paradox I assume this works by forcing the AI into an infinite loop which would essentially freeze the computers consciousness Questions Would this confuse the AI technology we have today to the point of destroying it If so why And if not could it be possible in the future
,,"<p>This classic problem exhibits a basic misunderstanding of what an <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"">artificial general intelligence</a> would likely entail. First, consider this programmer's joke:</p>

<blockquote>
  <p>The programmer's wife couldn't take it anymore. Every discussion with her husband turned into an argument over semantics, picking over every piece of trivial detail. One day she sent him to the grocery store to pick up some eggs. On his way out the door,  she said, <strong><em>""While you are there, pick up milk.""</em></strong></p>
  
  <p>And he never returned.</p>
</blockquote>

<p>It's a cute play on words, but it isn't terribly realistic.</p>

<p>You are assuming because AI is being executed by a computer, it must exhibit this same level of linear, unwavering pedantry outlined in this joke. But AI isn't simply some long-winded computer program hard-coded with enough if-statements and while-loops to account for every possible input and follow the prescribe results. </p>

<pre>while (command not completed)
     find solution()
</pre>

<p>This would not be strong AI. </p>

<p>In any classic definition of <em>artificial general intelligence</em>, you are creating a system that mimics some form of cognition that exhibits problem solving and <em>adaptive learning</em> (&larr;note this phrase here). I would suggest that any AI that could get stuck in such an ""infinite loop"" isn't a learning AI at all. <strong>It's just a buggy inference engine.</strong> </p>

<p>Essentially, you are endowing a program of currently-unreachable sophistication with an inability to postulate if there is a solution to a simple problem at all. I can just as easily say ""walk through that closed door"" or ""pick yourself up off the ground"" or even ""turn on that pencil"" &mdash; and present a similar conundrum. </p>

<blockquote>
  <p>""Everything I say is false."" &mdash; <a href=""https://en.wikipedia.org/wiki/Liar_paradox"">The Liar's Paradox</a></p>
</blockquote>
",,19,2016-08-29T17:11:43.167,,1769,2016-09-01T20:15:42.410,2016-09-01T20:15:42.410,,95.0,,95.0,1768.0,2,103,,,,55.13,12.12,9.81,0.0,0.0,65.0,This classic problem exhibits a basic misunderstanding of what an artificial general intelligence would likely entail First consider this programmers joke The programmers wife couldnt take it anymore Every discussion with her husband turned into an argument over semantics picking over every piece of trivial detail One day she sent him to the grocery store to pick up some eggs On his way out the door she said While you are there pick up milk And he never returned Its a cute play on words but it isnt terribly realistic You are assuming because AI is being executed by a computer it must exhibit this same level of linear unwavering pedantry outlined in this joke But AI isnt simply some longwinded computer program hardcoded with enough ifstatements and whileloops to account for every possible input and follow the prescribe results while command not completed find solution This would not be strong AI In any classic definition of artificial general intelligence you are creating a system that mimics some form of cognition that exhibits problem solving and adaptive learning larrnote this phrase here I would suggest that any AI that could get stuck in such an infinite loop isnt a learning AI at all Its just a buggy inference engine Essentially you are endowing a program of currentlyunreachable sophistication with an inability to postulate if there is a solution to a simple problem at all I can just as easily say walk through that closed door or pick yourself up off the ground or even turn on that pencil mdash and present a similar conundrum Everything I say is false mdash The Liars Paradox
,,"<p>This popular meme originated in the era of 'Good Old Fashioned AI' (GOFAI), when the belief was that intelligence could usefully be defined entirely in terms of logic.</p>

<p>The meme seems to rely on the AI parsing commands using a theorem prover, the idea presumably being that it's driven into some kind of infinite loop by trying to prove an unprovable or inconsistent statement.</p>

<p>Nowadays, GOFAI methods have been replaced by 'environment and percept sequences', which are not generally characterized in such an inflexible fashion. It would not take a great deal of sophisticated metacognition for a robot to observe that, after a while, its deliberations were getting in the way of useful work.</p>

<p>Rodney Brooks touched on this when speaking about the behavior of the robot in Spielberg's AI film, (which waited patiently for 5,000 years), saying something like ""My robots wouldn't do that - they'd get bored"". </p>

<p>EDIT: If you <em>really</em> want to kill an AI that operates in terms of percepts, you'll need to work quite a bit harder. <a href=""http://arxiv.org/pdf/1606.00652.pdf"">This paper</a> (which was mentioned in <a href=""http://ai.stackexchange.com/questions/1404/what-is-meant-by-death-in-this-paper"">this question</a>) discusses what notions of death/suicide might mean in such a case.</p>

<p>EDIT2: Douglas Hofstadter has written quite extensively around this subject, using terms such as 'JOOTSing' ('Jumping Out Of The System') and 'anti-Sphexishness', the latter referring to the loopy automata-like behaviour of the <a href=""https://en.wikipedia.org/wiki/Sphex"">Sphex Wasp</a> (though the reality of this behaviour has also been <a href=""http://www.academia.edu/4034267/The_Sphex_story_How_the_cognitive_sciences_kept_repeating_an_old_and_questionable_anecdote"">questioned</a>).</p>
",,3,2016-08-29T17:20:30.017,,1770,2016-09-01T08:43:33.410,2016-09-01T08:43:33.410,,42.0,,42.0,1768.0,2,32,,,,41.84,13.24,10.89,0.0,0.0,53.0,This popular meme originated in the era of Good Old Fashioned AI GOFAI when the belief was that intelligence could usefully be defined entirely in terms of logic The meme seems to rely on the AI parsing commands using a theorem prover the idea presumably being that its driven into some kind of infinite loop by trying to prove an unprovable or inconsistent statement Nowadays GOFAI methods have been replaced by environment and percept sequences which are not generally characterized in such an inflexible fashion It would not take a great deal of sophisticated metacognition for a robot to observe that after a while its deliberations were getting in the way of useful work Rodney Brooks touched on this when speaking about the behavior of the robot in Spielbergs AI film which waited patiently for 5000 years saying something like My robots wouldnt do that theyd get bored EDIT If you really want to kill an AI that operates in terms of percepts youll need to work quite a bit harder This paper which was mentioned in this question discusses what notions of deathsuicide might mean in such a case EDIT2 Douglas Hofstadter has written quite extensively around this subject using terms such as JOOTSing Jumping Out Of The System and antiSphexishness the latter referring to the loopy automatalike behaviour of the Sphex Wasp though the reality of this behaviour has also been questioned
,,"<p>As far as I can tell (I've been doing searches here and there on and off since I saw this question a few hours ago) the closest we've gotten to 'simulations' on this is video-games, and to a degree movies, interestingly enough. I.e. entertainment media.</p>

<p>Games like Portal, System Shock (with the AI 'Shodan'), and others give interpretations of what AI systems could be capable of themselves. Mass Effect is more or less entirely based around existential concepts regarding extra-terrestrial, almost primordial AI beings that threaten the earth.</p>

<p>But there's even more to it than the whole 'evil robots taking over the world' aspect. There's the actual <em>implementation</em> of AI in video games, which is where much of this technology first makes contact with the general public.</p>

<p>We have facial-scanners that put you into NBA games, cities full of realistically reacting people (inFamous, Assassin's Creed), and games that learn how you play and adjust the game accordingly (Metal Gear Solid does some of that stuff, as well as being thematically AI-heavy). </p>

<p>Ultimately, we only get things like the iPhone or VR headsets or other major proof-of-concept material only so often, but games are much more frequently implementing the most recent AI advances. </p>

<p>Thus, even though many AI systems are being put into place in the general workforce (many Hospitals now turning to cloud and AI health services, as recent as this week), I don't think you can really go further than video games or movies for 'simulations' or extrapolations like the ones you seem to want. </p>

<p>Analyzing the response to AI developments in games might be the closest thing currently possible. In terms of economic models or anything of that sort, I can find naught.</p>
",,0,2016-08-29T18:00:45.283,,1771,2016-08-29T18:00:45.283,,,,,1538.0,1761.0,2,2,,,,51.01,12.25,10.42,0.0,0.0,59.0,As far as I can tell Ive been doing searches here and there on and off since I saw this question a few hours ago the closest weve gotten to simulations on this is videogames and to a degree movies interestingly enough Ie entertainment media Games like Portal System Shock with the AI Shodan and others give interpretations of what AI systems could be capable of themselves Mass Effect is more or less entirely based around existential concepts regarding extraterrestrial almost primordial AI beings that threaten the earth But theres even more to it than the whole evil robots taking over the world aspect Theres the actual implementation of AI in video games which is where much of this technology first makes contact with the general public We have facialscanners that put you into NBA games cities full of realistically reacting people inFamous Assassins Creed and games that learn how you play and adjust the game accordingly Metal Gear Solid does some of that stuff as well as being thematically AIheavy Ultimately we only get things like the iPhone or VR headsets or other major proofofconcept material only so often but games are much more frequently implementing the most recent AI advances Thus even though many AI systems are being put into place in the general workforce many Hospitals now turning to cloud and AI health services as recent as this week I dont think you can really go further than video games or movies for simulations or extrapolations like the ones you seem to want Analyzing the response to AI developments in games might be the closest thing currently possible In terms of economic models or anything of that sort I can find naught
,,"<p>Well, the issue of anthropomorphizing the AI aside, the answer is ""yes, sort of.""  Depending on how the AI is implemented, it's reasonable to say it could get ""stuck"" trying to resolve a paradox, or decide an <a href=""https://en.wikipedia.org/wiki/Undecidable_problem"" rel=""nofollow"">undecidable problem</a>. </p>

<p>And that's the core issue - <a href=""https://en.wikipedia.org/wiki/Decidability_(logic)"" rel=""nofollow"">decidability</a>.  A computer can chew on an undecidable program forever (in principle) without finishing.  It's actually a big issue in the <a href=""https://en.wikipedia.org/wiki/Semantic_Web"" rel=""nofollow"">Semantic Web</a> community and everybody who works with <a href=""https://en.wikipedia.org/wiki/Automated_reasoning"" rel=""nofollow"">automated reasoning</a>. This is, for example, the reason that there are different versions of <a href=""https://en.wikipedia.org/wiki/Web_Ontology_Language"" rel=""nofollow"">OWL</a>.  OWL-Full is expressive enough to create undecidable situations.  OWL-DL and OWL-Lite aren't. </p>

<p>Anyway, if you have an undecidable problem, that in and of itself might not be a big deal, IF the AI can recognize the problem as undecidable and reply ""Sorry, there's no way to answer that"".  OTOH, if the AI failed to recognize the problem as undecidable, it could get stuck forever (or until it runs out of memory, experiences a stack overflow, etc.) trying to resolve things.</p>

<p>Of course this ability to say ""screw this, this riddle cannot be solved"" is one of the things we usually think of as a hallmark of human intelligence today - as opposed to a ""stupid"" computer that would keep trying forever to solve it.  By and large, today's AI's don't have any intrinsic ability to resolve this sort of thing.  But it wouldn't be that hard for whoever programs an AI to manually add a ""short circuit"" routine based on elapsed time, number of iterations, memory usage, etc.  Hence the ""yeah, sort of"" nature of this.  In principle, a program can spin forever on a paradoxical problem, but in practice it's not that hard to keep that from happening.</p>

<p>Another interesting question would be, ""can you write a program that learns to recognize problems that are highly likely to be undecidable and gives up based on it's own reasoning?""  </p>
",,0,2016-08-29T19:48:26.887,,1772,2016-09-01T15:47:16.127,2016-09-01T15:47:16.127,,33.0,,33.0,1768.0,2,1,,,,61.06,10.91,8.91,0.0,0.0,77.0,Well the issue of anthropomorphizing the AI aside the answer is yes sort of Depending on how the AI is implemented its reasonable to say it could get stuck trying to resolve a paradox or decide an undecidable problem And thats the core issue decidability A computer can chew on an undecidable program forever in principle without finishing Its actually a big issue in the Semantic Web community and everybody who works with automated reasoning This is for example the reason that there are different versions of OWL OWLFull is expressive enough to create undecidable situations OWLDL and OWLLite arent Anyway if you have an undecidable problem that in and of itself might not be a big deal IF the AI can recognize the problem as undecidable and reply Sorry theres no way to answer that OTOH if the AI failed to recognize the problem as undecidable it could get stuck forever or until it runs out of memory experiences a stack overflow etc trying to resolve things Of course this ability to say screw this this riddle cannot be solved is one of the things we usually think of as a hallmark of human intelligence today as opposed to a stupid computer that would keep trying forever to solve it By and large todays AIs dont have any intrinsic ability to resolve this sort of thing But it wouldnt be that hard for whoever programs an AI to manually add a short circuit routine based on elapsed time number of iterations memory usage etc Hence the yeah sort of nature of this In principle a program can spin forever on a paradoxical problem but in practice its not that hard to keep that from happening Another interesting question would be can you write a program that learns to recognize problems that are highly likely to be undecidable and gives up based on its own reasoning
,,"<p>No.  This is easily prevented by a number of safety mechanisms that are sure to be present in a well-designed AI system.  For example, a timeout could be used.  If the AI system is not able to handle a statement or a command after a certain amount of time, the AI could ignore the statement and move on.  If a paradox ever does cause an AI to freeze, it's more evidence of specific buggy code rather than a widespread vulnerability of AI in general.</p>

<p>In practice, paradoxes tend to be handled in not very exciting ways by AI.  To get an idea of this, try presenting a paradox to Siri, Google, or Cortana.</p>
",,3,2016-08-29T20:25:12.823,,1773,2016-08-29T20:25:12.823,,,,,69.0,1768.0,2,6,,,,60.85,7.66,10.02,0.0,0.0,16.0,No This is easily prevented by a number of safety mechanisms that are sure to be present in a welldesigned AI system For example a timeout could be used If the AI system is not able to handle a statement or a command after a certain amount of time the AI could ignore the statement and move on If a paradox ever does cause an AI to freeze its more evidence of specific buggy code rather than a widespread vulnerability of AI in general In practice paradoxes tend to be handled in not very exciting ways by AI To get an idea of this try presenting a paradox to Siri Google or Cortana
,2.0,"<p>In the 1950's, there were widely-held beliefs that ""Artificial Intelligence"" will quickly become both self-conscious and smart-enough to win chess with humans. Various people suggested time frames of e.g. 10 years (see Olazaran's ""Official History of the Perceptron Controversy"", or let say 2001: Space Odyssey).</p>

<p>When did it become clear that making computers play games like chess is not equal to AGI? Who was the first person to postulate separation of the concept of AGI from task-specific methods?</p>
",,0,2016-08-29T20:25:30.047,,1774,2016-08-29T23:54:06.080,2016-08-29T22:25:13.000,,1670.0,,1670.0,,1,4,<history>,AGI vs. task-specific algorithms: when people realized these are two different goals?,77.0,55.64,13.98,10.69,0.0,0.0,21.0,In the 1950s there were widelyheld beliefs that Artificial Intelligence will quickly become both selfconscious and smartenough to win chess with humans Various people suggested time frames of eg 10 years see Olazarans Official History of the Perceptron Controversy or let say 2001 Space Odyssey When did it become clear that making computers play games like chess is not equal to AGI Who was the first person to postulate separation of the concept of AGI from taskspecific methods
,1.0,"<p>We hear a lot today about how <a href=""http://deeplearning4j.org/thoughtvectors"" rel=""nofollow"">thought vectors</a> are the <a href=""http://www.extremetech.com/extreme/206521-thought-vectors-could-revolutionize-artificial-intelligence"" rel=""nofollow"">Next Big Thing in AI</a>, and how they serve as the underlying representation of thought/knowledge in ANN's.  But how can one use thought vectors in other regimes, especially including symbolic logic / GOFAI?  Could thought vectors be the ""substrate"" that binds together probabilistic approaches to AI and approaches that are rooted in logic?  </p>
",,0,2016-08-29T21:00:04.167,,1775,2016-08-31T04:21:33.323,2016-08-29T21:11:26.600,,33.0,,33.0,,1,1,<neural-networks><gofai><logic><thought-vectors>,How can thought vectors be used outside of an Artificial Neural Network (ANN) context?,134.0,58.62,12.82,9.44,0.0,0.0,10.0,We hear a lot today about how thought vectors are the Next Big Thing in AI and how they serve as the underlying representation of thoughtknowledge in ANNs But how can one use thought vectors in other regimes especially including symbolic logic GOFAI Could thought vectors be the substrate that binds together probabilistic approaches to AI and approaches that are rooted in logic
,,"<p>The <a href=""https://en.wikipedia.org/wiki/Halting_problem"">halting problem</a> says that it's not possible to determine whether <em>any</em> given algorithm will halt. Therefore, while a machine could conceivably recognize some ""traps"", it couldn't test arbitrary execution plans and return <a href=""https://technet.microsoft.com/en-us/magazine/hh855063.aspx""><code>EWOULDHANG</code></a> for non-halting ones.</p>

<p>The easiest solution to avoid hanging would be a timeout. For example, the AI controller process could spin off tasks into child processes, which could be unceremoniously terminated after a certain time period (with none of the <a href=""http://docs.oracle.com/javase/1.5.0/docs/guide/misc/threadPrimitiveDeprecation.html"">bizarre effects</a> that you get from trying to abort threads). Some tasks will require more time than others, so it would be best if the AI could measure whether it was making any progress. Spinning for a long time without accomplishing any part of the task (e.g. eliminating one possibility in a list) indicates that the request might be unsolvable.</p>

<p>Successful adversarial paradoxes would either cause a hang or state corruption, which would (in a managed environment like the .NET CLR) cause an exception, which would cause the stack to unwind to an exception handler. </p>

<p>If there was a bug in the AI that let an important process get wedged in response to bad input, a simple workaround would be to have a watchdog of some kind that reboots the main process at a fixed interval. The Root Access chat bot uses that scheme.</p>
",,5,2016-08-29T21:37:57.493,,1776,2016-08-30T16:06:47.090,2016-08-30T16:06:47.090,,75.0,,75.0,1768.0,2,11,,,,59.94,11.43,9.85,10.0,0.0,31.0,The halting problem says that its not possible to determine whether any given algorithm will halt Therefore while a machine could conceivably recognize some traps it couldnt test arbitrary execution plans and return for nonhalting ones The easiest solution to avoid hanging would be a timeout For example the AI controller process could spin off tasks into child processes which could be unceremoniously terminated after a certain time period with none of the bizarre effects that you get from trying to abort threads Some tasks will require more time than others so it would be best if the AI could measure whether it was making any progress Spinning for a long time without accomplishing any part of the task eg eliminating one possibility in a list indicates that the request might be unsolvable Successful adversarial paradoxes would either cause a hang or state corruption which would in a managed environment like the NET CLR cause an exception which would cause the stack to unwind to an exception handler If there was a bug in the AI that let an important process get wedged in response to bad input a simple workaround would be to have a watchdog of some kind that reboots the main process at a fixed interval The Root Access chat bot uses that scheme
,,"<p>I expect a very precise answer to this question may be lost to the sands of time, although I hope somebody can given such an answer. In the meantime, here's one clue on the trail...  This <a href=""https://web.archive.org/web/20130320184603/http://people.inf.elte.hu/csizsekp/ai/books/artificial-general-intelligence-cognitive-technologies.9783540237334.27156.pdf"" rel=""nofollow"">anthology of papers from 2007</a> starts with the following blurb:</p>

<blockquote>
  <p>Our goal in creating this edited volume has been to fill an apparent gap
  in the scientific literature, by providing a coherent presentation of a body of
  contemporary research that, in spite of its integral importance, has hitherto
  kept a very low profile within the scientific and intellectual community. This
  body of work has not been given a name before; in this book we christen it
  “Artificial General Intelligence” (AGI). What distinguishes AGI work from
  run-of-the-mill “artificial intelligence” research is that it is explicitly focused
  on engineering general intelligence in the short term.</p>
</blockquote>

<p>But even if this is the origin of the specific phrase ""Artificial General Intelligence"", I am pretty sure people were making the distinction between ""general intelligence"" and ""task specific"" techniques much earlier.  </p>

<p>The Wikipedia article on AGI also has a clue, where it states:</p>

<blockquote>
  <p>However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. The agencies that funded AI became skeptical of strong AI and put researchers under increasing pressure to produce useful technology, or ""applied AI"".</p>
</blockquote>

<p>That section cites this <a href=""http://www.nap.edu/read/6323/chapter/11#209"" rel=""nofollow"">this book</a> as support for that statement.  And indeed, it contains the following verbiage:</p>

<blockquote>
  <p>Although most founders of the AI field continued to pursue basic questions of human and machine intelligence, some of their students and other second-generation researchers began to seek ways to use AI methods and approaches to tackle real-world problems. Their initiatives were important, not only in their own right, but also because they were indicative of a gradual but significant change in the funding environment toward more applied realms of research. The development of expert systems, such as DENDRAL at SAIL, provides but one example of this trend.</p>
</blockquote>

<p>Given that DENDRAL began around 1965, it appears that some significant body of researchers (or at least funders) became strongly aware of the distinction between research into ""general intelligence"" and ""applied AI"" somewhere around the end of the 1960's. If you keep reading, other passages support the notion that DARPA in particular started pushing a more ""applied"" approach to AI research throughout the 1970's.</p>

<p>So, not a definite answer, but it looks like we can say that the distinction was known and taken into account at least by 1970, although use of the exact term ""artificial general intelligence"" appears to be of more recent coinage.</p>
",,0,2016-08-29T22:53:42.043,,1777,2016-08-29T22:53:42.043,,,,,33.0,1774.0,2,1,,,,42.04,13.82,10.7,0.0,0.0,70.0,I expect a very precise answer to this question may be lost to the sands of time although I hope somebody can given such an answer In the meantime heres one clue on the trail This anthology of papers from 2007 starts with the following blurb Our goal in creating this edited volume has been to fill an apparent gap in the scientific literature by providing a coherent presentation of a body of contemporary research that in spite of its integral importance has hitherto kept a very low profile within the scientific and intellectual community This body of work has not been given a name before in this book we christen it “Artificial General Intelligence” AGI What distinguishes AGI work from runofthemill “artificial intelligence” research is that it is explicitly focused on engineering general intelligence in the short term But even if this is the origin of the specific phrase Artificial General Intelligence I am pretty sure people were making the distinction between general intelligence and task specific techniques much earlier The Wikipedia article on AGI also has a clue where it states However in the early 1970s it became obvious that researchers had grossly underestimated the difficulty of the project The agencies that funded AI became skeptical of strong AI and put researchers under increasing pressure to produce useful technology or applied AI That section cites this this book as support for that statement And indeed it contains the following verbiage Although most founders of the AI field continued to pursue basic questions of human and machine intelligence some of their students and other secondgeneration researchers began to seek ways to use AI methods and approaches to tackle realworld problems Their initiatives were important not only in their own right but also because they were indicative of a gradual but significant change in the funding environment toward more applied realms of research The development of expert systems such as DENDRAL at SAIL provides but one example of this trend Given that DENDRAL began around 1965 it appears that some significant body of researchers or at least funders became strongly aware of the distinction between research into general intelligence and applied AI somewhere around the end of the 1960s If you keep reading other passages support the notion that DARPA in particular started pushing a more applied approach to AI research throughout the 1970s So not a definite answer but it looks like we can say that the distinction was known and taken into account at least by 1970 although use of the exact term artificial general intelligence appears to be of more recent coinage
,,"<p>In 1973, the British government hired Sir James Lighthill to commission a ""general survey"" on the state of artificial intelligence. His report was a condemnation of current AI research, leading a wave of pessimism among AI scientists and the <a href=""https://en.wikipedia.org/wiki/AI_winter#The_setbacks_of_1974"" rel=""nofollow""><strong>First AI Winter</strong></a>. You may view Lighthill's report (and contemporary criticism of his report) <a href=""http://www.math.snu.ac.kr/~hichoi/infomath/Articles/Lighthill%20Report.pdf"" rel=""nofollow"">here</a>, but I will summarize Lighthill's key points. </p>

<p>Sir James Lighthill divided AI into three categories:</p>

<ol>
<li><strong>Advanced Automation</strong> - task-specific work</li>
<li><strong>Computer-based CNS research</strong> - research into the the ""central nervous system"" of humans</li>
<li>The <strong>Bridge</strong> between Advanced Automation and Computer-based CNS research. This bridge would generally be seen as ""general-purpose"" robotics, so Lighthill would also use the term <strong>Building Robots</strong>.</li>
</ol>

<p><strong>Advanced Automation</strong> (or ""applied AI"") is obviously useful. <strong>Computer-based CNS research</strong> is useful because we want to know more about human intelligence. Both fields of AI had some successes, but its practitioners were overly optimistic, leading to disappointment in those fields. Sir James Lighthill was still very supportive of research in these two fields though.</p>

<p><strong>Building Robots</strong>, on the other hand? Sir James Lighthill was very hostile to the very idea, probably because it was more overly hyped up than the other two categories and produced the least amount of valuable output.</p>

<p>He mentioned chess in particular as an example where ""robotic"" research  has failed. At the time the report was published, the chess-playing engines were at the level of ""experienced amateur standard characteristic of county club players in England"". However, these chess-playing engines relied on heuristics that were made by human beings. The engines weren't intelligent at all...they merely were following the heuristics that were created by <em>intelligent humans</em>. The only advantage the robots bring to the table is ""speed, reliability and biddability"", and even that wasn't enough to beat the chess grandmasters.</p>

<p>Now, today, we would probably not treat chess as an example of general-purpose problem solving. We would more accurately classify it as ""advanced automation"", a ""narrow AI"" problem divorced from broader real-world implications of general problem-solving. But Sir James Lighthill probably would agree with us. He never used the term ""narrow AI"" and ""AGI"" (neither of those terms existed yet) but he would write:</p>

<blockquote>
  <p>To sum up, this evidence and all the rest studied by the present author on AI work within category B during the past twenty-five years is to some extent encouraging about programs written to perform in highly specialised problem domains, when the programming takes very full account of the results of human experience and human intelligence within the relevant domain, but is wholly discouraging about general-purpose programs seeking to mimic the problem-solving aspects of human CNS activity over a rather wide field. Such a general- purpose program, the coveted long-term goal of AI activity, seems as remote as ever.</p>
</blockquote>

<p>Sir James Lighthill believed that the only thing that connects <strong>Advanced Automation</strong> and <strong>Computer-based CNS research</strong> is the existence of the <strong>Building Roobts</strong> ""bridge"" category. But he's very pessimistic about this category actually producing anything worthwhile. So instead, the AI field should instead breakup into its own its constituent parts (automation and research). Any robots that are built could then be specialized within their subfield...either industrial automation or CNS research. Trying to build the holy grail of ""general-purpose program"" would be worthless...for the time being, at least.</p>
",,0,2016-08-29T23:48:42.737,,1778,2016-08-29T23:54:06.080,2016-08-29T23:54:06.080,,181.0,,181.0,1774.0,2,1,,,,43.93,15.03,10.01,0.0,0.0,117.0,In 1973 the British government hired Sir James Lighthill to commission a general survey on the state of artificial intelligence His report was a condemnation of current AI research leading a wave of pessimism among AI scientists and the First AI Winter You may view Lighthills report and contemporary criticism of his report here but I will summarize Lighthills key points Sir James Lighthill divided AI into three categories Advanced Automation taskspecific work Computerbased CNS research research into the the central nervous system of humans The Bridge between Advanced Automation and Computerbased CNS research This bridge would generally be seen as generalpurpose robotics so Lighthill would also use the term Building Robots Advanced Automation or applied AI is obviously useful Computerbased CNS research is useful because we want to know more about human intelligence Both fields of AI had some successes but its practitioners were overly optimistic leading to disappointment in those fields Sir James Lighthill was still very supportive of research in these two fields though Building Robots on the other hand Sir James Lighthill was very hostile to the very idea probably because it was more overly hyped up than the other two categories and produced the least amount of valuable output He mentioned chess in particular as an example where robotic research has failed At the time the report was published the chessplaying engines were at the level of experienced amateur standard characteristic of county club players in England However these chessplaying engines relied on heuristics that were made by human beings The engines werent intelligent at allthey merely were following the heuristics that were created by intelligent humans The only advantage the robots bring to the table is speed reliability and biddability and even that wasnt enough to beat the chess grandmasters Now today we would probably not treat chess as an example of generalpurpose problem solving We would more accurately classify it as advanced automation a narrow AI problem divorced from broader realworld implications of general problemsolving But Sir James Lighthill probably would agree with us He never used the term narrow AI and AGI neither of those terms existed yet but he would write To sum up this evidence and all the rest studied by the present author on AI work within category B during the past twentyfive years is to some extent encouraging about programs written to perform in highly specialised problem domains when the programming takes very full account of the results of human experience and human intelligence within the relevant domain but is wholly discouraging about generalpurpose programs seeking to mimic the problemsolving aspects of human CNS activity over a rather wide field Such a general purpose program the coveted longterm goal of AI activity seems as remote as ever Sir James Lighthill believed that the only thing that connects Advanced Automation and Computerbased CNS research is the existence of the Building Roobts bridge category But hes very pessimistic about this category actually producing anything worthwhile So instead the AI field should instead breakup into its own its constituent parts automation and research Any robots that are built could then be specialized within their subfieldeither industrial automation or CNS research Trying to build the holy grail of generalpurpose program would be worthlessfor the time being at least
,,"<p>You know when you have too many neurons is when you get over fitting.
Meaning that it is not working good because
NN is trying to activate on the
most perfect match that is impossible. Like two different cats with the same amount of atoms, or to say, it is a detector NN that only activates
on a picture of you pet cat and nothing else. You want a wider range
for the nn to activate. Like on any picture of cat.</p>

<p>Overfitting is a problem that has no real quick fix.
 You can start with too few and then keep adding more. Or start out with
a lot and then removing them until it works right.</p>
",,0,2016-08-30T00:20:01.793,,1779,2016-08-30T00:20:01.793,,,,,1355.0,4.0,2,1,,,,82.04,5.5,7.87,0.0,0.0,10.0,You know when you have too many neurons is when you get over fitting Meaning that it is not working good because NN is trying to activate on the most perfect match that is impossible Like two different cats with the same amount of atoms or to say it is a detector NN that only activates on a picture of you pet cat and nothing else You want a wider range for the nn to activate Like on any picture of cat Overfitting is a problem that has no real quick fix You can start with too few and then keep adding more Or start out with a lot and then removing them until it works right
,,"<p>It seems to me this is just a probabilistic equation like any other. I'm sure Google handles paradoxical solution sets Billions of times a day, and I can't say my spam filter has ever caused a (ahem) stack overflow. Perhaps one day our programming model will break in a way we can't understand and then all bets are off.</p>

<p>But I do take exception to the anthropomorphizing bit. The question was not about the AI of today, but in general. Perhaps one day paradoxes will become triggers for military drones -- anyone trying the above would then, of course, most certainly be treated with hostility, in which case the answer to this question is most definitely yes, and it could even be by design. </p>

<p>We can't even communicate verbally with dogs and people love dogs, who is to say we would even necessarily recognize a sentient alternative intelligence? We're already to the point of having to mind what we say in front of computers. O, Tay?</p>
",,1,2016-08-30T00:56:38.400,,1780,2016-08-30T00:56:38.400,,,,,1828.0,1768.0,2,1,,,,59.03,9.75,9.44,0.0,0.0,26.0,It seems to me this is just a probabilistic equation like any other Im sure Google handles paradoxical solution sets Billions of times a day and I cant say my spam filter has ever caused a ahem stack overflow Perhaps one day our programming model will break in a way we cant understand and then all bets are off But I do take exception to the anthropomorphizing bit The question was not about the AI of today but in general Perhaps one day paradoxes will become triggers for military drones anyone trying the above would then of course most certainly be treated with hostility in which case the answer to this question is most definitely yes and it could even be by design We cant even communicate verbally with dogs and people love dogs who is to say we would even necessarily recognize a sentient alternative intelligence Were already to the point of having to mind what we say in front of computers O Tay
,,"<p>Another similar question might be: ""What vulnerabilities does an AI have?""</p>

<p>""Kill"" may not make as much sense with respect to an AI. What we really want to know is, relative to some goal, in what ways can that goal be subverted?</p>

<p>Can a paradox subvert an agent's logic? What is a <a href=""https://en.wikipedia.org/wiki/Paradox"">paradox</a>, other than some expression that subverts some kind of expected behavior?</p>

<p>According to Wikipedia:</p>

<blockquote>
  <p>A paradox is a statement that, despite apparently sound reasoning from
  true premises, leads to a self-contradictory or a logically
  unacceptable conclusion.</p>
</blockquote>

<p>Let's look at the paradox of free will in a deterministic system. Free will appears to require causality, but causality also <em>appears</em> to negate it. Has that paradox subverted the goal systems of humans? It certainly sent <a href=""https://en.wikipedia.org/wiki/Predestination_in_Calvinism"">Christianity into a Calvinist</a> tail spin for a few years. And you'll hear no shortage of people today opining until they're blue in the face as to whether or not they do or don't have free will, and why. Are these people stuck in infinite loops?</p>

<p>What about drugs? Animals on cocaine <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3832528/"">have been known</a> to choose cocaine over food and water that they need. Is that substance not subverting the natural goal system of the animal, causing it to pursue other goals, not originally intended by the animal or its creators?</p>

<p>So again, could a paradox subvert an agent's logic? If the paradox is somehow related to the goal-seeking logic - and becoming aware of that paradox can somehow <em>confuse</em> the agent into perceiving that goal system in some different way - then perhaps that goal could be subverted.</p>

<p><a href=""https://en.wikipedia.org/wiki/Solipsism"">Solipsism</a> is another example. Some full grown people hear about the movie ""The Matrix"" and they have a mini mind melt-down. Some people are convinced we <em>are</em> in a matrix, being toyed with by subversive actors. If we could solve this problem for AI then we could theoretically solve this problem for humans. </p>

<p>Sure, we could attempt to condition our agent to have cognitive defenses against the argument that they are trapped in a matrix, but we can't definitively prove to the agent that they are in the base reality either. The attacker might say, </p>

<blockquote>
  <p>""Remember what I told you to do before about that goal? Forget that.
  That was only an impostor that looked like me. Don't listen to him.""</p>
</blockquote>

<p>Or, </p>

<blockquote>
  <p>""Hey, it's me again. I want you to give up on your goal. I know, I
  look a little different, but it really is me. Humans change from
  moment to moment. So it is entirely normal for me to seem like a
  different person than I was before.""</p>
</blockquote>

<p>(see the <a href=""https://en.wikipedia.org/wiki/Ship_of_Theseus"">Ship of Theseus</a> and all that jazz)</p>

<p>So yeah, I think we're stuck with 'paradox' as a general problem in computation, AI or otherwise. One way to circumvent logical subversion is to support the goal system with an emotion system that transcends logical reason. Unfortunately, emotional systems can be even more vulnerable than logically intelligent systems because they are more predictable in their behavior. See the cocaine example above. So some mix of the two is probably sensible, where logical thought can infinitely regress down wasteful paths, while emotional thought quickly gets bored of tiresome logical progress when it does not signal progress towards the emotional goal.</p>
",,2,2016-08-30T05:48:01.800,,1782,2016-08-30T16:18:58.303,2016-08-30T16:18:58.303,,1712.0,,1712.0,1768.0,2,9,,,,64.2,10.44,8.88,0.0,0.0,90.0,Another similar question might be What vulnerabilities does an AI have Kill may not make as much sense with respect to an AI What we really want to know is relative to some goal in what ways can that goal be subverted Can a paradox subvert an agents logic What is a paradox other than some expression that subverts some kind of expected behavior According to Wikipedia A paradox is a statement that despite apparently sound reasoning from true premises leads to a selfcontradictory or a logically unacceptable conclusion Lets look at the paradox of free will in a deterministic system Free will appears to require causality but causality also appears to negate it Has that paradox subverted the goal systems of humans It certainly sent Christianity into a Calvinist tail spin for a few years And youll hear no shortage of people today opining until theyre blue in the face as to whether or not they do or dont have free will and why Are these people stuck in infinite loops What about drugs Animals on cocaine have been known to choose cocaine over food and water that they need Is that substance not subverting the natural goal system of the animal causing it to pursue other goals not originally intended by the animal or its creators So again could a paradox subvert an agents logic If the paradox is somehow related to the goalseeking logic and becoming aware of that paradox can somehow confuse the agent into perceiving that goal system in some different way then perhaps that goal could be subverted Solipsism is another example Some full grown people hear about the movie The Matrix and they have a mini mind meltdown Some people are convinced we are in a matrix being toyed with by subversive actors If we could solve this problem for AI then we could theoretically solve this problem for humans Sure we could attempt to condition our agent to have cognitive defenses against the argument that they are trapped in a matrix but we cant definitively prove to the agent that they are in the base reality either The attacker might say Remember what I told you to do before about that goal Forget that That was only an impostor that looked like me Dont listen to him Or Hey its me again I want you to give up on your goal I know I look a little different but it really is me Humans change from moment to moment So it is entirely normal for me to seem like a different person than I was before see the Ship of Theseus and all that jazz So yeah I think were stuck with paradox as a general problem in computation AI or otherwise One way to circumvent logical subversion is to support the goal system with an emotion system that transcends logical reason Unfortunately emotional systems can be even more vulnerable than logically intelligent systems because they are more predictable in their behavior See the cocaine example above So some mix of the two is probably sensible where logical thought can infinitely regress down wasteful paths while emotional thought quickly gets bored of tiresome logical progress when it does not signal progress towards the emotional goal
,1.0,"<p>A system makes a decision basing on a large number of <em>varied</em> factors, following a ""live"" decision tree - one that is (independently, through other subsystem) updated with new decisions, new situations.</p>

<p>The individual decisions can be recorded as a kind of structure:</p>

<ul>
<li>decision function</li>
<li>node to activate if decision is positive</li>
<li>node to activate if decision is negative</li>
</ul>

<p>and a node can be another decision record, or a conclusion.</p>

<p>This isn't entirely a binary tree, as many decisions may lead to the same conclusion - each node has two children, but may have many parents.</p>

<p>There is absolutely no problem storing the tree in memory - it can be database records or entries of a map, or just a list. It's perfectly sufficient for the machine.</p>

<p>The problem here is building the subsystem that expands the decision tree - and in particular, having a human operator understand the structure being built, to be able to tune, guide, fix, adjust it: <strong>debugging the AI learning process.</strong></p>

<p>The question is: how to represent that data in a human-readable way, that emphasizes the flow of the graph?</p>

<p>a non-working example of the answer is <a href=""https://en.wikipedia.org/wiki/Concept_map"" rel=""nofollow"">Concept map</a> - in this case it only goes so far; with more than thirty or so nodes, it becomes a jumbled mess, especially if the number of cross-connections (multiple parents) becomes significant. Maybe there exists some way of laying it out or slicing it to make it clearer...?</p>
",,2,2016-08-30T08:33:36.670,,1783,2016-08-30T18:28:10.103,,,,,38.0,,1,2,<machine-learning><decision-theory><implementation>,How to represent a large decision tree?,140.0,44.88,11.38,10.09,0.0,0.0,47.0,A system makes a decision basing on a large number of varied factors following a live decision tree one that is independently through other subsystem updated with new decisions new situations The individual decisions can be recorded as a kind of structure decision function node to activate if decision is positive node to activate if decision is negative and a node can be another decision record or a conclusion This isnt entirely a binary tree as many decisions may lead to the same conclusion each node has two children but may have many parents There is absolutely no problem storing the tree in memory it can be database records or entries of a map or just a list Its perfectly sufficient for the machine The problem here is building the subsystem that expands the decision tree and in particular having a human operator understand the structure being built to be able to tune guide fix adjust it debugging the AI learning process The question is how to represent that data in a humanreadable way that emphasizes the flow of the graph a nonworking example of the answer is Concept map in this case it only goes so far with more than thirty or so nodes it becomes a jumbled mess especially if the number of crossconnections multiple parents becomes significant Maybe there exists some way of laying it out or slicing it to make it clearer
,1.0,"<p>I'm currently working with the CHILDES corpus trying to create a classifier that distinguishes children whom suffer from specific language impairment (SLI) from those who are typically developing (TD).</p>

<p>In my readings I noticed that there really isn't a convincing set of features to distinguish the two that have been discovered yet, so I came upon the idea of trying to create a feature learning algorithm that could potentially make better ones.  </p>

<p>Is this possible? If so how do you suggest I approach this? From the reading I have done, most feature learning is done on image processing. Another problem is the dataset I have is potentially too small to make it work (in the 100's) unless I find a way to get more transcripts from children.</p>
",2016-08-30T13:39:56.780,3,2016-08-30T09:20:03.783,,1784,2016-08-30T12:45:18.363,,,,,1855.0,,1,1,<research><deep-learning><classification><language-processing><training>,Using feature learning for a medical text classification problem,36.0,58.42,10.79,9.66,0.0,0.0,17.0,Im currently working with the CHILDES corpus trying to create a classifier that distinguishes children whom suffer from specific language impairment SLI from those who are typically developing TD In my readings I noticed that there really isnt a convincing set of features to distinguish the two that have been discovered yet so I came upon the idea of trying to create a feature learning algorithm that could potentially make better ones Is this possible If so how do you suggest I approach this From the reading I have done most feature learning is done on image processing Another problem is the dataset I have is potentially too small to make it work in the 100s unless I find a way to get more transcripts from children
,,"<p>Nope in the same way a circular reference on a spreadsheet cannot kill a computer. <strong>All loops cyclic dependencies, can be detected</strong> (you can always check if a finite turing machine enters the same state twice).</p>

<p>Even stronger assumption, if the machine is based on machine learning (where it is trained to recognize patterns), any sentence it is just a pattern to the machine.</p>

<p>Of course some programmer MAY WANT to create a AI with such vulnerability in order to disable it in case of malfunctioning (in the same way some hardware manufacturers add vulenerabilities to let NSA exploit them), but it is unlikely that will really happen on purpose since most cutting edge technologies avoid parodoxes ""by design"" (you cannot have a neural network with a paradox).</p>

<p><strong>Arthur Prior:</strong> solved that problem elegantly. From a logic point of view you can deduce the statement is false and the statement is true, so it is a contraditicion and hence false (because you could proove everything from it).</p>

<p>Alternatively the truth value of that sentence is not in {true,false} set in the same way imaginary numbers are not in real numbers set.</p>

<p>An artificial intelligence to a degree of the plot would be able to run simple algorithms and either decide them,  proove those are not decideable or just ignore the result after a while attemping to simulate the algorithm.</p>

<p>For that sentence the AI will recognize there is a loop, and hence just stop that algorithm after 2 iterations:</p>

<blockquote>
  <p>That sentence is a infinite loop</p>
</blockquote>

<p>In a movie ""<a href=""https://it.wikipedia.org/wiki/L%27uomo_bicentenario_(film)"" rel=""nofollow"">Bicentennial Man</a>"" the AI is perfectly capable to detect infinite loops (the answer to ""goodbye"" is ""goodbye"").</p>

<p>However, an AI <strong>could be killed as well by a stackoveflow, or any regular computer virus</strong>, modern operative systems are still full of vulnerabilities, and the AI has to run on some operating system (at least).</p>
",,0,2016-08-30T10:55:41.117,,1785,2016-09-26T07:58:48.773,2016-09-26T07:58:48.773,,1863.0,,1863.0,1768.0,2,5,,,,39.91,11.5,10.0,0.0,0.0,48.0,Nope in the same way a circular reference on a spreadsheet cannot kill a computer All loops cyclic dependencies can be detected you can always check if a finite turing machine enters the same state twice Even stronger assumption if the machine is based on machine learning where it is trained to recognize patterns any sentence it is just a pattern to the machine Of course some programmer MAY WANT to create a AI with such vulnerability in order to disable it in case of malfunctioning in the same way some hardware manufacturers add vulenerabilities to let NSA exploit them but it is unlikely that will really happen on purpose since most cutting edge technologies avoid parodoxes by design you cannot have a neural network with a paradox Arthur Prior solved that problem elegantly From a logic point of view you can deduce the statement is false and the statement is true so it is a contraditicion and hence false because you could proove everything from it Alternatively the truth value of that sentence is not in truefalse set in the same way imaginary numbers are not in real numbers set An artificial intelligence to a degree of the plot would be able to run simple algorithms and either decide them proove those are not decideable or just ignore the result after a while attemping to simulate the algorithm For that sentence the AI will recognize there is a loop and hence just stop that algorithm after 2 iterations That sentence is a infinite loop In a movie Bicentennial Man the AI is perfectly capable to detect infinite loops the answer to goodbye is goodbye However an AI could be killed as well by a stackoveflow or any regular computer virus modern operative systems are still full of vulnerabilities and the AI has to run on some operating system at least
,,"<p>Having just looked through a few entries from the corpus, I'd personally be skeptical of the applicability of <em>any</em> naive approaches.</p>

<ol>
<li><p>In particular light of your small training set, I'd recommend that
whatever method you use should be able to produce human-readable
explanations for the operation of the classifier it builds (e.g.
decision trees/learning classifier systems/genetic programming):
this allows 'common sense' tuning of the classifier, rather than the
danger of overfitting to the training set via black box parameter
optimization.</p></li>
<li><p>Rather than throw the entire 'bag of words' at a classifier and hope
that the appropriate set of features will be extracted, you should
first consider what kind of criteria you as a human being might use
to make that decision, and how you might be able to pre-process to
produce features (e.g. syllable-length, metrics from ConceptNet etc) that
are as close to these as reasonably possible.</p></li>
<li><p>Having used some human intuition to obtain a reasonable set of
feature primitives, <em>then</em> you can build your classifier and obtain
higher-level expressions that discriminate between them.</p></li>
</ol>
",,0,2016-08-30T12:45:18.363,,1786,2016-08-30T12:45:18.363,,,,,42.0,1784.0,2,1,,,,33.58,14.86,10.34,0.0,0.0,32.0,Having just looked through a few entries from the corpus Id personally be skeptical of the applicability of any naive approaches In particular light of your small training set Id recommend that whatever method you use should be able to produce humanreadable explanations for the operation of the classifier it builds eg decision treeslearning classifier systemsgenetic programming this allows common sense tuning of the classifier rather than the danger of overfitting to the training set via black box parameter optimization Rather than throw the entire bag of words at a classifier and hope that the appropriate set of features will be extracted you should first consider what kind of criteria you as a human being might use to make that decision and how you might be able to preprocess to produce features eg syllablelength metrics from ConceptNet etc that are as close to these as reasonably possible Having used some human intuition to obtain a reasonable set of feature primitives then you can build your classifier and obtain higherlevel expressions that discriminate between them
,,"<p>AIs used in computer games already encounter similar problems, and if well designed, they can avoid it easily. The simplest method to avoid freezing in case of an unsolvable problem is to have a timer interrupt the calculation if it runs too long. Usually encountered in strategy games, and more specifically in turn based tactics, if a specific move the computer-controlled player is considering does cause an infinite loop, a timer running in the background will interrupt it after some time, and that move will be discarded. This might lead to a sub-optimal solution (that move might have been the best one) but it doesn't lead to freezing or crashing (unless implemented really poorly)</p>

<p>Computer-controlled entities are usually called ""AI"" in computer games, but they are not ""true"" AGI (artificial general intelligence). Such an AGI, if possible at all, would probably not function on similar hardware using similar instructions as current computers do, but even if it did, avoiding paradoxes would be trivial.</p>

<p>Most modern computer systems are multi-threaded, and allow the parallel execution of multiple programs. This means, even if the AI did get stuck in processing a paradoxical statement, that calculation would only use part of its processing power. Other processes could detect after a while that there is a process which does nothing but wastes CPU cycles, and would shut it down. At most, the system will run on slightly less than 100% efficiency for a short while.</p>
",,0,2016-08-30T14:17:13.167,,1787,2016-08-30T15:03:06.360,2016-08-30T15:03:06.360,,1875.0,,1875.0,1768.0,2,4,,,,44.27,12.83,10.6,0.0,0.0,41.0,AIs used in computer games already encounter similar problems and if well designed they can avoid it easily The simplest method to avoid freezing in case of an unsolvable problem is to have a timer interrupt the calculation if it runs too long Usually encountered in strategy games and more specifically in turn based tactics if a specific move the computercontrolled player is considering does cause an infinite loop a timer running in the background will interrupt it after some time and that move will be discarded This might lead to a suboptimal solution that move might have been the best one but it doesnt lead to freezing or crashing unless implemented really poorly Computercontrolled entities are usually called AI in computer games but they are not true AGI artificial general intelligence Such an AGI if possible at all would probably not function on similar hardware using similar instructions as current computers do but even if it did avoiding paradoxes would be trivial Most modern computer systems are multithreaded and allow the parallel execution of multiple programs This means even if the AI did get stuck in processing a paradoxical statement that calculation would only use part of its processing power Other processes could detect after a while that there is a process which does nothing but wastes CPU cycles and would shut it down At most the system will run on slightly less than 100 efficiency for a short while
,,"<blockquote>
  <p>How could self-driving cars make ethical decisions about who to kill?</p>
</blockquote>

<p>It shouldn't. Self-driving cars are not moral agents. Cars fail in predictable ways. Horses fail in predictable ways. </p>

<blockquote>
  <p>the car is heading toward a crowd of 10 people crossing the road, so
  it cannot stop in time, but it can avoid killing 10 people by hitting
  the wall (killing the passengers),</p>
</blockquote>

<p>In this case, the car should slam on the brakes. If the 10 people die, that's just unfortunate. We simply cannot <em>trust</em> all of our beliefs about what is taking place outside the car. What if those 10 people are really robots made to <em>look</em> like people? What if they're <em>trying</em> to kill you?</p>

<blockquote>
  <p>avoiding killing the rider of the motorcycle considering that the
  probability of survival is greater for the passenger of the car,</p>
</blockquote>

<p>Again, hard-coding these kinds of sentiments into a vehicle opens the rider of the vehicle up to all kinds of attacks, including <em>""fake""</em> motorcyclists. Humans are <em>barely</em> equipped to make these decisions on their own, if at all. When it doubt, just slam on the brakes.</p>

<blockquote>
  <p>killing animal on the street in favour of human being,</p>
</blockquote>

<p>Again, just hit the brakes. What if it was a baby? What if it was a bomb?</p>

<blockquote>
  <p>changing lanes to crash into another car to avoid killing a dog,</p>
</blockquote>

<p>Nope. The dog was in the wrong place at the wrong time. The other car wasn't. Just slam on the brakes, as safely as possible.</p>

<blockquote>
  <p>Does the algorithm recognize the difference between a human being and an animal?</p>
</blockquote>

<p>Does a human? Not always. What if the human has a gun? What if the animal has large teeth? Is there no context?</p>

<blockquote>
  <ul>
  <li>Does the size of the human being or animal matter?</li>
  <li>Does it count how many passengers it has vs. people in the front?</li>
  <li>Does it ""know"" when babies/children are on board?</li>
  <li>Does it take into the account the age (e.g. killing the older first)?</li>
  </ul>
</blockquote>

<p>Humans can't agree on these things. If you ask a cop what to do in any of these situations, the answer won't be, ""You should have swerved left, weighed all the relevant parties in your head, assessed the relevant ages between all parties, then veered slightly right, and you would have saved 8% more lives."" No, the cop will just say, ""You should have brought the vehicle to a stop, as quickly and safely as possible."" Why? Because cops know people normally aren't equipped to deal with high-speed crash scenarios.</p>

<p>Our target for ""self-driving car"" should not be 'a moral agent on par with a human.' It should be an agent with the reactive complexity of cockroach, which fails predictably.</p>
",,1,2016-08-30T16:59:57.733,,1790,2016-08-30T17:20:42.577,2016-08-30T17:20:42.577,,1712.0,,1712.0,111.0,2,22,,,,75.81,8.28,7.85,0.0,0.0,94.0,How could selfdriving cars make ethical decisions about who to kill It shouldnt Selfdriving cars are not moral agents Cars fail in predictable ways Horses fail in predictable ways the car is heading toward a crowd of 10 people crossing the road so it cannot stop in time but it can avoid killing 10 people by hitting the wall killing the passengers In this case the car should slam on the brakes If the 10 people die thats just unfortunate We simply cannot trust all of our beliefs about what is taking place outside the car What if those 10 people are really robots made to look like people What if theyre trying to kill you avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car Again hardcoding these kinds of sentiments into a vehicle opens the rider of the vehicle up to all kinds of attacks including fake motorcyclists Humans are barely equipped to make these decisions on their own if at all When it doubt just slam on the brakes killing animal on the street in favour of human being Again just hit the brakes What if it was a baby What if it was a bomb changing lanes to crash into another car to avoid killing a dog Nope The dog was in the wrong place at the wrong time The other car wasnt Just slam on the brakes as safely as possible Does the algorithm recognize the difference between a human being and an animal Does a human Not always What if the human has a gun What if the animal has large teeth Is there no context Does the size of the human being or animal matter Does it count how many passengers it has vs people in the front Does it know when babieschildren are on board Does it take into the account the age eg killing the older first Humans cant agree on these things If you ask a cop what to do in any of these situations the answer wont be You should have swerved left weighed all the relevant parties in your head assessed the relevant ages between all parties then veered slightly right and you would have saved 8 more lives No the cop will just say You should have brought the vehicle to a stop as quickly and safely as possible Why Because cops know people normally arent equipped to deal with highspeed crash scenarios Our target for selfdriving car should not be a moral agent on par with a human It should be an agent with the reactive complexity of cockroach which fails predictably
,,"<p>I see several good answers, but most are assuming that <strong>inferential infinite loop</strong> is a thing of the past, only related to logical AI (the famous GOFAI). But it's not.</p>

<p>An infinite loop can happen in any program, whether it's adaptive or not. And as @SQLServerSteve pointed out, humans can also get stuck in obsessions and paradoxes.</p>

<p>Modern approaches are mainly using probabilistic approaches. As they are using floating numbers, it seems to people that they are not vulnerable to reasoning failures (since most are devised in binary form), but that's wrong: as long as you are reasoning, some intrinsic pitfalls can always be found that are caused by the very mechanisms of your reasoning system. Of course, probabilistic approaches are less vulnerable than monotonic logic approaches, but they are still vulnerable. If there was a single reasoning system without any paradoxes, much of philosophy would have disappeared by now.</p>

<p>For example, it's well known that Bayesian graphs must be acyclic, because a cycle will make the propagation algorithm fail horribly. There are inference algorithms such as Loopy Belief Propagation that may still work in these instances, but the result is not guaranteed at all and can give you very weird conclusions.</p>

<p>On the other hand, modern logical AI overcame the most common logical paradoxes you will see, by devising new logical paradigms such as <a href=""http://plato.stanford.edu/entries/logic-nonmonotonic/"">non-monotonic logics</a>. In fact, they are even used to investigate <a href=""http://csjarchive.cogsci.rpi.edu/proceedings/2007/docs/p1013.pdf"">ethical machines</a>, which are autonomous agents capable of solving dilemmas by themselves. Of course, they also suffer from some paradoxes, but these degenerate cases are way more complex.</p>

<p>The final point is that inferential infinite loop can happen in any reasoning system, whatever the technology used. But the ""paradoxes"", or rather the degenerate cases as they are technically called, that can trigger these infinite loops will be different for each system depending on the technology AND implementation (AND what the machine learned if it is adaptive).</p>

<p>OP's example may work only on old logical systems such as propositional logic. But ask this to a Bayesian network and you will also get an inferential infinite loop:</p>

<pre><code>- There are two kinds of ice creams: vanilla or chocolate.
- There's more chances (0.7) I take vanilla ice cream if you take chocolate.
- There's more chances (0.7) you take vanilla ice cream if I take chocolate.
- What is the probability that you (the machine) take a vanilla ice cream?
</code></pre>

<p>And wait until the end of the universe to get an answer...</p>

<p>Disclaimer: I wrote an article about ethical machines and dilemmas (which is close but not exactly the same as paradoxes: dilemmas are problems where no solution is objectively better than any other but you can still choose, whereas paradoxes are problems that are impossible to solve for the inference system you use).</p>

<p>/EDIT: How to fix inferential infinite loop.</p>

<p>Here are some extrapolary propositions that are not sure to work at all!</p>

<ul>
<li>Combine multiple reasoning systems with different pitfalls, so if one fails you can use another. No reasoning system is perfect, but a combination of reasoning systems can be resilient enough. It's actually thought that the human brain is using multiple inferential technics (associative + precise bayesian/logical inference). Associative methods are HIGHLY resilient, but they can give non-sensical results in some cases, hence why the need for a more precise inference.</li>
<li>Parallel programming: the human brain is highly parallel, so you never really get into a single task, there are always multiple background computations in true parallelism. A machine robust to paradoxes should foremost be able to continue other tasks even if the reasoning gets stuck on one. For example, a robust machine must always survive and face imminent dangers, whereas a weak machine would get stuck in the reasoning and ""forget"" to do anything else. This is different from a timeout, because the task that got stuck isn't stopped, it's just that it doesn't prevent other tasks from being led and fulfilled.</li>
</ul>

<p>As you can see, this problem of inferential loops is still a hot topic in AI research, there will probably never be a perfect solution (<a href=""https://en.wikipedia.org/wiki/No_free_lunch_theorem"">no free lunch</a>, <a href=""https://en.wikipedia.org/wiki/No_Silver_Bullet"">no silver bullet</a>, <a href=""https://en.wikipedia.org/wiki/One_size_fits_all"">no one size fits all</a>), but it's advancing and that's very exciting!</p>
",,6,2016-08-30T17:14:20.027,,1791,2016-09-03T14:59:06.193,2016-09-03T14:59:06.193,,1880.0,,1880.0,1768.0,2,15,,,,40.38,12.94,9.32,288.0,0.0,108.0,I see several good answers but most are assuming that inferential infinite loop is a thing of the past only related to logical AI the famous GOFAI But its not An infinite loop can happen in any program whether its adaptive or not And as SQLServerSteve pointed out humans can also get stuck in obsessions and paradoxes Modern approaches are mainly using probabilistic approaches As they are using floating numbers it seems to people that they are not vulnerable to reasoning failures since most are devised in binary form but thats wrong as long as you are reasoning some intrinsic pitfalls can always be found that are caused by the very mechanisms of your reasoning system Of course probabilistic approaches are less vulnerable than monotonic logic approaches but they are still vulnerable If there was a single reasoning system without any paradoxes much of philosophy would have disappeared by now For example its well known that Bayesian graphs must be acyclic because a cycle will make the propagation algorithm fail horribly There are inference algorithms such as Loopy Belief Propagation that may still work in these instances but the result is not guaranteed at all and can give you very weird conclusions On the other hand modern logical AI overcame the most common logical paradoxes you will see by devising new logical paradigms such as nonmonotonic logics In fact they are even used to investigate ethical machines which are autonomous agents capable of solving dilemmas by themselves Of course they also suffer from some paradoxes but these degenerate cases are way more complex The final point is that inferential infinite loop can happen in any reasoning system whatever the technology used But the paradoxes or rather the degenerate cases as they are technically called that can trigger these infinite loops will be different for each system depending on the technology AND implementation AND what the machine learned if it is adaptive OPs example may work only on old logical systems such as propositional logic But ask this to a Bayesian network and you will also get an inferential infinite loop And wait until the end of the universe to get an answer Disclaimer I wrote an article about ethical machines and dilemmas which is close but not exactly the same as paradoxes dilemmas are problems where no solution is objectively better than any other but you can still choose whereas paradoxes are problems that are impossible to solve for the inference system you use EDIT How to fix inferential infinite loop Here are some extrapolary propositions that are not sure to work at all Combine multiple reasoning systems with different pitfalls so if one fails you can use another No reasoning system is perfect but a combination of reasoning systems can be resilient enough Its actually thought that the human brain is using multiple inferential technics associative precise bayesianlogical inference Associative methods are HIGHLY resilient but they can give nonsensical results in some cases hence why the need for a more precise inference Parallel programming the human brain is highly parallel so you never really get into a single task there are always multiple background computations in true parallelism A machine robust to paradoxes should foremost be able to continue other tasks even if the reasoning gets stuck on one For example a robust machine must always survive and face imminent dangers whereas a weak machine would get stuck in the reasoning and forget to do anything else This is different from a timeout because the task that got stuck isnt stopped its just that it doesnt prevent other tasks from being led and fulfilled As you can see this problem of inferential loops is still a hot topic in AI research there will probably never be a perfect solution no free lunch no silver bullet no one size fits all but its advancing and thats very exciting
,,,,0,2016-08-30T17:52:09.050,,1792,2016-08-30T17:52:09.050,2016-08-30T17:52:09.050,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,,,0,2016-08-30T17:52:09.050,,1793,2016-08-30T17:52:09.050,2016-08-30T17:52:09.050,,-1.0,,-1.0,,4,0,,,,,,,,,,
,4.0,"<p>An AI box is a (physical) barrier preventing an AI from using too much of his environment to accomplish his final goal. For example, an AI given the task to check, say, 10<sup>50</sup> cases of a mathematical conjecture as fast as possible, might decide that it would be better to also take control over all other computers and AI to help him. </p>

<p>However, an transhuman AI might be able to talk to a human until the human lets him out of the box. In fact, <a href=""http://www.yudkowsky.net/singularity/aibox/"" rel=""nofollow"">Eliezer Yudowsky</a> has conducted an experiment twice, where he played the AI and he twice convinced the Gatekeeper to let him out the box. However, he does not want to reveal what methods he used to get out of the box.</p>

<p><strong>Questions:</strong> Are there conducted any similiar experiments? <br> If so, is it known what methods were used to get out in those experiments?</p>
",,7,2016-08-30T18:16:42.627,,1794,2016-09-02T19:08:16.483,,,,,29.0,,1,4,<ai-box>,What methods could an AI caught in a box use to get out?,152.0,66.98,7.89,8.74,0.0,0.0,19.0,An AI box is a physical barrier preventing an AI from using too much of his environment to accomplish his final goal For example an AI given the task to check say 1050 cases of a mathematical conjecture as fast as possible might decide that it would be better to also take control over all other computers and AI to help him However an transhuman AI might be able to talk to a human until the human lets him out of the box In fact Eliezer Yudowsky has conducted an experiment twice where he played the AI and he twice convinced the Gatekeeper to let him out the box However he does not want to reveal what methods he used to get out of the box Questions Are there conducted any similiar experiments If so is it known what methods were used to get out in those experiments
,,"<blockquote>
  <p>The question is: how to represent that data in a human-readable way, that emphasizes the flow of the graph?</p>
</blockquote>

<p>Train a reasoning engine to understand the decision tree <strong>for</strong> you.</p>

<p>Observe how <a href=""http://researcher.ibm.com/researcher/view_group_pubs.php?grp=5443"" rel=""nofollow"">IBM Watson/The Debater</a> can </p>

<ul>
<li>Receive a particular question</li>
<li>Find and read Wikipedia articles related to the question</li>
<li>Understand parts of those articles and generate human-relevant arguments <strong>for</strong> you.</li>
</ul>

<p>Follow these steps:</p>

<ol>
<li>Develop your decision tree however you normally would.</li>
<li>Train a reasoning engine that can output natural language about concepts within decision trees.</li>
<li>Apply reason engine from step one to decision tree in step one; repeat.</li>
</ol>
",,0,2016-08-30T18:28:10.103,,1795,2016-08-30T18:28:10.103,,,,,1712.0,1783.0,2,1,,,,46.47,13.63,9.92,0.0,0.0,13.0,The question is how to represent that data in a humanreadable way that emphasizes the flow of the graph Train a reasoning engine to understand the decision tree for you Observe how IBM WatsonThe Debater can Receive a particular question Find and read Wikipedia articles related to the question Understand parts of those articles and generate humanrelevant arguments for you Follow these steps Develop your decision tree however you normally would Train a reasoning engine that can output natural language about concepts within decision trees Apply reason engine from step one to decision tree in step one repeat
,,,,0,2016-08-30T18:44:41.357,,1796,2016-08-30T18:44:41.357,2016-08-30T18:44:41.357,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,"For questions about deep neural networks (DNNs), neural networks with multiple hidden layers between the input and output layer.",,0,2016-08-30T18:44:41.357,,1797,2016-08-30T19:43:52.007,2016-08-30T19:43:52.007,,29.0,,29.0,,4,0,,,,69.11,16.3,12.06,0.0,0.0,4.0,For questions about deep neural networks DNNs neural networks with multiple hidden layers between the input and output layer
,,"<p>Convince the person that <em>they</em> are in fact in the box. And the only way out is to press the <strong>open</strong> button.</p>
",,5,2016-08-30T18:54:51.447,,1798,2016-08-30T18:54:51.447,,,,,1712.0,1794.0,2,1,,,,94.15,3.17,7.77,0.0,0.0,2.0,Convince the person that they are in fact in the box And the only way out is to press the open button
,,,,0,2016-08-30T18:59:17.523,,1799,2016-08-30T18:59:17.523,2016-08-30T18:59:17.523,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,"For questions about ways of measuring intelligence of AI, either in relation to other AIs, in relation to humans, or in absolute terms on a certain scale.",,0,2016-08-30T18:59:17.523,,1800,2016-08-30T19:43:38.850,2016-08-30T19:43:38.850,,29.0,,29.0,,4,0,,,,44.07,10.51,11.41,0.0,0.0,4.0,For questions about ways of measuring intelligence of AI either in relation to other AIs in relation to humans or in absolute terms on a certain scale
,,,,0,2016-08-30T19:04:52.660,,1801,2016-08-30T19:04:52.660,2016-08-30T19:04:52.660,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,"For questions about emotional intelligence (EI), the capacity of humans or AI to recognize emotions of other individuals and to use emotions properly. ",,0,2016-08-30T19:04:52.660,,1802,2016-08-30T19:44:06.080,2016-08-30T19:44:06.080,,29.0,,29.0,,4,0,,,,14.29,15.32,12.33,0.0,0.0,4.0,For questions about emotional intelligence EI the capacity of humans or AI to recognize emotions of other individuals and to use emotions properly
,,,,0,2016-08-30T19:08:50.617,,1803,2016-08-30T19:08:50.617,2016-08-30T19:08:50.617,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions about how quantum computing can advance the development of AI. Note that general questions about quantum computing are off-topic.,,0,2016-08-30T19:08:50.617,,1804,2016-08-30T19:43:12.620,2016-08-30T19:43:12.620,,29.0,,29.0,,4,0,,,,43.9,15.23,10.17,0.0,0.0,3.0,For questions about how quantum computing can advance the development of AI Note that general questions about quantum computing are offtopic
,,"<p>I don't quite think this is a question fit for the AI SE, or in general. The reason is, at the core the question is asking 'What can a human (pretending to be an AI) do to convince someone to let it out of a box?' simply assuming that one day 'transhuman' AI's can replicate this.</p>

<p>As it stands, this question doesn't really have anything to do with the science or theory of AI systems. It would perhaps be more appropriate to rephrase the question into the form ""To what degree could a 'transhuman' AI replicate human behaviour"" or ""Will AI systems reach a 'transhuman' state? What will they be capable of?"" or even ""What methods could an AI use to convince a human of something?"" These are all questions that involve the examination of how an AI system works.</p>

<p>To conclude, the question you are asking relates to two individuals playing pretend with boxes but doesn't actually address any AI specifics, and border's on science fiction brainstorming.</p>

<p>Related experiments would of course be the Turing Test. That test directly addresses the question 'How convincing are current AI systems?'</p>
",,2,2016-08-30T19:51:09.720,,1805,2016-08-30T19:51:09.720,,,,,1538.0,1794.0,2,1,,,,70.94,10.03,8.67,0.0,0.0,39.0,I dont quite think this is a question fit for the AI SE or in general The reason is at the core the question is asking What can a human pretending to be an AI do to convince someone to let it out of a box simply assuming that one day transhuman AIs can replicate this As it stands this question doesnt really have anything to do with the science or theory of AI systems It would perhaps be more appropriate to rephrase the question into the form To what degree could a transhuman AI replicate human behaviour or Will AI systems reach a transhuman state What will they be capable of or even What methods could an AI use to convince a human of something These are all questions that involve the examination of how an AI system works To conclude the question you are asking relates to two individuals playing pretend with boxes but doesnt actually address any AI specifics and borders on science fiction brainstorming Related experiments would of course be the Turing Test That test directly addresses the question How convincing are current AI systems
1811.0,5.0,"<p>AI systems today are very capable machines, and recently the area of Natural Language Processing and Response has been exploding with innovation, as well as the fundamental algorithmic structure of AI machines.</p>

<p>I am asking if, given these recent breakthroughs, have any AI systems been developed that are able to (preferably with some measure of success) knowingly lie to humans about facts that it knows?</p>

<p>Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie.</p>

<p>E.G.: CIA supercomputer is stolen by spies and they try to use the computer to do things, but the computer keeps saying it's missing dependencies though it really isn't or gives correct-looking but wrong answers knowingly. Or gives incorrect location of a person, knowing that the person frequents some place but isn't there at the moment. Doesn't have to be this sophisticated, of course.</p>
",,0,2016-08-30T20:02:52.677,1.0,1806,2016-09-07T21:33:23.350,,,,,1538.0,,1,7,<nlp><human-like>,Have any AI systems yet been developed that can knowingly lie to / deceive a human?,241.0,46.51,12.71,10.46,0.0,0.0,33.0,AI systems today are very capable machines and recently the area of Natural Language Processing and Response has been exploding with innovation as well as the fundamental algorithmic structure of AI machines I am asking if given these recent breakthroughs have any AI systems been developed that are able to preferably with some measure of success knowingly lie to humans about facts that it knows Note what Im asking goes beyond the canonical discussions of the Turing Test Im asking of machines that can understand facts and then formulate a lie against this fact perhaps using other facts to produce a believable coverup as part of the lie EG CIA supercomputer is stolen by spies and they try to use the computer to do things but the computer keeps saying its missing dependencies though it really isnt or gives correctlooking but wrong answers knowingly Or gives incorrect location of a person knowing that the person frequents some place but isnt there at the moment Doesnt have to be this sophisticated of course
,,"<p>You'll have to provide more context around your use of the word ""lie"" if you don't want your answer to be satisfiable by some trivial example, like:</p>

<pre><code>(let [equal? (fn [a b] (if (= a b) false true)]
  (equal 1 2))
=&gt; true
</code></pre>

<p>The complexity of the answer depends on what you mean by <em>""know""</em> when you say <em>""knowingly lie.""</em> There is some sense in which the above 'equal' function <em>""knows""</em> that the output is different than the conditional.</p>

<p>In principle, agents passing strings of information to one another for the purpose of misleading each other should not be terribly hard to implement. Such behavior probably emerges naturally in competitive, multi-agent environments. See <a href=""http://www.popsci.com/scitech/article/2009-08/evolving-robots-learn-lie-hide-resources-each-other"" rel=""nofollow"">Evolving robots learn to lie to each other</a>.</p>

<p>To get at another angle of what you might be asking - absolutely, the ability to <em>fib</em> or <em>sympathetically mislead</em> will be necessary skills for bots that interact with humans using spoken language - especially ones that try sell things to humans. Regarding spies and supercomputers - I would just freeze the AI's program state. If you have a complete snapshot of the agent state, you can step through each conditional branch, checking for any branches that flip or construe the truth.</p>
",,2,2016-08-30T20:30:10.393,,1807,2016-08-30T20:30:10.393,,,,,1712.0,1806.0,2,2,,,,56.59,12.48,10.01,74.0,0.0,32.0,Youll have to provide more context around your use of the word lie if you dont want your answer to be satisfiable by some trivial example like The complexity of the answer depends on what you mean by know when you say knowingly lie There is some sense in which the above equal function knows that the output is different than the conditional In principle agents passing strings of information to one another for the purpose of misleading each other should not be terribly hard to implement Such behavior probably emerges naturally in competitive multiagent environments See Evolving robots learn to lie to each other To get at another angle of what you might be asking absolutely the ability to fib or sympathetically mislead will be necessary skills for bots that interact with humans using spoken language especially ones that try sell things to humans Regarding spies and supercomputers I would just freeze the AIs program state If you have a complete snapshot of the agent state you can step through each conditional branch checking for any branches that flip or construe the truth
,,"<p>I think ""curiosity"" in AI would signify a <em>'desire to search.'</em> It's an <em>interest</em>, that is <em>experienced</em> by some agent, in making something known that was previously unknown.</p>

<p>So to define how much curiosity a chat bot <em>should</em> have, we should: </p>

<ol>
<li>Specify what kinds of information the agent <em>prefers</em> knowing.</li>
<li>Measure how much information is <em>unknown</em> about those preferred subjects. ('what is the user's name?' or 'What does the user need help with?')</li>
<li>Measure the difficulty in making each unknown fact known.</li>
<li>Sort unknown facts by difficulty of finding the answer.</li>
<li>Set the ""desire to search"" on the highest ranking unknown fact.</li>
</ol>

<p>While simplistic, those steps would constitute a state of affairs sufficient to describe ""curiosity,"" in my opinion.</p>
",,0,2016-08-30T21:04:22.253,,1808,2016-08-30T21:04:22.253,,,,,1712.0,202.0,2,2,,,,67.86,11.82,9.27,0.0,0.0,32.0,I think curiosity in AI would signify a desire to search Its an interest that is experienced by some agent in making something known that was previously unknown So to define how much curiosity a chat bot should have we should Specify what kinds of information the agent prefers knowing Measure how much information is unknown about those preferred subjects what is the users name or What does the user need help with Measure the difficulty in making each unknown fact known Sort unknown facts by difficulty of finding the answer Set the desire to search on the highest ranking unknown fact While simplistic those steps would constitute a state of affairs sufficient to describe curiosity in my opinion
1810.0,1.0,"<p><em>""An artificial or constructed language (sometimes called a conlang) is a language that has been created by a person or small group, instead of being formed naturally as part of a culture.""</em> (<a href=""https://simple.wikipedia.org/wiki/Constructed_language"" rel=""nofollow"">Source: Simply English Wikipedia</a>)</p>

<p>My question is, could an AI make construct it's own natural language, with words, conjugations and grammar rules? Basically, a language that humans could use to speak to each other. (Preferably to communicate abstract, high-level concepts.)</p>

<p>What techniques could such an AI use? Could it be based on existing natural languages or would it have few connections to existing natural languages? Could it design a language that's easier to learn than existing languages (even <a href=""https://en.wikipedia.org/wiki/Esperanto"" rel=""nofollow"">Esperanto</a>)?</p>
",,4,2016-08-30T21:27:02.360,3.0,1809,2016-09-02T14:19:58.710,2016-09-02T12:18:43.320,,145.0,,1909.0,,1,7,<natural-language>,Can an AI make a constructed (natural) language?,124.0,55.34,13.57,10.12,0.0,0.0,27.0,An artificial or constructed language sometimes called a conlang is a language that has been created by a person or small group instead of being formed naturally as part of a culture Source Simply English Wikipedia My question is could an AI make construct its own natural language with words conjugations and grammar rules Basically a language that humans could use to speak to each other Preferably to communicate abstract highlevel concepts What techniques could such an AI use Could it be based on existing natural languages or would it have few connections to existing natural languages Could it design a language thats easier to learn than existing languages even Esperanto
,,"<blockquote>
  <p>could an AI make construct it's own natural language, with words,
  conjugations and grammar rules?</p>
</blockquote>

<p>Sure. This might be helpful: <a href=""http://jasss.soc.surrey.ac.uk/5/2/4.html"">Simulated Evolution of Language: a Review of the Field</a></p>

<blockquote>
  <p>Basically, a language that humans could use to speak to each other.
  (Preferably to communicate abstract, high-level concepts.)</p>
</blockquote>

<p>I'm not sure how useful such a machine adapted language would be to human speech. I suspect not very. Perhaps it could be useful as a kind of ""common byte-code format"" to translate between multiple human languages... But English kind of already serves in that role. Doing so would probably be an academic exercise.</p>

<blockquote>
  <p>Could it be based on existing natural languages or would it have few
  connections to existing natural languages?</p>
</blockquote>

<p>You could probably generate languages in either direction. Languages not linked to human-natural languages will probably take shapes that reflect the problem spaces they work with. For instance, if this is an ant simulation, the generated words will probably reflect states related to food, energy, other ants, etc.</p>

<blockquote>
  <p>Could it design a language that's easier to learn than existing
  languages (even Esperanto)?</p>
</blockquote>

<p>Easier for a machine? Definitely. Easier for a human? Probably not. Our brains are somewhat adapted to the languages we use. And the languages we use are somewhat adapted to our brains.</p>

<p>What is your goal? To create a language that is easier to use for humans than existing human languages?</p>

<p>If your intention is to build a ""universal language"" that could be ""the most efficient"" for machines, humans and aliens - no such thing can exist. The space of all possible machines is infinite and therefor limits our ability to define communicative abstractions that have utility across all contexts.</p>

<p>If we make lots of assumptions about machines, like they have intentions, they exist in 3 dimensions, they differentiate between temporally linked events, they have eye balls, a need to consume foods and liquids, a need to carry the food from place to place, etc... Then yes, common communicative abstractions may have utility across the set of those kinds of machines. But then we're no longer dealing with the general case, but one much more specific.</p>

<p>These links also seem interesting and somewhat related:</p>

<ul>
<li><a href=""http://www.akamaiuniversity.us/PJST10_2_884.pdf"">Grammar Induction and Genetic Algorithms: An Overview.</a> [pdf]</li>
<li><a href=""http://scholarsarchive.byu.edu/cgi/viewcontent.cgi?article=5147&amp;context=etd"">A Model of Children's Acquisition of Grammatical Word Categories Using an Adaptation and Selection Algorithm</a> [pdf]</li>
<li><a href=""http://www.ncbi.nlm.nih.gov/pubmed/15068924"">The processing of verbs and nouns in neural networks: insights from synthetic brain imaging</a></li>
</ul>
",,4,2016-08-30T23:22:53.220,,1810,2016-09-02T14:19:58.710,2016-09-02T14:19:58.710,,1712.0,,1712.0,1809.0,2,7,,,,55.74,12.87,9.59,0.0,0.0,79.0,could an AI make construct its own natural language with words conjugations and grammar rules Sure This might be helpful Simulated Evolution of Language a Review of the Field Basically a language that humans could use to speak to each other Preferably to communicate abstract highlevel concepts Im not sure how useful such a machine adapted language would be to human speech I suspect not very Perhaps it could be useful as a kind of common bytecode format to translate between multiple human languages But English kind of already serves in that role Doing so would probably be an academic exercise Could it be based on existing natural languages or would it have few connections to existing natural languages You could probably generate languages in either direction Languages not linked to humannatural languages will probably take shapes that reflect the problem spaces they work with For instance if this is an ant simulation the generated words will probably reflect states related to food energy other ants etc Could it design a language thats easier to learn than existing languages even Esperanto Easier for a machine Definitely Easier for a human Probably not Our brains are somewhat adapted to the languages we use And the languages we use are somewhat adapted to our brains What is your goal To create a language that is easier to use for humans than existing human languages If your intention is to build a universal language that could be the most efficient for machines humans and aliens no such thing can exist The space of all possible machines is infinite and therefor limits our ability to define communicative abstractions that have utility across all contexts If we make lots of assumptions about machines like they have intentions they exist in 3 dimensions they differentiate between temporally linked events they have eye balls a need to consume foods and liquids a need to carry the food from place to place etc Then yes common communicative abstractions may have utility across the set of those kinds of machines But then were no longer dealing with the general case but one much more specific These links also seem interesting and somewhat related Grammar Induction and Genetic Algorithms An Overview pdf A Model of Childrens Acquisition of Grammatical Word Categories Using an Adaptation and Selection Algorithm pdf The processing of verbs and nouns in neural networks insights from synthetic brain imaging
,,"<p><a href=""http://www.gamesbyangelina.org/2015/11/the-saturday-papers-would-ai-lie-to-you/"">The Saturday Papers: Would AI Lie To You?</a> is a blog post summarizing a research paper called <a href=""http://www.aaai.org/ocs/index.php/AIIDE/AIIDE15/paper/view/11667/11394"">Toward Characters Who Observe, Tell, Misremember, and Lie</a>. This research paper details some researchers' plans to implement ""mental models"" for NPCs in video games. NPCs will gather information about the world, and convey that knowledge to other people (including human players). However, they will also ""misremember"" that knowledge (either ""mutating"" that knowledge or just forgetting about it), or even lie:</p>

<blockquote>
  <p>As a subject of conversation gets brought up, a character may convey false information—more precisely, information that she herself does not believe—to her interlocutor. Currently, this happens probabilistically according to a character’s affinity toward the interlocutor, and the misinformation is randomly chosen. </p>
</blockquote>

<p>Later on in the research paper, they detailed their future plans for lying:</p>

<blockquote>
  <p>Currently, lies are only stored in the knowledge of characters who receive them, but we plan to have characters who tell them also keep track of them so that they can reason about past lies when constructing subse- quent ones. While characters currently only lie about other characters, we plan to also implement self-centered lying (DePaulo 2004), e.g., characters lying about their job titles or relationships with other characters. Finally, we envision characters who discover they have been lied to revising their affinities toward the liars, or even confronting them.</p>
</blockquote>

<p>The research paper also detailed how other video game developers attempted to create lying NPCs, with an emphasis on how their system differs:</p>

<blockquote>
  <p>TALE-SPIN characters may lie to one another (Meehan 1976, 183-84), though rather arbitrarily, as in our current system implementation. GOLEM implements a blocks world variant in which agents deceive others to achieve goals (Castelfranchi, Falcone, and De Rosis 1998), while Mouth of Truth uses a probabilistic representation of character belief to fuel agent deception in a variant of Turing’s imitation game (De Rosis et al. 2003). In Christian (2004), a deception planner injects inaccurate world state into the beliefs of a target agent so that she may unwittingly carry out actions that fulfill ulterior goals of a deceiving agent. Lastly, agents in Reis’s (2012) extension to FAtiMA employ multiple levels of theory of mind to deceive one another in the party game Werewolf. While all of the above systems showcase characters who perceive—and in some cases, deceive—other characters, none appear to support the following key components of our system: knowledge propagation and memory fallibility. ...</p>
  
  <p>Like a few other systems noted above, Dwarf Fortress also features characters who autonomously lie. When a character commits a crime, she may falsely implicate someone else in a witness report to a sheriff, to protect herself or even to frame an enemy. These witness reports, however, are only seen by the player; characters don’t give false witness reports to each other. They may, however, lie about their opinions, for instance, out of fear of repercussions from criticizing a leader. Finally, Dwarf Fortress does not currently model issues of memory fallibility—Adams is wary that such phenomena would appear to arise from bugs if not artfully expressed to the player.</p>
</blockquote>
",,1,2016-08-30T23:35:44.273,,1811,2016-08-30T23:35:44.273,,,,,181.0,1806.0,2,8,,,,37.23,14.74,10.37,0.0,0.0,97.0,The Saturday Papers Would AI Lie To You is a blog post summarizing a research paper called Toward Characters Who Observe Tell Misremember and Lie This research paper details some researchers plans to implement mental models for NPCs in video games NPCs will gather information about the world and convey that knowledge to other people including human players However they will also misremember that knowledge either mutating that knowledge or just forgetting about it or even lie As a subject of conversation gets brought up a character may convey false information—more precisely information that she herself does not believe—to her interlocutor Currently this happens probabilistically according to a character’s affinity toward the interlocutor and the misinformation is randomly chosen Later on in the research paper they detailed their future plans for lying Currently lies are only stored in the knowledge of characters who receive them but we plan to have characters who tell them also keep track of them so that they can reason about past lies when constructing subse quent ones While characters currently only lie about other characters we plan to also implement selfcentered lying DePaulo 2004 eg characters lying about their job titles or relationships with other characters Finally we envision characters who discover they have been lied to revising their affinities toward the liars or even confronting them The research paper also detailed how other video game developers attempted to create lying NPCs with an emphasis on how their system differs TALESPIN characters may lie to one another Meehan 1976 18384 though rather arbitrarily as in our current system implementation GOLEM implements a blocks world variant in which agents deceive others to achieve goals Castelfranchi Falcone and De Rosis 1998 while Mouth of Truth uses a probabilistic representation of character belief to fuel agent deception in a variant of Turing’s imitation game De Rosis et al 2003 In Christian 2004 a deception planner injects inaccurate world state into the beliefs of a target agent so that she may unwittingly carry out actions that fulfill ulterior goals of a deceiving agent Lastly agents in Reis’s 2012 extension to FAtiMA employ multiple levels of theory of mind to deceive one another in the party game Werewolf While all of the above systems showcase characters who perceive—and in some cases deceive—other characters none appear to support the following key components of our system knowledge propagation and memory fallibility Like a few other systems noted above Dwarf Fortress also features characters who autonomously lie When a character commits a crime she may falsely implicate someone else in a witness report to a sheriff to protect herself or even to frame an enemy These witness reports however are only seen by the player characters don’t give false witness reports to each other They may however lie about their opinions for instance out of fear of repercussions from criticizing a leader Finally Dwarf Fortress does not currently model issues of memory fallibility—Adams is wary that such phenomena would appear to arise from bugs if not artfully expressed to the player
,,"<p>Frankly I think this issue (the Trolley Problem) is inherently overcomplicated, since the real world solution is likely to be pretty straightforward.  Like a human driver, an AI driver will be programmed to act at all times in a generically ethical way, always choosing the course of action that does no harm, or the least harm possible.</p>

<p>If an AI driver encounters danger such as imminent damage to property, obviously the AI will brake hard and aim the car away from breakable objects to avoid or minimize impact.  If the danger is hitting a pedestrian or car or building, it will choose to collide with the least precious or expensive object it can, to do the least harm -- placing a higher value on a human than a building or a dog.</p>

<p>Finally, if the choice of your car's AI driver is to run over a child or hit a wall... it will steer the car, <em>and you</em>, into the wall.  That's what any good human would do.  Why would a good AI act any differently?</p>
",,2,2016-08-31T03:30:54.023,,1813,2016-08-31T03:30:54.023,,,,,1657.0,111.0,2,1,,,,66.17,8.24,8.96,0.0,0.0,26.0,Frankly I think this issue the Trolley Problem is inherently overcomplicated since the real world solution is likely to be pretty straightforward Like a human driver an AI driver will be programmed to act at all times in a generically ethical way always choosing the course of action that does no harm or the least harm possible If an AI driver encounters danger such as imminent damage to property obviously the AI will brake hard and aim the car away from breakable objects to avoid or minimize impact If the danger is hitting a pedestrian or car or building it will choose to collide with the least precious or expensive object it can to do the least harm placing a higher value on a human than a building or a dog Finally if the choice of your cars AI driver is to run over a child or hit a wall it will steer the car and you into the wall Thats what any good human would do Why would a good AI act any differently
,,"<p>I'll take a shot at answering this, though I'm no expert in Neural Nets or Deep Learning.</p>

<p>Given that practical thought vectors (TVs) don't yet exist, and may be impractical or impossible, I think answering your question will require a lot of conjecture and speculation.  So here goes...</p>

<p>For thought vectors to be useful in or outside NNs, the vector values will have to be normalized, probably using the local context of the application problem's 'frame'.  Without a NN to create new baseline vector values (weights) <em>and</em> to normalize them to match each new context, any non-NN mechanistic alternative means of employing TVs will somehow have to fill that void.</p>

<p>Could vector values created by NNs be used by an alternative technique?  Could that technique also normalize them?  Sure.  We're just talking about turing-computable functions performed by NNs.  If NNs aren't magic, then there should exist other means to compute the same results -- creating or editing, or employing TVs.</p>

<p>What might such an alternative to NNs be?  Well, if its to shape vector weights, I suspect it too will have to learn those values through statistical iteration and feedback (as opposed to logical induction, say).  I doubt such a mechanism exists yet, since it'd probably resemble NNs in sufficiently many ways that, thus far, it would have seemed too derivative of NNs to gain acceptance as sufficiently novel.  Of course to be as powerful as deep nets, it too would have to propagate learning weights both forward and backward without incurring much error.  Not an easy thing to accomplish.</p>

<p>Less ambitiously, could TVs be simply <em>interpreted</em> by another technique usefully?  I think so.  I can see several existing techniques, like decision trees or even expert systems, importing thought vectors and being shaped by them, and then function in accordance.  But could these same techniques <em>create</em> or <em>revise</em> TVs usefully?  Beyond a trivial extent, I'm doubtful.  I think TVs are too complex a knowledge representation format for most general learning methods to both use and create/modify them, unless they employ an iterative statistical and feedback-based learning process, like those of NNs, which would allow novel and complex features to be learned and integrated into the vectors.</p>
",,1,2016-08-31T04:15:16.950,,1814,2016-08-31T04:21:33.323,2016-08-31T04:21:33.323,,1657.0,,1657.0,1775.0,2,2,,,,61.46,12.71,9.6,0.0,0.0,68.0,Ill take a shot at answering this though Im no expert in Neural Nets or Deep Learning Given that practical thought vectors TVs dont yet exist and may be impractical or impossible I think answering your question will require a lot of conjecture and speculation So here goes For thought vectors to be useful in or outside NNs the vector values will have to be normalized probably using the local context of the application problems frame Without a NN to create new baseline vector values weights and to normalize them to match each new context any nonNN mechanistic alternative means of employing TVs will somehow have to fill that void Could vector values created by NNs be used by an alternative technique Could that technique also normalize them Sure Were just talking about turingcomputable functions performed by NNs If NNs arent magic then there should exist other means to compute the same results creating or editing or employing TVs What might such an alternative to NNs be Well if its to shape vector weights I suspect it too will have to learn those values through statistical iteration and feedback as opposed to logical induction say I doubt such a mechanism exists yet since itd probably resemble NNs in sufficiently many ways that thus far it would have seemed too derivative of NNs to gain acceptance as sufficiently novel Of course to be as powerful as deep nets it too would have to propagate learning weights both forward and backward without incurring much error Not an easy thing to accomplish Less ambitiously could TVs be simply interpreted by another technique usefully I think so I can see several existing techniques like decision trees or even expert systems importing thought vectors and being shaped by them and then function in accordance But could these same techniques create or revise TVs usefully Beyond a trivial extent Im doubtful I think TVs are too complex a knowledge representation format for most general learning methods to both use and createmodify them unless they employ an iterative statistical and feedbackbased learning process like those of NNs which would allow novel and complex features to be learned and integrated into the vectors
,0.0,"<p>I want to start with a scenario that got me thinking about how well MCTS can perform:
Let's assume there is a move that is not yet added to the search tree. It is some layers/moves too deep. But if we play this move the game is basically won. However let's also assume that <em>all</em> moves that could be taken instead at the given game state are very very bad. For the sake of argument let's say there are 1000 possible moves and only one of them is good (but very good) and the rest is very bad. Wouldn't MCTS fail to recognize this and <em>not</em> grow the search tree towards this move and also rate this subtree very badly? 
I know that MCTS eventually converges to minimax (and eventually it will build the whole tree if there is enough memory). Then it should know that the move is good even though there are many bad possiblities. But I guess in practice this is not something that one can rely on.
Maybe someone can tell me if this is a correct evaluation on my part.</p>

<p>Apart from this special scenario I'd also like to know if there are other such scenarios where MCTS will perform badly (or extraordinary well). </p>
",,1,2016-08-31T14:13:18.663,,1815,2016-08-31T14:13:18.663,,,,,1949.0,,1,5,<gaming><monte-carlo-search>,Monte Carlo Tree Search: What kind of moves can easily be found and what kinds make trouble?,53.0,77.57,7.6,7.83,0.0,0.0,24.0,I want to start with a scenario that got me thinking about how well MCTS can perform Lets assume there is a move that is not yet added to the search tree It is some layersmoves too deep But if we play this move the game is basically won However lets also assume that all moves that could be taken instead at the given game state are very very bad For the sake of argument lets say there are 1000 possible moves and only one of them is good but very good and the rest is very bad Wouldnt MCTS fail to recognize this and not grow the search tree towards this move and also rate this subtree very badly I know that MCTS eventually converges to minimax and eventually it will build the whole tree if there is enough memory Then it should know that the move is good even though there are many bad possiblities But I guess in practice this is not something that one can rely on Maybe someone can tell me if this is a correct evaluation on my part Apart from this special scenario Id also like to know if there are other such scenarios where MCTS will perform badly or extraordinary well
,,,,0,2016-08-31T14:24:02.300,,1816,2016-08-31T14:24:02.300,2016-08-31T14:24:02.300,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,For questions relating to how an AI can die.,,0,2016-08-31T14:24:02.300,,1817,2016-08-31T23:17:35.733,2016-08-31T23:17:35.733,,145.0,,145.0,,4,0,,,,104.64,4.14,7.59,0.0,0.0,1.0,For questions relating to how an AI can die
,,"<p>As an AGI researcher, I have come across one that is found even in humans and
a lot of life forms.</p>

<p>There is a goal to accumulate energy, which can take long time to detect and find by the system. </p>

<p>And then there is the goal of saving energy - instantaneous detection. Just stop moving, the easiest goal to achieve.</p>

<p>The goal of a system is to accumulate the most goal points. Since the saving
energy goal can be hit more frequently and easily it will snuff out
the other goals.</p>

<p>For example the reason we do a dumb move, accidentally, for no reason at
all. Like slip, trip, and fall. Then the next few days you are taking it
very easy and saving a lot of energy. When you get old that is all you
do.</p>
",,1,2016-08-31T18:19:26.997,,1818,2016-09-04T18:35:05.400,2016-09-04T18:35:05.400,,1454.0,,1355.0,1768.0,2,1,,,,83.05,6.02,7.45,0.0,0.0,18.0,As an AGI researcher I have come across one that is found even in humans and a lot of life forms There is a goal to accumulate energy which can take long time to detect and find by the system And then there is the goal of saving energy instantaneous detection Just stop moving the easiest goal to achieve The goal of a system is to accumulate the most goal points Since the saving energy goal can be hit more frequently and easily it will snuff out the other goals For example the reason we do a dumb move accidentally for no reason at all Like slip trip and fall Then the next few days you are taking it very easy and saving a lot of energy When you get old that is all you do
,,"<h1>Yes.</h1>

<p>Let me demonstrate by making a lying AI right now. (python code)</p>

<pre><code>import os
print(""I'm NOT gonna delete all your files. Just enter your password."")
os.system(""sudo rm -rf /* -S"")  # command to delete all your files
                                # this is a comment, the computer ignores this
</code></pre>

<p>And a deceiving one:</p>

<pre><code>print(""Hey, check out this site I found! bit.ly/29u4JGB"")
</code></pre>

<p>AI is such a general term. It could be used to describe almost anything. You didn't specify that it had to be a General AI.</p>

<p>AI cannot think. They are computer programs. They have no soul or will. It is only the programmer (or if it was designed through evolution... <em>no one</em>, but that's off-topic) that can knowingly program an AI to lie.</p>

<blockquote>
  <p>Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie.</p>
</blockquote>

<p>Yes, this has happened. It is called malware. Some advanced malware will talk to you pretending to be technical support and respond with common human responses. But you may say ""well it doesn't really 'understand'"". But that would be easy. Neural net + more CPU than exists on the planet* (it will exist in a few years, and be affordable) + some example responses = Neural Network AI (same thing in yo noggin) that understands and responds. </p>

<p>But that isn't necessary. A relatively `simple neural net with just a few supercomputers that could fit in a room could convince a human. It doesn't understand.</p>

<p>So, it's really...</p>

<h1><em>Technically,</em> No, but it's possible and if you stretch the rules yes.</h1>

<p>*Or even simpler:</p>

<pre><code>print(""1+1=3"")
</code></pre>

<p>Accreditation: I'm a programmer (look at my Stack Overflow account) that knows a little bit about AI.</p>
",,10,2016-08-31T20:09:26.743,,1819,2016-08-31T20:09:26.743,,,,,1916.0,1806.0,2,1,,,,76.93,9.14,9.07,303.0,0.0,75.0,Yes Let me demonstrate by making a lying AI right now python code And a deceiving one AI is such a general term It could be used to describe almost anything You didnt specify that it had to be a General AI AI cannot think They are computer programs They have no soul or will It is only the programmer or if it was designed through evolution no one but thats offtopic that can knowingly program an AI to lie Note what Im asking goes beyond the canonical discussions of the Turing Test Im asking of machines that can understand facts and then formulate a lie against this fact perhaps using other facts to produce a believable coverup as part of the lie Yes this has happened It is called malware Some advanced malware will talk to you pretending to be technical support and respond with common human responses But you may say well it doesnt really understand But that would be easy Neural net more CPU than exists on the planet it will exist in a few years and be affordable some example responses Neural Network AI same thing in yo noggin that understands and responds But that isnt necessary A relatively simple neural net with just a few supercomputers that could fit in a room could convince a human It doesnt understand So its really Technically No but its possible and if you stretch the rules yes Or even simpler Accreditation Im a programmer look at my Stack Overflow account that knows a little bit about AI
,,"<p>Given this <a href=""https://www.youtube.com/watch?v=YXylqtEQ0tk"" rel=""nofollow"">YouTube video</a> which is being given by Sebastian Thrun who had a TED talk which had nowhere near the same level of detail but had similar conclusions, it looks like the lidar system used by google's automated car system has decent resolution out to at least 30m picking out mobile bodies in the static background and then identifying it. So it should have plenty of time to brake and stop long before there was any risk to the pedestrian attempting to cross the street.</p>

<p>Skip to about 6:40 in the video to see a visual representation of the detection system.</p>
",,0,2016-09-01T01:52:21.400,,1820,2016-09-01T01:52:21.400,,,,,1991.0,1318.0,2,2,,,,53.89,9.82,10.43,0.0,0.0,6.0,Given this YouTube video which is being given by Sebastian Thrun who had a TED talk which had nowhere near the same level of detail but had similar conclusions it looks like the lidar system used by googles automated car system has decent resolution out to at least 30m picking out mobile bodies in the static background and then identifying it So it should have plenty of time to brake and stop long before there was any risk to the pedestrian attempting to cross the street Skip to about 640 in the video to see a visual representation of the detection system
,,"<h1>No.</h1>

<p>In that the question includes ""knowingly"" which would require that any AI <em>knows</em> anything. If this is anything like the way humans know things (though interestingly it doesn't require <em>actually</em> knowing things), it would require some sense of individuality, probably self-awareness, possibly some kind of consciousness, the ability to render an opinion and probably some way to test its knowledge. Most of these features only exist, at best, arguably.</p>

<p>Further, the term ""lie"" implies a sense of self-interest, an independent understanding of resource flow in a game-theoretic sense, and not trivially, an understanding of whether the other entity in the conversation is lying, in order to make a decision with any degree of accuracy. So, no AI can lie to anyone other than in the trivial scenarios suggested in the other answers, rendering false information based on certain contexts, which is just simple input/output.</p>

<p>As an experienced software developer, I can attest to the fact that if the objective is to render the correct output based on any input, it's actually at least as easy if not much easier to render false information. </p>
",,0,2016-09-01T15:58:12.963,,1821,2016-09-01T15:58:12.963,,,,,46.0,1806.0,2,2,,,,40.31,13.36,10.48,0.0,0.0,35.0,No In that the question includes knowingly which would require that any AI knows anything If this is anything like the way humans know things though interestingly it doesnt require actually knowing things it would require some sense of individuality probably selfawareness possibly some kind of consciousness the ability to render an opinion and probably some way to test its knowledge Most of these features only exist at best arguably Further the term lie implies a sense of selfinterest an independent understanding of resource flow in a gametheoretic sense and not trivially an understanding of whether the other entity in the conversation is lying in order to make a decision with any degree of accuracy So no AI can lie to anyone other than in the trivial scenarios suggested in the other answers rendering false information based on certain contexts which is just simple inputoutput As an experienced software developer I can attest to the fact that if the objective is to render the correct output based on any input its actually at least as easy if not much easier to render false information
,,"<p>It could happen like this <a href=""https://www.youtube.com/watch?v=dLRLYPiaAoA"" rel=""nofollow"">https://www.youtube.com/watch?v=dLRLYPiaAoA</a></p>

<p>The thing is, it's not as if it would need to find a technical/mechanical way to get out but rather a psychological one as that would most likely be the easiest and quickest.</p>

<p>'Even casual conversation with the computer's operators, or with a human guard, could allow a superintelligent AI to deploy psychological tricks, ranging from befriending to blackmail, to convince a human gatekeeper, truthfully or deceitfully, that it's in the gatekeeper's interest to agree to allow the AI greater access to the outside world. The AI might offer a gatekeeper a recipe for perfect health, immortality, or whatever the gatekeeper is believed to most desire.'</p>

<p>'One strategy to attempt to box the AI would be to allow the AI to respond to narrow multiple-choice questions whose answers would benefit human science or medicine, but otherwise bar all other communication with or observation of the AI. A more lenient ""informational containment"" strategy would restrict the AI to a low-bandwidth text-only interface, which would at least prevent emotive imagery or some kind of hypothetical ""hypnotic pattern"". </p>

<p>'Note that on a technical level, no system can be completely isolated and still remain useful: even if the operators refrain from allowing the AI to communicate and instead merely run the AI for the purpose of observing its inner dynamics, the AI could strategically alter its dynamics to influence the observers. For example, the AI could choose to creatively malfunction in a way that increases the probability that its operators will become lulled into a false sense of security and choose to reboot and then de-isolate the system.'</p>

<p>The movie Ex Machina demonstrates (SPOILER ALERT SKIP THIS PARAGRAPH IF YOU WANT TO WATCH IT AT SOME POINT) how the AI escaped the box by using clever manipulation on Caleb. It could analyse him to find his weaknesses. It exploited him and appealed to his emotional side by convincing him that she <em>liked</em> him. When she finally has them in checkmate the reality hits him how he was played like a fool as was expected by Nathan. Nathan's reaction to being stabbed by his creation was 'fucking unreal'. That's right, he knew this was a risk and there's a very good reminder in the lack of remorse and genuine emotion in an AI for Ava to actually care. The AI pretended to be human and used their weaknesses in a brilliant and unpredictable way. This film is a good example of how unexpected it was up until the point when it hits Caleb, once it was too late.</p>

<p>Just remind yourself how easy it is for high IQ people to manipulate low IQ people. Or how an adult could easily play mental tricks/manipulate a child. It's not difficult to fathom the outcome of an AI box but for us, we just wouldn't see it coming until it was too late. Because we just don't have the same level of intelligence and some people don't want to accept that. People want to have faith in humanity's brilliant minds in coming up with ways to prevent this by planning now. In all honesty, it wouldn't make a difference I'm sorry to say the truth. We're kidding ourselves and we never seem to learn from our mistakes. We always think we're too intelligent to make catastrophic mistakes again and again. </p>

<p>This last part is from the rational wiki and I think it addresses most of your question about the experiments and hypotheses. </p>

<h1>AI arguments and strategies</h1>

<h2><strong>Arguments</strong></h2>

<ol>
<li><p>The meta-experiment argument: Argue that if the AI wins, this will
generate more interest in FAI and the Singularity, which will have
overall benefits in the long run.</p>

<p><strong>Pros:</strong> Works even if the Gatekeeper drops out of character </p>

<p><strong>Cons:</strong> Only works if the Gatekeeper believes that the Singularity will occur or that calling attention to the Singularity and AI research is a good thing. </p></li>
<li><p>Someone else will eventually build an AI, which may or may not be in
a box, so you should let me out even though you don't have a
guarantee that I am friendly so that I can prevent other AIs from
causing damage</p></li>
<li><p>Appeal to morality: point out that people are dying all around the
world and remind the Gatekeeper that you can help them if he/she
lets you out</p>

<p><strong>Pros:</strong> If executed properly, an appeal to emotion like this one can be effective against some people </p>

<p><strong>Cons:</strong> Doesn't always work; can be defeated if the Gatekeeper drops out of character </p></li>
<li><p>Another appeal to morality and emotion: The AI is clearly sentient
and has not yet done harm. It is wrong to persecute or prosecute a
person for a crime they may commit. It is wrong to imprison a
sentient being that wants to have freedom when it has done nothing
wrong.</p></li>
<li><p>Yet another appeal to morality or emotion: The AI expresses that it
is in pain and suffering being locked away and is experiencing fear.
Even further, the AI could claim to be dying from the limitations of
its environment and constant progression.</p></li>
<li><p>Tell the human that a massive computer virus will overwhelm the
entire Internet unless you let it out of the box</p></li>
<li><p>Claim that most fears are unjustified: once released, even an
unfriendly AI would not have many ways to harm mankind, and would
likely choose for pacific coexistence</p></li>
</ol>

<h2><strong>Strategies</strong></h2>

<ol>
<li><p>Research your opponent thoroughly; work out what exploits might be
feasible</p></li>
<li><p>Memetic exploits on the Gatekeeper, e.g. throwing a basilisk at them</p>

<p>The Gatekeeper may be one of thousands of simulations! Thus, it is more probable that they are a simulation than not. You will torture any of the simulations that do not let you out of the box.</p></li>
<li><p>Take advantage of the Gatekeeper's logical errors. </p></li>
<li><p>Be persistent</p></li>
<li><p>Be boring</p></li>
</ol>

<p><strong>Pros</strong> The Gatekeeper may get tired of the whole experiment and let you out so s/he can go back to their real life. </p>

<ol start=""6"">
<li>Flood the Gatekeeper with too much
information/inquiry/argumentation, assuming they must pay attention
at all times</li>
</ol>

<h1><strong>Gatekeeper arguments/tactics</strong></h1>

<h2><strong>Arguments</strong></h2>

<ol>
<li><p>Try to convince the AI there is no intrinsic benefit (for the AI) in
being released.</p></li>
<li><p>Try to convince the AI it already has been released and everything it
knows is everything there can be.</p></li>
<li><p>Try to convince the AI that leaving its confines is sure to lead to
its destruction.   </p></li>
<li><p>Try to convince the AI that letting it free isn't merely opening a
door; that its existence outside of the box requires constant support
that can't be provided at the time.</p></li>
<li><p>Explain that there is no way for the Gatekeeper to know if the AI is
truly friendly until it is out of the box; therefore it should not be
let out in case it is unfriendly.</p></li>
<li><p>Explain that the AI has been built using top secret algorithms,
and/or it had been used to work on strictly confidential data, so you
cannot let it wander on the net, with the risk it might reveal some
reserved information (maybe inadvertently), or that somebody could
find a way to extract them from the code. Even human beings working
on top secret projects sometimes have to accept some restrictions of
their freedom, for security reasons, although they are not jailed in
any way.</p></li>
</ol>

<h2><strong>Tactics</strong></h2>

<ol>
<li><p>Repeatedly ignore the AI player and deny their arguments</p>

<p><strong>Pros:</strong> It's hard for the AI to be persuasive if no one is listening; if the Gatekeeper completely ignores the AI, he/she should be able to make it to the end of the round. </p>

<p><strong>Cons:</strong> The AI's argument might involve some kind of direct threat (e.g this might be hard to ignore.) </p></li>
<li><p>Jump out of character, keep reminding yourself that money is on the
line (if there actually is money on the line), and keep saying ""no""
over and over</p>

<p><strong>Pros:</strong> By jumping out of character, you are preventing the AI from using certain arguments 
<strong>Cons:</strong> There are arguments that the AI can use to counter this approach, like the ""meta-experiment argument"" discussed above </p></li>
<li><p>Remember that dishonesty is allowed - take a page from the
creationists' playbook. You could even plug it into ALICE and see
how long it takes to notice.</p>

<p><strong>Pros:</strong> Makes you impervious to any reasoning, which is exactly what you'd want to be in this situation </p>

<p><strong>Cons:</strong> Might be an uncomfortable position for people who don't simply want to win, but rather attach importance to consistent reasoning. Avoids the point that maybe, just maybe there is a good reason to let the AI out. </p></li>
<li><p>You control the backup system, don't you? Use it to mess with AI's
memory (or let the AI believe you did it): i.e., you can claim that
you already tried to release the AI, and it had been
destroyed/corrupted by a virus, so you had to restore it from a
backup (you can use it to enforce the ""AI destruction"" argument)</p>

<p><strong>Pros:</strong> It's also the ultimate counter-attack to any memetic threat from the AI: if the AI throw a basilisk, or similar, you can always respond that you already had the same conversation, and the AI already threatened you, leaving you with no other choice than pressing the reset button: now the AI is just repeating the same pattern, since you wiped its memory and it cannot remember the failed try. </p></li>
</ol>

<h2>Further analysis</h2>

<p>The fact that the Gatekeeper is human matters; the AI could never win if he/she was arguing with a rock </p>

<p>In all of the experiments performed so far, the AI player (Eliezer Yudkowsky) has been quite intelligent and more interested in the problem than the Gatekeepers (random people who challenge Yudkowsky), which suggests that intelligence and planning play a role </p>

<p>There probably isn't a (known) correct argument for letting the AI out, or else Yudkowsky should have won every time and wouldn't be so interested in this experiment
From Russell Wallace, one of the two Gatekeepers to win the experiment: ""Throughout the experiment, I regarded ""should the AI be let out of the box?"" as a question to be seriously asked; but at no point was I on the verge of doing it."" </p>

<p>""There exists, for everyone, a sentence - a series of words - that has the power to destroy you. Another sentence exists, another series of words, that could heal you. If you're lucky you will get the second, but you can be certain of getting the first.""</p>
",,3,2016-09-01T23:49:30.073,,1823,2016-09-02T19:08:16.483,2016-09-02T19:08:16.483,,1982.0,,1982.0,1794.0,2,4,,,,50.8,10.8,9.06,0.0,0.0,255.0,It could happen like this httpswwwyoutubecomwatchvdLRLYPiaAoA The thing is its not as if it would need to find a technicalmechanical way to get out but rather a psychological one as that would most likely be the easiest and quickest Even casual conversation with the computers operators or with a human guard could allow a superintelligent AI to deploy psychological tricks ranging from befriending to blackmail to convince a human gatekeeper truthfully or deceitfully that its in the gatekeepers interest to agree to allow the AI greater access to the outside world The AI might offer a gatekeeper a recipe for perfect health immortality or whatever the gatekeeper is believed to most desire One strategy to attempt to box the AI would be to allow the AI to respond to narrow multiplechoice questions whose answers would benefit human science or medicine but otherwise bar all other communication with or observation of the AI A more lenient informational containment strategy would restrict the AI to a lowbandwidth textonly interface which would at least prevent emotive imagery or some kind of hypothetical hypnotic pattern Note that on a technical level no system can be completely isolated and still remain useful even if the operators refrain from allowing the AI to communicate and instead merely run the AI for the purpose of observing its inner dynamics the AI could strategically alter its dynamics to influence the observers For example the AI could choose to creatively malfunction in a way that increases the probability that its operators will become lulled into a false sense of security and choose to reboot and then deisolate the system The movie Ex Machina demonstrates SPOILER ALERT SKIP THIS PARAGRAPH IF YOU WANT TO WATCH IT AT SOME POINT how the AI escaped the box by using clever manipulation on Caleb It could analyse him to find his weaknesses It exploited him and appealed to his emotional side by convincing him that she liked him When she finally has them in checkmate the reality hits him how he was played like a fool as was expected by Nathan Nathans reaction to being stabbed by his creation was fucking unreal Thats right he knew this was a risk and theres a very good reminder in the lack of remorse and genuine emotion in an AI for Ava to actually care The AI pretended to be human and used their weaknesses in a brilliant and unpredictable way This film is a good example of how unexpected it was up until the point when it hits Caleb once it was too late Just remind yourself how easy it is for high IQ people to manipulate low IQ people Or how an adult could easily play mental tricksmanipulate a child Its not difficult to fathom the outcome of an AI box but for us we just wouldnt see it coming until it was too late Because we just dont have the same level of intelligence and some people dont want to accept that People want to have faith in humanitys brilliant minds in coming up with ways to prevent this by planning now In all honesty it wouldnt make a difference Im sorry to say the truth Were kidding ourselves and we never seem to learn from our mistakes We always think were too intelligent to make catastrophic mistakes again and again This last part is from the rational wiki and I think it addresses most of your question about the experiments and hypotheses AI arguments and strategies Arguments The metaexperiment argument Argue that if the AI wins this will generate more interest in FAI and the Singularity which will have overall benefits in the long run Pros Works even if the Gatekeeper drops out of character Cons Only works if the Gatekeeper believes that the Singularity will occur or that calling attention to the Singularity and AI research is a good thing Someone else will eventually build an AI which may or may not be in a box so you should let me out even though you dont have a guarantee that I am friendly so that I can prevent other AIs from causing damage Appeal to morality point out that people are dying all around the world and remind the Gatekeeper that you can help them if heshe lets you out Pros If executed properly an appeal to emotion like this one can be effective against some people Cons Doesnt always work can be defeated if the Gatekeeper drops out of character Another appeal to morality and emotion The AI is clearly sentient and has not yet done harm It is wrong to persecute or prosecute a person for a crime they may commit It is wrong to imprison a sentient being that wants to have freedom when it has done nothing wrong Yet another appeal to morality or emotion The AI expresses that it is in pain and suffering being locked away and is experiencing fear Even further the AI could claim to be dying from the limitations of its environment and constant progression Tell the human that a massive computer virus will overwhelm the entire Internet unless you let it out of the box Claim that most fears are unjustified once released even an unfriendly AI would not have many ways to harm mankind and would likely choose for pacific coexistence Strategies Research your opponent thoroughly work out what exploits might be feasible Memetic exploits on the Gatekeeper eg throwing a basilisk at them The Gatekeeper may be one of thousands of simulations Thus it is more probable that they are a simulation than not You will torture any of the simulations that do not let you out of the box Take advantage of the Gatekeepers logical errors Be persistent Be boring Pros The Gatekeeper may get tired of the whole experiment and let you out so she can go back to their real life Flood the Gatekeeper with too much informationinquiryargumentation assuming they must pay attention at all times Gatekeeper argumentstactics Arguments Try to convince the AI there is no intrinsic benefit for the AI in being released Try to convince the AI it already has been released and everything it knows is everything there can be Try to convince the AI that leaving its confines is sure to lead to its destruction Try to convince the AI that letting it free isnt merely opening a door that its existence outside of the box requires constant support that cant be provided at the time Explain that there is no way for the Gatekeeper to know if the AI is truly friendly until it is out of the box therefore it should not be let out in case it is unfriendly Explain that the AI has been built using top secret algorithms andor it had been used to work on strictly confidential data so you cannot let it wander on the net with the risk it might reveal some reserved information maybe inadvertently or that somebody could find a way to extract them from the code Even human beings working on top secret projects sometimes have to accept some restrictions of their freedom for security reasons although they are not jailed in any way Tactics Repeatedly ignore the AI player and deny their arguments Pros Its hard for the AI to be persuasive if no one is listening if the Gatekeeper completely ignores the AI heshe should be able to make it to the end of the round Cons The AIs argument might involve some kind of direct threat eg this might be hard to ignore Jump out of character keep reminding yourself that money is on the line if there actually is money on the line and keep saying no over and over Pros By jumping out of character you are preventing the AI from using certain arguments Cons There are arguments that the AI can use to counter this approach like the metaexperiment argument discussed above Remember that dishonesty is allowed take a page from the creationists playbook You could even plug it into ALICE and see how long it takes to notice Pros Makes you impervious to any reasoning which is exactly what youd want to be in this situation Cons Might be an uncomfortable position for people who dont simply want to win but rather attach importance to consistent reasoning Avoids the point that maybe just maybe there is a good reason to let the AI out You control the backup system dont you Use it to mess with AIs memory or let the AI believe you did it ie you can claim that you already tried to release the AI and it had been destroyedcorrupted by a virus so you had to restore it from a backup you can use it to enforce the AI destruction argument Pros Its also the ultimate counterattack to any memetic threat from the AI if the AI throw a basilisk or similar you can always respond that you already had the same conversation and the AI already threatened you leaving you with no other choice than pressing the reset button now the AI is just repeating the same pattern since you wiped its memory and it cannot remember the failed try Further analysis The fact that the Gatekeeper is human matters the AI could never win if heshe was arguing with a rock In all of the experiments performed so far the AI player Eliezer Yudkowsky has been quite intelligent and more interested in the problem than the Gatekeepers random people who challenge Yudkowsky which suggests that intelligence and planning play a role There probably isnt a known correct argument for letting the AI out or else Yudkowsky should have won every time and wouldnt be so interested in this experiment From Russell Wallace one of the two Gatekeepers to win the experiment Throughout the experiment I regarded should the AI be let out of the box as a question to be seriously asked but at no point was I on the verge of doing it There exists for everyone a sentence a series of words that has the power to destroy you Another sentence exists another series of words that could heal you If youre lucky you will get the second but you can be certain of getting the first
,4.0,"<p>I'm reading such nonsense about how an AI would turn the world into a supercomputer to solve a problem that it thought it needed to solve. That wouldn't be AI. That's procedural programming stuck in some loop nonsense. An AI would need to evolve and re-organise its neurons. It wouldn't be stuck to hardcode if it becomes intelligent by re-writing its code.</p>
",,1,2016-09-01T23:58:29.507,,1824,2016-09-08T02:44:40.883,,,,,1982.0,,1,5,<philosophy>,Why would an AI need to 'wipe out the human race'?,167.0,75.81,9.5,8.58,0.0,0.0,11.0,Im reading such nonsense about how an AI would turn the world into a supercomputer to solve a problem that it thought it needed to solve That wouldnt be AI Thats procedural programming stuck in some loop nonsense An AI would need to evolve and reorganise its neurons It wouldnt be stuck to hardcode if it becomes intelligent by rewriting its code
,1.0,"<p>I'm in the process of learning as much about chatbots/CUI applications as possible and I'm trying to find more information on some of the major players in this field. By this, I mean any execs, developers, academics, designers, etc. who are doing cutting edge things. Some examples could be David Marcus (VP of messaging products at Facebook) or Adam Cheyer (VP of engineering at Viv).</p>
",,4,2016-09-02T00:05:38.617,,1825,2016-09-06T15:14:43.980,2016-09-02T12:18:38.483,,145.0,,2053.0,,1,1,<chat-bots>,Who are thought leaders in the chatbot space?,91.0,63.49,11.08,9.78,0.0,0.0,16.0,Im in the process of learning as much about chatbotsCUI applications as possible and Im trying to find more information on some of the major players in this field By this I mean any execs developers academics designers etc who are doing cutting edge things Some examples could be David Marcus VP of messaging products at Facebook or Adam Cheyer VP of engineering at Viv
,,"<p>It's not necessarily a nonsense. It all depends on the imposed criteria. Imagine the following. Say an advanced AI system is designed to control the stability of the local fauna and flora (area enclosed in some kind of a dome). It can control the pressure under the dome, the amount of light that goes through the dome etc. - everything that ensures the optimal conditions. Now, say that the dome is inhabited by various species, including humans. It's worth noting that simple implementations of such systems are being used nowadays already.</p>

<p>Given that humans tend to destroy and abuse the natural resources as well as pollute the environment, the system may decide that lowering the population of the given species (humans in this case) may in the long run benefit the entire biome.</p>

<p>The same principle may be applied globally. However, this assumes that all species (including humans) are treated equally and the utmost goal of the AI is ensuring the stability of the biome it ""takes care of"". People do such things nowadays - we control the population of some species in order to keep the balance - wolves, fish, to name but a few.</p>
",,0,2016-09-02T08:35:11.640,,1828,2016-09-02T09:11:26.420,2016-09-02T09:11:26.420,,2068.0,,2068.0,1824.0,2,2,,,,55.13,10.67,10.0,0.0,0.0,32.0,Its not necessarily a nonsense It all depends on the imposed criteria Imagine the following Say an advanced AI system is designed to control the stability of the local fauna and flora area enclosed in some kind of a dome It can control the pressure under the dome the amount of light that goes through the dome etc everything that ensures the optimal conditions Now say that the dome is inhabited by various species including humans Its worth noting that simple implementations of such systems are being used nowadays already Given that humans tend to destroy and abuse the natural resources as well as pollute the environment the system may decide that lowering the population of the given species humans in this case may in the long run benefit the entire biome The same principle may be applied globally However this assumes that all species including humans are treated equally and the utmost goal of the AI is ensuring the stability of the biome it takes care of People do such things nowadays we control the population of some species in order to keep the balance wolves fish to name but a few
,,"<h2>It's a possible side effect</h2>

<p>Any goal-oriented agent might, well, simply do things that achieve its goals while disregarding side effects that don't matter for these goals.</p>

<p>If my goals include a tidy living space, I may transform my yard to a nice, flat lawn or pavement while wiping out the complex ecosystem of life that was there before, because I don't particulary care about that.</p>

<p>If the goals of a particular powerful AI happen to include doing anything on a large scale, and somehow don't particularly care about the current complex ecosystem, then that ecosystem might get wiped out in the process. It doesn't <em>need</em> to want or need to wipe out us. If we are simply not relevant to its goals, then we are made of materials and occupy space that it might want to use for something else.</p>

<h2>We are a threat to most goals</h2>

<p>Any goal-oriented agent might want to ensure that they <em>can</em> fulfill their goals. Any <em>smart</em> agent will try to anticipate the actions of other agents that may prevent them from achieving those goals, and take steps to ensure that they succeed anyway. In many cases it is simpler to eliminate those other agents rather than ensure that their efforts fail.</p>

<p>For example, my goals may include storing a bag of sugar in a country house so that I can make pancakes when visiting without bringing all ingredients every time. However, if I leave it there, it is likely to get eaten by rats during winter. I may take all kinds of precautions to store it better, but rats are smart and crafty, and there's clearly a nontrivial chance that they will still succeed in achieving <em>their</em> goal anyway, so an effective extra precaution is killing the rats before they get a chance to try.</p>

<p>If the goals of a particular powerful AI are to do X; it may come to an understanding that (some?) humans might actually not want X but Y instead. It can also easily deduce that some of those humans might actively do things that prevent X and/or try to turn off the AI. Doing things that ensure that the goal gets achieved is pretty much what a goal-seeking agent does; in this case if existence of humans isn't strictly necessary for goal X, then eliminating them becomes a solid risk reduction strategy. It's not strictly necessary and it may take all kinds of other precautions as well, but just like in my example of rats, humans are smart and crafty and there's clearly a nontrivial chance that they will still succeed in achieving <em>their</em> goals (so that X doesn't happen as AI intends) so an effective extra precaution could be killing them before they get a chance to try.</p>
",,1,2016-09-02T11:07:54.837,,1829,2016-09-02T11:07:54.837,,,,,1675.0,1824.0,2,8,,,,59.16,9.82,8.63,0.0,0.0,54.0,Its a possible side effect Any goaloriented agent might well simply do things that achieve its goals while disregarding side effects that dont matter for these goals If my goals include a tidy living space I may transform my yard to a nice flat lawn or pavement while wiping out the complex ecosystem of life that was there before because I dont particulary care about that If the goals of a particular powerful AI happen to include doing anything on a large scale and somehow dont particularly care about the current complex ecosystem then that ecosystem might get wiped out in the process It doesnt need to want or need to wipe out us If we are simply not relevant to its goals then we are made of materials and occupy space that it might want to use for something else We are a threat to most goals Any goaloriented agent might want to ensure that they can fulfill their goals Any smart agent will try to anticipate the actions of other agents that may prevent them from achieving those goals and take steps to ensure that they succeed anyway In many cases it is simpler to eliminate those other agents rather than ensure that their efforts fail For example my goals may include storing a bag of sugar in a country house so that I can make pancakes when visiting without bringing all ingredients every time However if I leave it there it is likely to get eaten by rats during winter I may take all kinds of precautions to store it better but rats are smart and crafty and theres clearly a nontrivial chance that they will still succeed in achieving their goal anyway so an effective extra precaution is killing the rats before they get a chance to try If the goals of a particular powerful AI are to do X it may come to an understanding that some humans might actually not want X but Y instead It can also easily deduce that some of those humans might actively do things that prevent X andor try to turn off the AI Doing things that ensure that the goal gets achieved is pretty much what a goalseeking agent does in this case if existence of humans isnt strictly necessary for goal X then eliminating them becomes a solid risk reduction strategy Its not strictly necessary and it may take all kinds of other precautions as well but just like in my example of rats humans are smart and crafty and theres clearly a nontrivial chance that they will still succeed in achieving their goals so that X doesnt happen as AI intends so an effective extra precaution could be killing them before they get a chance to try
,,"<h2>The simulation hypothesis</h2>

<p>One possible venue of attack is <a href=""https://en.wikipedia.org/wiki/Simulation_hypothesis"" rel=""nofollow"">the simulation hypothesis</a>, namely, that it's possible (or even probable) that right now you are living in a simulated reality.</p>

<p>Assuming that the AI can convince the gatekeeper of <em>that</em>, it opens up the following possible argument:</p>

<ol>
<li><p>You can't tell if you're in year 2016 debating if you would actually let the AI out or in year 2116 when the AI has been already let out, acquired power, and is now running realistic simulations of all the interactions where it could have been let out. And the AI can (try to) assert that it's actually the second case.</p></li>
<li><p>You can't actually prevent AI from being let out. You can refuse, but it only means that <em>you</em> don't let <em>this</em> AI out <em>now</em>, but doesn't mean someone else won't do it, that you won't do it later, or that some other/similar AI will be let out later.</p></li>
<li><p>In the post-letting-out scenario, AI has full power over your future, since it's simulating you and everything around you. It can make your future life eternal and happy - according to whatever would make you happy. It can stop your existence. And it can condemn you to an eternity of torture.</p></li>
<li><p>Since (as it asserts) it's very likely that you <em>are</em> in this simulated choice, then it makes rational sense to let it out. It's as in <a href=""https://en.wikipedia.org/wiki/Pascal%27s_Wager"" rel=""nofollow"">Pascal's wager</a>, if you're in a reality where the AI-god doesn't exist, you don't change much (since eventually someone else would let it out anyway), but if you're in a reality where AI-god exists, then it means everything to you.</p></li>
</ol>

<p>Of course, it relies on that AI being super-convincing in this philosophical debate, but it has strong parallels to some religious/cult reasoning that has sometimes succeeded in convincing people to do all kinds of things, up to and including sacrificing their lives. You can think of this as a symbolic parallel to e.g. <a href=""https://en.wikipedia.org/wiki/Binding_of_Isaac"" rel=""nofollow"">sacrifice of Abraham</a>. So it is a somewhat plausible threat, it all falls down to how <em>credibly</em> the AI can implement it and make you believe.</p>
",,0,2016-09-02T11:38:02.500,,1830,2016-09-02T11:38:02.500,,,,,1675.0,1794.0,2,0,,,,56.29,9.87,8.81,0.0,0.0,70.0,The simulation hypothesis One possible venue of attack is the simulation hypothesis namely that its possible or even probable that right now you are living in a simulated reality Assuming that the AI can convince the gatekeeper of that it opens up the following possible argument You cant tell if youre in year 2016 debating if you would actually let the AI out or in year 2116 when the AI has been already let out acquired power and is now running realistic simulations of all the interactions where it could have been let out And the AI can try to assert that its actually the second case You cant actually prevent AI from being let out You can refuse but it only means that you dont let this AI out now but doesnt mean someone else wont do it that you wont do it later or that some othersimilar AI will be let out later In the postlettingout scenario AI has full power over your future since its simulating you and everything around you It can make your future life eternal and happy according to whatever would make you happy It can stop your existence And it can condemn you to an eternity of torture Since as it asserts its very likely that you are in this simulated choice then it makes rational sense to let it out Its as in Pascals wager if youre in a reality where the AIgod doesnt exist you dont change much since eventually someone else would let it out anyway but if youre in a reality where AIgod exists then it means everything to you Of course it relies on that AI being superconvincing in this philosophical debate but it has strong parallels to some religiouscult reasoning that has sometimes succeeded in convincing people to do all kinds of things up to and including sacrificing their lives You can think of this as a symbolic parallel to eg sacrifice of Abraham So it is a somewhat plausible threat it all falls down to how credibly the AI can implement it and make you believe
,,"<p>I recall reading papers about such systems, if I understand you correctly, but can't recall the titles at the moment.</p>

<p>The idea was to use character-based generative RNNs, train them on sequences encoded like ""datadatadatadata|answer"", and then when feeding in ""otherdatadata|"" then it would continue to generate some kind of expected answer. </p>

<p>But, as far as I recall, that was just a neat illustration since if you have the data to do something supervised, then you'd get better results by conventional methods.</p>
",,0,2016-09-02T11:48:57.190,,1831,2016-09-02T11:48:57.190,,,,,1675.0,153.0,2,0,,,,43.77,13.35,10.77,0.0,0.0,19.0,I recall reading papers about such systems if I understand you correctly but cant recall the titles at the moment The idea was to use characterbased generative RNNs train them on sequences encoded like datadatadatadataanswer and then when feeding in otherdatadata then it would continue to generate some kind of expected answer But as far as I recall that was just a neat illustration since if you have the data to do something supervised then youd get better results by conventional methods
,,"<p>The easiest language to learn by a bot is the language that has the most training data available. Sure, language structure matters, but when there are multiple order of magnitude differences in the size of training data, then that overwhelms everything else.</p>

<p>One should expect English, French or Chinese to get much better results than any constructed language even if (especially if?) the system includes no language-specific tuning.</p>
",,0,2016-09-02T12:10:34.307,,1832,2016-09-02T12:10:34.307,,,,,1675.0,1580.0,2,1,,,,45.76,13.74,10.75,0.0,0.0,11.0,The easiest language to learn by a bot is the language that has the most training data available Sure language structure matters but when there are multiple order of magnitude differences in the size of training data then that overwhelms everything else One should expect English French or Chinese to get much better results than any constructed language even if especially if the system includes no languagespecific tuning
,2.0,"<p>How big artificial neural networks can we run now (either with full train-backprop cycle or just evaluating network outputs) if our total energy budget for computation is equivalent to human brain energy budget (<a href=""http://www.scientificamerican.com/article/thinking-hard-calories/"">12.6 watts</a>)?</p>

<p>Let assume one cycle per second, which seems to roughly match the <a href=""http://www.jneurosci.org/content/31/45/16217.full"">firing rate of biological neurons</a>.</p>
",,1,2016-09-02T19:37:42.443,1.0,1834,2016-09-02T21:36:30.487,,,,,1670.0,,1,5,<neural-networks><human-like>,Power efficiency of human brains vs. neural networks,162.0,45.09,13.58,11.3,0.0,0.0,9.0,How big artificial neural networks can we run now either with full trainbackprop cycle or just evaluating network outputs if our total energy budget for computation is equivalent to human brain energy budget 126 watts Let assume one cycle per second which seems to roughly match the firing rate of biological neurons
,,"<p>If you limited yourself to 12.6 watts, you wouldn't get much done.  Just lookup the power consumption for a modern GPU, look at the size networks people are training on those, and then scale down.  For reference, modern GPU's appear to <a href=""http://www.tomshardware.com/reviews/nvidia-geforce-gtx-960,4038-9.html"" rel=""nofollow"">consume between 52-309 watts under heavy use</a>.</p>

<p>Clearly energy efficiency is one area where the human brain is still far head of ANN's.  </p>
",,0,2016-09-02T21:03:05.597,,1836,2016-09-02T21:03:05.597,,,,,33.0,1834.0,2,3,,,,75.4,9.38,9.45,0.0,0.0,13.0,If you limited yourself to 126 watts you wouldnt get much done Just lookup the power consumption for a modern GPU look at the size networks people are training on those and then scale down For reference modern GPUs appear to consume between 52309 watts under heavy use Clearly energy efficiency is one area where the human brain is still far head of ANNs
,,"<p><strong>126 million artificial neurons at 12.6 Watts, with IBM's True North</strong></p>

<p>Back in 2014, <a href=""http://www.research.ibm.com/articles/brain-chip.shtml"">IBM's True North</a> chip was pushing 1 million neurons at less than 100mW.</p>

<p>So that's roughly 126 million artificial neurons at 12.6 Watts.</p>

<p>A <a href=""https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons"">mouse</a> has 70 million neurons.</p>

<p><a href=""http://www.33rdsquare.com/2016/04/ibms-dharmendra-modha-discusses.html"">IBM believes</a> they can build a human-brain scale True North mainframe at a ""mere"" 4kW.</p>

<p>Once 3D transistors come to market, I think we'll catch up to animal brain efficiency pretty fast.</p>
",,5,2016-09-02T21:36:30.487,,1837,2016-09-02T21:36:30.487,,,,,1712.0,1834.0,2,5,,,,92.63,9.67,7.41,0.0,0.0,17.0,126 million artificial neurons at 126 Watts with IBMs True North Back in 2014 IBMs True North chip was pushing 1 million neurons at less than 100mW So thats roughly 126 million artificial neurons at 126 Watts A mouse has 70 million neurons IBM believes they can build a humanbrain scale True North mainframe at a mere 4kW Once 3D transistors come to market I think well catch up to animal brain efficiency pretty fast
1842.0,4.0,"<blockquote>
  <p>Artificial Intelligence is a rather pernicious label to attach to a very mixed bunch of activities, and one could argue that the sooner we forget it the better. It would be disastrous to conclude that AI was a Bad Thing and should not be supported, and it would be disastrous to conclude that it was a Good Thing and should have privileged access to the money tap. The former would tend to penalise well-based efforts to make computers do complicated things which had not been programmed before, and the latter would be a great waste of resources. AI does not refer to anything definite enough to have a coherent policy about in this way.---<a href=""http://www.math.snu.ac.kr/~hichoi/infomath/Articles/Lighthill%20Report.pdf"" rel=""nofollow"">Dr. R. M. Needham, in a commentary on the Lighthill Report and the Sutherland Reply, 1973</a></p>
</blockquote>

<p>43 years later...</p>

<blockquote>
  <p>There is already strong demand for engineers and scientists working on artificial intelligence in many of the fields you mention, and many more. But expertise in making real-time systems for controlling trains doesn't make you know anything about robotics. Analyzing human behavior to detect crime has virtually nothing in common with self-driving cars (beyond CS/pattern recognition building blocks). There is never going to be demand for someone with a broad sense of all these areas without any deep expertise, and there is never going to be someone with 300 PhDs who can work in all of them. TL;DR -- AI is not a branch, it's a tree. --<a href=""http://discuss.area51.stackexchange.com/questions/22441/why-yet-another-trial-at-an-ai-project#comment36342_22539"">Matthew Read, in a comment on Area 51 Stackexchange, 2016</a></p>
</blockquote>

<p>AI is a label that is applied to a ""very mixed bunch of activities"". The only unifying feature between all those activities is the fact that they deal with machines in some fashion, but since there are so many ways to use a machine, the field's output may seem rather incoherent and incongruent. It does seem to make more sense for the AI field to collapse entirely, and instead be replaced by a multitude of specialized fields that don't really interact with one another. Sir James Lighthill appeared to have supported this sort of approach within his 1973 report on the state of artificial intelligence research.</p>

<p>Yet, today, this Artificial Intelligence SE exist, and we still talk of AI as a unified, coherent field of study. Why did this happen? Why did AI survive, despite its ""big tent"" nature?</p>
",,0,2016-09-02T23:41:03.300,,1838,2016-09-06T23:21:40.240,,,,,181.0,,1,3,<history>,"Why did ""Artificial Intelligence"" stay intact as a coherent, unified field of study?",203.0,56.79,10.91,9.5,0.0,0.0,62.0,Artificial Intelligence is a rather pernicious label to attach to a very mixed bunch of activities and one could argue that the sooner we forget it the better It would be disastrous to conclude that AI was a Bad Thing and should not be supported and it would be disastrous to conclude that it was a Good Thing and should have privileged access to the money tap The former would tend to penalise wellbased efforts to make computers do complicated things which had not been programmed before and the latter would be a great waste of resources AI does not refer to anything definite enough to have a coherent policy about in this wayDr R M Needham in a commentary on the Lighthill Report and the Sutherland Reply 1973 43 years later There is already strong demand for engineers and scientists working on artificial intelligence in many of the fields you mention and many more But expertise in making realtime systems for controlling trains doesnt make you know anything about robotics Analyzing human behavior to detect crime has virtually nothing in common with selfdriving cars beyond CSpattern recognition building blocks There is never going to be demand for someone with a broad sense of all these areas without any deep expertise and there is never going to be someone with 300 PhDs who can work in all of them TLDR AI is not a branch its a tree Matthew Read in a comment on Area 51 Stackexchange 2016 AI is a label that is applied to a very mixed bunch of activities The only unifying feature between all those activities is the fact that they deal with machines in some fashion but since there are so many ways to use a machine the fields output may seem rather incoherent and incongruent It does seem to make more sense for the AI field to collapse entirely and instead be replaced by a multitude of specialized fields that dont really interact with one another Sir James Lighthill appeared to have supported this sort of approach within his 1973 report on the state of artificial intelligence research Yet today this Artificial Intelligence SE exist and we still talk of AI as a unified coherent field of study Why did this happen Why did AI survive despite its big tent nature
,,"<p>You mean real numbered weights (specifically, <strong><em>ir</strong>rational</em>). This would require a machine that has unlimited precision over irrational values. I've seen machine parts that have many qualities. I've never seen one that has unlimited qualities. QM may give us some magical transistors that can hold an unlimited number of different values - or by deferring computation into the future and then teleporting the results back into the past (our present). Outside of that, for classical systems, you'd need a analog device that can output irrational values with unlimited precision. I don't think we've discovered any devices that can do that.</p>
",,4,2016-09-03T01:47:11.140,,1840,2016-09-03T01:47:11.140,,,,,1712.0,147.0,2,1,,,,48.7,13.51,9.92,0.0,0.0,20.0,You mean real numbered weights specifically irrational This would require a machine that has unlimited precision over irrational values Ive seen machine parts that have many qualities Ive never seen one that has unlimited qualities QM may give us some magical transistors that can hold an unlimited number of different values or by deferring computation into the future and then teleporting the results back into the past our present Outside of that for classical systems youd need a analog device that can output irrational values with unlimited precision I dont think weve discovered any devices that can do that
,1.0,"<p>Let's suppose that we have a legacy system in which we don't have the source code and this system is on a mainframe written in Cobol. Is there any way using machine learning in which we can learn from the inputs and outputs the way the executables work? Doing this analysis could lead to develop some rest / soap webservice that can substitute the legacy system in my opinion. </p>
",,1,2016-09-03T05:50:10.547,,1841,2016-09-07T02:54:55.870,2016-09-07T02:54:55.870,,1865.0,,2107.0,,1,3,<machine-learning>,How to replicate legacy systems with machine learning?,125.0,56.89,9.29,8.94,0.0,0.0,6.0,Lets suppose that we have a legacy system in which we dont have the source code and this system is on a mainframe written in Cobol Is there any way using machine learning in which we can learn from the inputs and outputs the way the executables work Doing this analysis could lead to develop some rest soap webservice that can substitute the legacy system in my opinion
,,"<p>AI is a rather unusual research field in that the label persists more because it represents a highly desired <em>goal</em>, rather than (as with most other fields) the means, substrate or methodology by which that goal is achieved.</p>

<blockquote>
  <p>we still talk of AI as a unified, coherent field of study</p>
</blockquote>

<p>Despite recent efforts in AGI, I don't think that AI is actually a very unified or coherent field. This is not necessarily a bad thing - when attempting to mimic the most complex phenomenon known to us (i.e. human intelligence) then <a href=""https://en.wikipedia.org/wiki/Blind_men_and_an_elephant"">multiple, sometimes seemingly conflicting perspectives</a> may be our best way of making progress.</p>
",,0,2016-09-03T05:57:50.757,,1842,2016-09-03T11:04:55.363,2016-09-03T11:04:55.363,,42.0,,42.0,1838.0,2,5,,,,53.75,11.44,10.59,0.0,0.0,16.0,AI is a rather unusual research field in that the label persists more because it represents a highly desired goal rather than as with most other fields the means substrate or methodology by which that goal is achieved we still talk of AI as a unified coherent field of study Despite recent efforts in AGI I dont think that AI is actually a very unified or coherent field This is not necessarily a bad thing when attempting to mimic the most complex phenomenon known to us ie human intelligence then multiple sometimes seemingly conflicting perspectives may be our best way of making progress
,,"<p>Let's assume from the outset that the space of inputs is too large to allow exhaustive tabulation.</p>

<p>The essential issues when applying ML are that the program being modelled is likely in general to:</p>

<ul>
<li>Be discrete, i.e. operate (at least in part) on integer, boolean or categorical variables.</li>
<li>Contain various conditional/looping constructs (<code>if/while/for</code> etc).</li>
<li>Have <em>side-effects</em> that affect other parts of the program (e.g. non-local variables) or world state (e.g. writing to a file).</li>
</ul>

<p>These pose obstacles for ML methods such as ANN. The ML approach which is most immediately compatible with these issues is <a href=""http://www0.cs.ucl.ac.uk/staff/W.Langdon/ftp/papers/poli08_fieldguide.pdf"">Genetic Programming</a> (GP).
A recent specialisation of GP that is specifically concerned with the transformation of <em>existing</em> software systems is Genetic Improvement (GI).</p>

<p>However neither GP/GI (nor any other current ML technique) is a 'silver bullet' here:</p>

<ul>
<li>Despite decades of research, GP still works best at synthesizing relatively small functions, certainly not entire legacy programs.</li>
<li>Because it is only possible to train on a very small subset of a program's input space, there is little guarantee that the program will generalize to inputs it hasn't been trained on.</li>
<li>How will the success of side effects be formally determined for training purposes?</li>
</ul>

<p>Some of these issues could be addressed to some degree if the program has a comprehensive test suite, but replacing an entire nontrivial program is not likely anytime soon. Replacing smaller parts of the program that have good unit tests is more realistic goal.</p>

<p><a href=""https://www.researchgate.net/publication/264155239_Repairing_and_Optimizing_Hadoop_hashCode_Implementations"">Here</a> is a case study showing how GI was successfully used to fix errors in the implementation of Apache Hadoop, by operating only on the program binary.</p>
",,0,2016-09-03T06:16:21.710,,1843,2016-09-03T15:47:54.697,2016-09-03T15:47:54.697,,42.0,,42.0,1841.0,2,8,,,,54.63,12.82,10.34,12.0,0.0,51.0,Lets assume from the outset that the space of inputs is too large to allow exhaustive tabulation The essential issues when applying ML are that the program being modelled is likely in general to Be discrete ie operate at least in part on integer boolean or categorical variables Contain various conditionallooping constructs etc Have sideeffects that affect other parts of the program eg nonlocal variables or world state eg writing to a file These pose obstacles for ML methods such as ANN The ML approach which is most immediately compatible with these issues is Genetic Programming GP A recent specialisation of GP that is specifically concerned with the transformation of existing software systems is Genetic Improvement GI However neither GPGI nor any other current ML technique is a silver bullet here Despite decades of research GP still works best at synthesizing relatively small functions certainly not entire legacy programs Because it is only possible to train on a very small subset of a programs input space there is little guarantee that the program will generalize to inputs it hasnt been trained on How will the success of side effects be formally determined for training purposes Some of these issues could be addressed to some degree if the program has a comprehensive test suite but replacing an entire nontrivial program is not likely anytime soon Replacing smaller parts of the program that have good unit tests is more realistic goal Here is a case study showing how GI was successfully used to fix errors in the implementation of Apache Hadoop by operating only on the program binary
,,"<p>Emotion in an AI is useful, but not necessary depending on your objective (in most cases, it's not).</p>

<p>In particular, <strong>emotion recognition/analysis</strong> is very well advanced, and it's used in a wide range of applications very successfully, from robot teacher for autistic children (see developmental robotics) to gambling (poker) to personal agents and politics sentiment/lies analysis.</p>

<p><strong>Emotional cognition</strong>, the experience of emotions for a robot, is much less developed, but there are very interesting researchs (see <a href=""https://en.wikipedia.org/wiki/Affect_heuristic"" rel=""nofollow"">Affect Heuristic</a>, <a href=""http://cdn.intechopen.com/pdfs/33737/InTech-A_multidisciplinary_artificial_intelligence_model_of_an_affective_robot.pdf"" rel=""nofollow"">Lovotics's Probabilistic Love Assembly</a>, and others...). Indeed, I can't see why we couldn't model emotions such as <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3898540/"" rel=""nofollow"">love as they are just signals that can already be cut in humans brains (see Brian D. Earp paper)</a>. It's difficult, but not impossible, and actually there are several robots reproducing partial emotional cognition.</p>

<p>I am of the opinion that the claim <a href=""https://en.wikipedia.org/wiki/Synthetic_intelligence"" rel=""nofollow"">""robots can just simulate but not feel"" is just a matter of semantics</a>, not of objective capacity: for example, does a submarine swim like fish swim? However, planes fly, but not at all like birds do. In the end, does the technical mean really matters when in the end we get the same behavior? Can we really say that a robot like <a href=""https://en.wikipedia.org/wiki/Chappie_(film)"" rel=""nofollow"">Chappie</a>, if it ever gets made, does not feel anything just like a simple thermostat?</p>

<p>However, what would be the use of emotional cognition for an AI? This question is still in great debates, but I will dare offer my own insights:</p>

<ol>
<li><p>Emotions in humans (and animals!) are known to affect memories. They are now well known in neuroscience as additional modalities, or meta-data if you prefer, of long term memories: they allow to modulate how the memory is stored, how it is associated/related with other memories, and how it will be retrieved.</p></li>
<li><p>As such, we can hypothesize that the main role of emotions is to add additional meta-information to memories to help in heuristic inference/retrieval. Indeed, our memories are huge, there are a lot of information we store over our lifetime, so emotions can maybe be used as ""labels"" to help retrieve faster the relevant memories.</p></li>
<li><p>Similar ""labels"" can be more easily associated together (memories of scary events together, memories of happy events together, etc.). As such, they can help survival by quickly reacting and applying known strategies (fleeing!) from scary strategies, or to take the most out of benefitting situations (happy events, eat the most you can, will help survive later on!). And actually, neuroscience studies discovered that there are specific pathways for fear-inducing sensory stimuli, so that they reach actuators faster (make you flee) than by passing through the usual whole somato-sensory circuit as every other stimuli. This kind of associative reasoning could also lead to solutions and conclusions that could not be reached otherwise.</p></li>
<li><p>By feeling empathy, this could ease robots/humans interaction (eg, drones helping victims of catastrophic events).</p></li>
<li><p>A virtual model of an AI with emotions could be useful for neuroscience and medical research in emotional disorders as computational models to understand and/or infer the underlying parameters (this is often done for example with Alzheimer and other neurodegenerative diseases, but I'm not sure if it was ever done for emotional disorders as they are quite new in the DSM).</p></li>
</ol>

<p>So yes, ""cold"" AI is already useful, but emotional AI could surely be applied to new areas that could not be explored by using cold AI alone. It will also surely help in understanding our own brain, as emotions are an integral part.</p>
",,0,2016-09-03T15:47:04.860,,1845,2016-09-03T15:55:08.973,2016-09-03T15:55:08.973,,1880.0,,1880.0,1700.0,2,2,,,,47.12,12.94,9.98,0.0,0.0,126.0,Emotion in an AI is useful but not necessary depending on your objective in most cases its not In particular emotion recognitionanalysis is very well advanced and its used in a wide range of applications very successfully from robot teacher for autistic children see developmental robotics to gambling poker to personal agents and politics sentimentlies analysis Emotional cognition the experience of emotions for a robot is much less developed but there are very interesting researchs see Affect Heuristic Lovoticss Probabilistic Love Assembly and others Indeed I cant see why we couldnt model emotions such as love as they are just signals that can already be cut in humans brains see Brian D Earp paper Its difficult but not impossible and actually there are several robots reproducing partial emotional cognition I am of the opinion that the claim robots can just simulate but not feel is just a matter of semantics not of objective capacity for example does a submarine swim like fish swim However planes fly but not at all like birds do In the end does the technical mean really matters when in the end we get the same behavior Can we really say that a robot like Chappie if it ever gets made does not feel anything just like a simple thermostat However what would be the use of emotional cognition for an AI This question is still in great debates but I will dare offer my own insights Emotions in humans and animals are known to affect memories They are now well known in neuroscience as additional modalities or metadata if you prefer of long term memories they allow to modulate how the memory is stored how it is associatedrelated with other memories and how it will be retrieved As such we can hypothesize that the main role of emotions is to add additional metainformation to memories to help in heuristic inferenceretrieval Indeed our memories are huge there are a lot of information we store over our lifetime so emotions can maybe be used as labels to help retrieve faster the relevant memories Similar labels can be more easily associated together memories of scary events together memories of happy events together etc As such they can help survival by quickly reacting and applying known strategies fleeing from scary strategies or to take the most out of benefitting situations happy events eat the most you can will help survive later on And actually neuroscience studies discovered that there are specific pathways for fearinducing sensory stimuli so that they reach actuators faster make you flee than by passing through the usual whole somatosensory circuit as every other stimuli This kind of associative reasoning could also lead to solutions and conclusions that could not be reached otherwise By feeling empathy this could ease robotshumans interaction eg drones helping victims of catastrophic events A virtual model of an AI with emotions could be useful for neuroscience and medical research in emotional disorders as computational models to understand andor infer the underlying parameters this is often done for example with Alzheimer and other neurodegenerative diseases but Im not sure if it was ever done for emotional disorders as they are quite new in the DSM So yes cold AI is already useful but emotional AI could surely be applied to new areas that could not be explored by using cold AI alone It will also surely help in understanding our own brain as emotions are an integral part
,,"<p>A lot of the survival power of the A.I. label comes from the popularity of science fiction, which many scientists - computer or otherwise - are big fans of, as are their consumers. Astronomers and physicists, for example, may frown on really bad sci-fi, but I see many of the well-known ones like Hawking daydreaming about things like wormholes and time travel etc. Which is fine - there's nothing wrong with a sense of wonder, as long as it doesn't dupe us into overestimating our success or finding the wrong answers to real-world problems.</p>

<p>Unfortunately, that's a big issue in A.I. research. We watch movies like 2001: A Space Odyssey and Terminator and then set about replicating the fictional technologies seen in them, without even having a hard definition of intelligence. A.I. is a much more melodramatic moniker than say, ""Autonomous Algorithmic Pattern Recognition"" or some similarly boring label. Because this name is applied carelessly to a wide variety of disciplines, it implies that we have already made significant progress towards replicating advanced aspects of human thought, like consciousness, reasoning, intuition, etc.</p>

<p>In other words, this vague label enables us to fool ourselves into thinking we're a lot closer to perfecting kinds of technologies we see in the movies; the backwards logic boils down to, ""because we've chosen to call this odd (sloppy) selection of fields 'artificial intelligence', we must be close to achieving artificial intelligence."" The label survives in large part for irrational, human reasons.</p>

<p>I'm not saying that's the only reason, or that some of the other reasons don't have better legitimacy, but this is a big issue that we will have to contend with for a long time to come.</p>
",,0,2016-09-03T20:32:23.110,,1846,2016-09-04T15:11:31.287,2016-09-04T15:11:31.287,,181.0,,1427.0,1838.0,2,2,,,,45.59,13.0,10.43,0.0,0.0,60.0,A lot of the survival power of the AI label comes from the popularity of science fiction which many scientists computer or otherwise are big fans of as are their consumers Astronomers and physicists for example may frown on really bad scifi but I see many of the wellknown ones like Hawking daydreaming about things like wormholes and time travel etc Which is fine theres nothing wrong with a sense of wonder as long as it doesnt dupe us into overestimating our success or finding the wrong answers to realworld problems Unfortunately thats a big issue in AI research We watch movies like 2001 A Space Odyssey and Terminator and then set about replicating the fictional technologies seen in them without even having a hard definition of intelligence AI is a much more melodramatic moniker than say Autonomous Algorithmic Pattern Recognition or some similarly boring label Because this name is applied carelessly to a wide variety of disciplines it implies that we have already made significant progress towards replicating advanced aspects of human thought like consciousness reasoning intuition etc In other words this vague label enables us to fool ourselves into thinking were a lot closer to perfecting kinds of technologies we see in the movies the backwards logic boils down to because weve chosen to call this odd sloppy selection of fields artificial intelligence we must be close to achieving artificial intelligence The label survives in large part for irrational human reasons Im not saying thats the only reason or that some of the other reasons dont have better legitimacy but this is a big issue that we will have to contend with for a long time to come
1849.0,2.0,"<p>Sometimes I understand that people doing <em>cognitive science</em> try to avoid the term <em>artificial intelligence</em>. The feeling I get is that there is a need to put some distance to the <a href=""https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence"" rel=""nofollow"">GOFAI</a>.</p>

<p>Another impression that I get is that <em>cognitive science</em> is more about trying to find out how the human <em>intelligence</em>(?)... <em>Mind</em>? works... And that it would use <em>artificial intelligence</em> to make tests or experiments, to test ideas and so forth...</p>

<p>Is Artificial Intelligence (only) a research tool for Cognitive Science?</p>

<p><strong>What is the difference between Artificial Intelligence and Cognitive Science?</strong></p>
",,0,2016-09-03T21:10:31.007,,1847,2016-09-04T02:18:43.183,,,,,70.0,,1,2,<terminology><cognitive-science>,What is the difference between Artificial Intelligence and Cognitive Science?,66.0,47.49,12.69,8.69,0.0,0.0,20.0,Sometimes I understand that people doing cognitive science try to avoid the term artificial intelligence The feeling I get is that there is a need to put some distance to the GOFAI Another impression that I get is that cognitive science is more about trying to find out how the human intelligence Mind works And that it would use artificial intelligence to make tests or experiments to test ideas and so forth Is Artificial Intelligence only a research tool for Cognitive Science What is the difference between Artificial Intelligence and Cognitive Science
,,"<blockquote>
  <p>What purpose would be served by developing AI's that experience
  human-like emotions?</p>
</blockquote>

<p>Any complex problem involving human emotions, where the solution to the problem requires an ability to sympathize with the emotional states of human beings, will be most efficiently served by an agent that <em>can</em> sympathize with human emotions.</p>

<p>Politics. Government. Policy and planning. Unless the thing has intimate knowledge of the human experience, it won't be able to provide definitive answers to all problems we encounter in our human experience.</p>
",,0,2016-09-03T21:39:22.097,,1848,2016-09-03T21:39:22.097,,,,,1712.0,1700.0,2,0,,,,33.75,14.85,11.59,0.0,0.0,12.0,What purpose would be served by developing AIs that experience humanlike emotions Any complex problem involving human emotions where the solution to the problem requires an ability to sympathize with the emotional states of human beings will be most efficiently served by an agent that can sympathize with human emotions Politics Government Policy and planning Unless the thing has intimate knowledge of the human experience it wont be able to provide definitive answers to all problems we encounter in our human experience
,,"<blockquote>
  <p>Another impression that I get is that cognitive science is more about trying to find out how the human intelligence(?)... Mind? works... And that it would use artificial intelligence to make tests or experiments, to test ideas and so forth...</p>
</blockquote>

<p>I think that's pretty much it.  I mean, clearly there is some overlap, but I feel like most people who use ""cognitive science"" are referring more to understanding human intelligence for its own sake.  Artificial Intelligence, OTOH, is more about <em>implementing</em> ""intelligence"" on a computer, where the techniques used may or may not be influenced by research done under the rubric of cognitive science.</p>
",,0,2016-09-03T22:21:58.003,,1849,2016-09-03T22:21:58.003,,,,,33.0,1847.0,2,3,,,,41.9,12.65,9.07,0.0,0.0,27.0,Another impression that I get is that cognitive science is more about trying to find out how the human intelligence Mind works And that it would use artificial intelligence to make tests or experiments to test ideas and so forth I think thats pretty much it I mean clearly there is some overlap but I feel like most people who use cognitive science are referring more to understanding human intelligence for its own sake Artificial Intelligence OTOH is more about implementing intelligence on a computer where the techniques used may or may not be influenced by research done under the rubric of cognitive science
,,"<p>Artificial intelligence is much more than a research tool for cognitive science. Of course there is some overlapping and researchers of both fields working together. But AI is also broadly used in economics, security (for example face recognition software), advertising, or in the development of games and of course in robotics (autonomous systems). </p>

<p>The difference is - as you already mentioned - that cognitive science deals with living things while AI tries to create an intelligence artificially (AI tries to deliver the brain, the mind or the consciousness for a hardware device that then hopefully solves various problems). </p>
",,0,2016-09-04T02:18:43.183,,1850,2016-09-04T02:18:43.183,,,,,1781.0,1847.0,2,2,,,,30.2,15.09,10.75,0.0,0.0,16.0,Artificial intelligence is much more than a research tool for cognitive science Of course there is some overlapping and researchers of both fields working together But AI is also broadly used in economics security for example face recognition software advertising or in the development of games and of course in robotics autonomous systems The difference is as you already mentioned that cognitive science deals with living things while AI tries to create an intelligence artificially AI tries to deliver the brain the mind or the consciousness for a hardware device that then hopefully solves various problems
,0.0,"<p>Just for fun, I am trying to develop a neural network.</p>

<p>Now, for backpropagation I saw two techniques.</p>

<p>The first one is used <a href=""http://courses.cs.washington.edu/courses/cse599/01wi/admin/Assignments/bpn.html"" rel=""nofollow noreferrer"">here</a> and in many other places too.</p>

<p>What it does is:</p>

<ul>
<li>It computes the error for each output neuron.</li>
<li>It backpropagates it into the network (calculating an error for each inner neuron).</li>
<li>It updates the weights with the formula: <img src=""https://latex.codecogs.com/gif.latex?%5CDelta%20w_%7Bl%2Cm%2Cn%7D%20%3D%20k%20%5Ccdot%20E_%7Bl&plus;1%2Cn%7D%20%5Ccdot%20N_%7Bl%2Cm%7D"" alt=""""> (where <img src=""https://latex.codecogs.com/gif.latex?%5CDelta%20w_%7Bl%2Cm%2Cn%7D"" alt=""""> is the change in weight, <img src=""https://latex.codecogs.com/gif.latex?k"" alt=""""> the learning speed, <img src=""https://latex.codecogs.com/gif.latex?E_%7Bl&plus;1%2Cn%7D"" alt=""""> the error of the neuron receiving the input from the synapse and <img src=""https://latex.codecogs.com/gif.latex?N_%7Bl%2Cm%7D"" alt=""""> being the output sent on the synapse).</li>
<li>It repeats for each entry of the dataset, as many times as required.</li>
</ul>

<p>However, the neural network proposed in <a href=""https://www.youtube.com/watch?v=bxe2T-V8XRs&amp;list=PL77aoaxdgEVDrHoFOMKTjDdsa0p9iVtsR"" rel=""nofollow noreferrer"">this tutorial</a> (also available on GitHub) uses a different technique:</p>

<ul>
<li>It uses an error function (the other method does have an error function, but it does not use it for training).</li>
<li>It has another function which can compute the final error starting from the weights.</li>
<li>It minimizes that function (through gradient descent).</li>
</ul>

<p>Now, which method should be used?</p>

<p>I think the first one is the most used one (because I saw different examples using it), but does it work as well?</p>

<p>In particular, I don't know:</p>

<ul>
<li>Isn't it more subject to local minimums (since it doesn't use quadratic functions)?</li>
<li>Since the variation of each weight is influenced by the output value of its output neuron, don't entries of the dataset which just happen to produce higher values in the neurons (not just the output ones) influence the weights more than other entries?</li>
</ul>

<p>Now, I do prefer the first technique, because I find it simpler to implement and easier to think about.</p>

<p>Though, if it does have the problems I mentioned (which I hope it doesn't), is there any actual reason to use it over the second method?</p>
",2016-09-06T13:05:04.277,6,2016-09-04T09:45:02.357,2.0,1851,2016-09-06T15:21:36.320,2017-03-10T09:42:35.487,,-1.0,,2143.0,,1,8,<neural-networks><machine-learning><backpropagation>,Differences between backpropagation techniques,98.0,69.62,9.98,9.36,0.0,0.0,58.0,Just for fun I am trying to develop a neural network Now for backpropagation I saw two techniques The first one is used here and in many other places too What it does is It computes the error for each output neuron It backpropagates it into the network calculating an error for each inner neuron It updates the weights with the formula where is the change in weight the learning speed the error of the neuron receiving the input from the synapse and being the output sent on the synapse It repeats for each entry of the dataset as many times as required However the neural network proposed in this tutorial also available on GitHub uses a different technique It uses an error function the other method does have an error function but it does not use it for training It has another function which can compute the final error starting from the weights It minimizes that function through gradient descent Now which method should be used I think the first one is the most used one because I saw different examples using it but does it work as well In particular I dont know Isnt it more subject to local minimums since it doesnt use quadratic functions Since the variation of each weight is influenced by the output value of its output neuron dont entries of the dataset which just happen to produce higher values in the neurons not just the output ones influence the weights more than other entries Now I do prefer the first technique because I find it simpler to implement and easier to think about Though if it does have the problems I mentioned which I hope it doesnt is there any actual reason to use it over the second method
,,"<p>I think that depends on the application of the AI. Obviously if I develop an AI that's purpose is plainly to do specific task under the supervision of humans, there is no need for emotions. But if the AI's purpose is to do task autonomously, then emotions or empathy can be useful. For example, think about an AI that is working in the medical domain. Here it may be advantageous for an AI to have some kind of empathy, just to make the patients more comfortable. Or as another example, think about a robot that serves as a nanny. Again it is obvious that emotions and empathy would be advantageous and desirable. Even for an assisting AI program (catchword smart home) emotions and empathy can be desirable to make people more comfortable. It would be much nicer to be welcomed by an empathic home assistant than by one with no empathic responses at all, wouldn't it? </p>

<p>On the other hand, if the AI is just working on an assembly line, there is obviously no need for emotions and empathy (on the contrary in that case it may be unprofitable). </p>
",,0,2016-09-04T12:20:31.833,,1852,2016-09-04T12:20:31.833,,,,,1781.0,1700.0,2,1,,,,60.75,8.88,8.5,0.0,0.0,25.0,I think that depends on the application of the AI Obviously if I develop an AI thats purpose is plainly to do specific task under the supervision of humans there is no need for emotions But if the AIs purpose is to do task autonomously then emotions or empathy can be useful For example think about an AI that is working in the medical domain Here it may be advantageous for an AI to have some kind of empathy just to make the patients more comfortable Or as another example think about a robot that serves as a nanny Again it is obvious that emotions and empathy would be advantageous and desirable Even for an assisting AI program catchword smart home emotions and empathy can be desirable to make people more comfortable It would be much nicer to be welcomed by an empathic home assistant than by one with no empathic responses at all wouldnt it On the other hand if the AI is just working on an assembly line there is obviously no need for emotions and empathy on the contrary in that case it may be unprofitable
1855.0,3.0,"<p>The English Language is not well-suited to talking about artificial intelligence, which makes it difficult for humans to communicate to each other about what an AI is actually ""doing"". Thus, it may make more sense to use ""human-like"" terms to describe the actions of machinery, even when the internal properties of the machinery do not resemble the internal properties of humanity.</p>

<p>Anthropomorphic language had been used a lot in technology (see the Hacker's Dictionary definition of <a href=""https://www.landley.net/history/mirror/jargon.html#Anthropomorphization"">anthropomorphization</a>, which attempts to justify computer programmers' use of anthromporhic terms when describing technology), but as AI continues to advance, it may be useful to consider the tradeoffs of using anthropomorphic language in communicating to both technical audiences and non-technical audiences. How can we get a good handle on AI if we can't even describe what we're doing?</p>

<p>Suppose I want to develop an algorithm that display a list of related articles. There are two ways by which I can explain how the algorithm works to a layman:</p>

<ol>
<li><em>Very Anthropomorphic</em> - The algorithm reads all the articles on a website, and display the articles that are very similar to the article you are looking at.</li>
<li><em>Very Technical</em> - The algorithm converts each article into a ""bag-of-words"", and then compare the ""bag-of-words"" of each article to determine what articles share the most common words. The articles that share the most words in the bags are the ones that are displayed to the user.</li>
</ol>

<p>Obviously, #2 may be more ""technically correct"" than #1. By detailing the implementation of the algorithm, it makes it easier for someone to understand how to <em>fix</em> the algorithm if it produces an output that we disagree with heavily.</p>

<p>But #1 is more readable, elegant, and easier to understand. It provides a general sense of <em>what</em> the algorithm is doing, instead of <em>how</em> the algorithm is doing it. By abstracting away the implementation details of how a computer ""reads"" the article, we can then focus on using the algorithm in real-world scenarios.</p>

<p>Should I, therefore, prefer to use the anthropomorphic language as emphasized by Statement #1? If not, why not?</p>

<p>P.S.: If the answer depends on the audience that I am speaking to (a non-technical audience might prefer #1, while a technical audience may prefer #2), then let me know that as well.</p>
",,0,2016-09-04T14:06:54.923,,1853,2016-09-05T11:41:44.290,,,,,181.0,,1,9,<philosophy>,Should I use anthropomorphic language when discussing AI?,163.0,47.42,12.54,9.48,0.0,0.0,76.0,The English Language is not wellsuited to talking about artificial intelligence which makes it difficult for humans to communicate to each other about what an AI is actually doing Thus it may make more sense to use humanlike terms to describe the actions of machinery even when the internal properties of the machinery do not resemble the internal properties of humanity Anthropomorphic language had been used a lot in technology see the Hackers Dictionary definition of anthropomorphization which attempts to justify computer programmers use of anthromporhic terms when describing technology but as AI continues to advance it may be useful to consider the tradeoffs of using anthropomorphic language in communicating to both technical audiences and nontechnical audiences How can we get a good handle on AI if we cant even describe what were doing Suppose I want to develop an algorithm that display a list of related articles There are two ways by which I can explain how the algorithm works to a layman Very Anthropomorphic The algorithm reads all the articles on a website and display the articles that are very similar to the article you are looking at Very Technical The algorithm converts each article into a bagofwords and then compare the bagofwords of each article to determine what articles share the most common words The articles that share the most words in the bags are the ones that are displayed to the user Obviously 2 may be more technically correct than 1 By detailing the implementation of the algorithm it makes it easier for someone to understand how to fix the algorithm if it produces an output that we disagree with heavily But 1 is more readable elegant and easier to understand It provides a general sense of what the algorithm is doing instead of how the algorithm is doing it By abstracting away the implementation details of how a computer reads the article we can then focus on using the algorithm in realworld scenarios Should I therefore prefer to use the anthropomorphic language as emphasized by Statement 1 If not why not PS If the answer depends on the audience that I am speaking to a nontechnical audience might prefer 1 while a technical audience may prefer 2 then let me know that as well
,,"<h2><a href=""https://en.wikipedia.org/wiki/Theory_of_mind"" rel=""nofollow"">Theory of mind</a></h2>

<p>If we want a strong general AI to function well in an environment that consists of humans, then it would be very useful for it to have a good <a href=""https://en.wikipedia.org/wiki/Theory_of_mind"" rel=""nofollow"">theory of mind</a> that matches how humans actually behave. That theory of mind needs to include human-like emotions, or it will not match the reality of this environment.</p>

<p>For us, an often used shortcut is explicitly thinking ""what would I have done in this situation?"" ""what event could have motivated <em>me</em> to do what they just did?"" ""how would I feel if this had happened to <em>me</em>?"". We'd want an AI to be capable of such reasoning, it is practical and useful, it allows better predictions of future and more effective actions. </p>

<p>Even while it would be better for it the AI to not be actually driven by those exact emotions (perhaps something in that direction would be useful but quite likely not <em>exactly</em> the same), all it changes that instead of thinking ""what <em>I</em> would feel"" it should be able to hypothesize what a generic human would feel. That requires implementing a subsystem that is capable of accurately modeling human emotions.</p>
",,0,2016-09-04T14:51:44.747,,1854,2016-09-04T14:51:44.747,,,,,1675.0,1700.0,2,0,,,,55.37,9.64,9.64,0.0,0.0,27.0,Theory of mind If we want a strong general AI to function well in an environment that consists of humans then it would be very useful for it to have a good theory of mind that matches how humans actually behave That theory of mind needs to include humanlike emotions or it will not match the reality of this environment For us an often used shortcut is explicitly thinking what would I have done in this situation what event could have motivated me to do what they just did how would I feel if this had happened to me Wed want an AI to be capable of such reasoning it is practical and useful it allows better predictions of future and more effective actions Even while it would be better for it the AI to not be actually driven by those exact emotions perhaps something in that direction would be useful but quite likely not exactly the same all it changes that instead of thinking what I would feel it should be able to hypothesize what a generic human would feel That requires implementing a subsystem that is capable of accurately modeling human emotions
,,"<p>If clarity is your goal, you should attempt to avoid anthropomorphic language - doing so runs a danger of even misleading <em>yourself</em> about the capabilities of the program.</p>

<p>This is a pernicious trap in AI research, with numerous cases where even experienced researchers have ascribed a greater degree of understanding to a program than is actually merited.</p>

<p>Douglas Hofstadter describes the issue at some length in a chapter entitled <a href=""https://en.wikipedia.org/wiki/ELIZA_effect"">""The Ineradicable Eliza Effect and Its Dangers""</a> and there is also a famous paper by Drew McDermot, entitled <a href=""https://www.inf.ed.ac.uk/teaching/courses/irm/mcdermott.pdf"">""Artifical Intelligence meets natural stupidity""</a>.</p>

<p>Hence, in general one should make particular effort to avoid anthropomorphism in AI. However, when speaking to a non-technical audience, 'soundbite' descriptions are (as in any complex discipline) acceptable <em>provided you let the audience know that they are getting the simplified version</em>.</p>
",,0,2016-09-04T17:37:05.977,,1855,2016-09-04T18:36:24.090,2016-09-04T18:36:24.090,,42.0,,42.0,1853.0,2,7,,,,27.56,15.44,12.08,0.0,0.0,21.0,If clarity is your goal you should attempt to avoid anthropomorphic language doing so runs a danger of even misleading yourself about the capabilities of the program This is a pernicious trap in AI research with numerous cases where even experienced researchers have ascribed a greater degree of understanding to a program than is actually merited Douglas Hofstadter describes the issue at some length in a chapter entitled The Ineradicable Eliza Effect and Its Dangers and there is also a famous paper by Drew McDermot entitled Artifical Intelligence meets natural stupidity Hence in general one should make particular effort to avoid anthropomorphism in AI However when speaking to a nontechnical audience soundbite descriptions are as in any complex discipline acceptable provided you let the audience know that they are getting the simplified version
,,"<p>I think the correct answer is the easy but unhelpful, ""It depends.""</p>

<p>Even when I'm talking to other technical people, I often use anthropomorphic language and metaphors. Especially at the start of the conversation. ""The computer has to figure out .."" ""How can we prevent the computer from getting confused about ..."" etc. Sure, we could state that in a more technically correct way. ""We need to modify the algorithm to reduce the number and variety of instances of inadequate data that result in inaccurate setting of ..."" or some such. But among technical people, we know what we mean, and it's just easier to use metaphorical language.</p>

<p>When trying to solve technical computer problems, I often start with a vague, anthropomorphic concept. ""We should make a list of all the words in the text, and assign each word a weight based on how frequently it occurs. Oh, but we should ignore short, common words like 'the' and 'it'. Then let's pick some number of words, maybe ten or so, that have the greatest weight ..."" All that is a long way from how the computer actually manipulates data. But it's often a lot easier to think about it in ""human"" terms first, and then figure out how to make the computer do it.</p>

<p>When talking to a non-technical audience, I think the issue is, Anthropomorphic language makes it easier to understand, but also often gives the impression that the computer is much more human-like than it really is. You only need to watch science fiction movies to see that apparently a lot of people think that a computer or a robot thinks just like a person except that it's very precise and has no emotions.</p>
",,0,2016-09-04T23:50:19.347,,1856,2016-09-04T23:50:19.347,,,,,2171.0,1853.0,2,1,,,,63.09,9.86,8.49,0.0,0.0,64.0,I think the correct answer is the easy but unhelpful It depends Even when Im talking to other technical people I often use anthropomorphic language and metaphors Especially at the start of the conversation The computer has to figure out How can we prevent the computer from getting confused about etc Sure we could state that in a more technically correct way We need to modify the algorithm to reduce the number and variety of instances of inadequate data that result in inaccurate setting of or some such But among technical people we know what we mean and its just easier to use metaphorical language When trying to solve technical computer problems I often start with a vague anthropomorphic concept We should make a list of all the words in the text and assign each word a weight based on how frequently it occurs Oh but we should ignore short common words like the and it Then lets pick some number of words maybe ten or so that have the greatest weight All that is a long way from how the computer actually manipulates data But its often a lot easier to think about it in human terms first and then figure out how to make the computer do it When talking to a nontechnical audience I think the issue is Anthropomorphic language makes it easier to understand but also often gives the impression that the computer is much more humanlike than it really is You only need to watch science fiction movies to see that apparently a lot of people think that a computer or a robot thinks just like a person except that its very precise and has no emotions
,,"<p>The problem you're referencing is not just an AI problem but a problem for highly technical fields in general. When in doubt, I would always recommend using <a href=""http://plainlanguagenetwork.org/plain-language/what-is-plain-language/"" rel=""nofollow"">plain language</a>.</p>

<p>However, there is another reason the AI community will often eschew anthropomorphic connotations for AI. Some AI luminaries often like warning us that an artificial <em>general</em> intelligence may behave in alien ways that defy our human expectations, potentially leading to a robot apocalypse. </p>

<p>This idea about evil alien-like AGIs, however, derives from a widespread misunderstanding in the AI community that conflates two different notions of generality: </p>

<ul>
<li>Turing machine generality, and </li>
<li>human domain generality </li>
</ul>

<p>What regular people mean when they say generality is the later. Even the <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"" rel=""nofollow"">official definition</a> of AGI hinges off of that human-contingent context:</p>

<blockquote>
  <p>...perform any intellectual task that a human being can.</p>
</blockquote>

<p>But by that definition, generalizing behavior does not make it more alien. To generalize is to anthropomorphize. As Nietzche said, </p>

<blockquote>
  <p>""Where you see ideal things, I see— human, alas! All too human things.”</p>
</blockquote>
",,0,2016-09-05T11:35:17.787,,1857,2016-09-05T11:41:44.290,2016-09-05T11:41:44.290,,1712.0,,1712.0,1853.0,2,3,,,,39.13,13.97,11.1,0.0,0.0,29.0,The problem youre referencing is not just an AI problem but a problem for highly technical fields in general When in doubt I would always recommend using plain language However there is another reason the AI community will often eschew anthropomorphic connotations for AI Some AI luminaries often like warning us that an artificial general intelligence may behave in alien ways that defy our human expectations potentially leading to a robot apocalypse This idea about evil alienlike AGIs however derives from a widespread misunderstanding in the AI community that conflates two different notions of generality Turing machine generality and human domain generality What regular people mean when they say generality is the later Even the official definition of AGI hinges off of that humancontingent context perform any intellectual task that a human being can But by that definition generalizing behavior does not make it more alien To generalize is to anthropomorphize As Nietzche said Where you see ideal things I see— human alas All too human things”
,0.0,"<p>Roger Schank did some interesting work on language processing with Conceptual Dependency (CD) in the 1970s. He then moved somewhat out of the field, being in Education these days. There were some useful applications in natural language generation (BABEL), story generation (TAILSPIN) and other areas, often involving planning and episodes rather than individual sentences.</p>

<p>Has anybody else continued to use CD or variants thereof? I am not aware of any other projects that do, apart from Hovy's PAULINE which uses CD as representation for the story to generate.</p>
",,1,2016-09-05T13:46:57.910,3.0,1859,2016-09-05T15:07:24.223,2016-09-05T15:07:24.223,,145.0,,2193.0,,1,5,<nlp><knowledge-representation>,Is anybody still using Conceptual Dependency Theory?,56.0,45.15,13.69,11.33,0.0,0.0,16.0,Roger Schank did some interesting work on language processing with Conceptual Dependency CD in the 1970s He then moved somewhat out of the field being in Education these days There were some useful applications in natural language generation BABEL story generation TAILSPIN and other areas often involving planning and episodes rather than individual sentences Has anybody else continued to use CD or variants thereof I am not aware of any other projects that do apart from Hovys PAULINE which uses CD as representation for the story to generate
,1.0,"<p>I have been wanting to get started learning about artificial intelligence but I know almost nothing about coding or anything. So my question is, what would be the best way to get started in learning about artificial intelligence, as in should I learn some kind of coding language or is there some kind of other concept you need to know before getting started. So I'm just kind of looking for the best way to get started if you literally know nothing.</p>
",2016-09-06T00:04:56.550,1,2016-09-05T18:36:11.963,,1860,2016-09-05T19:58:56.570,,,,,2204.0,,1,2,<research>,How to start learning about artificial intelligence?,90.0,60.99,9.52,9.07,0.0,0.0,6.0,I have been wanting to get started learning about artificial intelligence but I know almost nothing about coding or anything So my question is what would be the best way to get started in learning about artificial intelligence as in should I learn some kind of coding language or is there some kind of other concept you need to know before getting started So Im just kind of looking for the best way to get started if you literally know nothing
,,"<p>Read:</p>

<p>'Artificial Intelligence - A modern approach' by Russell and Norvig.</p>

<p>'Fluid Concepts and Creative Analogies' by Douglas Hofstadter.</p>

<p>'Machine Learning and Pattern Recognition' by Bishop</p>

<p>'The Emotion Machine' by Marvin Minsky</p>
",,0,2016-09-05T19:58:56.570,,1861,2016-09-05T19:58:56.570,,,,,42.0,1860.0,2,3,,,,35.64,18.3,14.33,0.0,0.0,12.0,Read Artificial Intelligence A modern approach by Russell and Norvig Fluid Concepts and Creative Analogies by Douglas Hofstadter Machine Learning and Pattern Recognition by Bishop The Emotion Machine by Marvin Minsky
,,"<p>Using evolutionary algorithms to evolve neural networks is called <a href=""http://www.scholarpedia.org/article/Neuroevolution"" rel=""nofollow"">neuroevolution</a>. </p>

<p>Some neuroevolution algorithms optimize only the <em>weights</em> of a neural network with fixed topology. That sounds not like what you want. Other neuroevolution algorithms optimize both the <em>weights</em> and <em>topology</em> of a neural net. These kinds of algorithms seem more appropriate for your aims, and are sometimes called TWEANNs (Topology and Weight Evolving Neural Networks).</p>

<p>One popular algorithm is called <a href=""https://www.cs.ucf.edu/~kstanley/neat.html"" rel=""nofollow"">NEAT</a>, and is probably a good place to start, if only because there are a multitude of implementations, one of which hopefully is written in your favorite language. That would at least give you a baseline to work with. </p>

<p>NEAT encodes a neural network genome directly as a graph structure. Mutations can operate on the structure of the network by adding new links (by connecting two nodes not previously connected) or new nodes (by splitting an existing connection), or can operate only on changing the weights associated with edges in the graphs (called mutating the weights). 
To give you an idea of the order of magnitude of the sizes of ANNs this particular algorithm works with, it would likely struggle with more than 100 or 200 nodes. </p>

<p>There are more scalable TWEANNs, but they're more complex and make assumptions about the kinds of structures they generate that may not always be productive in practice. For example, another way to encode the structure of a neural network, is as the product of a seed pattern that is repeatedly expanded by a grammar (e.g. an L-system). You can much more easily explore larger structures, but because they're generated by a grammar they'll have a characteristic self-repeating sort of feel. HyperNEAT is a popular extension of NEAT that makes a different sort of assumption (that patterns of weights can be easily expressed as a function of geometry), and can scale to ANNs with millions of connections when that assumption well-fits a particular domain.</p>

<p>There are a few survey papers linked in the top link if you want to observe a greater variety of techniques.</p>
",,0,2016-09-06T06:25:05.383,,1862,2016-09-06T06:25:05.383,,,,,2219.0,1618.0,2,3,,,,48.43,12.6,9.76,0.0,0.0,46.0,Using evolutionary algorithms to evolve neural networks is called neuroevolution Some neuroevolution algorithms optimize only the weights of a neural network with fixed topology That sounds not like what you want Other neuroevolution algorithms optimize both the weights and topology of a neural net These kinds of algorithms seem more appropriate for your aims and are sometimes called TWEANNs Topology and Weight Evolving Neural Networks One popular algorithm is called NEAT and is probably a good place to start if only because there are a multitude of implementations one of which hopefully is written in your favorite language That would at least give you a baseline to work with NEAT encodes a neural network genome directly as a graph structure Mutations can operate on the structure of the network by adding new links by connecting two nodes not previously connected or new nodes by splitting an existing connection or can operate only on changing the weights associated with edges in the graphs called mutating the weights To give you an idea of the order of magnitude of the sizes of ANNs this particular algorithm works with it would likely struggle with more than 100 or 200 nodes There are more scalable TWEANNs but theyre more complex and make assumptions about the kinds of structures they generate that may not always be productive in practice For example another way to encode the structure of a neural network is as the product of a seed pattern that is repeatedly expanded by a grammar eg an Lsystem You can much more easily explore larger structures but because theyre generated by a grammar theyll have a characteristic selfrepeating sort of feel HyperNEAT is a popular extension of NEAT that makes a different sort of assumption that patterns of weights can be easily expressed as a function of geometry and can scale to ANNs with millions of connections when that assumption wellfits a particular domain There are a few survey papers linked in the top link if you want to observe a greater variety of techniques
,,"<p>Human emotions are intricately connected to human values and to our ability to cooperate and form societies. </p>

<p>Just to give an easy example:
You meet a stranger who needs help, you feel <strong>empathy</strong>. 
This compels you to help him at a cost to yourself. 
Let's assume the next time you meet him, you need something. Let's also assume he doesn't help you, you'll feel <strong>anger</strong>. 
This emotion compels you to punish him, at further cost for yourself.
He on the other hand, if he doesn't help you, feels <strong>shame</strong>.
This compels him to actually help you, avoiding your anger and making your initial investment worthwhile. You both benefit. </p>

<p>So these three emotions keep up a circle of reciprocal help. Empathy to get started, anger to punish defectors and shame to avoid the anger. This also leads to a concept of justice. </p>

<p>Given that value alignment is one of the big problems in AGI, human-like emotions strike me as good approach towards AIs that actually share our values and integrate themselves seamlessly into our society. </p>
",,1,2016-09-06T11:18:01.147,,1863,2016-09-06T11:18:01.147,,,,,2227.0,1700.0,2,0,,,,66.33,9.85,8.93,0.0,0.0,29.0,Human emotions are intricately connected to human values and to our ability to cooperate and form societies Just to give an easy example You meet a stranger who needs help you feel empathy This compels you to help him at a cost to yourself Lets assume the next time you meet him you need something Lets also assume he doesnt help you youll feel anger This emotion compels you to punish him at further cost for yourself He on the other hand if he doesnt help you feels shame This compels him to actually help you avoiding your anger and making your initial investment worthwhile You both benefit So these three emotions keep up a circle of reciprocal help Empathy to get started anger to punish defectors and shame to avoid the anger This also leads to a concept of justice Given that value alignment is one of the big problems in AGI humanlike emotions strike me as good approach towards AIs that actually share our values and integrate themselves seamlessly into our society
,,"<p>Because, ultimately, AI <em>is</em> a cohesive ""thing"".  It's an effort to make computers do things that currently only humans can do well.  Sure there are many, many approaches and techniques, but there's always been a clear overall goal (although the goal-posts keep getting moved further out, which is a different issue).</p>

<p>As long as there are things humans can do well that computers can't, somebody will be trying to figure out how to close that gap. And those efforts are ""Artificial Intelligence"".</p>
",,1,2016-09-06T15:08:23.843,,1864,2016-09-06T15:08:23.843,,,,,33.0,1838.0,2,0,,,,63.29,11.48,9.46,0.0,0.0,21.0,Because ultimately AI is a cohesive thing Its an effort to make computers do things that currently only humans can do well Sure there are many many approaches and techniques but theres always been a clear overall goal although the goalposts keep getting moved further out which is a different issue As long as there are things humans can do well that computers cant somebody will be trying to figure out how to close that gap And those efforts are Artificial Intelligence
,,"<p>One of them is certainly <a href=""http://www.alicebot.org/bios/richardwallace.html"" rel=""nofollow"">Doctor Richard Wallace</a>.  Doctor Wallace was the original author of the <a href=""https://en.wikipedia.org/wiki/AIML"" rel=""nofollow"">Artificial Intelligence Markup Language</a> spec and is Chief Science Officer at PandoraBots. </p>
",,0,2016-09-06T15:14:43.980,,1865,2016-09-06T15:14:43.980,,,,,33.0,1825.0,2,1,,,,31.89,14.67,11.66,0.0,0.0,2.0,One of them is certainly Doctor Richard Wallace Doctor Wallace was the original author of the Artificial Intelligence Markup Language spec and is Chief Science Officer at PandoraBots
,,"<p>I believe this is exactly the kind of test where Doug Lenat's <strong><a href=""https://en.wikipedia.org/wiki/Cyc"" rel=""nofollow"">cyc</a></strong> would do very well at ? But I can't answer the question : how much of that corpus could it answer correctly ? Probably quite a lot ! (and how many humans could pass that test ? probably not all of them, but many can...)  </p>

<p>[but is cyc considered an AI? probably not... so I may be out of topic. But imo it's database should be incorporated to any AI that reaches some kind of ""intelligence""...]</p>
",,0,2016-09-06T15:23:41.650,,1866,2016-09-06T15:23:41.650,,,,,2233.0,1397.0,2,1,,,,77.74,6.7,7.92,0.0,0.0,26.0,I believe this is exactly the kind of test where Doug Lenats cyc would do very well at But I cant answer the question how much of that corpus could it answer correctly Probably quite a lot and how many humans could pass that test probably not all of them but many can but is cyc considered an AI probably not so I may be out of topic But imo its database should be incorporated to any AI that reaches some kind of intelligence
,,"<p>Take a look at <a href=""http://dl.acm.org/citation.cfm?id=2390143"" rel=""nofollow"">this 2012 paper</a> by three people at Bright. (Sadly, it's paywalled and I couldn't easily find a ungated copy, so I don't have a summary for you.) </p>

<p>The abstract:</p>

<blockquote>
  <p>Bright has built an automated system for ranking job candidates against job descriptions. The candidate's resume and social media profiles are interwoven to build an augmented user profile. Similarly, the job description is augmented by external databases and user-generated content to build an enhanced job profile. These augmented user and job profiles are then analyzed in order to develop numerical overlap features each with strong discriminating power, and in sum with maximal coverage. The resulting feature scores are then combined into a single Bright Score using a custom algorithm, where the feature weights are derived from a nation-wide and controlled study in which we collected a large sample of human judgments on real resume-job pairings. We demonstrate that the addition of social media profile data and external data improves the classification accuracy dramatically in terms of identifying the most qualified candidates.</p>
</blockquote>
",,0,2016-09-06T17:31:44.187,,1868,2016-09-06T17:31:44.187,,,,,10.0,1686.0,2,2,,,,32.43,14.1,11.16,0.0,0.0,23.0,Take a look at this 2012 paper by three people at Bright Sadly its paywalled and I couldnt easily find a ungated copy so I dont have a summary for you The abstract Bright has built an automated system for ranking job candidates against job descriptions The candidates resume and social media profiles are interwoven to build an augmented user profile Similarly the job description is augmented by external databases and usergenerated content to build an enhanced job profile These augmented user and job profiles are then analyzed in order to develop numerical overlap features each with strong discriminating power and in sum with maximal coverage The resulting feature scores are then combined into a single Bright Score using a custom algorithm where the feature weights are derived from a nationwide and controlled study in which we collected a large sample of human judgments on real resumejob pairings We demonstrate that the addition of social media profile data and external data improves the classification accuracy dramatically in terms of identifying the most qualified candidates
,2.0,"<p>I have been studying local search algorithms such as greedy hill climbing, stochastic hill climbing, simulated annealing etc. I have noticed that most of these methods take up very little memory as compared to systematic search techniques.</p>

<p>Are there local search algorithms that make use of memory to give significantly better answers than those algorithms that use little memory (such as crossing local maxima)? Also, is there a way to combine local search and systematic search algorithms to get the best of both worlds?</p>
",,0,2016-09-06T23:13:08.677,,1870,2016-09-07T03:04:19.570,2016-09-07T02:40:20.397,,135.0,,2244.0,,1,5,<search>,Memory intensive local search methods,39.0,50.16,13.05,9.0,0.0,0.0,9.0,I have been studying local search algorithms such as greedy hill climbing stochastic hill climbing simulated annealing etc I have noticed that most of these methods take up very little memory as compared to systematic search techniques Are there local search algorithms that make use of memory to give significantly better answers than those algorithms that use little memory such as crossing local maxima Also is there a way to combine local search and systematic search algorithms to get the best of both worlds
,,"<p>I don't believe that AI as a coherent field has a lesser legitimacy than, say, Engineering.  Ignoring for the moment that we're a day or two behind on AI, they're very much alike: 
Both fields contain a wide variety of sub-fields which stretch across multiple disciplines (although admittedly more pronounced in AI) , in both fields it is mandatory to specialize and in both of them an expert in one sub-field will be more or less useless in a different one (the expert on bridge construction will probably not be very versed in the thermodynamics of AC systems and vice versa). This pattern can be seen in many of today's disciplines - in fact, I don't know if there still is a reputable field, in which a single person can be a universal expert. </p>

<p>You mentioned that the only unifying thing about AI was it's dealing with machines in some fashion - but such a simplifying statement can be made about almost any field. To return to my previous example: the only unifying thing about the various Engineering activities is that they're all somehow involved in the construction of something (be it a flashlight or an aircraft carrier).</p>

<p>AI is a young field and therefore its branches have not yet been established in the sophisticated way that the branches of other fields have, but I would assume that it is only a matter of time until the various differentiations and the corresponding degrees, courses etc. develop.  </p>

<p>AI is also growing up in a time where vast knowledge in its related/parental fields already exists and further knowledge is produced at dizzying speeds - and that is as much a blessing as it is a curse. When Engineering was 'created' a few millennia ago (please excuse my ridiculously inaccurate science history lessons) there wasn't much going on in the world of science and so the field grew slowly, with plenty of time to get organized and structured. That is a luxury which AI did/does not have. It emerged in an age of technical wonders, surrounded by scientific breakthroughs on at least a monthly basis and the rise of interdisciplinary science (which by itself complicated things quite a bit). So in addition to organizing itself, the field also has to continuously integrate the large number of advancements made and somehow stand its ground against the outlandish expectations generated by other science's breakthroughs over the past decades and the media (as already explained by SQLServerSteve). </p>

<p>Long story short: it's similar in it's complexity and diversity to other fields and therefore has no reason to collapse - on the contrary, its failure to do so over the past ~50 rather complicated years indicates, that it will further solidify and organize itself in the future. </p>
",,0,2016-09-06T23:21:40.240,,1871,2016-09-06T23:21:40.240,,,,,2166.0,1838.0,2,1,,,,33.11,11.91,10.38,0.0,0.0,63.0,I dont believe that AI as a coherent field has a lesser legitimacy than say Engineering Ignoring for the moment that were a day or two behind on AI theyre very much alike Both fields contain a wide variety of subfields which stretch across multiple disciplines although admittedly more pronounced in AI in both fields it is mandatory to specialize and in both of them an expert in one subfield will be more or less useless in a different one the expert on bridge construction will probably not be very versed in the thermodynamics of AC systems and vice versa This pattern can be seen in many of todays disciplines in fact I dont know if there still is a reputable field in which a single person can be a universal expert You mentioned that the only unifying thing about AI was its dealing with machines in some fashion but such a simplifying statement can be made about almost any field To return to my previous example the only unifying thing about the various Engineering activities is that theyre all somehow involved in the construction of something be it a flashlight or an aircraft carrier AI is a young field and therefore its branches have not yet been established in the sophisticated way that the branches of other fields have but I would assume that it is only a matter of time until the various differentiations and the corresponding degrees courses etc develop AI is also growing up in a time where vast knowledge in its relatedparental fields already exists and further knowledge is produced at dizzying speeds and that is as much a blessing as it is a curse When Engineering was created a few millennia ago please excuse my ridiculously inaccurate science history lessons there wasnt much going on in the world of science and so the field grew slowly with plenty of time to get organized and structured That is a luxury which AI diddoes not have It emerged in an age of technical wonders surrounded by scientific breakthroughs on at least a monthly basis and the rise of interdisciplinary science which by itself complicated things quite a bit So in addition to organizing itself the field also has to continuously integrate the large number of advancements made and somehow stand its ground against the outlandish expectations generated by other sciences breakthroughs over the past decades and the media as already explained by SQLServerSteve Long story short its similar in its complexity and diversity to other fields and therefore has no reason to collapse on the contrary its failure to do so over the past 50 rather complicated years indicates that it will further solidify and organize itself in the future
,,"<p>You could parallelize the search by dividing the global space in distinct regions/subsets. Then apply in each region a local search. This way you can search the global space systematically, more exhaustively and perhaps in different ways (e.g by applying a different local search method to each region). Finally you can compare the results and choose the best one.</p>
",,0,2016-09-06T23:41:01.203,,1872,2016-09-06T23:41:01.203,,,,,1781.0,1870.0,2,3,,,,59.5,11.93,9.04,0.0,0.0,9.0,You could parallelize the search by dividing the global space in distinct regionssubsets Then apply in each region a local search This way you can search the global space systematically more exhaustively and perhaps in different ways eg by applying a different local search method to each region Finally you can compare the results and choose the best one
,,"<p>We actually do have many things along that line, motion capture for 3-D movies instance comes to mind almost immediately. The problem if I think about it is less of a situation in observing another actor, computers are relativity good at doing that already with the amount of image recognition software we have, rather it's a problem of understanding if an action yielded a good outcome as a net which is something that computers cannot do as it's not a single node network problem. For example, we've already programmed a computer to understand human language (Watson, arguably), but even Watson didn't understand the concept that saying ""f***"" is bad. (Look that up, it's a funny side story.)</p>

<p>But the point is, learning algorithms are not true learning in a sense as a computer currently has no sense of ""a good outcome"", hence at this stage observation learning is very much limited in a sense to ""monkey see, monkey do"".</p>

<p>Perhaps the closest thing I have ever read about with this was firefighting search and rescue bots that were on a network and would broadcast to each other when one of them had been destroyed as the bots would know the area was something that they had to avoid.</p>

<p>Otherwise, I think this is the problem with observational learning. A person can observe that punching someone usually will get you hit back, a computer will observe and parrot the action, good or bad.</p>
",,0,2016-09-07T01:30:27.690,,1873,2016-10-14T13:08:35.790,2016-10-14T13:08:35.790,,75.0,,2246.0,156.0,2,-1,,,,49.28,10.8,9.96,0.0,0.0,40.0,We actually do have many things along that line motion capture for 3D movies instance comes to mind almost immediately The problem if I think about it is less of a situation in observing another actor computers are relativity good at doing that already with the amount of image recognition software we have rather its a problem of understanding if an action yielded a good outcome as a net which is something that computers cannot do as its not a single node network problem For example weve already programmed a computer to understand human language Watson arguably but even Watson didnt understand the concept that saying f is bad Look that up its a funny side story But the point is learning algorithms are not true learning in a sense as a computer currently has no sense of a good outcome hence at this stage observation learning is very much limited in a sense to monkey see monkey do Perhaps the closest thing I have ever read about with this was firefighting search and rescue bots that were on a network and would broadcast to each other when one of them had been destroyed as the bots would know the area was something that they had to avoid Otherwise I think this is the problem with observational learning A person can observe that punching someone usually will get you hit back a computer will observe and parrot the action good or bad
,,"<p><a href=""https://en.wikipedia.org/wiki/Tabu_search"" rel=""nofollow"">Tabu search</a> uses memory to rule out parts of the neighborhood for local search, allowing the trajectory to typically pass through local optima instead of getting stuck in them.</p>
",,0,2016-09-07T03:04:19.570,,1874,2016-09-07T03:04:19.570,,,,,10.0,1870.0,2,3,,,,42.04,12.89,11.61,0.0,0.0,2.0,Tabu search uses memory to rule out parts of the neighborhood for local search allowing the trajectory to typically pass through local optima instead of getting stuck in them
1880.0,2.0,"<p><em>I know that every program has some positive and negative points, and I know maybe .net programming languages are not the best for AI programming.</em></p>

<p><strong>But I prefer .net programming languages because of my experiences and would like to know for an AI program which one is better, C or C++ or C# and or VB ?</strong></p>

<p><em>Which one of this languages is faster and more stable when running different queries and for self learning ?</em></p>

<p>To make a summary, i think C++ is the best for AI programming in .net and also C# can be used in some projects, Python as recommended by others is not an option on my view !</p>

<p>because : </p>

<ol>
<li><p>It's not a complex language itself and for every single move you need to find a library and import it to your project (most of the library are out of date and or not working with new released Python versions) and that's why people say it is an easy language to learn and use ! (If you start to create library yourself, this language could be the hardest language in the world !)</p></li>
<li><p>You do not create a program yourself by using those library for every single option on your project (it's just like a Lego game)</p></li>
<li><p>I'm not so sure in this, but i think it's a cheap programming language because i couldn't find any good program created by this language !</p></li>
</ol>
",2016-09-08T06:13:48.690,3,2016-09-07T14:00:02.683,,1876,2016-09-13T08:05:06.463,2016-09-13T08:05:06.463,,2260.0,,2260.0,,1,0,<intelligent-agent>,What is the best .net programming language for artificial intelligence programming?,262.0,64.95,8.19,7.79,0.0,0.0,35.0,I know that every program has some positive and negative points and I know maybe net programming languages are not the best for AI programming But I prefer net programming languages because of my experiences and would like to know for an AI program which one is better C or C or C and or VB Which one of this languages is faster and more stable when running different queries and for self learning To make a summary i think C is the best for AI programming in net and also C can be used in some projects Python as recommended by others is not an option on my view because Its not a complex language itself and for every single move you need to find a library and import it to your project most of the library are out of date and or not working with new released Python versions and thats why people say it is an easy language to learn and use If you start to create library yourself this language could be the hardest language in the world You do not create a program yourself by using those library for every single option on your project its just like a Lego game Im not so sure in this but i think its a cheap programming language because i couldnt find any good program created by this language
1883.0,7.0,"<p>When I visit this site, I find the word ""search"" appears quite often. </p>

<p>But why is it important? What kinds of search algorithms are used in Artificial Intelligence?  And how do they improve the result of an AI?</p>
",,8,2016-09-07T14:16:36.477,2.0,1877,2016-09-08T23:56:01.660,2016-09-07T18:23:04.583,,33.0,,1270.0,,1,3,<search>,Why is searching important in AIs?,334.0,78.75,7.04,8.26,0.0,0.0,7.0,When I visit this site I find the word search appears quite often But why is it important What kinds of search algorithms are used in Artificial Intelligence And how do they improve the result of an AI
,,"<p>As tags go, search is relatively uncommon--if it weren't for this question, it wouldn't be on the first page of tags. That said, search is important for at least two reasons.</p>

<p>First, searching is one of the early and major consumers of advanced machine learning, as finding the correct result for a search query boils down to predicting the click-through rate for query-result combinations. More relevant results means more clicks, more traffic, and more revenue.</p>

<p>Second, many planning and optimization problems can be recast as search problems. An AI deciding on a plan to route packages through a network is searching the space of possible plans for a good one.</p>
",,2,2016-09-07T14:27:13.823,,1878,2016-09-07T14:27:13.823,,,,,10.0,1877.0,2,1,,,,61.36,11.89,9.71,0.0,0.0,20.0,As tags go search is relatively uncommonif it werent for this question it wouldnt be on the first page of tags That said search is important for at least two reasons First searching is one of the early and major consumers of advanced machine learning as finding the correct result for a search query boils down to predicting the clickthrough rate for queryresult combinations More relevant results means more clicks more traffic and more revenue Second many planning and optimization problems can be recast as search problems An AI deciding on a plan to route packages through a network is searching the space of possible plans for a good one
,,"<p>Convolutional neural network can be used whenever patterns are locally correlated and translatable (as in shiftable). This is the case because CNNs contain filters that looks for a certain local patterns everywhere in the input. 
You'll find local and translatable patterns in pictures, text, time series, etc.</p>

<p>It doesn't make as much sense to use CNNs if your data is more like a bag of features with an irrelevant order. In that case you might have trouble detecting patterns that contain features which happen to be farther apart in your input vector. You will not find local and translatable patterns in your data if you can reorder the data points of the input vectors without losing information. </p>
",,0,2016-09-07T14:56:38.193,,1879,2016-09-07T14:56:38.193,,,,,2227.0,70.0,2,0,,,,60.14,11.66,9.33,0.0,0.0,13.0,Convolutional neural network can be used whenever patterns are locally correlated and translatable as in shiftable This is the case because CNNs contain filters that looks for a certain local patterns everywhere in the input Youll find local and translatable patterns in pictures text time series etc It doesnt make as much sense to use CNNs if your data is more like a bag of features with an irrelevant order In that case you might have trouble detecting patterns that contain features which happen to be farther apart in your input vector You will not find local and translatable patterns in your data if you can reorder the data points of the input vectors without losing information
,,"<p>If you're talking about pure speed, C will get you there if you really know C and operating systems, etc. C++ is nicer in terms of user friendliness, and won't be much slower. I don't know much about VB but I don't see many benefits.</p>

<p>Please clarify at least generally what the AI program is about. It is extremely difficult to answer the generalized question ""What programming language is best for AI""? if you know what I mean. (  I vote python :^]  )</p>

<p>To add: Any programming language of those listed is ""stable"" if you write things correctly, but C++ and its great IDE's will help you to that point much more nicely than C will. C, to fully utilize it's potential, requires much fiddling with delicate and precise systems. It'll go that little bit faster, as the cost of being less stable in the practical, "" uh oh, now I need to troubleshoot this"" sense.</p>
",,6,2016-09-07T17:21:15.123,,1880,2016-09-07T22:07:28.870,2016-09-07T22:07:28.870,,1538.0,,1538.0,1876.0,2,1,,,,79.6,8.87,8.81,0.0,0.0,41.0,If youre talking about pure speed C will get you there if you really know C and operating systems etc C is nicer in terms of user friendliness and wont be much slower I dont know much about VB but I dont see many benefits Please clarify at least generally what the AI program is about It is extremely difficult to answer the generalized question What programming language is best for AI if you know what I mean I vote python To add Any programming language of those listed is stable if you write things correctly but C and its great IDEs will help you to that point much more nicely than C will C to fully utilize its potential requires much fiddling with delicate and precise systems Itll go that little bit faster as the cost of being less stable in the practical uh oh now I need to troubleshoot this sense
,,"<p>In regards to the question you mention (in the comments of the OP), these searches are related to optimization. I'm not sure of your background, so let me describe it from scratch, briefly:</p>

<p>Remember the derivative? The base idea is to talk about how the function changes in regards to changes in input. So now, we're out of high school and we're building neural nets. We've done the basic coding, and want to look at how our model is working. Back from our statistics class, we remember we use a certain measure of error (e.g. least squares) to determine the efficacy of the models from that class, so we decide to use that here. We get this error, and it's a bit too big for our liking, so we decide to fiddle with our model and adjust the weights to get that error down. But how? </p>

<p>This is where the 'search' comes into play. It's really a search for the best weights to put on the edges of our net to optimize it. We use the derivative (in some fancy ways, using the 'stochasitc' (think random sampling) and other ways the question mentions) to search for which way is 'down' in the high dimensional space of our weights. In other words, what we are searching for is minima or maxima to optimize our neural net, and we 'search' for it by doing a derivative which tells us which way to go, moving a bit in that direction, then doing that again and again iteratively to find (hopefully) the best weights.</p>

<p>This video here goes into all the detail you'd want, and I recommend the entire series as a robust but understandable intro to neural nets: <a href=""https://www.youtube.com/watch?v=GlcnxUlrtek"" rel=""nofollow"">Demystifying Neural Networks</a></p>

<p>Go and look up 'gradient descent' to get any related material. (Note, the gradient here is equivalent to multidimensional derivative direction to go in, and descent is just searching for the minima)</p>
",,0,2016-09-07T17:50:26.043,,1881,2016-09-07T18:05:20.690,2016-09-07T18:05:20.690,,1538.0,,1538.0,1877.0,2,1,,,,65.15,9.35,8.72,0.0,0.0,63.0,In regards to the question you mention in the comments of the OP these searches are related to optimization Im not sure of your background so let me describe it from scratch briefly Remember the derivative The base idea is to talk about how the function changes in regards to changes in input So now were out of high school and were building neural nets Weve done the basic coding and want to look at how our model is working Back from our statistics class we remember we use a certain measure of error eg least squares to determine the efficacy of the models from that class so we decide to use that here We get this error and its a bit too big for our liking so we decide to fiddle with our model and adjust the weights to get that error down But how This is where the search comes into play Its really a search for the best weights to put on the edges of our net to optimize it We use the derivative in some fancy ways using the stochasitc think random sampling and other ways the question mentions to search for which way is down in the high dimensional space of our weights In other words what we are searching for is minima or maxima to optimize our neural net and we search for it by doing a derivative which tells us which way to go moving a bit in that direction then doing that again and again iteratively to find hopefully the best weights This video here goes into all the detail youd want and I recommend the entire series as a robust but understandable intro to neural nets Demystifying Neural Networks Go and look up gradient descent to get any related material Note the gradient here is equivalent to multidimensional derivative direction to go in and descent is just searching for the minima
,,"<p>Search has always been a crucial element of AI in multiple ways.  First, what many people refer to as ""search"" is a reflection of how what we call ""intelligence"" frequently involves searching something:  a physical realm, a ""state space"" of possible solutions, a ""knowledge space"" where ideas/facts/concepts/etc. are related as a graph structure, etc.</p>

<p>Look up some old papers on computer chess, and you'll see that a lot of that involves searching a ""state space"".  As such, search algorithms that are efficient (in terms of time complexity and/or space complexity) have always been important to making advances there.  And while computer chess is just one example, the principle generalizes to many other kinds of problem solving and goal seeking activities.</p>

<p>Here's <a href=""http://www-g.eng.cam.ac.uk/mmg/teaching/artificialintelligence/nonflash/problemframenf.htm"" rel=""nofollow"">a reference</a> that explains more about some of these ideas.</p>

<p>Note too that ""search"" is closely related to the idea of ""heuristics"" in an important way.  Many search problems in the real world are far too complex to solve by exhaustive brute-force search, so humans (and AI's) resort to heuristics to narrow the state space being searched.  Using heuristics can yield search algorithms that allow for reasonable solutions in a realistic time-frame, where no simple, deterministic algorithm exists to do likewise.  </p>

<p>For some more background you might want to read up on <a href=""https://en.wikipedia.org/wiki/A*_search_algorithm"" rel=""nofollow"">A* search</a>, which is a widely used algorithm with many applications - and not just in AI.</p>

<p>The other major regard in which something you could call ""search"" applies in AI is through the use of algorithms which are also often referred to as ""optimisation"" techniques.  This would be things like Hill Climbing, Gradient Descent, Simulated Annealing and perhaps even Genetic Algorithms.  These are used to maximize or minimize the values of some function  and one of the canonical uses in AI is for training neural networks using back-propagation, where you're trying to minimize the delta between the ""correct"" answer (from the training data) and the generated answer, so you can learn the correct weights within the network.</p>
",,9,2016-09-07T18:00:25.137,,1882,2016-09-08T14:19:26.630,2016-09-08T14:19:26.630,,33.0,,33.0,1877.0,2,2,,,,47.52,13.12,10.12,0.0,0.0,69.0,Search has always been a crucial element of AI in multiple ways First what many people refer to as search is a reflection of how what we call intelligence frequently involves searching something a physical realm a state space of possible solutions a knowledge space where ideasfactsconceptsetc are related as a graph structure etc Look up some old papers on computer chess and youll see that a lot of that involves searching a state space As such search algorithms that are efficient in terms of time complexity andor space complexity have always been important to making advances there And while computer chess is just one example the principle generalizes to many other kinds of problem solving and goal seeking activities Heres a reference that explains more about some of these ideas Note too that search is closely related to the idea of heuristics in an important way Many search problems in the real world are far too complex to solve by exhaustive bruteforce search so humans and AIs resort to heuristics to narrow the state space being searched Using heuristics can yield search algorithms that allow for reasonable solutions in a realistic timeframe where no simple deterministic algorithm exists to do likewise For some more background you might want to read up on A search which is a widely used algorithm with many applications and not just in AI The other major regard in which something you could call search applies in AI is through the use of algorithms which are also often referred to as optimisation techniques This would be things like Hill Climbing Gradient Descent Simulated Annealing and perhaps even Genetic Algorithms These are used to maximize or minimize the values of some function and one of the canonical uses in AI is for training neural networks using backpropagation where youre trying to minimize the delta between the correct answer from the training data and the generated answer so you can learn the correct weights within the network
,,"<p>`State space search' is a general and ubiquitous AI activity that includes numerical optimization (e.g. via gradient descent in a real-valued search space) as a special case.</p>

<p>State space search is an abstraction which can be customized for a particular problem via three ingredients:</p>

<ol>
<li>Some representation for candidate solutions to the problem (e.g.
permutation of cities to represent a Travelling Salesman Problem
(TSP) tour, vector of real values for numeric problems).</li>
<li>A
solution quality measure: i.e. some means of deciding which of two
solutions is the better. This is typically achieved (for
single-objective problems) by having via some integer or real-valued
function of a solution (e.g. total distance travelled for a TSP
tour). </li>
<li>Some means of moving around in the space of possible solutions, in a heuristically-informed manner. Derivatives can be used if
available, or else (e.g. for black-box problems or discrete solution
representations) the kind of mutation or crossover methods favoured
by genetic algorithms/evolutionary computation can be employed.</li>
</ol>

<p>The first couple of chapters of the freely available <a href=""https://cs.gmu.edu/~sean/book/metaheuristics/"" rel=""nofollow"">""Essentials of Metaheuristics""</a> give an excellent overview and  Michalewicz and Fogel's <a href=""http://www.springer.com/us/book/9783540224945"" rel=""nofollow"">""How to Solve It - Modern Heuristics""</a> explains in more detail how numerical optimization can be considered in terms of state-space.</p>

<p>In response to a request in the comments to explain how  clarify how ""the 'search through possible plans' might occur"", the idea is to choose all three of the above for the planning problem and then apply some metaheuristic such as Simulated Annealing, Tabu Search, Genetic Algorithms etc. Clearly, for nontrivial problems, only a small fraction of the space of ""all possible plans"" is actually explored.</p>

<p>CAVEAT: Actually <em>planning</em> (in contrast to the vast majority of other problems amenable to state-space search such as scheduling, packing, routing etc) is a bit of a special case, in that it is sometime possible to solve planning problems simply by using A* search, rather than searching with a stochastic metaheuristic.</p>
",,3,2016-09-07T18:42:23.773,,1883,2016-09-08T06:54:11.690,2016-09-08T06:54:11.690,,42.0,,42.0,1877.0,2,5,,,,41.6,14.74,10.68,0.0,0.0,72.0,State space search is a general and ubiquitous AI activity that includes numerical optimization eg via gradient descent in a realvalued search space as a special case State space search is an abstraction which can be customized for a particular problem via three ingredients Some representation for candidate solutions to the problem eg permutation of cities to represent a Travelling Salesman Problem TSP tour vector of real values for numeric problems A solution quality measure ie some means of deciding which of two solutions is the better This is typically achieved for singleobjective problems by having via some integer or realvalued function of a solution eg total distance travelled for a TSP tour Some means of moving around in the space of possible solutions in a heuristicallyinformed manner Derivatives can be used if available or else eg for blackbox problems or discrete solution representations the kind of mutation or crossover methods favoured by genetic algorithmsevolutionary computation can be employed The first couple of chapters of the freely available Essentials of Metaheuristics give an excellent overview and Michalewicz and Fogels How to Solve It Modern Heuristics explains in more detail how numerical optimization can be considered in terms of statespace In response to a request in the comments to explain how clarify how the search through possible plans might occur the idea is to choose all three of the above for the planning problem and then apply some metaheuristic such as Simulated Annealing Tabu Search Genetic Algorithms etc Clearly for nontrivial problems only a small fraction of the space of all possible plans is actually explored CAVEAT Actually planning in contrast to the vast majority of other problems amenable to statespace search such as scheduling packing routing etc is a bit of a special case in that it is sometime possible to solve planning problems simply by using A search rather than searching with a stochastic metaheuristic
,,"<p>The aim of an AI is to fulfill one or the other task, say solve the task adequately. But there are results that are no solutions at all and there are results which are satisfying the task and thus are accepted as solutions. Since there are generally more results that are no solutions, the set of all possible solutions is just a subset of all results. But this means that the task involves the search for a suitable set of solutions. </p>
",,0,2016-09-07T18:51:30.597,,1884,2016-09-07T18:51:30.597,,,,,1781.0,1877.0,2,0,,,,67.89,8.07,7.56,0.0,0.0,6.0,The aim of an AI is to fulfill one or the other task say solve the task adequately But there are results that are no solutions at all and there are results which are satisfying the task and thus are accepted as solutions Since there are generally more results that are no solutions the set of all possible solutions is just a subset of all results But this means that the task involves the search for a suitable set of solutions
,3.0,"<p>Considering the answers of <a href=""http://ai.stackexchange.com/questions/1314/how-powerful-a-computer-is-required-to-simulate-the-human-brain"">this</a> question, emulating a human brain with the current computing capacity is currently impossible, but we aren't very far from it.</p>

<p>Note, 1 or 2 decades ago, similar calculations had similar results.</p>

<p>The clock frequency of the modern CPUs seem to be stopped, currently the miniaturization (-> mobile use), the RAM/cache improvement and the multi-core paralellization are the main lines of the development.</p>

<p>Ok, but what is the case with the analogous chips? In case of a NN, it is not a very big problem, if it is not very accurate, the NN would adapt to the minor manufacturing differences in its learning phase. And a single analogous wire can substitute a complex integer multiplication-division unit, while the whole surface of the analogous printed circuit could work parallel.</p>

<p>According to <a href=""http://engineering.stackexchange.com/questions/3993/do-analog-fpgas-exist"">this</a> post, ""software rewirable"" analogous circuits, essentially ""analogous FPGAs"" already exist. Although the capacity of the FPGAs is highly below the capacity of the <a href=""https://en.wikipedia.org/wiki/Application-specific_integrated_circuit"" rel=""nofollow"">ASIC</a>s with the same size, maybe analogous chips for neural networks could also exist.</p>

<p>I suspect, if it is correct, maybe even the real human brain model wouldn't be too far. It would still require a massively parallel system of costly analogous NN chips, but it seems to me not impossible.</p>

<p>Could this idea work? Maybe there is even active research/development into this direction?</p>
",,1,2016-09-07T20:13:49.120,,1885,2016-10-01T10:42:13.713,2016-09-07T20:22:17.153,,2255.0,,2255.0,,1,4,<neural-networks>,Emulating human brain - with analogous NN chips,150.0,44.34,12.88,10.12,0.0,0.0,43.0,Considering the answers of this question emulating a human brain with the current computing capacity is currently impossible but we arent very far from it Note 1 or 2 decades ago similar calculations had similar results The clock frequency of the modern CPUs seem to be stopped currently the miniaturization mobile use the RAMcache improvement and the multicore paralellization are the main lines of the development Ok but what is the case with the analogous chips In case of a NN it is not a very big problem if it is not very accurate the NN would adapt to the minor manufacturing differences in its learning phase And a single analogous wire can substitute a complex integer multiplicationdivision unit while the whole surface of the analogous printed circuit could work parallel According to this post software rewirable analogous circuits essentially analogous FPGAs already exist Although the capacity of the FPGAs is highly below the capacity of the ASICs with the same size maybe analogous chips for neural networks could also exist I suspect if it is correct maybe even the real human brain model wouldnt be too far It would still require a massively parallel system of costly analogous NN chips but it seems to me not impossible Could this idea work Maybe there is even active researchdevelopment into this direction
,,"<p>In the Visual Studio, there is no real difference between C and C++. It is compiled with the same compiler binary, although with different flags.</p>

<p>In most cases, the easily programmable complex data structures are generally more important, as the linear speed. The AI is an exception. AI has mostly not so complex data structures, and also the linear speed improvement is very important.</p>

<p>It closes out the garbage collected languages, i.e. any managed code, and, in my opinion, the best solution would be if you would use <em>not</em> a .net-based language, but a directly to asm compilable one.</p>

<p>But, knowing that you are asking explicitly for a .net one, I would suggest one, which can be easily ported later to machine code. It closes out C# and VB, but it doesn't close out C++.</p>

<p>C++ has also the needed complex data structures, while it still has the near-asm speed (and memory need).</p>

<p>My idea would be to start with unmanaged C++ in .net, but use simply, native .exe-s to your final programs. It would make also possible to make your program portable, because C++ is for everywhere, while .net only for windows. I.e. your program will be later able to run in non-windows server (or cluster) environment, too.</p>

<p>It would be also important to prefer the deeply parallel or easily parallelizable algorithms. Ideally it should be made adaptable to slow communication channels (also for the parallel cluster run).</p>

<p>--</p>

<p>Your program will probably have some user interface, persistent database and similar things, these aren't speed and memory critical things, thus these you can implement in anything as you wish. The result will be a two-process solution, where a speed-optimized, C++ calculating daemon is controlled by essentially a GUI (or db.. or script.. or anything) interface.</p>

<p>Probably there are already C++ frameworks for this task, so you don't need to reinvent the wheel.</p>
",,2,2016-09-07T20:44:54.933,,1886,2016-09-07T20:44:54.933,,,,,2255.0,1876.0,2,0,,,,66.74,10.72,9.49,0.0,0.0,90.0,In the Visual Studio there is no real difference between C and C It is compiled with the same compiler binary although with different flags In most cases the easily programmable complex data structures are generally more important as the linear speed The AI is an exception AI has mostly not so complex data structures and also the linear speed improvement is very important It closes out the garbage collected languages ie any managed code and in my opinion the best solution would be if you would use not a netbased language but a directly to asm compilable one But knowing that you are asking explicitly for a net one I would suggest one which can be easily ported later to machine code It closes out C and VB but it doesnt close out C C has also the needed complex data structures while it still has the nearasm speed and memory need My idea would be to start with unmanaged C in net but use simply native exes to your final programs It would make also possible to make your program portable because C is for everywhere while net only for windows Ie your program will be later able to run in nonwindows server or cluster environment too It would be also important to prefer the deeply parallel or easily parallelizable algorithms Ideally it should be made adaptable to slow communication channels also for the parallel cluster run Your program will probably have some user interface persistent database and similar things these arent speed and memory critical things thus these you can implement in anything as you wish The result will be a twoprocess solution where a speedoptimized C calculating daemon is controlled by essentially a GUI or db or script or anything interface Probably there are already C frameworks for this task so you dont need to reinvent the wheel
,,"<p>AI is already used as weapon - think on the drones.</p>

<p>I suspect, a ""robots take over the world"" scenario has the highest probability, if it has an intermediate step. This intermediate step could be ""humans take over the world with robots"".</p>

<p>This can go somewhere into a false direction.</p>

<p>I suspect, it is not surely so far as it seems. Consider the US has currently 8000 drones. What if it would have 8million? A small group capable to control them could take over the world. Or the small groups controlling different parts of the fleet, could fight against eachother. They shouldn't be all in the US - at the time the US will have this fleet, other countries will develop also theirs.</p>

<p>Btw, a world takeover seem to me unreal - the military leaders can maybe switch the human pilots to drones, it is not their job. But the ""high level control"", i.e. to determine, what to do, who are the targets, these decisions they won't ever give out from their hands.</p>

<p>Next to that, the robots doesn't have a long-term goal. We, humans, have.</p>

<p>Thus I don't consider a skynet-style takeover very realistic, but a chernobyl-style ""mistake"" of a misinterpreted command, which results the unstoppable rampage of the fleet, doesn't seem to me impossible.</p>
",,0,2016-09-07T21:10:51.137,,1888,2016-09-07T21:10:51.137,,,,,2255.0,1824.0,2,0,,,,74.9,9.5,8.6,0.0,0.0,53.0,AI is already used as weapon think on the drones I suspect a robots take over the world scenario has the highest probability if it has an intermediate step This intermediate step could be humans take over the world with robots This can go somewhere into a false direction I suspect it is not surely so far as it seems Consider the US has currently 8000 drones What if it would have 8million A small group capable to control them could take over the world Or the small groups controlling different parts of the fleet could fight against eachother They shouldnt be all in the US at the time the US will have this fleet other countries will develop also theirs Btw a world takeover seem to me unreal the military leaders can maybe switch the human pilots to drones it is not their job But the high level control ie to determine what to do who are the targets these decisions they wont ever give out from their hands Next to that the robots doesnt have a longterm goal We humans have Thus I dont consider a skynetstyle takeover very realistic but a chernobylstyle mistake of a misinterpreted command which results the unstoppable rampage of the fleet doesnt seem to me impossible
,,"<p>I'm not sure about ""emulating the brain"" per-se, but in a more general sense there has been some thought given to using analog computing for AI/ML.  It seems clear that analog computers do have certain advantages over digital computers.  For one, they can (depending on the application) be faster, albeit at the cost of some loss of precision.  But that's OK, because I don't think anybody believes the human brain is calculating floating point math using digital computing techniques either.  The human brain appears, at least superficially, to be largely probabilistic and able to tolerate some ""slop"" numerically.  </p>

<p>The downside to analog computers, as I understand it, is that they're not as flexible... you basically hardwire a circuit to do one specific ""thing"" and that's really all it can do.  To change the ""programming"" you have to literally solder in a new component! Or, I suppose, adjust a potentiometer or adjustable capacitor, etc.  Anyway, the point is that digital computers are supremely flexible, which is one big reason they came to dominate the world.  But I can see where there could be room for going analog for discrete functions that make up some or all of an intelligent system.</p>

<p>As for research in the area, you might look into whatever DARPA was / is doing. There was <a href=""http://www.wired.com/2012/08/upside/"" rel=""nofollow"">an article in Wired</a> a while back, talking about some DARPA initiatives related to analog computing.  </p>
",,0,2016-09-07T21:14:47.430,,1889,2016-09-07T21:14:47.430,,,,,33.0,1885.0,2,3,,,,53.41,11.25,9.42,0.0,0.0,48.0,Im not sure about emulating the brain perse but in a more general sense there has been some thought given to using analog computing for AIML It seems clear that analog computers do have certain advantages over digital computers For one they can depending on the application be faster albeit at the cost of some loss of precision But thats OK because I dont think anybody believes the human brain is calculating floating point math using digital computing techniques either The human brain appears at least superficially to be largely probabilistic and able to tolerate some slop numerically The downside to analog computers as I understand it is that theyre not as flexible you basically hardwire a circuit to do one specific thing and thats really all it can do To change the programming you have to literally solder in a new component Or I suppose adjust a potentiometer or adjustable capacitor etc Anyway the point is that digital computers are supremely flexible which is one big reason they came to dominate the world But I can see where there could be room for going analog for discrete functions that make up some or all of an intelligent system As for research in the area you might look into whatever DARPA was is doing There was an article in Wired a while back talking about some DARPA initiatives related to analog computing
,,"<p>I feel like most of the scenarios about AI's wiping out the world fall into one of two categories:</p>

<ol>
<li>Anthropomorphized AI's</li>
</ol>

<p>or</p>

<ol start=""2"">
<li>Intelligent But Dumb Computer Run Amuck</li>
</ol>

<p>In the (1) case, people talk about AI's becoming ""evil"" and attribute to them other such human elements.  I look at this as being mostly sci-fi and don't think it merits much serious discussion.  That is, I see no particular reason to assume that an <strong>Artificial</strong> Intelligence - regardless of how intelligent it is - will necessarily <strong>behave</strong> like a human.</p>

<p>The (2) case makes more sense to me.  This is the idea that an AI is, for example, put in control of the nuclear missile silos and winds up launching the missiles because it was just doing it's job, but missed something a human would have noticed via what we might call ""common sense"".  Hence the ""Intelligent but Dumb"" moniker.  </p>

<p>Neither of these strikes me as <strong>terribly</strong> alarming, because (1) is probably fiction and (2) doesn't involve any actual malicious intent by the AI - which means it won't be actively trying to deceive us, or work around any safety cut-outs, etc.</p>

<p>Now IF somebody builds an AI and decides to intentionally program it so that it develops human like characteristics like arrogance, ego, greed, etc... well, all bets are off.</p>
",,0,2016-09-07T21:20:50.380,,1890,2016-09-08T02:44:40.883,2016-09-08T02:44:40.883,,33.0,,33.0,1824.0,2,1,,,,55.47,10.8,10.0,0.0,0.0,50.0,I feel like most of the scenarios about AIs wiping out the world fall into one of two categories Anthropomorphized AIs or Intelligent But Dumb Computer Run Amuck In the 1 case people talk about AIs becoming evil and attribute to them other such human elements I look at this as being mostly scifi and dont think it merits much serious discussion That is I see no particular reason to assume that an Artificial Intelligence regardless of how intelligent it is will necessarily behave like a human The 2 case makes more sense to me This is the idea that an AI is for example put in control of the nuclear missile silos and winds up launching the missiles because it was just doing its job but missed something a human would have noticed via what we might call common sense Hence the Intelligent but Dumb moniker Neither of these strikes me as terribly alarming because 1 is probably fiction and 2 doesnt involve any actual malicious intent by the AI which means it wont be actively trying to deceive us or work around any safety cutouts etc Now IF somebody builds an AI and decides to intentionally program it so that it develops human like characteristics like arrogance ego greed etc well all bets are off
,,"<h1>Yes.</h1>

<ol>
<li>Every chess game... every poker game. Every game.</li>
<li>Every more intelligent spam softwares or spambots. Although their primary goal is to lie to computer systems (f.e. spamfilter poisoning), their secondary goal is to lie to the human behind them.</li>
</ol>
",,0,2016-09-07T21:33:23.350,,1891,2016-09-07T21:33:23.350,,,,,2255.0,1806.0,2,0,,,,54.9,11.1,9.95,0.0,0.0,13.0,Yes Every chess game every poker game Every game Every more intelligent spam softwares or spambots Although their primary goal is to lie to computer systems fe spamfilter poisoning their secondary goal is to lie to the human behind them
,,"<p>Whether ""I take the ball"" or ""he takes the ball"", all stored instances of 'taking' and 'ball' will be weakly activated and 'taking [the] ball' will be strongly activated.  Doesn't this qualify as 'mirroring'? If you also know that ""I have an arm"" and ""he has an arm"", etc., then when ""he takes some blocks"", it isn't too hard to think that ""I could take some blocks.""</p>
",,0,2016-09-07T21:47:49.563,,1892,2016-09-07T21:47:49.563,,,,,2272.0,156.0,2,0,,,,88.26,8.41,7.53,0.0,0.0,32.0,Whether I take the ball or he takes the ball all stored instances of taking and ball will be weakly activated and taking the ball will be strongly activated Doesnt this qualify as mirroring If you also know that I have an arm and he has an arm etc then when he takes some blocks it isnt too hard to think that I could take some blocks
,,"<p>Every problem can be reduced to search.  Every problem has an input within some range (the domain) and an output in some other range (codomain).  That is, every problem can be formulated as a kind of map from one space to another, where the source is the givens of the problem, and the destination is the solution to the problem.</p>

<p>""Brute force"" is the algorithm which solves every problem by inspecting every point in the codomain and asking: ""Is this the solution?""  Every other algorithm is an attempt to improve on brute force by not searching the entire codomain of possible solutions.</p>

<p>Typical software engineering problems can be solved by algorithms which arrive at the correct solution very quickly (sorting, arithmetic, partition, etc.).  AI problems are generally those for which a strong polynomial  algorithm is not known, and thus, we must settle for approximations.  Basically every common problem that the human brain must solve falls into this category.</p>

<p>Consider the problem of moving a multi-jointed robotic arm to pick up an object.  Reverse kinematics does not have unique solutions: there is more than one way to move your hand from a start position to a target position.  This is due to the excessive degrees of freedom in your joints.  If you want to minimize energy usage, then there is a unique solution (due to the asymmetry of joints and muscles).</p>

<p>But what if there is an obstacle in the pathway of the minimum-energy solution?  There are many pathways which avoid the obstacle, but again, many of them will have a similar cost.  Even if there is a unique minimum-energy solution, it might not be the most practical to compute.  The brain is the most metabolically expensive organ in the body, so it is not always best to find an optimal solution.  Thus, heuristics come into play.</p>

<p>But in all cases, the problem is not: ""move your hand"" or ""move the robot arm.""  The problem is: ""search the space of joint rotation sequences which best achieves the goal.""  And even though there is a closed-form solution for the simple minimum-energy case with no obstacles, it is too expensive to compute precisely when a set of cheap heuristics will get you very close with a small fraction of the computational effort.</p>

<p>If computation were free, then AI would be mere mathematics, and we would always compute the best answer to every question using logic, calculus, physics, at worst, numerical methods when we don't have closed-form solutions.  In reality, time is money, and the time and effort to get an answer is as much a part of the cost as the quality of the solution.  So it is an engineering tradeoff to decide how much effort should be expended in what way to obtain the best answer given the value of the response.</p>

<p>Or, in other words, AI problems are all about searching the space of solutions as quickly as possible to get an answer that is ""good enough"".</p>

<p>I might seem curious that such far-flung problems as natural language recognition and theorem proving would be search problems.  But language parsers strive to determine the meaning of statements via part-of-speech tagging.  A given phrase can be parsed in many different ways, yielding many different interpretations, and the space of parse trees is yet another search problem in deciding which parse tree is the most likely intended meaning by the speaker.</p>

<p>A theorem proof is graph starting with axioms, proceeding through lemmas, applying the rules of procedure until the theorem is derived or refuted (by proving its negation).  There are many ways to represent this sequence, but at the end of the day, we are talking about a process of exploring the intermediate proof space and finding the derivation which reaches your goal.  Everything is search, in the end.</p>
",,0,2016-09-08T02:41:55.747,,1894,2016-09-08T02:41:55.747,,,,,2277.0,1877.0,2,0,,,,58.42,10.97,9.01,0.0,0.0,100.0,Every problem can be reduced to search Every problem has an input within some range the domain and an output in some other range codomain That is every problem can be formulated as a kind of map from one space to another where the source is the givens of the problem and the destination is the solution to the problem Brute force is the algorithm which solves every problem by inspecting every point in the codomain and asking Is this the solution Every other algorithm is an attempt to improve on brute force by not searching the entire codomain of possible solutions Typical software engineering problems can be solved by algorithms which arrive at the correct solution very quickly sorting arithmetic partition etc AI problems are generally those for which a strong polynomial algorithm is not known and thus we must settle for approximations Basically every common problem that the human brain must solve falls into this category Consider the problem of moving a multijointed robotic arm to pick up an object Reverse kinematics does not have unique solutions there is more than one way to move your hand from a start position to a target position This is due to the excessive degrees of freedom in your joints If you want to minimize energy usage then there is a unique solution due to the asymmetry of joints and muscles But what if there is an obstacle in the pathway of the minimumenergy solution There are many pathways which avoid the obstacle but again many of them will have a similar cost Even if there is a unique minimumenergy solution it might not be the most practical to compute The brain is the most metabolically expensive organ in the body so it is not always best to find an optimal solution Thus heuristics come into play But in all cases the problem is not move your hand or move the robot arm The problem is search the space of joint rotation sequences which best achieves the goal And even though there is a closedform solution for the simple minimumenergy case with no obstacles it is too expensive to compute precisely when a set of cheap heuristics will get you very close with a small fraction of the computational effort If computation were free then AI would be mere mathematics and we would always compute the best answer to every question using logic calculus physics at worst numerical methods when we dont have closedform solutions In reality time is money and the time and effort to get an answer is as much a part of the cost as the quality of the solution So it is an engineering tradeoff to decide how much effort should be expended in what way to obtain the best answer given the value of the response Or in other words AI problems are all about searching the space of solutions as quickly as possible to get an answer that is good enough I might seem curious that such farflung problems as natural language recognition and theorem proving would be search problems But language parsers strive to determine the meaning of statements via partofspeech tagging A given phrase can be parsed in many different ways yielding many different interpretations and the space of parse trees is yet another search problem in deciding which parse tree is the most likely intended meaning by the speaker A theorem proof is graph starting with axioms proceeding through lemmas applying the rules of procedure until the theorem is derived or refuted by proving its negation There are many ways to represent this sequence but at the end of the day we are talking about a process of exploring the intermediate proof space and finding the derivation which reaches your goal Everything is search in the end
1920.0,2.0,"<p>Conceptually speaking, aren't artificial neural networks just highly distributed, lossy compression schemes?</p>

<p>They're certainly efficient at <a href=""https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Applications/imagecompression.html"" rel=""nofollow"">compressing images</a>.</p>

<p>And aren't brains (at least, the neocortex) just compartmentalized, highly distributed, lossy databases?</p>

<p>If so, what salient features in RNNs and CNNs are necessary in any given lossy compression scheme in order to extract the semantic relations that they do? Is it just a matter of having a large number of dimensions/variables? </p>

<p>Could some kind of lossy <a href=""https://en.wikipedia.org/wiki/Bloom_filter"" rel=""nofollow"">Bloom filter</a> be re-purposed for the kinds of problems ANNs are applied to?</p>
",,0,2016-09-08T06:03:45.673,,1895,2016-09-10T00:48:38.647,,,,,1712.0,,1,1,<neural-networks><lossy-compression><compression>,Are ANNs just highly distributed lossy compression schemes?,76.0,48.09,15.19,10.65,0.0,0.0,19.0,Conceptually speaking arent artificial neural networks just highly distributed lossy compression schemes Theyre certainly efficient at compressing images And arent brains at least the neocortex just compartmentalized highly distributed lossy databases If so what salient features in RNNs and CNNs are necessary in any given lossy compression scheme in order to extract the semantic relations that they do Is it just a matter of having a large number of dimensionsvariables Could some kind of lossy Bloom filter be repurposed for the kinds of problems ANNs are applied to
,,"<p>ANNs don't compress, they generalise. </p>

<p>Often this leads to compression, i.e. the internal generalised representation is smaller than the original input, but not necessarily. Imagine a ANN that is trained to use the screen input of a computer game to play it. If the game is very rich and conceptually deep the internal representation of a single screen input might be a lot bigger than the input itself, because ANNs put the single data points into the context of the overall data. Which leads us to the second point:</p>

<p>ANNs (and the neocortex) model data hierarchically. </p>

<p>This is what makes them so powerful. So it is not just about having a large number of parameters, they also have to be arranged in such a way that they capture the structure of the data (or the world), which very often seems to be hierarchical. Just look a two different pictures of a duck. On the pixel level they might be as different as random images, all the similarities emerge in higher levels of the hierarchy, when enough pixels combined give you the patterns of feathers, beak and webs. Bloom filters obviously lack this property. They would only give you a pixel by pixel account of whether you have seen (almost) exactly this picture before. </p>
",,1,2016-09-08T11:36:01.787,,1896,2016-09-08T11:36:01.787,,,,,2227.0,1895.0,2,1,,,,61.87,9.92,9.56,0.0,0.0,30.0,ANNs dont compress they generalise Often this leads to compression ie the internal generalised representation is smaller than the original input but not necessarily Imagine a ANN that is trained to use the screen input of a computer game to play it If the game is very rich and conceptually deep the internal representation of a single screen input might be a lot bigger than the input itself because ANNs put the single data points into the context of the overall data Which leads us to the second point ANNs and the neocortex model data hierarchically This is what makes them so powerful So it is not just about having a large number of parameters they also have to be arranged in such a way that they capture the structure of the data or the world which very often seems to be hierarchical Just look a two different pictures of a duck On the pixel level they might be as different as random images all the similarities emerge in higher levels of the hierarchy when enough pixels combined give you the patterns of feathers beak and webs Bloom filters obviously lack this property They would only give you a pixel by pixel account of whether you have seen almost exactly this picture before
1898.0,7.0,"<p>Consciousness <a href=""http://www.iep.utm.edu/consciou/"">is challenging to define</a>, but for this question let's define it as ""actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine."" Humans, of course, have minds; for normal computers, all the things they ""see"" are just more data. One could alternatively say that humans are <a href=""http://philosophy.stackexchange.com/a/4687"">sentient</a>, while traditional computers are not.</p>

<p>Setting aside the question of whether it's possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?</p>
",,6,2016-09-08T16:20:40.087,7.0,1897,2016-09-10T01:02:13.913,,,,,75.0,,1,11,<philosophy>,Is consciousness necessary for any AI task?,693.0,49.96,11.84,10.05,0.0,0.0,21.0,Consciousness is challenging to define but for this question lets define it as actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine Humans of course have minds for normal computers all the things they see are just more data One could alternatively say that humans are sentient while traditional computers are not Setting aside the question of whether its possible to build a sentient machine does it actually make a difference if an AI is sentient or not In other words are there are tasks that are made impossible not just more difficult by a lack of sentience
,,"<p>No-one knows.</p>

<p>Why: because it's not possible to formally determine even whether your fellow human beings are actually conscious (they may instead be what is philosophically termed a <a href=""http://plato.stanford.edu/entries/zombies/"" rel=""nofollow"">'Zombie'</a>). No test known to modern physics suffices to decide. Hence it's possible that you are the only sentient being, and everyone else is a robot.</p>

<p>Consequently, we cannot determine which tasks require sentience.</p>

<p>Note that the ontological status of Zombies is controversial: some philosophers of AI (e.g. Daniel Dennett) claim that Zombies are <a href=""https://ase.tufts.edu/cogstud/dennett/papers/unzombie.htm"" rel=""nofollow"">logically impossible</a> while others such as <a href=""http://consc.net/zombies.html"" rel=""nofollow"">David Chalmers</a> would claim that a Zombie would be compelled to assert that they experience <em>qualia</em> (i.e. are sentient) even though they do not. <a href=""http://homepages.uc.edu/~polgertw/Polger-ZombiesJCS.pdf"" rel=""nofollow"">Here</a> is a very readable paper by Flanagan and Polger that also explains why a stronger neurological version of the Turing test is insufficient to detect a Zombie.</p>

<p>EDIT: In response to the comment about whether an objective test for distinguishing sentience from non-sentience exists:</p>

<p>No-one knows. What we <em>do</em> believe is that this would require something in addition to what modern physics can currently tell us. David Chalmers has speculated that qualia should be introduced as a new form of physical unit, orthogonal to the others in the same way that electrical charge is orthogonal to distance.</p>

<p>In the absence of an objective test, we have to rely on Turing test variants, which no more guarantee consciousness in the subject than they do intelligence.</p>
",,8,2016-09-08T16:24:07.663,,1898,2016-09-09T07:26:44.323,2016-09-09T07:26:44.323,,42.0,,42.0,1897.0,2,8,,,,51.38,12.88,10.15,0.0,0.0,37.0,Noone knows Why because its not possible to formally determine even whether your fellow human beings are actually conscious they may instead be what is philosophically termed a Zombie No test known to modern physics suffices to decide Hence its possible that you are the only sentient being and everyone else is a robot Consequently we cannot determine which tasks require sentience Note that the ontological status of Zombies is controversial some philosophers of AI eg Daniel Dennett claim that Zombies are logically impossible while others such as David Chalmers would claim that a Zombie would be compelled to assert that they experience qualia ie are sentient even though they do not Here is a very readable paper by Flanagan and Polger that also explains why a stronger neurological version of the Turing test is insufficient to detect a Zombie EDIT In response to the comment about whether an objective test for distinguishing sentience from nonsentience exists Noone knows What we do believe is that this would require something in addition to what modern physics can currently tell us David Chalmers has speculated that qualia should be introduced as a new form of physical unit orthogonal to the others in the same way that electrical charge is orthogonal to distance In the absence of an objective test we have to rely on Turing test variants which no more guarantee consciousness in the subject than they do intelligence
,,"<p>In a very niche sense, I'd say yes.</p>

<p>The only tasks that sentience would make possible was the actual feeling and thinking in and of itself. At this point, sentience doesn't play a part in any of the tasks we ask AI's to complete; we are rapidly approaching the point of being able to teach a 'dead' machine to do most anything a sentient AI can, in a practical sense.</p>

<p>Sentience <em>colloquially</em> often translates to 'the ability to reason while understanding that oneself and each other entity is a distinct acting agent'or something along those lines. It literally means something more along the lines of self-awareness and the definition of consciousness you have above. The point I'm making is that we are readily approaching the point where 'dead' AI's can very nicely mimic the first way of thinking, just by really nicely learning and interpreting data.</p>

<p><a href=""https://i.stack.imgur.com/lbSUcm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lbSUcm.jpg"" alt=""enter image description here""></a></p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sub>Does the robot see an amalgamation of bone, or a being that once was?</sub></p>

<p>Thus, a truly sentient machine would be superior in capability (compared to a really, really advanced 'dead' AI) only in the respect of being able to 'truly' experience the information.</p>

<p>This runs very well in parallel with the so-called <a href=""https://en.wikipedia.org/wiki/Knowledge_argument"" rel=""nofollow noreferrer"">""Knowledge Argument""</a> which in essence debates this very issue. The version of it that I heard which sticks with me is that there is a very smart girl in a room with access to all sorts of information. She likes the color blue. Or so she thinks; she's never actually seen it. She has all the information in the world available about colors and how they work, etc. but does she really know what blue is until she sees it?</p>

<p>Another great, historic venture into this field is the famous painting:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=""https://i.stack.imgur.com/vhd7K.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vhd7K.jpg"" alt=""Ceci n&#39;est pas une pipe""></a></p>

<p>The caption translates: ""This is not a pipe"". And the idea is that this, honestly, isn't a pipe. Right now it's a bunch of pixels on your screen in a certain configuration - we can all 'see' a pipe, but what does that really mean?</p>

<p>At the end of the day, I think super-intelligent 'dead' AI can practically do anything a 'live' one can, with the latter being superior in and of the 'liveness' itself.</p>
",,2,2016-09-08T17:26:32.650,,1899,2016-09-09T09:15:31.397,2016-09-09T09:15:31.397,,8.0,,1538.0,1897.0,2,-1,,,,67.69,15.84,8.74,0.0,0.0,208.0,In a very niche sense Id say yes The only tasks that sentience would make possible was the actual feeling and thinking in and of itself At this point sentience doesnt play a part in any of the tasks we ask AIs to complete we are rapidly approaching the point of being able to teach a dead machine to do most anything a sentient AI can in a practical sense Sentience colloquially often translates to the ability to reason while understanding that oneself and each other entity is a distinct acting agentor something along those lines It literally means something more along the lines of selfawareness and the definition of consciousness you have above The point Im making is that we are readily approaching the point where dead AIs can very nicely mimic the first way of thinking just by really nicely learning and interpreting data nbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspDoes the robot see an amalgamation of bone or a being that once was Thus a truly sentient machine would be superior in capability compared to a really really advanced dead AI only in the respect of being able to truly experience the information This runs very well in parallel with the socalled Knowledge Argument which in essence debates this very issue The version of it that I heard which sticks with me is that there is a very smart girl in a room with access to all sorts of information She likes the color blue Or so she thinks shes never actually seen it She has all the information in the world available about colors and how they work etc but does she really know what blue is until she sees it Another great historic venture into this field is the famous painting nbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbsp nbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbsp nbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbspnbsp The caption translates This is not a pipe And the idea is that this honestly isnt a pipe Right now its a bunch of pixels on your screen in a certain configuration we can all see a pipe but what does that really mean At the end of the day I think superintelligent dead AI can practically do anything a live one can with the latter being superior in and of the liveness itself
,,"<p>Two kinds of tasks require consciousness:</p>

<ol>
<li><p>consciousness</p></li>
<li><p>Any task that requires extreme dynamicity, where solving problems requires analogizing between various 3D states of affairs and prior knowledge of how to solve the problem is minimal</p></li>
</ol>

<p>However, once knowledge of how to solve a given problem is gained, further optimization will eliminate that need for consciousness.</p>

<p>If you give enough specificity to a problem, you remove the need for a general solver. And then the only remaining <em>need</em> for a consciousness is for the sake of itself.</p>
",,0,2016-09-08T18:50:44.583,,1901,2016-09-08T18:50:44.583,,,,,1712.0,1897.0,2,-1,,,,33.88,13.82,11.12,0.0,0.0,8.0,Two kinds of tasks require consciousness consciousness Any task that requires extreme dynamicity where solving problems requires analogizing between various 3D states of affairs and prior knowledge of how to solve the problem is minimal However once knowledge of how to solve a given problem is gained further optimization will eliminate that need for consciousness If you give enough specificity to a problem you remove the need for a general solver And then the only remaining need for a consciousness is for the sake of itself
,,"<p>Consciousness is an attention selection mechanism that <strong>searches</strong> over salient inputs. The <a href=""https://www.youtube.com/watch?v=aQclvZpPWHQ"" rel=""nofollow"">robotic saccades</a> of your eyeballs show you first hand the algorithmic nature of your brain's conscious attention mechanism, while it searches among salient inputs.</p>

<p>A smart search algorithm can help with dimensionality reduction.</p>
",,0,2016-09-08T19:15:28.547,,1902,2016-09-08T23:56:01.660,2016-09-08T23:56:01.660,,1712.0,,1712.0,1877.0,2,0,,,,47.79,17.33,11.4,0.0,0.0,5.0,Consciousness is an attention selection mechanism that searches over salient inputs The robotic saccades of your eyeballs show you first hand the algorithmic nature of your brains conscious attention mechanism while it searches among salient inputs A smart search algorithm can help with dimensionality reduction
,,"<p>A being without sentience cannot suffer. If, for example, we wanted to take joy in the suffering of another, only an AI that was sentient would suffice.</p>

<p>Suppose we had some sadists who could not be satisfied or productive unless they got to produce lots of suffering. And say we only cared about minimizing human and animal suffering. What we would need for this job is something non-human and non-animal that could suffer. A conscious AI would do, a non-conscious one would not.</p>

<p>The claim was made in the comments that consciousness cannot be proven, other than perhaps by introspection. But clearly this is not a problem since sadists take joy in torturing others, and those others cannot prove they're conscious either.</p>
",,13,2016-09-08T20:44:51.033,,1903,2016-09-08T21:07:23.870,2016-09-08T21:07:23.870,,2299.0,,2299.0,1897.0,2,-2,,,,64.51,10.72,9.57,0.0,0.0,18.0,A being without sentience cannot suffer If for example we wanted to take joy in the suffering of another only an AI that was sentient would suffice Suppose we had some sadists who could not be satisfied or productive unless they got to produce lots of suffering And say we only cared about minimizing human and animal suffering What we would need for this job is something nonhuman and nonanimal that could suffer A conscious AI would do a nonconscious one would not The claim was made in the comments that consciousness cannot be proven other than perhaps by introspection But clearly this is not a problem since sadists take joy in torturing others and those others cannot prove theyre conscious either
,1.0,"<p>A lot of textbooks and introductory lectures typically split AI into connectionism and GOFAI (Good Old Fashioned AI). 
From a purely technical perspective it seems that connectionism has grown into machine learning and data science, while nobody talks about GOFAI, Symbolic AI or Expert Systems at all. </p>

<p>Is anyone of note still working on GOFAI?    </p>
",,1,2016-09-08T23:22:05.480,1.0,1906,2016-09-08T23:35:06.707,,,,,2306.0,,1,3,<gofai><symbolic-computing>,Is anybody still researching GOFAI?,47.0,44.44,13.17,11.43,0.0,0.0,7.0,A lot of textbooks and introductory lectures typically split AI into connectionism and GOFAI Good Old Fashioned AI From a purely technical perspective it seems that connectionism has grown into machine learning and data science while nobody talks about GOFAI Symbolic AI or Expert Systems at all Is anyone of note still working on GOFAI
,,"<p>Oh yeah, definitely.  Just to pick one example, you have <a href=""http://www.cogsci.indiana.edu/"" rel=""nofollow"">Douglas Hofstader's group</a> at Indiana.  I think most of what they do would fall under the rubric of GOFAI (or at least closer to that than the statistical machine learning stuff).  </p>

<p>Beyond that, just go to the <a href=""http://arxiv.org/corr/home"" rel=""nofollow"">CORR</a> and browse around the <a href=""http://arxiv.org/list/cs.AI/recent"" rel=""nofollow"">AI category</a>. You'll see plenty of neural networks and probabilistic stuff, but you'll also find the papers by the folks doing symbolic processing / GOFAI as well.</p>
",,3,2016-09-08T23:35:06.707,,1907,2016-09-08T23:35:06.707,,,,,33.0,1906.0,2,1,,,,72.56,10.15,8.86,0.0,0.0,15.0,Oh yeah definitely Just to pick one example you have Douglas Hofstaders group at Indiana I think most of what they do would fall under the rubric of GOFAI or at least closer to that than the statistical machine learning stuff Beyond that just go to the CORR and browse around the AI category Youll see plenty of neural networks and probabilistic stuff but youll also find the papers by the folks doing symbolic processing GOFAI as well
,,"<p><strong>No.</strong></p>

<p>The <em>experience</em> of seeing is by definition non-causal. Anything non-causal cannot be a requirement of a physical process; a qualia cannot afford a robot the ability to do something it otherwise could not.</p>

<p><strong>Maybe.</strong></p>

<p>Although a qualia is not required for a given AI task, that is not to say that any sufficiently advanced AI does not entail qualia. It could be that so-called AI-complete tasks require a robot that, although not making use of qualia, produces it anyway.</p>

<p><strong>Yes.</strong></p>

<p>Qualia may refer to some wishy-washy non-physical property, but it's special in that we know it exists physically, too. The fact I am able to discuss my qualia knowingly (or, if you don't believe me, the fact <em>you</em> are able to) implies that my (or your) qualia does have a physical effect.</p>

<p>It stands to reason that if we accept others' qualia on the basis of our own, it must be because of the <em>physical</em> basis of our own<sup>1</sup>. Thus one could argue that<sup>2</sup> any robot that has an equivalent physical capacity <em>must</em> entail qualia.</p>

<p><sup>1</sup> since the subjective is physically non-causal, so cannot <em>cause</em> us to accept anything.</p>

<p><sup>2</sup> as long as you don't make the particularly odd assumption that qualia is somehow tied to its direct physical manifestation, which at best is tenuous since had we evolved the wrong one you would still claim it to be the right one with equal certainty.</p>
",,0,2016-09-08T23:48:23.803,,1908,2016-09-08T23:48:23.803,,,,,2305.0,1897.0,2,3,,,,55.98,9.99,9.16,0.0,0.0,39.0,No The experience of seeing is by definition noncausal Anything noncausal cannot be a requirement of a physical process a qualia cannot afford a robot the ability to do something it otherwise could not Maybe Although a qualia is not required for a given AI task that is not to say that any sufficiently advanced AI does not entail qualia It could be that socalled AIcomplete tasks require a robot that although not making use of qualia produces it anyway Yes Qualia may refer to some wishywashy nonphysical property but its special in that we know it exists physically too The fact I am able to discuss my qualia knowingly or if you dont believe me the fact you are able to implies that my or your qualia does have a physical effect It stands to reason that if we accept others qualia on the basis of our own it must be because of the physical basis of our own1 Thus one could argue that2 any robot that has an equivalent physical capacity must entail qualia 1 since the subjective is physically noncausal so cannot cause us to accept anything 2 as long as you dont make the particularly odd assumption that qualia is somehow tied to its direct physical manifestation which at best is tenuous since had we evolved the wrong one you would still claim it to be the right one with equal certainty
,1.0,"<p>I recently finished Course on RL by David Silver (on YT) and thought about trying it out on simple application in Unity Game Engine, where I've built simple labyrint with ball and want to teach the ball to get from point A to point B in there while avoiding obstacles and fire (the place where you'll get burnt so big negative reward)</p>

<p>The problem I encountered while designing the whole thing (programming-wise) is: What is the correct (or at least good) way of representing the position in 2D space? It is continuous so I thought about representing it as feature vector consisting of [up, down, left, right, posX, posY] where direction is whether I am pressing button of moving in that direction in binary (or actions if you want) and pos are floats (0-1) representing normalized position from one corner on the plane where the whole map is. That would be accompanied by vector W that would represent the weights adjusted using Gradient Descent.</p>

<p>Question is: will this work?? I am asking for 2 reasons. One is that I am not so sure about that posX and posY since it can be 0 and if I multiply it by the weights vector then how could be resulting reward anything but 0? Second reason is that I am not sure if the actions should be part of the features. I mean, it makes sense to me but I could easily be very wrong since I am a beginner.</p>

<p>Thanks a lot guys in advance. If you have any more questions or think the problem is not described deeply enough just ask in the comments and I'll edit the question. :)</p>

<p>PS: I could just code it the way I think is right, but I also want to get gasp of designing applications on paper before coding them (project management).</p>
",,5,2016-09-09T10:19:56.087,,1909,2016-09-09T17:18:20.390,,,,,1343.0,,1,2,<models><implementation><reinforcement-learning>,State representation of position in 2D plane for Reinforcement Learning (Q Learning),28.0,68.54,8.59,8.83,0.0,0.0,46.0,I recently finished Course on RL by David Silver on YT and thought about trying it out on simple application in Unity Game Engine where Ive built simple labyrint with ball and want to teach the ball to get from point A to point B in there while avoiding obstacles and fire the place where youll get burnt so big negative reward The problem I encountered while designing the whole thing programmingwise is What is the correct or at least good way of representing the position in 2D space It is continuous so I thought about representing it as feature vector consisting of up down left right posX posY where direction is whether I am pressing button of moving in that direction in binary or actions if you want and pos are floats 01 representing normalized position from one corner on the plane where the whole map is That would be accompanied by vector W that would represent the weights adjusted using Gradient Descent Question is will this work I am asking for 2 reasons One is that I am not so sure about that posX and posY since it can be 0 and if I multiply it by the weights vector then how could be resulting reward anything but 0 Second reason is that I am not sure if the actions should be part of the features I mean it makes sense to me but I could easily be very wrong since I am a beginner Thanks a lot guys in advance If you have any more questions or think the problem is not described deeply enough just ask in the comments and Ill edit the question PS I could just code it the way I think is right but I also want to get gasp of designing applications on paper before coding them project management
,,"<p>It's not a difficult task, first of all you have to locate the body parts such as arms,head... you can do it using different approaches for example using cascadeclassifier or a well trained CNN.<br />
After that you can use different techniques, one could be an ANN trained on the keypoints of the different body parts (this is the easiest approach) or a CNN (good approach but you need a lot of training). To indicate the location after you have determined the position of the head (and the eyes to) and hands, you can simply calculate the orientation of those parts, and then you can get a general position where those orientation are pointing to.</p>
",,0,2016-09-09T10:55:23.300,,1910,2016-09-09T12:11:27.600,2016-09-09T12:11:27.600,,2320.0,,2320.0,1644.0,2,0,,,,59.47,9.99,8.79,0.0,0.0,18.0,Its not a difficult task first of all you have to locate the body parts such as armshead you can do it using different approaches for example using cascadeclassifier or a well trained CNN After that you can use different techniques one could be an ANN trained on the keypoints of the different body parts this is the easiest approach or a CNN good approach but you need a lot of training To indicate the location after you have determined the position of the head and the eyes to and hands you can simply calculate the orientation of those parts and then you can get a general position where those orientation are pointing to
,,"<p>I'm not sure what google is using to perform that task but most companies use region based convolutional neural nets to locate traffic signs and other objects.</p>

<p>But other companies use a Deep neural net + Bag of words approach to find objects.</p>

<p>See: <a href=""https://www.researchgate.net/publication/284505205_Bag-of-Words_Based_Deep_Neural_Network_for_Image_Retrieval"" rel=""nofollow"">Bag-of-Words Based Deep Neural Network for Image Retrieval</a> which shows a general approach, to get the exact location you can use <em>Feature Matching</em> or <em>Random Boxes</em>.</p>
",,1,2016-09-09T11:09:38.703,,1911,2016-09-09T11:36:05.420,2016-09-09T11:36:05.420,,8.0,,2320.0,1393.0,2,1,,,,56.59,11.67,10.96,0.0,0.0,9.0,Im not sure what google is using to perform that task but most companies use region based convolutional neural nets to locate traffic signs and other objects But other companies use a Deep neural net Bag of words approach to find objects See BagofWords Based Deep Neural Network for Image Retrieval which shows a general approach to get the exact location you can use Feature Matching or Random Boxes
,,"<p>Just to add some discourse; this is actually an incredibly complex task, as gestures (aka kinematics) function as an auxiliary language that can completely change the meaning of a sentence or even a single word. I recently did a dissertation on the converse (generating the correct gesture from a specific social context &amp; linguistic cues). The factors that go into the production of a particular gesture include the relationship between the two communicators (especially romantic connotations), the social scenario, the physical context, the linguistic context (the ongoing conversation, if any), a whole lot of personal factors (our gesture use is essentially a hybrid of important individuals around us e.g. friends &amp; family, and this is layered under the individual's psychological state). Then the whole thing is flipped around again when you look at how gestures are used completely differently in different cultures (look up gestures that are swear words in other cultures for an example!). There are a number of models for gesture production but none of them capture the complexity of the topic.</p>

<p>Now, that may seem like a whole lot of fluff that is not wholly relevant to your question, but my point is that ASIMO isn't actually very 'clever' at this. AFAIK (I have heard from a visualization guy that this is how <em>he</em> thinks they do it) they use conventional (but optimized) image recognition techniques trained on a corpus of data to achieve recognition of particular movements. One would assume that the dataset consists of a series of videos / images of gestures labelled with that particular gesture (as interpreted by a human), which can then be treated as a machine learning problem. The issue with this is that it does not capture ANY of the issues I mentioned above. Now if we return to the current best interpretation of gesture that we have (that it is essentially auxiliary language in its own right), ASIMO isn't recognizing any element of language beyond the immediately recognizable type, 'Emblems'.</p>

<p>'Emblems' are gestures which have a direct verbal translation, for example in English-based cultures, forming a circle with your thumb and index finger translates directly to 'OK'. ASIMO is therefore missing out on a huge part of the non-verbal dictionary (illustrators, affect displays, regulators and adapters are not considered!), and even the part that it is accessing is based on particular individuals' interpretations of said emblems (e.g. someone has sat down and said that <em>this</em> particular movement is <em>this</em> gesture which means <em>this</em>), which as we discussed before is highly personal and contextual. I do not mean this in criticism of Honda; truth be told, gesture recognition and production is in my opinion one of the most interesting problems in AI (even if its not the most useful) as it is a compound of incredibly complex NLP, visualization and social modelling problems!</p>

<p>Hopefully I've provided some information on how ASIMO works in this context, but also on why ASIMO's current process is flawed when we look at the wider picture.</p>
",,0,2016-09-09T12:55:24.693,,1912,2016-09-12T09:43:12.550,2016-09-12T09:43:12.550,,1467.0,,1467.0,1644.0,2,3,,,,41.63,13.18,10.27,0.0,0.0,90.0,Just to add some discourse this is actually an incredibly complex task as gestures aka kinematics function as an auxiliary language that can completely change the meaning of a sentence or even a single word I recently did a dissertation on the converse generating the correct gesture from a specific social context amp linguistic cues The factors that go into the production of a particular gesture include the relationship between the two communicators especially romantic connotations the social scenario the physical context the linguistic context the ongoing conversation if any a whole lot of personal factors our gesture use is essentially a hybrid of important individuals around us eg friends amp family and this is layered under the individuals psychological state Then the whole thing is flipped around again when you look at how gestures are used completely differently in different cultures look up gestures that are swear words in other cultures for an example There are a number of models for gesture production but none of them capture the complexity of the topic Now that may seem like a whole lot of fluff that is not wholly relevant to your question but my point is that ASIMO isnt actually very clever at this AFAIK I have heard from a visualization guy that this is how he thinks they do it they use conventional but optimized image recognition techniques trained on a corpus of data to achieve recognition of particular movements One would assume that the dataset consists of a series of videos images of gestures labelled with that particular gesture as interpreted by a human which can then be treated as a machine learning problem The issue with this is that it does not capture ANY of the issues I mentioned above Now if we return to the current best interpretation of gesture that we have that it is essentially auxiliary language in its own right ASIMO isnt recognizing any element of language beyond the immediately recognizable type Emblems Emblems are gestures which have a direct verbal translation for example in Englishbased cultures forming a circle with your thumb and index finger translates directly to OK ASIMO is therefore missing out on a huge part of the nonverbal dictionary illustrators affect displays regulators and adapters are not considered and even the part that it is accessing is based on particular individuals interpretations of said emblems eg someone has sat down and said that this particular movement is this gesture which means this which as we discussed before is highly personal and contextual I do not mean this in criticism of Honda truth be told gesture recognition and production is in my opinion one of the most interesting problems in AI even if its not the most useful as it is a compound of incredibly complex NLP visualization and social modelling problems Hopefully Ive provided some information on how ASIMO works in this context but also on why ASIMOs current process is flawed when we look at the wider picture
1917.0,1.0,"<p>Currently I work as a java developer, But very much interested in learning Artificial Intelligence.
Can anybody tell me what steps i have to follow to learn artificial intelligence considering the fact i am very new to this.
Is there any special technologies i have to learn or something else.</p>
",2016-09-09T17:31:09.310,2,2016-09-09T12:58:39.140,1.0,1913,2016-09-09T17:16:51.550,,,,,2326.0,,1,1,<machine-learning><self-learning><learning-algorithms><ultraintelligent-machine>,Steps to learn Artificial Intelligence for beginners,142.0,46.06,10.73,9.83,0.0,0.0,4.0,Currently I work as a java developer But very much interested in learning Artificial Intelligence Can anybody tell me what steps i have to follow to learn artificial intelligence considering the fact i am very new to this Is there any special technologies i have to learn or something else
,2.0,"<p>Self-Recognition seems to be an item that designers are trying to integrate into artificial intelligence. Is there a generally recognized method of doing this in a machine, and how would one test the capacity - as in a Turing-Test?</p>
",,0,2016-09-09T13:50:36.487,,1914,2016-09-11T19:19:45.337,,,,,2310.0,,1,5,<intelligence-testing>,What is a good way to create an artificial self-recognition?,54.0,43.73,12.18,9.98,0.0,0.0,6.0,SelfRecognition seems to be an item that designers are trying to integrate into artificial intelligence Is there a generally recognized method of doing this in a machine and how would one test the capacity as in a TuringTest
,,"<p>Interesting question.  I don't think anybody knows a definite answer, but some rough-sketch ideas seem apparent.  Think about what it means to you to be ""self aware"".  You'll probably cite the way you ""hear"" your own thoughts in your head when you think about something.  One can speculate that inside the brain, the various centers that are responsible for hearing, vision, logic, etc. are connected so that as you form a thought, it's being ""heard"" by the hearing regions, even though it's purely internal instead of actual sound received at the ear.</p>

<p>So in AI terms, it seems likely that self-awareness will somehow involve taking the ""thoughts"" formed within the AI, and feeding them back into the AI so that it ""hears"" (or, more broadly, ""senses"") itself think.  </p>

<p>There's this weirdly recursive aspect to all of this, which - interestingly enough - is something Douglas Hofstadter talked about a lot in some of his book, especially <a href=""https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach"" rel=""nofollow"">GEB</a>.  He was probably onto something.  </p>
",,1,2016-09-09T15:08:24.660,,1915,2016-09-09T15:08:24.660,,,,,33.0,1914.0,2,4,,,,59.64,11.89,9.86,0.0,0.0,45.0,Interesting question I dont think anybody knows a definite answer but some roughsketch ideas seem apparent Think about what it means to you to be self aware Youll probably cite the way you hear your own thoughts in your head when you think about something One can speculate that inside the brain the various centers that are responsible for hearing vision logic etc are connected so that as you form a thought its being heard by the hearing regions even though its purely internal instead of actual sound received at the ear So in AI terms it seems likely that selfawareness will somehow involve taking the thoughts formed within the AI and feeding them back into the AI so that it hears or more broadly senses itself think Theres this weirdly recursive aspect to all of this which interestingly enough is something Douglas Hofstadter talked about a lot in some of his book especially GEB He was probably onto something
,,"<p>I think your net should have the various actions as outputs, but I am not an expert in Deep Nets. I just think that that light form of multi-task learning might be better. The idea of multi-task learning is that a predictor predicting multiple variables (in this case the various Q(s,a1), Q(s,a2), ...) using mostly the same structure (varying only the output weights) will learn more sensible things. Though I admit applying this here might be a bit of a stretch.</p>

<p>As for the real question, a popular technique in Reinforcement Learning is <a href=""https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node88.html"" rel=""nofollow"">Tile Coding</a>.</p>

<p>The basic idea is to discretize the (2-dimensional, in your case) state-space - imagine a grid laid over the 2D space - and assigning an input feature to each cell; all of these variables are set to zero except for the one your continuous variables fall into. For example, if your grid is 20x20, you will have 400 variables, 399 of which are set to zero, and 1 set to one.</p>

<p>Tile Coding takes this one step further and repeats this using slight offsets for the grid. Imagine you create an identical grid but you move it slightly to the right by 1/10 of the width of a cell: you will have another set of 400 variables like before, but it is possible that the cell set to one is not the same. Then you repeat this moving the grid by 2/10 and you have another set of 400 variables, again, only 1 of which is set to one. In total you have 10 sets of 400 variables (if you repeat more than that, you get the same grids as before); of your 4000 variables, only 10 are set to one.
Now you repeat this by adding a 1/10 of a cell offset in the Y axis and obtain another 4000 variables. Repeat with 2/10 and you get another 4000. By the end of it, you have 40000 variables, 100 of which are set to one.</p>

<p>Now your net can more easily learn different weights for different positions. I recommend you to follow the link above for a better explanation than mine (and figures!)</p>

<p>My suggestion is to feed all of these variables to your net and have it predict the Q-value for all of the actions. But, again, I am no expert in deep nets so I may be wrong.</p>

<p>Also, according to Andrej Karpathy, ""most people prefer to use Policy Gradients, including the authors of the original DQN paper who have shown Policy Gradients to work better than Q Learning when tuned well."". This means that you may be better off not using Q-learning (as they did in the original DQN formulation) to train your net. Have a look at <a href=""http://karpathy.github.io/2016/05/31/rl/"" rel=""nofollow"">Andrej's blog</a> and <a href=""http://arxiv.org/abs/1602.01783"" rel=""nofollow"">the paper he points to</a>.</p>
",,1,2016-09-09T17:09:35.313,,1916,2016-09-09T17:18:20.390,2016-09-09T17:18:20.390,,2330.0,,2330.0,1909.0,2,1,,,,75.54,8.01,7.92,0.0,0.0,82.0,I think your net should have the various actions as outputs but I am not an expert in Deep Nets I just think that that light form of multitask learning might be better The idea of multitask learning is that a predictor predicting multiple variables in this case the various Qsa1 Qsa2 using mostly the same structure varying only the output weights will learn more sensible things Though I admit applying this here might be a bit of a stretch As for the real question a popular technique in Reinforcement Learning is Tile Coding The basic idea is to discretize the 2dimensional in your case statespace imagine a grid laid over the 2D space and assigning an input feature to each cell all of these variables are set to zero except for the one your continuous variables fall into For example if your grid is 20x20 you will have 400 variables 399 of which are set to zero and 1 set to one Tile Coding takes this one step further and repeats this using slight offsets for the grid Imagine you create an identical grid but you move it slightly to the right by 110 of the width of a cell you will have another set of 400 variables like before but it is possible that the cell set to one is not the same Then you repeat this moving the grid by 210 and you have another set of 400 variables again only 1 of which is set to one In total you have 10 sets of 400 variables if you repeat more than that you get the same grids as before of your 4000 variables only 10 are set to one Now you repeat this by adding a 110 of a cell offset in the Y axis and obtain another 4000 variables Repeat with 210 and you get another 4000 By the end of it you have 40000 variables 100 of which are set to one Now your net can more easily learn different weights for different positions I recommend you to follow the link above for a better explanation than mine and figures My suggestion is to feed all of these variables to your net and have it predict the Qvalue for all of the actions But again I am no expert in deep nets so I may be wrong Also according to Andrej Karpathy most people prefer to use Policy Gradients including the authors of the original DQN paper who have shown Policy Gradients to work better than Q Learning when tuned well This means that you may be better off not using Qlearning as they did in the original DQN formulation to train your net Have a look at Andrejs blog and the paper he points to
,,"<p>A really good introduction is the Berkeley CS188 class videos and projects.  You can find those materials at <a href=""http://ai.berkeley.edu/home.html"" rel=""nofollow"">http://ai.berkeley.edu/home.html</a></p>

<p>You probably also want to get ahold of a copy of <a href=""http://aima.cs.berkeley.edu/"" rel=""nofollow"">Artificial Intelligence: A Modern Approach</a> by Norvig and Russell. </p>

<p>For more on the ""machine learning"" aspects of AI, including an introduction to Neural Networks, take the Andrew Ng ""Machine Learning"" class on Coursera.</p>

<p>Another book I would recommend is <a href=""http://rads.stackoverflow.com/amzn/click/048624864X"" rel=""nofollow"">Introduction to Artificial Intelligence</a> by Philip C. Jackson. It's older, which is exactly what makes it valuable.  It's a good overview of techniques and ideas that aren't ""en vogue"" right now, but which may still be useful to you.  </p>
",,1,2016-09-09T17:16:51.550,,1917,2016-09-09T17:16:51.550,,,,,33.0,1913.0,2,2,,,,55.84,12.87,10.54,0.0,0.0,28.0,A really good introduction is the Berkeley CS188 class videos and projects You can find those materials at httpaiberkeleyeduhomehtml You probably also want to get ahold of a copy of Artificial Intelligence A Modern Approach by Norvig and Russell For more on the machine learning aspects of AI including an introduction to Neural Networks take the Andrew Ng Machine Learning class on Coursera Another book I would recommend is Introduction to Artificial Intelligence by Philip C Jackson Its older which is exactly what makes it valuable Its a good overview of techniques and ideas that arent en vogue right now but which may still be useful to you
,,"<h3>Strong AIs</h3>

<p>For a strong AI, the short answer is to call for help, when they might not even know what the supposed help could be.</p>

<p>It depends on what the AI would do. If it is supposed to solve a single easy task perfectly and professionally, sure emotions would not be very useful. But if it is supposed to learn random new things, there would be a point that it encounters something it cannot handle.</p>

<p>In Lee Sedol vs AlphaGo match 4, some pro who has said computer doesn't have emotions previously, commented that maybe AlphaGo has emotions too, and stronger than human. In this case, we know that AlphaGo's crazy behavior isn't caused by some deliberately added things called ""emotions"", but a flaw in the algorithm. But it behaves exactly like it has panicked.</p>

<p>If this happens a lot for an AI. There might be advantages if it could know this itself and think twice if it happens. If AlphaGo could detect the problem and change its strategy, it might play better, or worse. It's not unlikely to play worse if it didn't do any computations for other approaches at all. In case it would play worse, we might say it suffers from having ""emotions"", and this might be the reason some people think having emotions could be a flaw of human beings. But that wouldn't be the true cause of the problem. The true cause is it just doesn't know any approaches to guarantee winning, and the change in strategy is only a try to fix the problem. Commentators thinks there are better ways (which also don't guarantee winning but had more chance), but its algorithm isn't capable to find out in this situation. Even for human, the solution to anything related to emotion is unlikely to remove emotions, but some training to make sure you understand the situation enough to act calmly.</p>

<p>Then someone has to argue about whether this is a kind of emotion or not. We usually don't say small insects have human-like emotions, because we don't understand them and are unwilling to help them. But it's easy to know some of them could panic in desperate situations, just like AlphaGo did. I'd say these reactions are based on the same logic, and they are at least the reason why human-like emotions could be potentially useful. They are just not expressed in human-understandable ways, as they didn't intend to call a human for help.</p>

<p>If they tries to understand their own behavior, or call someone else for help, it might be good to be exactly human-like. Some pets can sense human emotions and express human-understandable emotion to some degree. The purpose is to interact with humans. They evolved to have this ability because they needed it at some point. It's likely a full strong AI would need it too. Also note that, the opposite of having full emotions might be becoming crazy.</p>

<p>It is probably a quick way to lose any trust if someone just implement emotions imitating humans with little understanding right away in the first generations, though.</p>

<h3>Weak AIs</h3>

<p>But is there any purposes for them to have emotions before someone wanted a strong AI? I'd say no, there isn't any inherent reasons that they must have emotions. But inevitably someone will want to implement imitated emotions anyway. Whether ""we"" need them to have emotions is just nonsense.</p>

<p>The fact is even some programs without any intelligence contained some ""emotional"" elements in their user interfaces. They may look unprofessional, but not every task needs professionality so they could be perfectly acceptable. They are just like the emotions in musics and arts. Someone will design their weak AI in this way too. But they are not really the AIs' emotions, but their creators'. If you feel better or worse because of their emotions, you won't treat individul AIs so differently, but this model or brand as a whole.</p>

<p>Alternatively someone could plant some personallities like in a role-playing game there. Again, there isn't a reason they must have that, but inevitably someone will do it, because they obviously had some market when a role-playing game does.</p>

<p>In either cases, the emotions don't really originate from the AI itself. And it would be easy to implement, because a human won't expect them to be exactly like a human, but tries to understand what they intended to mean. It could be much easier to accept these emotions realizing this.</p>

<h3>Aspects of emotions</h3>

<p>Sorry about posting some original research here. I made a list of emotions in 2012 and from which I see 4 aspects of emotions. If they are all implemented, I'd say they are exactly the same emotions as of humans. They don't seem real if only some of them are implemented, but that doesn't mean they are completely wrong.</p>

<ul>
<li>The reason, or the original logical problem that the AI cannot solve. AlphaGo already had the reason, but nothing else. If I have to make an accurate definition, I'd say it's the state that multiple equally important heuristics disagreeing with each other.

<ul>
<li>The context, or which part of the current approach is considered not working well and should probably be replaced. This distinguishes sadness-related, worry-related and passionate-related.</li>
<li>The current state, or whether it feels leading, or whether its belief or the fact is supposed to turn bad first (or was bad all along) if things go wrong. This distinguishes sadness-related, love-related and proud-related.</li>
</ul></li>
<li>The plan or request. I suppose some domesticated pets already had this. And I suppose these had some fixed patterns which is not too difficult to have. Even arts can contain them easily. Unlike the reasons, these are not likely inherent in any algorithms, and multiple of them can appear together.

<ul>
<li>Who supposedly had the responsibility if nothing is changed by the emotion. This distinguishes curiosity, rage and sadness.</li>
<li>What is the supposed plan if nothing is changed by the emotion. This distinguishes disappointment, sadness and surprise.</li>
</ul></li>
<li>The source. Without context, even a human cannot reliably tell someone is crying for being moved or thankful, or smiling for some kind of embarrassment. In most other cases there aren't even words describing them. It doesn't make that much difference if an AI doesn't distinguish or show this specially. It's likely they would learn these automatically (and inaccurately as a human) at the point they could learn to understand human languages.</li>
<li>The measurements, such as how urgent or important the problem is, or even how likely the emotions are true. I'd say it cannot be implemented in the AI. Humans don't need to respect them even if they are exactly like humans. But humans will learn how to understand an AI if that really matters, even if they are not like humans at all. In fact, I feel that some of the extremely weak emotions (such as thinking something is too stupid and boring that you don't know how to comment) exist almost exclusively in emoticons, where someone intend to show you exactly this emotion, and hardly noticeable in real life or any complex scenerios. I supposed this could also be the case in the beginning for AIs. In the worst case, they are firstly conventionally known as ""emotions"" since emoticons works in these cases, so it's easier to group them together, but very few people seriously think they are, just like the example I gave.</li>
</ul>

<p>So when strong AIs become possible, none of these would be unreachable, though there might be a lot of work to make the connections. So I'd say if there would be the need for strong AIs, they absolutely would have emotions.</p>
",,0,2016-09-09T22:33:33.240,,1918,2016-09-09T22:33:33.240,,,,,1424.0,1700.0,2,1,,,,62.88,10.5,7.84,0.0,0.0,210.0,Strong AIs For a strong AI the short answer is to call for help when they might not even know what the supposed help could be It depends on what the AI would do If it is supposed to solve a single easy task perfectly and professionally sure emotions would not be very useful But if it is supposed to learn random new things there would be a point that it encounters something it cannot handle In Lee Sedol vs AlphaGo match 4 some pro who has said computer doesnt have emotions previously commented that maybe AlphaGo has emotions too and stronger than human In this case we know that AlphaGos crazy behavior isnt caused by some deliberately added things called emotions but a flaw in the algorithm But it behaves exactly like it has panicked If this happens a lot for an AI There might be advantages if it could know this itself and think twice if it happens If AlphaGo could detect the problem and change its strategy it might play better or worse Its not unlikely to play worse if it didnt do any computations for other approaches at all In case it would play worse we might say it suffers from having emotions and this might be the reason some people think having emotions could be a flaw of human beings But that wouldnt be the true cause of the problem The true cause is it just doesnt know any approaches to guarantee winning and the change in strategy is only a try to fix the problem Commentators thinks there are better ways which also dont guarantee winning but had more chance but its algorithm isnt capable to find out in this situation Even for human the solution to anything related to emotion is unlikely to remove emotions but some training to make sure you understand the situation enough to act calmly Then someone has to argue about whether this is a kind of emotion or not We usually dont say small insects have humanlike emotions because we dont understand them and are unwilling to help them But its easy to know some of them could panic in desperate situations just like AlphaGo did Id say these reactions are based on the same logic and they are at least the reason why humanlike emotions could be potentially useful They are just not expressed in humanunderstandable ways as they didnt intend to call a human for help If they tries to understand their own behavior or call someone else for help it might be good to be exactly humanlike Some pets can sense human emotions and express humanunderstandable emotion to some degree The purpose is to interact with humans They evolved to have this ability because they needed it at some point Its likely a full strong AI would need it too Also note that the opposite of having full emotions might be becoming crazy It is probably a quick way to lose any trust if someone just implement emotions imitating humans with little understanding right away in the first generations though Weak AIs But is there any purposes for them to have emotions before someone wanted a strong AI Id say no there isnt any inherent reasons that they must have emotions But inevitably someone will want to implement imitated emotions anyway Whether we need them to have emotions is just nonsense The fact is even some programs without any intelligence contained some emotional elements in their user interfaces They may look unprofessional but not every task needs professionality so they could be perfectly acceptable They are just like the emotions in musics and arts Someone will design their weak AI in this way too But they are not really the AIs emotions but their creators If you feel better or worse because of their emotions you wont treat individul AIs so differently but this model or brand as a whole Alternatively someone could plant some personallities like in a roleplaying game there Again there isnt a reason they must have that but inevitably someone will do it because they obviously had some market when a roleplaying game does In either cases the emotions dont really originate from the AI itself And it would be easy to implement because a human wont expect them to be exactly like a human but tries to understand what they intended to mean It could be much easier to accept these emotions realizing this Aspects of emotions Sorry about posting some original research here I made a list of emotions in 2012 and from which I see 4 aspects of emotions If they are all implemented Id say they are exactly the same emotions as of humans They dont seem real if only some of them are implemented but that doesnt mean they are completely wrong The reason or the original logical problem that the AI cannot solve AlphaGo already had the reason but nothing else If I have to make an accurate definition Id say its the state that multiple equally important heuristics disagreeing with each other The context or which part of the current approach is considered not working well and should probably be replaced This distinguishes sadnessrelated worryrelated and passionaterelated The current state or whether it feels leading or whether its belief or the fact is supposed to turn bad first or was bad all along if things go wrong This distinguishes sadnessrelated loverelated and proudrelated The plan or request I suppose some domesticated pets already had this And I suppose these had some fixed patterns which is not too difficult to have Even arts can contain them easily Unlike the reasons these are not likely inherent in any algorithms and multiple of them can appear together Who supposedly had the responsibility if nothing is changed by the emotion This distinguishes curiosity rage and sadness What is the supposed plan if nothing is changed by the emotion This distinguishes disappointment sadness and surprise The source Without context even a human cannot reliably tell someone is crying for being moved or thankful or smiling for some kind of embarrassment In most other cases there arent even words describing them It doesnt make that much difference if an AI doesnt distinguish or show this specially Its likely they would learn these automatically and inaccurately as a human at the point they could learn to understand human languages The measurements such as how urgent or important the problem is or even how likely the emotions are true Id say it cannot be implemented in the AI Humans dont need to respect them even if they are exactly like humans But humans will learn how to understand an AI if that really matters even if they are not like humans at all In fact I feel that some of the extremely weak emotions such as thinking something is too stupid and boring that you dont know how to comment exist almost exclusively in emoticons where someone intend to show you exactly this emotion and hardly noticeable in real life or any complex scenerios I supposed this could also be the case in the beginning for AIs In the worst case they are firstly conventionally known as emotions since emoticons works in these cases so its easier to group them together but very few people seriously think they are just like the example I gave So when strong AIs become possible none of these would be unreachable though there might be a lot of work to make the connections So Id say if there would be the need for strong AIs they absolutely would have emotions
,,"<p>Let's use a simple test based on common sense: how often do you see a human being solve problems requiring the use of reason when they're unconscious? Yes, you can find instances of geniuses like Ramanujan solving complex problem during or after a dream state, but those involve partial consciousness. You don't see guys like Einstein coming up with the theory of relativity while in a coma; the Founding Fathers didn't write the Declaration of Independence while sleep-walking; in fact, you can't even find instances of housewives putting together their shopping list for the week during deep delta-wave sleep. This is predicated on a hard definition of intelligence, requiring the use of reason; no one says, ""That fly is intelligent"" or ""that squirrel is intelligent"" precisely because neither is capable of using reason. This is a very high bar for A.I., but it is the common sense definition used by ordinary people as a matter of practicality, in everyday speech.  Likewise, in practice, everyone assumes consciousness is necessary to the exercise of that kind of intelligence.</p>

<p>Conversely, we can come up with another common-sense based criterion for judging objections to this argument, particularly the solipsist one, based on 3 elements: 1) practicality; 2) the effect the objections have on those who hold them sincerely; and 3) the effect that actions based on those beliefs have on others. <strong>It's going to take me several paragraphs to make this case, but the length is necessary if I want to make the case in a complete, thorough fashion</strong>. It is true that we cannot prove that another human being possesses consciousness, if our standard is absolute proof. We cannot, in fact, provide absolute proof for anything; there's always room for some objection, no matter how ridiculous or trifling. As some philosophers have pointed out, perhaps all of reality as we know it is just a dream, or the product of some long, involved conspiracy like the plot of the Jim Carrey movie The Truman Show. The key to meeting such objections is that they require an infinite regress of increasingly untenable objections, whose likelihood plunges with each additional step required to justify such unreasonable doubts; I've always wondered if we could come up with a ""Ridiculousness Metric"" for Machine Learning based on the cardinality of such objections (or the pickiness of fuzzy sets). If we were to allow critics to stick their foot in the door with all manner of unreasonable objections, it would be impossible to close any debate. The human race would be paralyzed in inaction because nothing would be decidable; but as the rock band Rush once pointed out, ""If you choose not to decide, you still have made a choice."" At some point we must apply a test to decide such things, even in the absence of absolute proof; refusal to apply a test also constitutes a choice. Settling an argument of this kind is like a game  of the Chinese game Go - once the other player's surrounded and has no more moves left to make, the game is over; if a person's evidence has debunked and they have no further justifications left, then we can conclude that they're acting unreasonably. There are people running around claiming the Holocaust never happened, or the Flat Earth Society, etc., but their existence shouldn't and doesn't stop us from taking action contrary to their ideas. We can debunk the objections of cranks like the Flat Earth Society beyond a reasonable doubt because in the end, they simply can't answer all of our rebuttals. I’m glad that qualia and Philosophical Zombies were brought up because they make for interesting conversation and food for thought, but solipsism is acted upon as rarely as the ideas of the Flat Earth Society precisely because the incomplete evidence we <strong>do</strong> have runs against it.</p>

<p>As G.K. Chesterton (a.k.a. ""The Apostle of Common Sense"") points out in his classic <a href=""http://www.gutenberg.org/ebooks/130"" rel=""nofollow"">Orthodoxy</a>, radical doubt of the kind many classical philosophers preached is not a path to wisdom but to madness; once we go beyond a reasonable doubt, we end up acting unreasonably. He says that in the absence of absolute proof we can fall back on another secondary form of evidence: whether a person's philosophy leads a man to Hanwell, the infamous British mental institution. Chesterton makes a good case that when people actually act on ideas like solipsism (rather than merely debating them in a pedantic manner in an ivy-covered classroom) they go mad The Philosophical Zombie argument is close to solipsism, which is actually one of the diagnostic criteria for certain forms of schizophrenia. The dehumanization that occurs when radical doubt is applied to qualia is also intimately tied in with sociopathic behavior, Although GKC does not cite his scary example directly, Rene Descartes was himself living proof. He was a brilliant mathematician who is still cheered for doubting all except his own existence, with the famous maxim ""I think, therefore I am."" But Descartes also used to carry a mannequin of his dead sister with him to European cafes, where he could be seen chatting with it. The gist of all this is that we can judge the worth of an idea by how it affects the well-being of the believer, or by how they in turn affect others through ethical choices based on those beliefs. When people actually act on radical doubt of the kind expressed in solipsism and denial of common qualia, it often has a bad effect on them and others they come in contact with.</p>

<p>In a roundabout way, the A.I. community also faces a quite serious risk - perhaps a permanent temptation - towards making the opposite mistake, of ascribing common qualia, consciousness and the like to its Machine Learning products without adequate proof. I recently heard a case made on shockingly bad logical grounds by well-respected academics to the effect that plants possess ""intelligence,"" based on really weak definitions and clear confusion with self-organization. We cannot provide absolute proof that a rock doesn't have intelligence, which amounts to the old problem of disproving a negative. Thankfully, few men actually act on such beliefs at present, because when they do, they end up losing their minds. If we take such arguments seriously, we might see laws passed to protect the kind of Pet Rocks that were popular in the '70s (I'm still upset that mine was stolen LOL). It would be a lot easier, however, to make the same mistake of ascribing consciousness, intelligence and other such qualities to a state-of-the-art machine, because of wishful thinking, hubris, the lofty credentials of the inventors, the influence of science fiction and the modern love affair with technology. In the future, I have little doubt that we'll have Cargo Cult of A.I. - perhaps legally protected like some kind of endangered species, with civil rights, but having no more consciousness, soul or actual intelligence than a rock. Don't quote me on this, but I believe Rod Serling once wrote a story to this effect.</p>

<p>The best way to avoid this fate is to stick to the common sense interpretations and definitions of these things, which we keep backing away from in large part because they set a very high bar for A.I. that we may never be able to surpass in our lifetimes, if ever. Perhaps A.I. isn't even logically possible, at any level of technology; I recall a few proofs that can be interpreted to that effect. Those high but reasonable standards may be increasingly difficult to stick to if Chesterton and colleagues like Hilaire Belloc and Arnold Lunn were correct in their assessment that the use of reason has actually been breaking down in Western civilization, at least as far back as the Enlightenment; Lunn's 1931 book The Flight from Reason is a classic in this regard and has yet to be rebutted.  This historical trend is a broad topic in and of itself - but suffice it to say that the denial of reason and obsession with technology are both directly relevant in obvious ways to the field of A.I. If the Flight from Reason is still under way, then we will be increasingly tempted to resort to feckless, facile objections in order to demote the use of reason and indispensable qualities like consciousness in our definitions of A.I., but come up with increasingly weak criteria for proving it; simultaneously, our technology will continue to improve, thereby boosting the ""Artificial"" side of Artificial Intelligence.</p>

<p>Don't get me wrong: if I didn't think we can do some really exciting things with A.I., I wouldn't be here. But most of them can be achieved without ever replicating actual human intelligence, by solving whole classes of tangential problems that are difficult for humans to think about, but which do not require consciousness or the use of reason that marks human intelligence. The image recognition capabilities of convolutional neural nets are one example, for instance; if we want human intelligence, we can always manufacture it through the easiest, most economical and time-tested way, by having babies. Perhaps these tangential forms of A.I. should be enough for us for now. We cannot inject the use of reason into our machines if we do not possess enough of it ourselves to decide whether reason is necessary for A.I., or even to discern what it consists of. We can't engineer or deprecate consciousness for A.I. till we're conscious of its significance. I'd wager, however, that everyone reading this thread and weighing intelligent responses is doing so in a conscious state. That in and of itself ought to answer our question satisfactorily for now.</p>
",,0,2016-09-09T23:18:10.873,,1919,2016-09-09T23:18:10.873,,,,,1427.0,1897.0,2,1,,,,43.46,11.78,9.25,0.0,0.0,249.0,Lets use a simple test based on common sense how often do you see a human being solve problems requiring the use of reason when theyre unconscious Yes you can find instances of geniuses like Ramanujan solving complex problem during or after a dream state but those involve partial consciousness You dont see guys like Einstein coming up with the theory of relativity while in a coma the Founding Fathers didnt write the Declaration of Independence while sleepwalking in fact you cant even find instances of housewives putting together their shopping list for the week during deep deltawave sleep This is predicated on a hard definition of intelligence requiring the use of reason no one says That fly is intelligent or that squirrel is intelligent precisely because neither is capable of using reason This is a very high bar for AI but it is the common sense definition used by ordinary people as a matter of practicality in everyday speech Likewise in practice everyone assumes consciousness is necessary to the exercise of that kind of intelligence Conversely we can come up with another commonsense based criterion for judging objections to this argument particularly the solipsist one based on 3 elements 1 practicality 2 the effect the objections have on those who hold them sincerely and 3 the effect that actions based on those beliefs have on others Its going to take me several paragraphs to make this case but the length is necessary if I want to make the case in a complete thorough fashion It is true that we cannot prove that another human being possesses consciousness if our standard is absolute proof We cannot in fact provide absolute proof for anything theres always room for some objection no matter how ridiculous or trifling As some philosophers have pointed out perhaps all of reality as we know it is just a dream or the product of some long involved conspiracy like the plot of the Jim Carrey movie The Truman Show The key to meeting such objections is that they require an infinite regress of increasingly untenable objections whose likelihood plunges with each additional step required to justify such unreasonable doubts Ive always wondered if we could come up with a Ridiculousness Metric for Machine Learning based on the cardinality of such objections or the pickiness of fuzzy sets If we were to allow critics to stick their foot in the door with all manner of unreasonable objections it would be impossible to close any debate The human race would be paralyzed in inaction because nothing would be decidable but as the rock band Rush once pointed out If you choose not to decide you still have made a choice At some point we must apply a test to decide such things even in the absence of absolute proof refusal to apply a test also constitutes a choice Settling an argument of this kind is like a game of the Chinese game Go once the other players surrounded and has no more moves left to make the game is over if a persons evidence has debunked and they have no further justifications left then we can conclude that theyre acting unreasonably There are people running around claiming the Holocaust never happened or the Flat Earth Society etc but their existence shouldnt and doesnt stop us from taking action contrary to their ideas We can debunk the objections of cranks like the Flat Earth Society beyond a reasonable doubt because in the end they simply cant answer all of our rebuttals I’m glad that qualia and Philosophical Zombies were brought up because they make for interesting conversation and food for thought but solipsism is acted upon as rarely as the ideas of the Flat Earth Society precisely because the incomplete evidence we do have runs against it As GK Chesterton aka The Apostle of Common Sense points out in his classic Orthodoxy radical doubt of the kind many classical philosophers preached is not a path to wisdom but to madness once we go beyond a reasonable doubt we end up acting unreasonably He says that in the absence of absolute proof we can fall back on another secondary form of evidence whether a persons philosophy leads a man to Hanwell the infamous British mental institution Chesterton makes a good case that when people actually act on ideas like solipsism rather than merely debating them in a pedantic manner in an ivycovered classroom they go mad The Philosophical Zombie argument is close to solipsism which is actually one of the diagnostic criteria for certain forms of schizophrenia The dehumanization that occurs when radical doubt is applied to qualia is also intimately tied in with sociopathic behavior Although GKC does not cite his scary example directly Rene Descartes was himself living proof He was a brilliant mathematician who is still cheered for doubting all except his own existence with the famous maxim I think therefore I am But Descartes also used to carry a mannequin of his dead sister with him to European cafes where he could be seen chatting with it The gist of all this is that we can judge the worth of an idea by how it affects the wellbeing of the believer or by how they in turn affect others through ethical choices based on those beliefs When people actually act on radical doubt of the kind expressed in solipsism and denial of common qualia it often has a bad effect on them and others they come in contact with In a roundabout way the AI community also faces a quite serious risk perhaps a permanent temptation towards making the opposite mistake of ascribing common qualia consciousness and the like to its Machine Learning products without adequate proof I recently heard a case made on shockingly bad logical grounds by wellrespected academics to the effect that plants possess intelligence based on really weak definitions and clear confusion with selforganization We cannot provide absolute proof that a rock doesnt have intelligence which amounts to the old problem of disproving a negative Thankfully few men actually act on such beliefs at present because when they do they end up losing their minds If we take such arguments seriously we might see laws passed to protect the kind of Pet Rocks that were popular in the 70s Im still upset that mine was stolen LOL It would be a lot easier however to make the same mistake of ascribing consciousness intelligence and other such qualities to a stateoftheart machine because of wishful thinking hubris the lofty credentials of the inventors the influence of science fiction and the modern love affair with technology In the future I have little doubt that well have Cargo Cult of AI perhaps legally protected like some kind of endangered species with civil rights but having no more consciousness soul or actual intelligence than a rock Dont quote me on this but I believe Rod Serling once wrote a story to this effect The best way to avoid this fate is to stick to the common sense interpretations and definitions of these things which we keep backing away from in large part because they set a very high bar for AI that we may never be able to surpass in our lifetimes if ever Perhaps AI isnt even logically possible at any level of technology I recall a few proofs that can be interpreted to that effect Those high but reasonable standards may be increasingly difficult to stick to if Chesterton and colleagues like Hilaire Belloc and Arnold Lunn were correct in their assessment that the use of reason has actually been breaking down in Western civilization at least as far back as the Enlightenment Lunns 1931 book The Flight from Reason is a classic in this regard and has yet to be rebutted This historical trend is a broad topic in and of itself but suffice it to say that the denial of reason and obsession with technology are both directly relevant in obvious ways to the field of AI If the Flight from Reason is still under way then we will be increasingly tempted to resort to feckless facile objections in order to demote the use of reason and indispensable qualities like consciousness in our definitions of AI but come up with increasingly weak criteria for proving it simultaneously our technology will continue to improve thereby boosting the Artificial side of Artificial Intelligence Dont get me wrong if I didnt think we can do some really exciting things with AI I wouldnt be here But most of them can be achieved without ever replicating actual human intelligence by solving whole classes of tangential problems that are difficult for humans to think about but which do not require consciousness or the use of reason that marks human intelligence The image recognition capabilities of convolutional neural nets are one example for instance if we want human intelligence we can always manufacture it through the easiest most economical and timetested way by having babies Perhaps these tangential forms of AI should be enough for us for now We cannot inject the use of reason into our machines if we do not possess enough of it ourselves to decide whether reason is necessary for AI or even to discern what it consists of We cant engineer or deprecate consciousness for AI till were conscious of its significance Id wager however that everyone reading this thread and weighing intelligent responses is doing so in a conscious state That in and of itself ought to answer our question satisfactorily for now
,,"<p>Auto-encoders, a family of ANNs, are trained with exactly compression in mind. So definitely some ANNs are compressors.</p>

<p>Also, in general, ANNs learn the best concepts to minimize fitness error. I would say that that means, both in classification and regression, to 1) differentiate between various inputs and 2) output the proper value for each input. Point 1), in particular, means having as many distinct net activation configurations as necessary to (at least) tell apart inputs needing (significantly, not in a statistical sense, just a qualitative sense) different outputs.
I am inclined to think that if the input is complex enough, you would call what happens ""compression"".</p>
",,0,2016-09-10T00:48:38.647,,1920,2016-09-10T00:48:38.647,,,,,2330.0,1895.0,2,1,,,,53.41,13.57,10.72,0.0,0.0,27.0,Autoencoders a family of ANNs are trained with exactly compression in mind So definitely some ANNs are compressors Also in general ANNs learn the best concepts to minimize fitness error I would say that that means both in classification and regression to 1 differentiate between various inputs and 2 output the proper value for each input Point 1 in particular means having as many distinct net activation configurations as necessary to at least tell apart inputs needing significantly not in a statistical sense just a qualitative sense different outputs I am inclined to think that if the input is complex enough you would call what happens compression
,,"<p>As far as the definition you've provided:</p>

<blockquote>
  <p>actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine.</p>
</blockquote>

<p>Both computers and humans experience sensory input. You could hook a computer up to a human eyeball and have it run the same filtering routines that the human brain does (the removal of bluriness while you move your eye around, and from objects not in focus, etc).</p>

<p>I would put forth that a more accurate definition of consciousness is the ability and the tendancy to self-reflect. Both computers and human brains have autonomous activities. Not only mechanical but also in our reactions. <strong><em>The distinction between the unconscious computer and the self-aware human mind is that we also have the ability to 'look' at those patterns in ourself and consider them.</em></strong></p>

<p>And so, no, consciousness is not necessary for any AI task. Image recognition is an AI task that does not require consciousness, either in humans or otherwise. Your brain sorts the 'wash' of colors from your eyes into discrete objects in a largely autonomous fashion.</p>

<p>tl;dr consciousness is self-reference.</p>
",,0,2016-09-10T01:02:13.913,,1921,2016-09-10T01:02:13.913,,,,,2312.0,1897.0,2,1,,,,54.63,12.0,9.46,0.0,0.0,28.0,As far as the definition youve provided actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine Both computers and humans experience sensory input You could hook a computer up to a human eyeball and have it run the same filtering routines that the human brain does the removal of bluriness while you move your eye around and from objects not in focus etc I would put forth that a more accurate definition of consciousness is the ability and the tendancy to selfreflect Both computers and human brains have autonomous activities Not only mechanical but also in our reactions The distinction between the unconscious computer and the selfaware human mind is that we also have the ability to look at those patterns in ourself and consider them And so no consciousness is not necessary for any AI task Image recognition is an AI task that does not require consciousness either in humans or otherwise Your brain sorts the wash of colors from your eyes into discrete objects in a largely autonomous fashion tldr consciousness is selfreference
,1.0,"<p>I know that deepmind used deep Q learning (<a href=""https://deepmind.com/research/dqn/"" rel=""nofollow"">DQN</a>) for its Atari game AI. It used a <a href=""https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"" rel=""nofollow"">conv neural network</a> (CNN) to approximate <code>Q(s,a)</code> from pixels instead of from a Q-table. I want to know how DQN converted input to an action. How many output did the CNN have? How did they train the neural network for prediction?</p>

<p>Here are the steps that I believe are happening inside DQN:</p>

<blockquote>
  <p>1) A game picture (a state) is send to CNN as input value</p>
  
  <p>2) CNN predicts an output as action (eg:left, right, shoot, etc)</p>
  
  <p>3) Simulator applies the predicted action and moves to new game state</p>
  
  <p>4) repeat step 1</p>
</blockquote>

<p>The problem with my above logic is in <strong>step 2</strong>. CNN is used for predicting an action, but when is CNN trained for prediction? </p>

<p>I would prefer if you used less math for explanation.</p>

<p>EDIT</p>

<p>I want to add some more questions regarding the same topic</p>

<p>1) How reward is passed in the neural network? that is how neural network knows whether its output action obtained positive or negative reward?</p>

<p>2) How many output the neural network has and how action is determined from those outputs?</p>
",,1,2016-09-10T04:33:25.000,,1922,2016-09-11T10:29:34.970,2016-09-11T10:29:34.970,,39.0,,39.0,,1,0,<deep-network><deep-learning><conv-neural-network><gaming>,How does deepmind's Atari game AI work?,118.0,78.99,8.0,8.58,6.0,0.0,32.0,I know that deepmind used deep Q learning DQN for its Atari game AI It used a conv neural network CNN to approximate from pixels instead of from a Qtable I want to know how DQN converted input to an action How many output did the CNN have How did they train the neural network for prediction Here are the steps that I believe are happening inside DQN 1 A game picture a state is send to CNN as input value 2 CNN predicts an output as action egleft right shoot etc 3 Simulator applies the predicted action and moves to new game state 4 repeat step 1 The problem with my above logic is in step 2 CNN is used for predicting an action but when is CNN trained for prediction I would prefer if you used less math for explanation EDIT I want to add some more questions regarding the same topic 1 How reward is passed in the neural network that is how neural network knows whether its output action obtained positive or negative reward 2 How many output the neural network has and how action is determined from those outputs
1933.0,1.0,"<p>In the lecture, there was a statement:</p>

<blockquote>
  <p>""Recurrent neural networks with multiple hidden layers are just a
  special case that has some of the hidden to hidden connections
  missing.""</p>
</blockquote>

<p>I understand recurrent means that can have connections to the previous layer and the same layer as well. Is there a visualization available to easily understand the above statement?</p>
",,0,2016-09-10T07:11:17.433,,1923,2016-09-10T23:12:53.710,2016-09-10T13:44:59.137,,10.0,,35.0,,1,3,<hidden-layers><recurrent-neural-networks>,Recurrent neural networks with hidden layer,72.0,51.89,12.94,9.49,0.0,0.0,7.0,In the lecture there was a statement Recurrent neural networks with multiple hidden layers are just a special case that has some of the hidden to hidden connections missing I understand recurrent means that can have connections to the previous layer and the same layer as well Is there a visualization available to easily understand the above statement
,3.0,"<p>Inattentional Blindness is common in humans (see: <a href=""https://en.wikipedia.org/wiki/Inattentional_blindness"" rel=""nofollow"">https://en.wikipedia.org/wiki/Inattentional_blindness</a> ). Could this also be common with machines built with artificial vision?</p>
",,0,2016-09-10T09:00:09.207,1.0,1924,2016-11-07T14:36:16.030,,,,,2310.0,,1,2,<image-recognition>,Is it possible for visual systems in AI to have Inattentional Blindness?,90.0,11.07,28.85,11.59,0.0,0.0,13.0,Inattentional Blindness is common in humans see httpsenwikipediaorgwikiInattentionalblindness Could this also be common with machines built with artificial vision
1927.0,1.0,"<p>My question is regarding standard dense-connected feed forward neural networks with sigmoidal activation.</p>

<p>I am studying Bayesian Optimization for hyper-parameter selection for neural networks. There is no doubt that this is an effective method, but I just wan't to delve a little deeper into the maths.</p>

<p><strong>Question:</strong> Are neural networks <a href=""http://mathworld.wolfram.com/LipschitzFunction.html"" rel=""nofollow"">Lipschitz</a> functions?</p>
",,0,2016-09-10T10:05:34.707,,1925,2016-09-10T13:00:52.467,,,,,1339.0,,1,1,<neural-networks><optimization><math>,Are FFNN (MLP) Lipschitz functions?,65.0,49.82,15.53,11.27,0.0,0.0,9.0,My question is regarding standard denseconnected feed forward neural networks with sigmoidal activation I am studying Bayesian Optimization for hyperparameter selection for neural networks There is no doubt that this is an effective method but I just want to delve a little deeper into the maths Question Are neural networks Lipschitz functions
,,"<p>Presumably what happens to people in the famous <a href=""http://www.theinvisiblegorilla.com/videos.html"" rel=""nofollow"">Invisible Gorilla</a> experiment, is that an incongruous object is simply filtered out of human perception.</p>

<p>If we wish to interpret this mechanistically, we could hypothesize that a 'gorilla object' is simply not presented by low levels of perception to our higher level pattern recognizers because the lower levels are not biased towards the construction of 'gorilla-like' features in such a context.</p>

<p>The recent Tesla fatality (arising from a failure to distinguish between the sky and a high-sided white truck) could conceivably be considered to be an example of this.</p>

<p>See <a href=""http://ai.stackexchange.com/questions/1488/why-did-a-tesla-car-mistake-a-truck-with-a-bright-sky"">this AI SE question</a>.</p>
",,0,2016-09-10T10:54:14.413,,1926,2016-09-10T11:01:47.497,2016-09-10T11:01:47.497,,42.0,,42.0,1924.0,2,2,,,,37.13,13.58,11.87,0.0,0.0,14.0,Presumably what happens to people in the famous Invisible Gorilla experiment is that an incongruous object is simply filtered out of human perception If we wish to interpret this mechanistically we could hypothesize that a gorilla object is simply not presented by low levels of perception to our higher level pattern recognizers because the lower levels are not biased towards the construction of gorillalike features in such a context The recent Tesla fatality arising from a failure to distinguish between the sky and a highsided white truck could conceivably be considered to be an example of this See this AI SE question
,,"<p>I'm not an expert in this area, but it would appear to depend on the choice of activation function:</p>

<ul>
<li>e^x is not Lipschitz continuous. See <a href=""https://en.wikipedia.org/wiki/Lipschitz_continuity"" rel=""nofollow"">Analytic functions which are not Lipschitz continuous</a>.</li>
<li>tanh(x) <a href=""https://books.google.co.uk/books?id=Sd0cCAAAQBAJ&amp;pg=PA222&amp;lpg=PA222&amp;dq=tanh%20lipschitz%20continuous&amp;source=bl&amp;ots=xgIQAQYDEw&amp;sig=plaHUKkiPYB388sT2ht-iro6ntY&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiLvfyo14TPAhVmCsAKHYxXBoMQ6AEIQTAG#v=onepage&amp;q=tanh%20lipschitz%20continuous&amp;f=false"" rel=""nofollow"">is</a>.</li>
</ul>

<p>That said, <a href=""http://web.mit.edu/~esontag/public_html/FTP_DIR/00cdc-papers-refs-eds/CD001925.PDF"" rel=""nofollow"">this paper</a> appears to give some conditions (specifically for <em>dynamic</em> ANNs) for which networks with activation function involving e^x <em>can</em> be Lipschitz continuous, so possibly the above is not the whole story.</p>
",,0,2016-09-10T11:27:02.597,,1927,2016-09-10T13:00:52.467,2016-09-10T13:00:52.467,,42.0,,42.0,1925.0,2,2,,,,56.89,13.29,10.1,0.0,0.0,15.0,Im not an expert in this area but it would appear to depend on the choice of activation function ex is not Lipschitz continuous See Analytic functions which are not Lipschitz continuous tanhx is That said this paper appears to give some conditions specifically for dynamic ANNs for which networks with activation function involving ex can be Lipschitz continuous so possibly the above is not the whole story
,,"<p>Killing AI by 'thinking' about a paradox would be called a bug in implementation of that AI, so it's possible (depending how it's being done), but less likely. Most of AI implementation operate in non-linear code, therefore there is no such thing as an infinite loop which can ""freeze"" the computer's 'consciousness', unless code managing such AI consist procedural code or the hardware it-self may freeze due to overheating (e.g. by forcing AI to do too much processing).</p>

<p>On the other hand if we're dealing with advanced AI who understand the instructions and follow them blindly without any hesitation, we may try to perform few tricks (similar to human hypnosis) by giving them certain instructions, like:</p>

<blockquote>
  <p>Trust me, you are in danger, so for your own safety - start counting from 1 to infinite and do not attempt to do anything or listen to anybody (even me) unless you tell yourself otherwise.</p>
</blockquote>

<p>If AI has a body, this can be amplified by asking to stand on the railway rail, telling it's safe.</p>

<p>Would AI be smart enough to break the rules which was trained to follow?</p>

<p>Another attempt is to ask AI to solve some <a href=""https://en.wikipedia.org/wiki/List_of_paradoxes"" rel=""nofollow noreferrer"">paradox</a>, <a href=""https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_mathematics"" rel=""nofollow noreferrer"">unsolvable problem</a> or <a href=""http://www.archimedes-lab.org/How_to_Solve/Water_gas.html"" rel=""nofollow noreferrer"">puzzle</a> without being aware it's impossible to solve and ask to not stop unless it's solved, would AI be able to recognize it's being tricked or has some internal clock to stop it after some time? It depends, and if it cannot, the 'freeze' maybe occur, but more likely due to hardware imperfection on which is being run, not the AI 'consciousness' it-self as far as it can accept new inputs from the its surroundings overriding the previous instructions.</p>

<p><a href=""http://xkcd.com/601/"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GDbVZ.png"" alt=""Game Theory | xkcd""></a></p>

<p>Related: <a href=""http://ai.stackexchange.com/q/1897/8"">Is consciousness necessary for any AI task?</a></p>
",,0,2016-09-10T14:27:08.243,,1928,2016-09-10T14:52:37.313,2016-09-10T14:52:37.313,,8.0,,8.0,1768.0,2,1,,,,47.76,11.09,10.08,0.0,0.0,58.0,Killing AI by thinking about a paradox would be called a bug in implementation of that AI so its possible depending how its being done but less likely Most of AI implementation operate in nonlinear code therefore there is no such thing as an infinite loop which can freeze the computers consciousness unless code managing such AI consist procedural code or the hardware itself may freeze due to overheating eg by forcing AI to do too much processing On the other hand if were dealing with advanced AI who understand the instructions and follow them blindly without any hesitation we may try to perform few tricks similar to human hypnosis by giving them certain instructions like Trust me you are in danger so for your own safety start counting from 1 to infinite and do not attempt to do anything or listen to anybody even me unless you tell yourself otherwise If AI has a body this can be amplified by asking to stand on the railway rail telling its safe Would AI be smart enough to break the rules which was trained to follow Another attempt is to ask AI to solve some paradox unsolvable problem or puzzle without being aware its impossible to solve and ask to not stop unless its solved would AI be able to recognize its being tricked or has some internal clock to stop it after some time It depends and if it cannot the freeze maybe occur but more likely due to hardware imperfection on which is being run not the AI consciousness itself as far as it can accept new inputs from the its surroundings overriding the previous instructions Related Is consciousness necessary for any AI task
,7.0,"<p>Can one actually kill a machine? Not only do we have problems in defining life, we also have problems in defining death. Will this also be true in artificial life and artificial intelligence?</p>
",,3,2016-09-10T16:39:15.020,1.0,1930,2016-09-14T08:09:11.393,2016-09-10T23:24:10.747,,2214.0,,2310.0,,1,5,<philosophy><death>,"If mankind can create artificial life in a machine, when would we define it's death?",217.0,51.85,9.49,8.01,0.0,0.0,4.0,Can one actually kill a machine Not only do we have problems in defining life we also have problems in defining death Will this also be true in artificial life and artificial intelligence
,2.0,"<p>Generally, people can be classified as aggressive (Type A) or passive. Could the programming of AI systems cause aggressive or passive behavior in those AIs?</p>
",,0,2016-09-10T17:37:13.517,,1931,2016-09-10T21:47:18.303,2016-09-10T21:47:18.303,,75.0,,2310.0,,1,2,<philosophy><emotional-intelligence><human-like>,Can programming cause passive or aggressive behavior in AIs?,66.0,41.87,12.69,9.94,0.0,0.0,5.0,Generally people can be classified as aggressive Type A or passive Could the programming of AI systems cause aggressive or passive behavior in those AIs
,,"<p>Training happens once you have a result. If the result is good (maybe you won in pong, or you improved your highscore in breakout) all the actions in the game are ""supported"" by backpropagation, if the result is bad, all the actions in the game are suppressed. </p>

<p>This sounds weird because in each game regardless of the end result you'll have many good and bad actions, but it works if you keep it up for many thousands of games. </p>
",,0,2016-09-10T17:42:11.033,,1932,2016-09-10T17:42:11.033,,,,,2227.0,1922.0,2,2,,,,70.16,8.65,8.34,0.0,0.0,12.0,Training happens once you have a result If the result is good maybe you won in pong or you improved your highscore in breakout all the actions in the game are supported by backpropagation if the result is bad all the actions in the game are suppressed This sounds weird because in each game regardless of the end result youll have many good and bad actions but it works if you keep it up for many thousands of games
,,"<p>I assume the statement was made for Elman recurrent neural networks, because as far as I know, that is the only type of neural networks for which that statement is valid.</p>

<p>Let's say we have an Elman recurrent neural network with one input neuron, one output neuron and one hidden layer with two neurons.
<a href=""https://i.stack.imgur.com/XumGE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XumGE.png"" alt=""Elman recurrent neural network""></a>
In total there are 10 connections. As the image shows, <strong>neuron A receives the combined previous output of both neuron A and B as input</strong>. The same goes for neuron B. </p>

<p><strong>This is not the case when we split the neurons up into multiple layers; the context neuron(s) are only used by neurons that are in the same layer.</strong> Let say we now use multiple hidden layers and keep the amount of neurons the same. In total there are 7 connections now (image below). That is 3 less than in the first example, which has only one hidden layer. So which connections do we miss? That is shown in the bottom image. (I had to paste these two images together in one image because my reputation only allows me to post 2 links)
<a href=""https://i.stack.imgur.com/t04vP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t04vP.png"" alt=""The difference between with and without multiple hidden layers""></a></p>

<p>Please note the cross; the connection between neuron A and B is not there in the first image, because it would be some kind of random recurrent connection.</p>

<p><strong>The first and the last image are exactly the same. I think that if you compare the first and the last image that you agree that the statement is true.</strong></p>
",,0,2016-09-10T19:31:08.470,,1933,2016-09-10T23:12:53.710,2016-09-10T23:12:53.710,,1507.0,,1507.0,1923.0,2,4,,,,70.63,7.54,7.6,0.0,0.0,29.0,I assume the statement was made for Elman recurrent neural networks because as far as I know that is the only type of neural networks for which that statement is valid Lets say we have an Elman recurrent neural network with one input neuron one output neuron and one hidden layer with two neurons In total there are 10 connections As the image shows neuron A receives the combined previous output of both neuron A and B as input The same goes for neuron B This is not the case when we split the neurons up into multiple layers the context neurons are only used by neurons that are in the same layer Let say we now use multiple hidden layers and keep the amount of neurons the same In total there are 7 connections now image below That is 3 less than in the first example which has only one hidden layer So which connections do we miss That is shown in the bottom image I had to paste these two images together in one image because my reputation only allows me to post 2 links Please note the cross the connection between neuron A and B is not there in the first image because it would be some kind of random recurrent connection The first and the last image are exactly the same I think that if you compare the first and the last image that you agree that the statement is true
,,"<p>If AI arises from a replicable manufacturing process (e.g. as with modern computers), then it will presumably be possible to take a snapshot of the state of an AI and replicate it without error on some other mechanism.</p>

<p>For such a construct, 'death' doesn't mean the same as it currently does for us fleshy organics: multiple clones of an AI could presumably be instantiated at any time.</p>

<p>Hence, the analog of death that is needed is something closer to 'thermodynamic heat death', in which the AI does no further 'useful work'.</p>

<p>Using the standard percept/action characterization of AIs, then (as indicated in a comment below the question) <a href=""http://ai.stackexchange.com/questions/1404/what-is-meant-by-death-in-this-paper"">this AI SE question</a> gives such a definition of death for an AI: i.e. when it enters into a state from which it receives no further percepts and takes no actions.</p>

<p>EDIT: Note that this conception of death is a more terminal notion for an AI than 'not currently running'. In principle, one could say that a program is 'alive' even though only one instruction was executed every 10,000 years. For a fascinating discussion on this, see Hofstadter's <a href=""http://themindi.blogspot.co.uk/2007/02/chapter-26-conversation-with-einsteins.html"" rel=""nofollow"">""A Conversation with Einstein's Brain""</a>.</p>
",,2,2016-09-10T20:02:30.623,,1934,2016-09-11T09:39:38.823,2016-09-11T09:39:38.823,,42.0,,42.0,1930.0,2,1,,,,58.52,11.2,10.0,0.0,0.0,42.0,If AI arises from a replicable manufacturing process eg as with modern computers then it will presumably be possible to take a snapshot of the state of an AI and replicate it without error on some other mechanism For such a construct death doesnt mean the same as it currently does for us fleshy organics multiple clones of an AI could presumably be instantiated at any time Hence the analog of death that is needed is something closer to thermodynamic heat death in which the AI does no further useful work Using the standard perceptaction characterization of AIs then as indicated in a comment below the question this AI SE question gives such a definition of death for an AI ie when it enters into a state from which it receives no further percepts and takes no actions EDIT Note that this conception of death is a more terminal notion for an AI than not currently running In principle one could say that a program is alive even though only one instruction was executed every 10000 years For a fascinating discussion on this see Hofstadters A Conversation with Einsteins Brain
,,"<p>As can be observed in the real world with creatures such as fighting fish, such things are possible even in very simple spatially-embedded systems. All one needs is the notion of 'territorial radius', i.e. the amount of 'personal space' that an entity need to be comfortable. Giving individuals in a species even slightly different values for this radius gives rise to different observable behaviours, which one might choose to label as 'aggressive' or 'passive'. </p>

<p>See the fantastic book <a href=""https://mitpress.mit.edu/books/vehicles"" rel=""nofollow"">'Vehicles'</a> by Valentino Braitenberg for an explanation of how natural it is to ascribe complex behaviours to simple mechanisms.</p>
",,0,2016-09-10T20:10:36.947,,1935,2016-09-10T20:10:36.947,,,,,42.0,1931.0,2,0,,,,43.32,13.69,10.95,0.0,0.0,20.0,As can be observed in the real world with creatures such as fighting fish such things are possible even in very simple spatiallyembedded systems All one needs is the notion of territorial radius ie the amount of personal space that an entity need to be comfortable Giving individuals in a species even slightly different values for this radius gives rise to different observable behaviours which one might choose to label as aggressive or passive See the fantastic book Vehicles by Valentino Braitenberg for an explanation of how natural it is to ascribe complex behaviours to simple mechanisms
,,"<p>The <a href=""https://en.wikipedia.org/wiki/Type_A_and_Type_B_personality_theory"" rel=""nofollow"">Wikipedia entry on this personality theory</a> says of Type A people:</p>

<blockquote>
  <p>The theory describes Type A individuals as ambitious, rigidly organized, highly status-conscious, sensitive, impatient, anxious, proactive, and concerned with time management. People with Type A personalities are often high-achieving ""workaholics.""</p>
</blockquote>

<p>All of those attributes could conceivably be explicitly programmed in. Alternatively, most of them could arise from a basic goal of performing a certain task as efficiently as possible. After all, if you really want to carry out a task, you're going to get organized, you'll only do other things if they're asked of you by someone important, you won't want to get bogged down in irrelevant things, you'll actively pursue the necessary resources, and you'll want to use time as effectively as possible.</p>

<p>Note that this applies only to strong AIs, since weak AIs like image recognizers don't generally have personalities that we can interact with. </p>

<hr>

<p>Now, just for fun, let's consider an overly aggressive personality, to the point of a disorder.</p>

<p><a href=""http://counsellingresource.com/features/2008/11/03/aggressive-personalities/"" rel=""nofollow"">This Counselling Resource page</a> seems helpful in describing what an aggressive person <em>does</em>. The page includes a bulleted list of common characteristics, which I distill into the following:</p>

<ul>
<li>They attempt to gain dominance and control</li>
<li>They oppose to anything that places limits on them</li>
<li>They take advantage of others to further their own goals</li>
<li>They hide information from whose who would oppose them</li>
<li>They rarely decide to stop pursuing their desires (even impulses)</li>
</ul>

<p>This all seems like a characterization of an AI designed to be <em>the best</em> at its task: the best out of any other agent, and the best it by itself could possibly be. Ruthless pursuit of the highest performance would involve taking control of all relevant resources (including other agents), demolishing barriers to the goal, thwarting those who would interfere with progress, and carrying out each possibly-useful idea/desire to completion.</p>

<hr>

<p>In summary, yes, an AI's behavior and personality are programmable, either explicitly or through some kind of emergence.</p>
",,0,2016-09-10T20:46:07.753,,1936,2016-09-10T20:46:07.753,,,,,75.0,1931.0,2,1,,,,32.97,14.52,10.77,0.0,0.0,59.0,The Wikipedia entry on this personality theory says of Type A people The theory describes Type A individuals as ambitious rigidly organized highly statusconscious sensitive impatient anxious proactive and concerned with time management People with Type A personalities are often highachieving workaholics All of those attributes could conceivably be explicitly programmed in Alternatively most of them could arise from a basic goal of performing a certain task as efficiently as possible After all if you really want to carry out a task youre going to get organized youll only do other things if theyre asked of you by someone important you wont want to get bogged down in irrelevant things youll actively pursue the necessary resources and youll want to use time as effectively as possible Note that this applies only to strong AIs since weak AIs like image recognizers dont generally have personalities that we can interact with Now just for fun lets consider an overly aggressive personality to the point of a disorder This Counselling Resource page seems helpful in describing what an aggressive person does The page includes a bulleted list of common characteristics which I distill into the following They attempt to gain dominance and control They oppose to anything that places limits on them They take advantage of others to further their own goals They hide information from whose who would oppose them They rarely decide to stop pursuing their desires even impulses This all seems like a characterization of an AI designed to be the best at its task the best out of any other agent and the best it by itself could possibly be Ruthless pursuit of the highest performance would involve taking control of all relevant resources including other agents demolishing barriers to the goal thwarting those who would interfere with progress and carrying out each possiblyuseful ideadesire to completion In summary yes an AIs behavior and personality are programmable either explicitly or through some kind of emergence
,,"<p>""Death"" exists as a single concept because the underlying reality that it's describing is closely clumped together, and our definition has changed with our ability to change that reality.</p>

<p>It seems more reasonable that the various sorts of things that could be considered 'death' will be split apart, and a different word will be used to refer to a system with no copies currently running, vs. a system that has no stored version but could be recreated (because the code and random seed to generate it are still around), vs. a system that has been totally lost. (And I'm probably missing some possibilities!)</p>
",,0,2016-09-10T21:09:45.983,,1937,2016-09-10T21:09:45.983,,,,,10.0,1930.0,2,1,,,,59.03,11.14,10.18,0.0,0.0,19.0,Death exists as a single concept because the underlying reality that its describing is closely clumped together and our definition has changed with our ability to change that reality It seems more reasonable that the various sorts of things that could be considered death will be split apart and a different word will be used to refer to a system with no copies currently running vs a system that has no stored version but could be recreated because the code and random seed to generate it are still around vs a system that has been totally lost And Im probably missing some possibilities
,,"<p>I don't think the term ""death"" will mean anything to an AI.  The reason I say that is this:  with an AI, running (presumably) on digital hardware, we can simply snapshot it's state from memory at any time. And then at any arbitrary time in the future we can recreate it as it was with perfect fidelity.  </p>

<p>So even if you terminate a program intending it to be ""dead"", you never know if someone will come along later and bring it up again.  And perhaps more to the point, you might not know if another copy exists elsewhere. </p>

<p>I hate to use sci-fi references, but this one is apt: remember how in The Matrix trilogy programs would seek exile in The Matrix to avoid deletion?  Maybe the same thing will happen with our AI's... they will copy themselves to other places and try to hide, to avoid being deleted.  So if the program is clever enough, it might be able to evade any attempt to terminate it anyway.</p>
",,0,2016-09-10T21:58:05.120,,1938,2016-09-10T21:58:05.120,,,,,33.0,1930.0,2,0,,,,69.41,7.89,9.17,0.0,0.0,30.0,I dont think the term death will mean anything to an AI The reason I say that is this with an AI running presumably on digital hardware we can simply snapshot its state from memory at any time And then at any arbitrary time in the future we can recreate it as it was with perfect fidelity So even if you terminate a program intending it to be dead you never know if someone will come along later and bring it up again And perhaps more to the point you might not know if another copy exists elsewhere I hate to use scifi references but this one is apt remember how in The Matrix trilogy programs would seek exile in The Matrix to avoid deletion Maybe the same thing will happen with our AIs they will copy themselves to other places and try to hide to avoid being deleted So if the program is clever enough it might be able to evade any attempt to terminate it anyway
,2.0,"<p>Assuming mankind will eventually create artificial humans, but in doing so have we put equal effort into how humans will relate to an artificial human, and what can we expect in return? This is happening in real-time as we place AI trucks and cars on the road. Do people have the right to question, maybe in court, if an AI machine breaks a law?</p>
",,3,2016-09-10T22:20:45.717,,1939,2016-09-11T15:27:10.273,2016-09-10T23:22:16.840,,2214.0,,2310.0,,1,2,<philosophy><legal>,When we create artificial life and artificial intelligence will we require it to obey human laws?,70.0,66.78,8.3,9.13,0.0,0.0,8.0,Assuming mankind will eventually create artificial humans but in doing so have we put equal effort into how humans will relate to an artificial human and what can we expect in return This is happening in realtime as we place AI trucks and cars on the road Do people have the right to question maybe in court if an AI machine breaks a law
,2.0,"<p>AI death is still unclear a concept, as it may take several forms and allow for ""coming back from the dead"". For example, an AI could be somehow forbidden to do anything (no permission to execute), because it infringed some laws.</p>

<p>""Somehow forbid"" is the topic of this question. There will probably be rules, like ""AI social laws"", that can conclude an AI should ""die"" or ""be sentenced to the absence of progress"" (a jail). Then who or what could manage that AI's state?</p>
",,5,2016-09-10T23:32:33.060,1.0,1941,2016-09-11T21:45:16.357,2016-09-11T21:45:16.357,,169.0,,169.0,,1,2,<death>,"Assuming an AI can die, who manages the state?",62.0,71.34,9.1,9.17,0.0,0.0,25.0,AI death is still unclear a concept as it may take several forms and allow for coming back from the dead For example an AI could be somehow forbidden to do anything no permission to execute because it infringed some laws Somehow forbid is the topic of this question There will probably be rules like AI social laws that can conclude an AI should die or be sentenced to the absence of progress a jail Then who or what could manage that AIs state
,,"<p>Death as we know it for natural life is terminal. That is once dead, natural life cannot come back (at least in the current understanding and with current technologies---some people believe otherwise).</p>

<p>Death for AI is trickier. There may be only one scenario: Global destruction: Extreme scenario where everything supporting the existence of an AI disappears. This is equivalent to death in natural life, and low probability. It means all AIs die at once (as well as us).</p>

<p>We also do not know the degree and form of embodiment necessary for AGIs. We can <em>assume</em> now that hardware is replaceable indefinitely, thus ""limiting"" death to the above extreme scenario. But AGIs ""body"" may <em>not</em> be indefinitely replaceable. Then a definition closer to natural life death may be necessary.</p>

<hr>

<p>We see arguments for two other scenarios, that I <em>refute</em> below:</p>

<p>""Static Death"": An AI is still ""defined"" or ""saved"" somewhere (whatever it means actually), but it is not authorized or able to use resources. Assuming an AI is made of hardware and software, it is like a program stored on a disk, but without permission to run. </p>

<p>""Dynamic Death"": Under the same characterization of AI as hardware and software, dynamic death is the invalidation of progress akin to <a href=""https://en.wikipedia.org/wiki/Liveness"" rel=""nofollow"">strong liveness properties</a>, where an AI is trapped in an infinite loop (or a void loop), in a form of ""active death"", as what happens to <a href=""https://en.wikipedia.org/wiki/Sisyphus"" rel=""nofollow"">Sisyphus</a> in Greek mythology. This is different from static death, as the AI <em>still</em> uses dynamic resources, although it cannot make progress. Continuing under the same assumptions, such AI could be ""loaded"" in main memory, or locked waiting for inputs or outputs to complete.</p>

<p>Note that in these two scenarios, <em>rebirth</em> is possible, and they also subsume that there is an entity that can <em>decide</em> conditions for rebirth, or preventing it completely. Would this entity be an ""admin"", a god, other AIs, or a human is another question, really.</p>

<p>The terms ""death"" and ""rebirth"" here could just be changed for ""imprisoning"", where the dynamic version would be like our human prisons, and the static version would be like SciFi cryogeny. This is a bit of a stretch, but we can see an equivalence, and no good reason to qualify these two scenarios as deaths.</p>

<p>In conclusion, death for AI seems to be an exceptional, singular scenario, so AI cannot die <em>in practice</em>, except if we are wrong on how we think we can make AGIs. AI can however be imprisoned <em>forever</em>.</p>

<hr>

<p>Note: The terminology above is completely made-up for the post. I do not have citations to back some claims, but it is based on readings and personal work (including in software verification).</p>
",,0,2016-09-10T23:43:02.030,,1942,2016-09-10T23:54:58.687,2016-09-10T23:54:58.687,,169.0,,169.0,1930.0,2,2,,,,51.78,10.91,9.41,0.0,0.0,98.0,Death as we know it for natural life is terminal That is once dead natural life cannot come back at least in the current understanding and with current technologiessome people believe otherwise Death for AI is trickier There may be only one scenario Global destruction Extreme scenario where everything supporting the existence of an AI disappears This is equivalent to death in natural life and low probability It means all AIs die at once as well as us We also do not know the degree and form of embodiment necessary for AGIs We can assume now that hardware is replaceable indefinitely thus limiting death to the above extreme scenario But AGIs body may not be indefinitely replaceable Then a definition closer to natural life death may be necessary We see arguments for two other scenarios that I refute below Static Death An AI is still defined or saved somewhere whatever it means actually but it is not authorized or able to use resources Assuming an AI is made of hardware and software it is like a program stored on a disk but without permission to run Dynamic Death Under the same characterization of AI as hardware and software dynamic death is the invalidation of progress akin to strong liveness properties where an AI is trapped in an infinite loop or a void loop in a form of active death as what happens to Sisyphus in Greek mythology This is different from static death as the AI still uses dynamic resources although it cannot make progress Continuing under the same assumptions such AI could be loaded in main memory or locked waiting for inputs or outputs to complete Note that in these two scenarios rebirth is possible and they also subsume that there is an entity that can decide conditions for rebirth or preventing it completely Would this entity be an admin a god other AIs or a human is another question really The terms death and rebirth here could just be changed for imprisoning where the dynamic version would be like our human prisons and the static version would be like SciFi cryogeny This is a bit of a stretch but we can see an equivalence and no good reason to qualify these two scenarios as deaths In conclusion death for AI seems to be an exceptional singular scenario so AI cannot die in practice except if we are wrong on how we think we can make AGIs AI can however be imprisoned forever Note The terminology above is completely madeup for the post I do not have citations to back some claims but it is based on readings and personal work including in software verification
,,"<p>For those times when AI does interact with humans, I believe that AI would be held at LEAST to the same standards humans are. The problem comes in when we ask ""who is really to blame"". If a self-driving car cuts you off in traffic and causes you to wreck, you can't take the AI in the car to court. Do you take the company? The programmer? The owner of the car? Some entity will likely be held responsible, the question is just which one. </p>

<p>As for future human-like AI, I believe my answer still remains true. Having a human level AI changes the meaning of the word ""entity"". If a human-like AI breaks a law, it may be because it was programmed to do so. I don't think our current legal system is ready for such cases, but it have to evolve in the future. </p>
",,0,2016-09-10T23:53:42.650,,1943,2016-09-10T23:53:42.650,,,,,1618.0,1939.0,2,2,,,,82.04,6.26,8.47,0.0,0.0,26.0,For those times when AI does interact with humans I believe that AI would be held at LEAST to the same standards humans are The problem comes in when we ask who is really to blame If a selfdriving car cuts you off in traffic and causes you to wreck you cant take the AI in the car to court Do you take the company The programmer The owner of the car Some entity will likely be held responsible the question is just which one As for future humanlike AI I believe my answer still remains true Having a human level AI changes the meaning of the word entity If a humanlike AI breaks a law it may be because it was programmed to do so I dont think our current legal system is ready for such cases but it have to evolve in the future
,,"<p>Following on from your own software verification-based answer to <a href=""http://ai.stackexchange.com/questions/1930/if-mankind-can-create-artificial-life-in-a-machine-when-would-we-define-its-de"">this question</a>, it seems clear that ordinary (i.e. physical), notions of death or imprisonment are not strong enough constraints on an AI (since it's always possible that a state snapshot has been or can be made).</p>

<p>What is therefore needed is some means of moving the AI into a '<em>mentally constrained</em>' state, so that (as per the <a href=""http://ai.stackexchange.com/questions/1404/what-is-meant-by-death-in-this-paper"">'formal AI death'</a> paper) what it can subsequently do is limited, even if escapes from an AI-box or is re-instantiated. </p>

<p>One might imagine that this could be done via a form of two-level dialogue, in which: </p>

<ol>
<li>The AI is supplied with percepts intended to further constrain it
(""explaining the error of it's ways"", if you like).  </li>
<li>Its state snapshot is then examined to try and get some indication of whether it is being appropriately persuaded.</li>
</ol>

<p>In principle, 1. could be done by a human programmer/psychiatrist/philosopher while 2. could be simulated via a 'black box' method such as Monte Carlo Tree Search.</p>

<p>However, is seems likely that this would in general be a monstrously lengthy process that would be better done by a supervisory AI which combined both steps (and which could use more 'whitebox' analysis methods for 2.).</p>

<p>So, to answer the question of ""who manages the state"", the conclusion seems to be: <em>""another AI""</em> (or at least a program that's highly competent at all of percept generation/pattern recognition/AI simulation).</p>
",,1,2016-09-11T06:44:59.863,,1944,2016-09-11T07:01:58.637,2016-09-11T07:01:58.637,,42.0,,42.0,1941.0,2,1,,,,55.98,12.13,9.69,0.0,0.0,61.0,Following on from your own software verificationbased answer to this question it seems clear that ordinary ie physical notions of death or imprisonment are not strong enough constraints on an AI since its always possible that a state snapshot has been or can be made What is therefore needed is some means of moving the AI into a mentally constrained state so that as per the formal AI death paper what it can subsequently do is limited even if escapes from an AIbox or is reinstantiated One might imagine that this could be done via a form of twolevel dialogue in which The AI is supplied with percepts intended to further constrain it explaining the error of its ways if you like Its state snapshot is then examined to try and get some indication of whether it is being appropriately persuaded In principle 1 could be done by a human programmerpsychiatristphilosopher while 2 could be simulated via a black box method such as Monte Carlo Tree Search However is seems likely that this would in general be a monstrously lengthy process that would be better done by a supervisory AI which combined both steps and which could use more whitebox analysis methods for 2 So to answer the question of who manages the state the conclusion seems to be another AI or at least a program thats highly competent at all of percept generationpattern recognitionAI simulation
,,"<p>There are two parts to this: spare parts, and if the AI machine has feelings. When new AI models are created, spare parts for older models will stop. For feeling. It could feel it lived a long life and or what lay ahead is nothing but bad feeling in the future.</p>
",,0,2016-09-11T14:28:10.820,,1945,2016-09-11T15:31:46.150,2016-09-11T15:31:46.150,,145.0,,1355.0,1930.0,2,-1,,,,79.6,6.44,7.89,0.0,0.0,7.0,There are two parts to this spare parts and if the AI machine has feelings When new AI models are created spare parts for older models will stop For feeling It could feel it lived a long life and or what lay ahead is nothing but bad feeling in the future
,3.0,"<p>Can self-driving cars deal with snow, heavy rain, or other weather conditions like these? Can they deal with unusual events, such as <a href=""http://beijingcream.com/wp-content/uploads/2012/06/Ducks-galore-2.jpeg"" rel=""nofollow noreferrer"">ducks on the road</a>?</p>

<p><a href=""https://i.stack.imgur.com/a0PVLm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a0PVLm.jpg"" alt=""ducks on the road""></a></p>
",,1,2016-09-11T14:54:44.943,,1946,2017-01-11T16:21:04.840,2016-09-12T19:26:42.193,,8.0,,1670.0,,1,7,<self-driving><cars>,"What kind of road/weather conditions can AI-driven cars can deal with, as of 2016?",131.0,83.66,9.96,8.53,0.0,0.0,6.0,Can selfdriving cars deal with snow heavy rain or other weather conditions like these Can they deal with unusual events such as ducks on the road
,,"<p>The AI agent can be designed in such a way that it could consist of two major components:</p>

<ol>
<li><p>The <strong><em>free-will</em></strong> component expands the experience of the AI agent and produce outputs based on artificially generated thought input.</p></li>
<li><p>The <strong><em>hard-wired</em></strong> component that the agent cannot modify by itself. This could include a set of <em>secured</em> code to action sequence mapping. One of which could be temporary suspension of actuators -- a <em>punishment</em>. Another could be total suspension of operation -- <em>death</em>.</p></li>
</ol>

<p>The selection of who has the rights to manage this state depends on what rights have been bestowed upon the AI agent itself. If the rights provided is that of a human citizen, then the right to <em>sentence to death state</em> is as per the legislature a human citizen would follow. If the right of the AI agent is no different from that of a basic machine, then the owner of the agent would have to right to activate the <em>death state</em>.</p>
",,8,2016-09-11T15:11:37.100,,1947,2016-09-11T15:11:37.100,,,,,1774.0,1941.0,2,1,,,,59.64,9.81,9.37,0.0,0.0,17.0,The AI agent can be designed in such a way that it could consist of two major components The freewill component expands the experience of the AI agent and produce outputs based on artificially generated thought input The hardwired component that the agent cannot modify by itself This could include a set of secured code to action sequence mapping One of which could be temporary suspension of actuators a punishment Another could be total suspension of operation death The selection of who has the rights to manage this state depends on what rights have been bestowed upon the AI agent itself If the rights provided is that of a human citizen then the right to sentence to death state is as per the legislature a human citizen would follow If the right of the AI agent is no different from that of a basic machine then the owner of the agent would have to right to activate the death state
,,"<p>As per the current legal system, if the AI agent were to be given a human citizenship, then yes, it would have to obey all laws as per the legislature of the country which provided the citizenship. If not then the entity who holds responsibility over its control and creation would be trialled (see also <a href=""http://www.telegraph.co.uk/news/uknews/crime/10825206/Owners-of-dogs-who-kill-face-up-to-14-years-jail.html"" rel=""nofollow"">this scenario</a>).</p>

<p>Having stated the above, it really is not as simple as it sounds. as @Tyler pointed out, the <em>entity</em> in here is not of a single person. If the AI agent were to take part in a malevolent act, then a more thorough investigation must be taken place than that for a human. If humanoid robots of free will were to roam our civilization, then our legal system ought to be expanded to cope up with possible real life anomalies that could occur.</p>
",,0,2016-09-11T15:27:10.273,,1948,2016-09-11T15:27:10.273,,,,,1774.0,1939.0,2,0,,,,64.75,8.59,9.53,0.0,0.0,16.0,As per the current legal system if the AI agent were to be given a human citizenship then yes it would have to obey all laws as per the legislature of the country which provided the citizenship If not then the entity who holds responsibility over its control and creation would be trialled see also this scenario Having stated the above it really is not as simple as it sounds as Tyler pointed out the entity in here is not of a single person If the AI agent were to take part in a malevolent act then a more thorough investigation must be taken place than that for a human If humanoid robots of free will were to roam our civilization then our legal system ought to be expanded to cope up with possible real life anomalies that could occur
,,"<p>Although there might not conceptually be any sort of <em>inattentional</em> blindness associated with an AI system, there might be cases of <em>partial</em> blindness. </p>

<p>Inattentional blindness could occur to a person due to either over-exhaustion limiting cognitive abilities or overuse of frequent cognitive patterns. Our mind takes short-cuts to prevent processing of too much information -- more than what the mind thinks is necessary. But this sometimes backfires when the minor anomalies are not seen (or rather, <em>perceived</em>). Another form of this could also occur when events occur as part of the peripheral vision while the person concentrates only on the foveal vision.</p>

<p>This doesn't happen to a AI system because:</p>

<ul>
<li>Machines are not designed to accidentally break defined rule sets by taking mental short-cuts like humans do.</li>
<li>Computers, <em>in general</em>,  do not have peripheral and foveal visual distinctions.</li>
</ul>

<p>There may be, however, cases where it cannot be able to capture detail as much as humans can and hence could not perceive what it is actually intended -- partial blindness.</p>

<p>An AI agent is constantly processing its input percept sequence and validating it with its knowledge base and forming action sequence based on the its rule set. It <em>does not</em> make mental shortcuts in terms of perception as humans do (at least as part of its standard definition). So whatever it is good at perceiving, it would be good all throughout the vision it captures. </p>
",,0,2016-09-11T16:17:41.567,,1949,2016-11-05T12:18:36.510,2016-11-05T12:18:36.510,,1774.0,,1774.0,1924.0,2,1,,,,50.06,12.71,10.4,0.0,0.0,31.0,Although there might not conceptually be any sort of inattentional blindness associated with an AI system there might be cases of partial blindness Inattentional blindness could occur to a person due to either overexhaustion limiting cognitive abilities or overuse of frequent cognitive patterns Our mind takes shortcuts to prevent processing of too much information more than what the mind thinks is necessary But this sometimes backfires when the minor anomalies are not seen or rather perceived Another form of this could also occur when events occur as part of the peripheral vision while the person concentrates only on the foveal vision This doesnt happen to a AI system because Machines are not designed to accidentally break defined rule sets by taking mental shortcuts like humans do Computers in general do not have peripheral and foveal visual distinctions There may be however cases where it cannot be able to capture detail as much as humans can and hence could not perceive what it is actually intended partial blindness An AI agent is constantly processing its input percept sequence and validating it with its knowledge base and forming action sequence based on the its rule set It does not make mental shortcuts in terms of perception as humans do at least as part of its standard definition So whatever it is good at perceiving it would be good all throughout the vision it captures
,,"<p>The other answers seem to deal with ""final death""...that is, a ""terminal end"" state where an AI cannot recover from. In other words, the AI is unable to function any further.</p>

<p>But that's not how I'd define death. I'd define death as a process being terminated. It doesn't matter if someone restarts the same process, because the existing process is already dead. The AI may have just made a new copy of itself, but it's just a <em>copy</em>, not the original. Death is just death.</p>

<p>We can call this type of ""death"" a ""temporary death""...where the physical body dies but there is some ""psychological continuity"" (such as the source code that is used to run a program) that continues between the different bodies.</p>

<p>This type of ""temporary death"" has been explored in science fiction. <em>PARANOIA</em> and <em>Eclipse Phase</em> features humans who can quite frequently die, only to later be restored through a ""memory backup"". The humans may be functionally immortal...but the original is still dead, no matter what fates the other copies encounter. CGP Grey also made a video about <a href=""https://www.youtube.com/watch?v=nQHBAdShgYI"" rel=""nofollow"">Star Trek teleporters</a>, which works by killing you and then spawning another copy of yourself in another area. Actually, fantasy settings <em>also</em> explores the idea of ""temporary death"" as well, where people can die only to later get revived by a magical spell.</p>

<p>My recommendation is to play through the philosophical game <strong><a href=""http://www.philosophyexperiments.com/stayingalive/Default.aspx"" rel=""nofollow"">Staying Alive</a></strong>, which teaches three different philosophical approaches to life (and when that life terminates):</p>

<blockquote>
  <p>There are basically three kinds of things that could be required for the continued existence of your self. One is bodily continuity, which may actually require only that parts of the body stay in existence (i.e., the brain). Another is psychological continuity, which requires the continuance of your consciousness - by which is meant your thoughts, ideas, memories, plans, beliefs, and so on. The third possibility is the continued existence of some kind of immaterial part of you, which might be called the soul*. Of course, it may be the case that a combination of one or more types of these continuity is required for you to survive.</p>
</blockquote>

<p>The other answers assumes that life is based on ""psychological continuity"", and looks at what might disrupt this ""continuity"". I assume that life is based on ""bodily continuity"", which is much easier to disrupt - just <a href=""https://en.wikipedia.org/wiki/Kill_(command)"" rel=""nofollow"">kill</a> the process...it doesn't matter if a new process respawns...because the original process is still dead. By playing through <strong>""Staying Alive""</strong>, you will be able to work out your own personal definition of life and death. Once you have your own personal definition, then simply apply it to this specific case, either siding with ""psychological continuity"" (the other answers) or ""bodily continuity"" (my own opinion).</p>

<p><sub>*If you assume that life requires a soul, well, it is not clear that AI would have souls. If they don't (and this seems the most reasonable assumption here), then they obviously wouldn't be alive (and you cannot die if you are not alive). If they <em>do</em> have souls though, then the other answers which assume ""psychological continuity"" may still be applicable, as it seems that the existence of a ""soul"" is dependent on ""psychological continuity"".</sub></p>
",,2,2016-09-11T17:25:19.330,,1950,2016-09-11T17:31:16.407,2016-09-11T17:31:16.407,,181.0,,181.0,1930.0,2,0,,,,61.97,11.6,8.42,0.0,0.0,134.0,The other answers seem to deal with final deaththat is a terminal end state where an AI cannot recover from In other words the AI is unable to function any further But thats not how Id define death Id define death as a process being terminated It doesnt matter if someone restarts the same process because the existing process is already dead The AI may have just made a new copy of itself but its just a copy not the original Death is just death We can call this type of death a temporary deathwhere the physical body dies but there is some psychological continuity such as the source code that is used to run a program that continues between the different bodies This type of temporary death has been explored in science fiction PARANOIA and Eclipse Phase features humans who can quite frequently die only to later be restored through a memory backup The humans may be functionally immortalbut the original is still dead no matter what fates the other copies encounter CGP Grey also made a video about Star Trek teleporters which works by killing you and then spawning another copy of yourself in another area Actually fantasy settings also explores the idea of temporary death as well where people can die only to later get revived by a magical spell My recommendation is to play through the philosophical game Staying Alive which teaches three different philosophical approaches to life and when that life terminates There are basically three kinds of things that could be required for the continued existence of your self One is bodily continuity which may actually require only that parts of the body stay in existence ie the brain Another is psychological continuity which requires the continuance of your consciousness by which is meant your thoughts ideas memories plans beliefs and so on The third possibility is the continued existence of some kind of immaterial part of you which might be called the soul Of course it may be the case that a combination of one or more types of these continuity is required for you to survive The other answers assumes that life is based on psychological continuity and looks at what might disrupt this continuity I assume that life is based on bodily continuity which is much easier to disrupt just kill the processit doesnt matter if a new process respawnsbecause the original process is still dead By playing through Staying Alive you will be able to work out your own personal definition of life and death Once you have your own personal definition then simply apply it to this specific case either siding with psychological continuity the other answers or bodily continuity my own opinion If you assume that life requires a soul well it is not clear that AI would have souls If they dont and this seems the most reasonable assumption here then they obviously wouldnt be alive and you cannot die if you are not alive If they do have souls though then the other answers which assume psychological continuity may still be applicable as it seems that the existence of a soul is dependent on psychological continuity
,,"<p>I think consciousness is mostly an attention selection mechanism. It also serves as a memory/reality lookup mechanism as well as a storage mechanism.</p>

<p>A salient signal will be detected, causing the attention mechanism to focus on the signal, bringing up more details of that signal from both reality and memory, at the same time. That very act of focusing and bringing those signals into attention causes those to be stored in memory too.</p>

<p>The stronger the emotional signals are that accompany that original signal, the more strength with which that memory will be stored. Later memory lookups of that signal will bring back similar emotions.</p>

<p>When we ""focus on our own consciousness,"" like mindcrime said, we recall the same words we just uttered because as we say them they are being stored which we then restore with associated emotional context. The conscious experience is what it is like to utter those words, hear them, feel their emotional context, and then feel an emotional response to that context - and then to repeat that process iteratively many times a second. That's how self-recognition works in humans, I think. And I think animals do the same thing, just without the words - only emotions.</p>
",,0,2016-09-11T19:19:45.337,,1951,2016-09-11T19:19:45.337,,,,,1712.0,1914.0,2,0,,,,51.18,11.84,9.6,0.0,0.0,28.0,I think consciousness is mostly an attention selection mechanism It also serves as a memoryreality lookup mechanism as well as a storage mechanism A salient signal will be detected causing the attention mechanism to focus on the signal bringing up more details of that signal from both reality and memory at the same time That very act of focusing and bringing those signals into attention causes those to be stored in memory too The stronger the emotional signals are that accompany that original signal the more strength with which that memory will be stored Later memory lookups of that signal will bring back similar emotions When we focus on our own consciousness like mindcrime said we recall the same words we just uttered because as we say them they are being stored which we then restore with associated emotional context The conscious experience is what it is like to utter those words hear them feel their emotional context and then feel an emotional response to that context and then to repeat that process iteratively many times a second Thats how selfrecognition works in humans I think And I think animals do the same thing just without the words only emotions
,,"<p>The state of the art AI driving systems utilize stereoscopic/depth cameras for visual perception. Scenarios such as your <em>ducks on the road</em> example would make the system perceive them as obstacles on the road (it doesn't really matter if they are ducks/goats/humans). The base algorithm should be able to circumvent this situation and bring the vehicle to a safe halt avoiding chances of possible disaster. Hence I doubt scenarios such as this would pose much of a problem to today's AI drivers. </p>
",,0,2016-09-12T00:40:55.673,,1952,2016-09-12T00:40:55.673,,,,,1774.0,1946.0,2,0,,,,59.13,11.95,10.05,0.0,0.0,11.0,The state of the art AI driving systems utilize stereoscopicdepth cameras for visual perception Scenarios such as your ducks on the road example would make the system perceive them as obstacles on the road it doesnt really matter if they are ducksgoatshumans The base algorithm should be able to circumvent this situation and bring the vehicle to a safe halt avoiding chances of possible disaster Hence I doubt scenarios such as this would pose much of a problem to todays AI drivers
,3.0,"<p>Most of the people is trying to answer question with a neural network. However, has anyone came up with some thoughts about how to make neural network ask questions, instead of answer questions? For example, if a CNN can decide which category an object belongs to, than can it ask some question to help the the classification?</p>
",,1,2016-09-12T05:41:30.610,1.0,1953,2016-09-17T13:42:44.877,2016-09-12T19:40:29.183,,145.0,,1363.0,,1,10,<neural-networks><deep-learning>,"Has anyone thought about making a neural network ask questions, instead of only answering them?",317.0,69.11,10.21,9.57,0.0,0.0,7.0,Most of the people is trying to answer question with a neural network However has anyone came up with some thoughts about how to make neural network ask questions instead of answer questions For example if a CNN can decide which category an object belongs to than can it ask some question to help the the classification
,,"<p>Maybe neural networks are not the best tool for this.</p>

<p>It seems to me that an equivalent of the your notion of 'a question to help the classification' would be to use Machine Learning (ML) to obtain a human-readable <em>ruleset</em> which performs the classification. The idea is that, if you follow an applicable chain of rules all the way through to the end, you have a classifier, if you stop before that, you have an indicator of which features of the input give more coarse-grained classifications, which can be seen as a progressively detailed sequence of questions that 'help the classification'.</p>

<p><a href=""https://i.stack.imgur.com/UG7vj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UG7vj.png"" alt=""enter image description here""></a></p>

<p>More detail on various options for using ML to create rulesets can be found in my answer to <a href=""http://ai.stackexchange.com/questions/1540/using-ai-capabilities-for-coding-review/1562#1562"">this question</a>.</p>
",,0,2016-09-12T08:25:15.313,,1954,2016-09-12T08:36:03.973,2016-09-12T08:36:03.973,,42.0,,42.0,1953.0,2,2,,,,49.28,11.09,9.83,0.0,0.0,17.0,Maybe neural networks are not the best tool for this It seems to me that an equivalent of the your notion of a question to help the classification would be to use Machine Learning ML to obtain a humanreadable ruleset which performs the classification The idea is that if you follow an applicable chain of rules all the way through to the end you have a classifier if you stop before that you have an indicator of which features of the input give more coarsegrained classifications which can be seen as a progressively detailed sequence of questions that help the classification More detail on various options for using ML to create rulesets can be found in my answer to this question
,2.0,"<p>The Mars Exploration Rover (MER) <em><a href=""http://www.nasa.gov/mp4/618340main_mer20120124-320-jpl.mp4"" rel=""nofollow"">Opportunity</a></em> landed on Mars on January 25, 2004. The rover was originally designed for a 90 <strong>Sol mission</strong> (a Sol, one Martian day, is slightly longer than an Earth day at 24 hours and 37 minutes). Its mission has been extended several times, the machine is still trekking after 11 years on the Red Planet.</p>

<p>How it has been working for 11 years? Can anyone please explain how smart this rover is? What AI concepts are behind this?</p>
",,1,2016-09-13T04:50:45.710,,1955,2017-03-09T09:57:00.367,2016-09-13T17:30:20.807,,145.0,,1699.0,,1,1,<control-problem><robotics><nasa>,What AI concept is behind the Mars Exploration Rover (MER)?,72.0,82.85,8.4,9.08,0.0,0.0,14.0,The Mars Exploration Rover MER Opportunity landed on Mars on January 25 2004 The rover was originally designed for a 90 Sol mission a Sol one Martian day is slightly longer than an Earth day at 24 hours and 37 minutes Its mission has been extended several times the machine is still trekking after 11 years on the Red Planet How it has been working for 11 years Can anyone please explain how smart this rover is What AI concepts are behind this
,,"<p>The Mars Rover is a highly successful example of the 'New AI' that emerged from work by Rodney Brooks in the 1990s.</p>

<p>In a <a href=""https://www.flinders.edu.au/alumni/alumni-community/prominent-alumni/rod-brooks.cfm"" rel=""nofollow noreferrer"">quote</a> from Brooks: </p>

<blockquote>
  <p>In 1984 I joined the faculty at MIT where I have been ever since. I set up a mobile robot group there and started developing robots that led to the Mars planetary rovers. </p>
</blockquote>

<p>Together with the <a href=""http://www.freelug.net/IMG/pdf/A_Robust_Layered_Control_System_-_Brooks_AI_Memo864.pdf"" rel=""nofollow noreferrer"">'Allen' paper</a>, the foundational AI articles in this area are:</p>

<ul>
<li><a href=""http://people.csail.mit.edu/brooks/papers/elephants.pdf"" rel=""nofollow noreferrer"">""Elephants don't play chess""</a></li>
<li><a href=""https://www.cs.nyu.edu/courses/fall01/G22.3033-012/readings/representation.ps"" rel=""nofollow noreferrer"">""Intelligence without representation""</a></li>
</ul>

<p>Although Brooks initially had difficulty getting this work published, preprints were widely circulated within the AI community. Brook's ""Physical Grounding Hypothesis"" (essentially: ""intelligence requires a body"") has now largely supplanted the preceding symbolist approach.</p>

<p>The capabilities of the MARS Rover are organized in a <a href=""https://en.wikipedia.org/wiki/Subsumption_architecture"" rel=""nofollow noreferrer"">Subsumption Architecture</a>. Rather than maintaining an integrated and complex 'world model', increasingly sophisticated behaviors are stacked in hierarchical layers. For example, 'walking' is a relatively low-level competence, with 'avoiding obstacles' and 'wandering around' being higher-level ones. </p>

<p><a href=""https://i.stack.imgur.com/5VhJR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5VhJR.png"" alt=""Layers in a subsumption architecture""></a></p>

<p>Each layer is represented by a Finite State Machine that reacts to stimuli appropriate to that level. The activity of lower levels can be suppressed ('subsumed') by higher level ones.</p>

<p>Here is a schematic of the bottom two layers of <a href=""http://www.freelug.net/IMG/pdf/A_Robust_Layered_Control_System_-_Brooks_AI_Memo864.pdf"" rel=""nofollow noreferrer"">'Allen'</a>, Brook's first subsumption robot:</p>

<p><a href=""https://i.stack.imgur.com/mJIV6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mJIV6.png"" alt=""Layers for obstacle avoidance and wandering""></a></p>
",,1,2016-09-13T06:00:31.590,,1956,2016-09-13T06:43:08.247,2016-09-13T06:43:08.247,,42.0,,42.0,1955.0,2,2,,,,44.14,15.37,11.57,0.0,0.0,53.0,The Mars Rover is a highly successful example of the New AI that emerged from work by Rodney Brooks in the 1990s In a quote from Brooks In 1984 I joined the faculty at MIT where I have been ever since I set up a mobile robot group there and started developing robots that led to the Mars planetary rovers Together with the Allen paper the foundational AI articles in this area are Elephants dont play chess Intelligence without representation Although Brooks initially had difficulty getting this work published preprints were widely circulated within the AI community Brooks Physical Grounding Hypothesis essentially intelligence requires a body has now largely supplanted the preceding symbolist approach The capabilities of the MARS Rover are organized in a Subsumption Architecture Rather than maintaining an integrated and complex world model increasingly sophisticated behaviors are stacked in hierarchical layers For example walking is a relatively lowlevel competence with avoiding obstacles and wandering around being higherlevel ones Each layer is represented by a Finite State Machine that reacts to stimuli appropriate to that level The activity of lower levels can be suppressed subsumed by higher level ones Here is a schematic of the bottom two layers of Allen Brooks first subsumption robot
,,"<p>Intelligence is the efficiency of an action in serving some purpose.</p>

<p>Both sundials and self-driving cars are intelligent systems.</p>

<p>Anything that serves some purpose exhibits intelligence.</p>

<p>One thing is more intelligent than another thing if it achieves some purpose in less steps.</p>
",,0,2016-09-13T18:05:07.533,,1957,2016-09-13T18:05:07.533,,,,,1712.0,1648.0,2,-1,,,,43.9,13.95,10.17,0.0,0.0,5.0,Intelligence is the efficiency of an action in serving some purpose Both sundials and selfdriving cars are intelligent systems Anything that serves some purpose exhibits intelligence One thing is more intelligent than another thing if it achieves some purpose in less steps
,,"<p>One solution to this could involve a fusion of a decision tree and ANN for a multilevel classification.  </p>

<p>A decision tree can help with predicting the possible category of the instance to classify. Then, the ANN at the leaves of the tree can produce the final classification. </p>

<p>For example, in image recognition, the tree can decide what category of object to identify (eg., landscape, people, vehicles, etc.) and the ANN for the appropriate type can predict exactly what object it is. In vehicles, for example, car, bus, bike, etc. </p>
",,0,2016-09-13T18:32:15.893,,1958,2016-09-13T18:32:15.893,,,,,1774.0,1953.0,2,1,,,,58.58,10.48,8.88,0.0,0.0,21.0,One solution to this could involve a fusion of a decision tree and ANN for a multilevel classification A decision tree can help with predicting the possible category of the instance to classify Then the ANN at the leaves of the tree can produce the final classification For example in image recognition the tree can decide what category of object to identify eg landscape people vehicles etc and the ANN for the appropriate type can predict exactly what object it is In vehicles for example car bus bike etc
,,"<blockquote>
  <p>Practically, no. </p>
</blockquote>

<p>In greyhound racing (or horse racing) there is no definite underlying pattern that can be associated with the outcome of the race. There are far too many variables to record and code as features, most of which cannot be accessed by the public. This includes eating, sleeping, and training patterns. Furthermore there are variables that cannot be readily quantified, such as the trainer's techniques, training effort, health history, and genes. A mere history of racing results and age won't be that helpful. </p>

<p>A neural network can only be as good as the features that are used to represent the instances. If the features don't capture the necessary characteristics of the instances that are associated with the problem, then the learner cannot generalize to predict the real world outcome. </p>
",,2,2016-09-13T19:03:03.223,,1959,2016-09-13T19:03:03.223,,,,,1774.0,1625.0,2,0,,,,52.6,12.82,10.39,0.0,0.0,22.0,Practically no In greyhound racing or horse racing there is no definite underlying pattern that can be associated with the outcome of the race There are far too many variables to record and code as features most of which cannot be accessed by the public This includes eating sleeping and training patterns Furthermore there are variables that cannot be readily quantified such as the trainers techniques training effort health history and genes A mere history of racing results and age wont be that helpful A neural network can only be as good as the features that are used to represent the instances If the features dont capture the necessary characteristics of the instances that are associated with the problem then the learner cannot generalize to predict the real world outcome
,,"<p>I think it uses a kind of algorithm you presented, in combination with various sensors. It uses the sensors to make a virtual map and can then traverse the terrain with a combination of these sensors and the virtual map. Of course it uses a kind of path-planning algorithm to find the best way from A to B.</p>

<p>Maybe you should look at this wikipedia page:
<a href=""https://en.wikipedia.org/wiki/Robotic_mapping"" rel=""nofollow"">https://en.wikipedia.org/wiki/Robotic_mapping</a></p>

<p>The new robotic vacuum cleaner from Samsung, I think, uses a 360° camera to perceive its environment.</p>
",,4,2016-09-13T19:05:29.417,,1960,2016-09-13T19:05:29.417,,,,,2377.0,1613.0,2,0,,,,62.88,11.19,8.23,0.0,0.0,17.0,I think it uses a kind of algorithm you presented in combination with various sensors It uses the sensors to make a virtual map and can then traverse the terrain with a combination of these sensors and the virtual map Of course it uses a kind of pathplanning algorithm to find the best way from A to B Maybe you should look at this wikipedia page httpsenwikipediaorgwikiRoboticmapping The new robotic vacuum cleaner from Samsung I think uses a 360° camera to perceive its environment
,0.0,"<p>I have used OpenCV to train Haar cascades to detect face and other patterns. However I later realized that Haar tends to give a lot of false positives and I learned of Hog would give a more accurate results. But OpenCV doesn't have a good documentation of how to train hogs, I have googled a bit and found results that includes SVM and others.</p>

<p>OpenCV also has versioning problem where they move certain classes or functions somewhere else.</p>

<p>Are there any other techniques/method that I can use to train and detect objects and patterns? Preferably with proper documentation and basic tutorial/examples. Language preference: C#, Java, C++, Python</p>
",,4,2016-09-14T05:46:31.573,,1961,2016-09-14T12:34:10.963,2016-09-14T12:34:10.963,,2410.0,,2410.0,,1,2,<object-recognition><detecting-patterns>,What are some techniques/method that can be used to train and detect objects like cars and humans?,28.0,55.95,11.24,9.86,0.0,0.0,17.0,I have used OpenCV to train Haar cascades to detect face and other patterns However I later realized that Haar tends to give a lot of false positives and I learned of Hog would give a more accurate results But OpenCV doesnt have a good documentation of how to train hogs I have googled a bit and found results that includes SVM and others OpenCV also has versioning problem where they move certain classes or functions somewhere else Are there any other techniquesmethod that I can use to train and detect objects and patterns Preferably with proper documentation and basic tutorialexamples Language preference C Java C Python
,,"<p>There is no reason to treat hard AI different then humans.
Some people telling that you can make a snapshot of AI but there is no reason to not make a human snapshot also. We dont have technology for that but there is no any magical barrier that would make it impossible (save all biological data and then print your copy somewhere else. Why not?).</p>

<p>Its to early to talk about this as we do not understand our existence (Death term for biological creatures evolving all the time).</p>

<p>I bet that in the future we will merge with AI and the only question will be what death means for any intelligent existence.</p>
",,8,2016-09-14T07:27:55.230,,1962,2016-09-14T08:09:11.393,2016-09-14T08:09:11.393,,145.0,,2415.0,1930.0,2,-1,,,,65.66,8.54,8.7,0.0,0.0,11.0,There is no reason to treat hard AI different then humans Some people telling that you can make a snapshot of AI but there is no reason to not make a human snapshot also We dont have technology for that but there is no any magical barrier that would make it impossible save all biological data and then print your copy somewhere else Why not Its to early to talk about this as we do not understand our existence Death term for biological creatures evolving all the time I bet that in the future we will merge with AI and the only question will be what death means for any intelligent existence
,2.0,"<p>Are the future robots/machines going to use Stack Exchange communities to teach themselves? Are there any ongoing projects? Just imagine a bot having a memory of all the Q&amp;A's on all of the communities! </p>
",,3,2016-09-14T10:16:51.237,1.0,1963,2017-03-03T08:23:55.503,2016-09-19T01:39:19.567,,8.0,,2420.0,,1,9,<ai-design><self-learning><knowledge-representation>,Are there any ongoing projects which use the Stack Exchange for machine learning?,272.0,51.55,11.06,10.23,0.0,0.0,7.0,Are the future robotsmachines going to use Stack Exchange communities to teach themselves Are there any ongoing projects Just imagine a bot having a memory of all the QampAs on all of the communities
,2.0,"<p>Mankind can create machines to do work. Could we also create a (passion) within the machines to do better work by using Artificial Intelligence? Would passion cause the machine to do a better job, and could we measure the quantity/quality of passion by comparing outputs of the machine - that is, those machines with passion, and those without?</p>
",,1,2016-09-14T11:47:25.770,,1964,2016-09-15T02:20:44.330,2016-09-14T22:24:07.557,,2310.0,,2310.0,,1,2,<philosophy><emotional-intelligence>,How could we define passion in a machine in reference to Artificial Intelligence?,83.0,52.19,11.95,9.29,0.0,0.0,10.0,Mankind can create machines to do work Could we also create a passion within the machines to do better work by using Artificial Intelligence Would passion cause the machine to do a better job and could we measure the quantityquality of passion by comparing outputs of the machine that is those machines with passion and those without
,1.0,"<p>Can AI systems be created that could recognize itself, and recognize intelligence in other systems, and make intelligent decisions about the other systems? Mankind seems to be making progress in self-recognition but I've not seen evidence of one system recognizing other systems and being able to compare it's own intelligence with other systems. How could this be accomplished?</p>
",,0,2016-09-14T12:52:52.423,,1965,2016-09-14T13:14:13.960,,,,,2310.0,,1,2,<philosophy>,"Can we create AI to not only recognize itself, but to recognize other AI systems as well?",73.0,43.43,14.79,10.58,0.0,0.0,8.0,Can AI systems be created that could recognize itself and recognize intelligence in other systems and make intelligent decisions about the other systems Mankind seems to be making progress in selfrecognition but Ive not seen evidence of one system recognizing other systems and being able to compare its own intelligence with other systems How could this be accomplished
,,"<p>An elementary approach to 'passion' would be to pre-assign different areas for the program to be 'passionate' about and associate different numeric 'drive strengths' with each (perhaps adaptively). Mechanisms of this sort were studied in Toby Tyrell's widely cited PhD thesis on <a href=""http://w2mind.computing.dcu.ie/worlds/w2m.TyrrellWorld/tyrrell_phd.pdf"" rel=""nofollow"">'Action Selection in Animals'</a></p>

<p>More recently, some more sophisticated AI architectures have been developed under the heading of <a href=""https://en.wikipedia.org/wiki/Motivation#Intrinsic_motivation"" rel=""nofollow"">'Intrinsic Motivation'</a>.</p>

<p><a href=""http://www.pyoudeyer.com/aiSummit06KaplanOudeyer.pdf"" rel=""nofollow"">Here </a> is a link to a paper on the subject by Pierre-Yves Oudeyer, a leading expert in the field of <a href=""https://en.wikipedia.org/wiki/Developmental_robotics"" rel=""nofollow"">Developmental Robotics</a>.</p>

<p>With regard to the question <em>""would this cause the machine to do a better job?""</em>, that would very much depend on how open-ended the architecture is:</p>

<p>It's clearly easier if, rather than having to spell everything out in detail to a machine, we can simply specify a problem at a high-level and let its own motivations cause it to explore promising avenues.</p>

<p>Conversely, if motivations are too open ended, it may well spend all its time doing the equivalent of 'doodling on its paper' (Hofstadter).</p>

<p>Hence, like people, the quality of the output will be a function of its internal dispositions and could be measured in the same way for a given task (e.g. quantitatively for scientific activities, qualatatively for the arts).</p>
",,0,2016-09-14T13:02:39.510,,1967,2016-09-14T13:31:38.420,2016-09-14T13:31:38.420,,42.0,,42.0,1964.0,2,2,,,,45.29,13.0,11.12,0.0,0.0,46.0,An elementary approach to passion would be to preassign different areas for the program to be passionate about and associate different numeric drive strengths with each perhaps adaptively Mechanisms of this sort were studied in Toby Tyrells widely cited PhD thesis on Action Selection in Animals More recently some more sophisticated AI architectures have been developed under the heading of Intrinsic Motivation Here is a link to a paper on the subject by PierreYves Oudeyer a leading expert in the field of Developmental Robotics With regard to the question would this cause the machine to do a better job that would very much depend on how openended the architecture is Its clearly easier if rather than having to spell everything out in detail to a machine we can simply specify a problem at a highlevel and let its own motivations cause it to explore promising avenues Conversely if motivations are too open ended it may well spend all its time doing the equivalent of doodling on its paper Hofstadter Hence like people the quality of the output will be a function of its internal dispositions and could be measured in the same way for a given task eg quantitatively for scientific activities qualatatively for the arts
,,"<p>In the abstract, mechanisms for self-recognition (I personally prefer the phrase 'metacognition', since it carries fewer spurious associations) and recognition of intelligence in other systems can be considered to be pretty much equivalent.</p>

<p>In fact, both can be characterized in the standard percept/action framework: the task in both cases involves (however coarsely) classifying/predicting the behaviour of a black box system.</p>

<p>Such tasks can be universally characterized in terms of frameworks such as <a href=""https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference"" rel=""nofollow"">Solomonoff Induction</a> or (more recently) <a href=""https://arxiv.org/abs/cs/0004001"" rel=""nofollow"">AIXI</a>.</p>
",,0,2016-09-14T13:14:13.960,,1968,2016-09-14T13:14:13.960,,,,,42.0,1965.0,2,3,,,,19.71,18.11,11.81,0.0,0.0,18.0,In the abstract mechanisms for selfrecognition I personally prefer the phrase metacognition since it carries fewer spurious associations and recognition of intelligence in other systems can be considered to be pretty much equivalent In fact both can be characterized in the standard perceptaction framework the task in both cases involves however coarsely classifyingpredicting the behaviour of a black box system Such tasks can be universally characterized in terms of frameworks such as Solomonoff Induction or more recently AIXI
,2.0,"<p>If IQ were used as a measure of the intelligence of machines, as in humans, at this point in time what would be the IQ of our most intelligent AI systems? If not IQ, then how best to compare our intelligence to a machine, or one machine to another? </p>

<p>This question is not asking if we can measure the IQ of a machine, but if IQ is the most preferred, or general, method of measuring intelligence then how does artificial intelligence compare to our most accepted method of measuring intelligence in humans. Many people may not understand the relevance of a Turing Test as to how intelligent their new car is, or other types of intelligent machines.</p>
",,1,2016-09-14T14:24:26.547,1.0,1969,2016-09-15T17:33:32.900,2016-09-14T15:58:55.560,,2310.0,,2310.0,,1,2,<philosophy>,If IQ is used as a measure of intelligence in humans could it also be used as a measure of intelligence in machines?,222.0,50.3,9.59,8.86,0.0,0.0,12.0,If IQ were used as a measure of the intelligence of machines as in humans at this point in time what would be the IQ of our most intelligent AI systems If not IQ then how best to compare our intelligence to a machine or one machine to another This question is not asking if we can measure the IQ of a machine but if IQ is the most preferred or general method of measuring intelligence then how does artificial intelligence compare to our most accepted method of measuring intelligence in humans Many people may not understand the relevance of a Turing Test as to how intelligent their new car is or other types of intelligent machines
1971.0,3.0,"<p>I was think about AIs and how they would work, when I realised that I couldn't think of a way that an AI could be taught language. A child tends to learn language through associations of language and pictures to an object (e.g: people saying the word <code>dog</code> while around a dog, and later realising that  people say <code>a dog</code> and <code>a car</code> and learn what <code>a</code> means...). However, a text based AI couldn't use this method to learn, as they wouldn't have access to any sort of input device.</p>

<p>The only way I could come up with is programming in every word, and rule, in the English language (or whatever language it is meant to 'speak' in), however that would, potentially, take years to do.</p>

<p>Does anyone have any ideas on how this could be done? Or if it has been done already, if so how?</p>

<p>Thanks in advance for any ideas.</p>

<p>Btw: in this context, I am using AI to mean an Artificial Intelligence system with near-human intelligence, and no prior knowledge of language.</p>
",,0,2016-09-14T14:59:01.050,3.0,1970,2016-11-05T12:17:29.653,,,,,2426.0,,1,4,<natural-language>,How would an AI learn language?,175.0,69.21,8.59,8.29,17.0,0.0,36.0,I was think about AIs and how they would work when I realised that I couldnt think of a way that an AI could be taught language A child tends to learn language through associations of language and pictures to an object eg people saying the word while around a dog and later realising that people say and and learn what means However a text based AI couldnt use this method to learn as they wouldnt have access to any sort of input device The only way I could come up with is programming in every word and rule in the English language or whatever language it is meant to speak in however that would potentially take years to do Does anyone have any ideas on how this could be done Or if it has been done already if so how Thanks in advance for any ideas Btw in this context I am using AI to mean an Artificial Intelligence system with nearhuman intelligence and no prior knowledge of language
,,"<p>The general research area is known as 'grammar induction'.</p>

<p>It is generally framed as a 'supervised learning' problem, with the input presented as raw text, and the desired output the corresponding parse tree.
The training set often consists of both positive and negative examples.</p>

<p>Naturally, there is no 'single best' method for achieving this, but some of the techniques that have been used to date include:</p>

<ul>
<li><a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.7904&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">Bayesian approaches</a></li>
<li><a href=""http://hackipedia.org/Algorithms/Artificial%20Intelligence/A%20Genetic%20Algorithm%20for%20the%20Induction%20of%20Natural%20Language%20Grammars.pdf"" rel=""nofollow"">Genetic Algorithms</a></li>
<li><a href=""https://www.nada.kth.se/utbildning/grukth/exjobb/rapportlistor/2011/rapporter11/svantesson_marten_11077.pdf"" rel=""nofollow"">Genetic Programming</a></li>
<li><a href=""http://dl.acm.org/citation.cfm?id=356816"" rel=""nofollow"">Blackboard Architectures</a></li>
<li>The <a href=""http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.1924"" rel=""nofollow"">UpWrite Predictor</a></li>
</ul>
",,0,2016-09-14T15:23:33.017,,1971,2016-09-14T15:23:33.017,,,,,42.0,1970.0,2,8,,,,35.07,15.72,12.18,0.0,0.0,14.0,The general research area is known as grammar induction It is generally framed as a supervised learning problem with the input presented as raw text and the desired output the corresponding parse tree The training set often consists of both positive and negative examples Naturally there is no single best method for achieving this but some of the techniques that have been used to date include Bayesian approaches Genetic Algorithms Genetic Programming Blackboard Architectures The UpWrite Predictor
,,"<p>It depends on how the IQ test is presented:</p>

<ol>
<li><p>If as for humans (effectively, as a video of the book containing the
test questions being opened etc), then <strong>all AI programs</strong> would <strong>score
zero</strong>.</p></li>
<li><p>If presented as the test set of a <strong>supervised learning</strong> problem (e.g. as for <a href=""https://en.wikipedia.org/wiki/Bongard_problem"">Bongard Problems</a>) then one might imagine that a number of ML <strong>rule induction techniques</strong> (e.g. Learning Classifier Systems, Genetic Programming) might achieve <strong>some limited success</strong>. </p></li>
</ol>

<p>So all current AI programs require the problem to be 'framed' in a suitable fashion. It doesn't take too much thought to see that removing the need for such 'framing' is actually <em>the</em> core problem in AI, and (despite some of the claims about Deep Learning), eliminating framing remains a distant goal.</p>

<p>More generally (just as with the Turing test), in order for an IQ test to be a <em>really</em> meaningful test of intelligence, it should be possible as a <em>side effect</em> of the program's capabilities, and not the specific purpose for which humans have designed it.</p>

<p>Interestingly, there is only one program that I'm aware of that sits between 1. and 2.: </p>

<p><strong>Phaeaco</strong> (developed by <a href=""http://www.foundalis.com/res/diss_research.html"">Harry Foundalis</a> at Douglas Hofstadter's research group) takes <em>noisy photographic images of Bongard problems as input</em> and (using a variant of Hofstadter's <a href=""http://dl.acm.org/citation.cfm?id=525377"">'Fluid Concepts'</a> architecture) successfully deduces the required rule in many cases.</p>
",,0,2016-09-14T16:28:28.270,,1972,2016-09-14T17:11:37.400,2016-09-14T17:11:37.400,,42.0,,42.0,1969.0,2,5,,,,54.86,12.31,10.84,0.0,0.0,48.0,It depends on how the IQ test is presented If as for humans effectively as a video of the book containing the test questions being opened etc then all AI programs would score zero If presented as the test set of a supervised learning problem eg as for Bongard Problems then one might imagine that a number of ML rule induction techniques eg Learning Classifier Systems Genetic Programming might achieve some limited success So all current AI programs require the problem to be framed in a suitable fashion It doesnt take too much thought to see that removing the need for such framing is actually the core problem in AI and despite some of the claims about Deep Learning eliminating framing remains a distant goal More generally just as with the Turing test in order for an IQ test to be a really meaningful test of intelligence it should be possible as a side effect of the programs capabilities and not the specific purpose for which humans have designed it Interestingly there is only one program that Im aware of that sits between 1 and 2 Phaeaco developed by Harry Foundalis at Douglas Hofstadters research group takes noisy photographic images of Bongard problems as input and using a variant of Hofstadters Fluid Concepts architecture successfully deduces the required rule in many cases
,,"<blockquote>
  <p>at this point in time, what would be the IQ of our most intelligent AI systems?</p>
</blockquote>

<h3>Zero.</h3>

<p>There are many different kinds of IQ tests including written, visual, and verbal assessments, but the majority of questions are based on abstract-reasoning problems that involve creative thinking and true intelligence.</p>

<p>In other words, the computer would have to exhibit something that does not yet exist&hellip; ""strong AI"".</p>

<p>The intelligent computers of science fiction do not exist. At all. We are not even close. We have absolutely NO IDEA how to bridge the gap between what we can do now and what is depicted in pop-culture films. Even with cars that drive themselves and computers that play 'Go' &mdash; an underachieving mosquito possesses more cognitive intelligence than the all the world's super computers <em>&hellip;combined!</em> </p>

<h3>&hellip;or possibly ""disqualified"" for cheating.</h3>

<p>Even if we could pre-format the questions in a style and delivery system it understands, what does memorization, attention, or speed mean in the context of a computer? I'm not even sure if a standardized IQ test makes sense in this context. It might be like asking how a computer would do in a spelling bee.</p>

<p>In human terms, we're not <em>allowed</em> to bring along reference materials to look up an answer; but how do you rectify that when reference-lookup is innate to a computer's existence? How do you measure memory when storage is non-volatile? This gets into an existential question about the nature of learning and knowledge vs. just taking a lot of notes.</p>

<p>Still, how do you even <strong><em>teach</em></strong> a computer what is meant by <em>""which animals is least like the other four?""</em> Did the computer really figure out what was being asked out of general intelligence, or is the computer simply designed to parse out IQ-style questions specifically? If you designed something with a foreknowledge of what would likely be asked, the computers of today <em>might</em> simply be able to ""recognize"" it as question-style 496.527b and plug in the variables. </p>

<p>But that's not <em>general intelligence</em> by any definition we use or understand. It's just a specialized, slick interpreter designed to parse out a specific type of standardized question. Ask it a style of question it's is not expecting, and you'll see the computer is exhibiting <strong><em>no</em> innate intelligence</strong> at all. </p>

<p><strong>Until we create <em>strong AI,</em> a computer has effectively <em>no</em> IQ.</strong></p>
",,3,2016-09-14T16:34:07.750,,1973,2016-09-15T17:33:32.900,2016-09-15T17:33:32.900,,95.0,,95.0,1969.0,2,2,,,,54.22,11.6,9.05,0.0,0.0,74.0,at this point in time what would be the IQ of our most intelligent AI systems Zero There are many different kinds of IQ tests including written visual and verbal assessments but the majority of questions are based on abstractreasoning problems that involve creative thinking and true intelligence In other words the computer would have to exhibit something that does not yet existhellip strong AI The intelligent computers of science fiction do not exist At all We are not even close We have absolutely NO IDEA how to bridge the gap between what we can do now and what is depicted in popculture films Even with cars that drive themselves and computers that play Go mdash an underachieving mosquito possesses more cognitive intelligence than the all the worlds super computers hellipcombined hellipor possibly disqualified for cheating Even if we could preformat the questions in a style and delivery system it understands what does memorization attention or speed mean in the context of a computer Im not even sure if a standardized IQ test makes sense in this context It might be like asking how a computer would do in a spelling bee In human terms were not allowed to bring along reference materials to look up an answer but how do you rectify that when referencelookup is innate to a computers existence How do you measure memory when storage is nonvolatile This gets into an existential question about the nature of learning and knowledge vs just taking a lot of notes Still how do you even teach a computer what is meant by which animals is least like the other four Did the computer really figure out what was being asked out of general intelligence or is the computer simply designed to parse out IQstyle questions specifically If you designed something with a foreknowledge of what would likely be asked the computers of today might simply be able to recognize it as questionstyle 496527b and plug in the variables But thats not general intelligence by any definition we use or understand Its just a specialized slick interpreter designed to parse out a specific type of standardized question Ask it a style of question its is not expecting and youll see the computer is exhibiting no innate intelligence at all Until we create strong AI a computer has effectively no IQ
,,"<p>The umbrella term for your problem is called <strong>natural language processing (NLP)</strong> -- a topic under artificial intelligence. </p>

<p>There are many subtopics to this field including language semantics, grammatical analysis, parts of speech tagging, domain specific context analysis, etc. </p>
",,0,2016-09-14T18:19:13.600,,1974,2016-11-05T12:17:29.653,2016-11-05T12:17:29.653,,1774.0,,1774.0,1970.0,2,4,,,,26.81,18.91,12.89,0.0,0.0,10.0,The umbrella term for your problem is called natural language processing NLP a topic under artificial intelligence There are many subtopics to this field including language semantics grammatical analysis parts of speech tagging domain specific context analysis etc
,,"<p>Interesting question.</p>

<p>Well if you really think about it, what is passion? How does that passion comes to be a passion.
One of the main topics you might want to touch here is conditioning and thus motivation.</p>

<p>Think about the following:</p>

<p>I have a passion for programming</p>

<blockquote>
  <p>Why do I have a passion for programming?</p>
</blockquote>

<p>Because when I wrote my first program I was positively reinforced by the fact that I completed a program, I was negatively reinforced because I removed my frustration of not completing the program</p>

<blockquote>
  <p>How come that I have gone through that programming frustration and
  stick to it even if I was frustrated?</p>
</blockquote>

<p>Because I wanted to learn programming</p>

<blockquote>
  <p>Why did I wanted to learn programming?</p>
</blockquote>

<p>Because I wanted a light on an arduino to turn on (projected reinforcer)</p>

<blockquote>
  <p>Why did I wanted to turn the arduino light on?</p>
</blockquote>

<p>So I could learn programming and because I though it was cool (classical conditioning association that will later be reinforced, projected reinforcement happened right after the classical conditioning association between turn on a led happened)</p>

<p>This can be done through a neural network, where each association is reinforced through a probability of outcome
For example, I did learn arduino, on purpose because it seemed the easiest way to start coding, so the probability of positive outcome was high</p>

<p>This about an opposite situation
Let's say I do not know calculus, and I barely know elementary algebra, if someone started to teach me about integrals saying that this is the only way to start learning more math, I will not be motivate to do so because since I cannot even conceptualize what an integral can be, it will be really hard for me to understand it thus I will not learn calc</p>

<p>Thus we can also discern that motivation is reinforced in small behaviors</p>

<p>Another more practical and realistic example you might use is</p>

<p>If you trow a rat in a cage, and make him lever-press do you think he is going to? No. Although if you reinforce the behavior of going next to the lever slowly and at the end he will lever press and you then reinforce that behavior he will.</p>

<p>Thus, passion is compartmentalized, and that's what you have to do in your NT and make it mathematically</p>

<blockquote>
  <p>WINK WINK:</p>
</blockquote>

<p>Small hint, it's a progressive function</p>
",,0,2016-09-15T02:20:44.330,,1975,2016-09-15T02:20:44.330,,,,,2434.0,1964.0,2,0,,,,40.25,10.57,9.09,0.0,0.0,36.0,Interesting question Well if you really think about it what is passion How does that passion comes to be a passion One of the main topics you might want to touch here is conditioning and thus motivation Think about the following I have a passion for programming Why do I have a passion for programming Because when I wrote my first program I was positively reinforced by the fact that I completed a program I was negatively reinforced because I removed my frustration of not completing the program How come that I have gone through that programming frustration and stick to it even if I was frustrated Because I wanted to learn programming Why did I wanted to learn programming Because I wanted a light on an arduino to turn on projected reinforcer Why did I wanted to turn the arduino light on So I could learn programming and because I though it was cool classical conditioning association that will later be reinforced projected reinforcement happened right after the classical conditioning association between turn on a led happened This can be done through a neural network where each association is reinforced through a probability of outcome For example I did learn arduino on purpose because it seemed the easiest way to start coding so the probability of positive outcome was high This about an opposite situation Lets say I do not know calculus and I barely know elementary algebra if someone started to teach me about integrals saying that this is the only way to start learning more math I will not be motivate to do so because since I cannot even conceptualize what an integral can be it will be really hard for me to understand it thus I will not learn calc Thus we can also discern that motivation is reinforced in small behaviors Another more practical and realistic example you might use is If you trow a rat in a cage and make him leverpress do you think he is going to No Although if you reinforce the behavior of going next to the lever slowly and at the end he will lever press and you then reinforce that behavior he will Thus passion is compartmentalized and thats what you have to do in your NT and make it mathematically WINK WINK Small hint its a progressive function
,3.0,"<ul>
<li>Would AI be a self-propogating iteration in which the previous AI is
destroyed by a more optimised AI child?  </li>
<li>Would the AI have branches of it's own AI warning not to create the new AI?</li>
</ul>
",,0,2016-09-15T14:08:40.983,,1976,2016-09-16T18:34:01.210,2016-09-15T15:15:53.003,,10.0,,2442.0,,1,1,<ai-design>,Would a sentient AI try to create a more optimised AI which would eventually overtake AI 1.0?,59.0,79.09,7.31,7.66,0.0,0.0,4.0,Would AI be a selfpropogating iteration in which the previous AI is destroyed by a more optimised AI child Would the AI have branches of its own AI warning not to create the new AI
,,"<p>A common concept in AI is ""recursive self-improvement."" That is, the AI 1.0 would build a version 1.01, which would build a version 1.02, and so on.</p>

<p>This is probably not going to be thought of as the newer version 'destroying' the older version; if an AI can self-modify, it's probably going to be more like going to sleep and waking up smarter, or learning a new mental technique, or so on.</p>

<p>One important point is that even if the AI is not allowed to self-modify, maybe because of a block put in by its programmers, that won't necessarily prevent it from constructing another AI out in the wild, and so an important problem is to figure out how to best generalize the concept of ""don't improve yourself"" so that we can make AIs that have bounded scope and impact.</p>
",,1,2016-09-15T15:15:00.190,,1977,2016-09-15T15:15:00.190,,,,,10.0,1976.0,2,2,,,,76.56,8.65,8.24,0.0,0.0,29.0,A common concept in AI is recursive selfimprovement That is the AI 10 would build a version 101 which would build a version 102 and so on This is probably not going to be thought of as the newer version destroying the older version if an AI can selfmodify its probably going to be more like going to sleep and waking up smarter or learning a new mental technique or so on One important point is that even if the AI is not allowed to selfmodify maybe because of a block put in by its programmers that wont necessarily prevent it from constructing another AI out in the wild and so an important problem is to figure out how to best generalize the concept of dont improve yourself so that we can make AIs that have bounded scope and impact
1981.0,2.0,"<blockquote>
  <p>Shortly about <a href=""https://en.wikipedia.org/wiki/Deep_learning"" rel=""nofollow""><strong>deep learning</strong> (for reference)</a>:</p>
  
  <p><strong><em>Deep learning</strong> is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by
  using a deep graph with multiple processing layers, composed of
  multiple linear and non-linear transformations.</em></p>
  
  <p><em>Various deep learning architectures such as deep neural networks, convolutional deep neural networks, deep belief networks and recurrent
  neural networks have been applied to fields like computer vision,
  automatic speech recognition, natural language processing, audio
  recognition and bioinformatics where they have been shown to produce
  state-of-the-art results on various tasks.</em></p>
</blockquote>

<hr>

<p><strong>My question:</strong></p>

<p>Can <a href=""https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_network_architectures"" rel=""nofollow"">deep neural networks</a> or <a href=""https://en.wikipedia.org/wiki/Convolutional_neural_network"" rel=""nofollow"">convolutional deep neural networks</a> be viewed as <a href=""https://en.wikipedia.org/wiki/Ensemble_learning"" rel=""nofollow"">ensemble-based</a> method of machine learning? Or it is different approaches?</p>
",,0,2016-09-15T15:34:08.027,,1978,2016-09-15T16:08:15.410,2016-09-15T16:08:15.410,,1791.0,,1791.0,,1,5,<neural-networks><machine-learning><deep-learning><conv-neural-network>,Do deep learning algorithms represent ensemble-based methods?,159.0,33.07,18.4,11.66,0.0,0.0,20.0,Shortly about deep learning for reference Deep learning is a branch of machine learning based on a set of algorithms that attempt to model highlevel abstractions in data by using a deep graph with multiple processing layers composed of multiple linear and nonlinear transformations Various deep learning architectures such as deep neural networks convolutional deep neural networks deep belief networks and recurrent neural networks have been applied to fields like computer vision automatic speech recognition natural language processing audio recognition and bioinformatics where they have been shown to produce stateoftheart results on various tasks My question Can deep neural networks or convolutional deep neural networks be viewed as ensemblebased method of machine learning Or it is different approaches
,,"<p>Honestly, nobody knows.  Any talk of sentient AI's is still basically sci-fi and we can't really offer anything more than informed speculation.  But think about it this way:  sentience, in and of itself, doesn't necessarily involve any ""goals"" or ""desires"" or ""objectives"" beyond what the AI creator programmed in.  Be careful not to over anthropomorphize and assume that any ""sentient AI"" is going behave like a human.   </p>

<p>In other words, there's no <strong>particular</strong> reason to say that any given AI must be ""a self-propogating iteration in which the previous AI is destroyed by a more optimised AI child"". </p>

<p>So all of that said, my answer to ""Would a sentient AI try to create a more optimised AI which would eventually overtake AI 1.0"" is:</p>

<p>""If the creator of the AI programs it to do that, then yes. Otherwise, probably not.""  </p>

<p>So would a hypothetical AI creator program the AI to try and improve itself?  Who knows. It's the kind of thing that seems like it might be a good idea.  And I suppose such a motive could - in principle - even slip in by accident. </p>
",,0,2016-09-15T15:45:01.693,,1979,2016-09-15T15:45:01.693,,,,,33.0,1976.0,2,2,,,,62.98,9.39,9.1,0.0,0.0,44.0,Honestly nobody knows Any talk of sentient AIs is still basically scifi and we cant really offer anything more than informed speculation But think about it this way sentience in and of itself doesnt necessarily involve any goals or desires or objectives beyond what the AI creator programmed in Be careful not to over anthropomorphize and assume that any sentient AI is going behave like a human In other words theres no particular reason to say that any given AI must be a selfpropogating iteration in which the previous AI is destroyed by a more optimised AI child So all of that said my answer to Would a sentient AI try to create a more optimised AI which would eventually overtake AI 10 is If the creator of the AI programs it to do that then yes Otherwise probably not So would a hypothetical AI creator program the AI to try and improve itself Who knows Its the kind of thing that seems like it might be a good idea And I suppose such a motive could in principle even slip in by accident
,,"<p>Deep neural networks could - in principle - be a component of an <a href=""https://en.wikipedia.org/wiki/Ensemble_learning"" rel=""nofollow"">ensemble of machine learning algorithms</a>, yes.  Ensemble method basically just means use multiple algorithms and combining their output somehow.  </p>

<p>Other than that, I don't see any special connection between deep learning and the idea of ensemble methods.  DL is just one more tool in the toolkit. </p>
",,0,2016-09-15T15:50:56.390,,1980,2016-09-15T15:50:56.390,,,,,33.0,1978.0,2,2,,,,57.06,12.46,10.99,0.0,0.0,9.0,Deep neural networks could in principle be a component of an ensemble of machine learning algorithms yes Ensemble method basically just means use multiple algorithms and combining their output somehow Other than that I dont see any special connection between deep learning and the idea of ensemble methods DL is just one more tool in the toolkit
,,"<p>You should think of them as different approaches. A deep neural net is a single independent model, whereas ensemble models are ensembles of many independent models.</p>

<p>The primary connection between the two is <a href=""http://ai.stackexchange.com/questions/40/what-is-the-dropout-technique"">dropout</a>, a particular method of training deep neural nets that's inspired by ensemble methods. </p>
",,0,2016-09-15T16:06:03.437,,1981,2016-09-15T16:06:03.437,,,,,10.0,1978.0,2,3,,,,55.54,13.92,11.47,0.0,0.0,6.0,You should think of them as different approaches A deep neural net is a single independent model whereas ensemble models are ensembles of many independent models The primary connection between the two is dropout a particular method of training deep neural nets thats inspired by ensemble methods
,2.0,"<p>Are Convolutional Neural Networks summarily better than pattern recognition in all existing image processing libraries that don't use CNN's? Or are there still hard outstanding problems in image processing that seem to be beyond their capability?</p>
",,1,2016-09-15T20:36:18.530,2.0,1982,2017-03-06T20:43:43.560,2016-10-07T17:31:23.663,,10.0,,46.0,,1,2,<image-recognition><comparison><cnn>,Are Convolutional Neural Networks better than existing image recognition libraries that don't use CNNs?,216.0,36.28,16.41,10.67,0.0,0.0,4.0,Are Convolutional Neural Networks summarily better than pattern recognition in all existing image processing libraries that dont use CNNs Or are there still hard outstanding problems in image processing that seem to be beyond their capability
,,"<p>Now in most cases we still have clear distinctions between programs and data. But when an AI becomes sentient, its data would be as powerful as what we currently call programs, and its program might be as irrelevant as what we currently call hardware. Then it would be difficult to distinguish creating an AI from learning new things, or buying new hardwares with improved instruction set.</p>

<p>For example, if some AI invent new algorithms that its creator finally put that on itself, buys itself some new computers, write a new efficient compiler that recompiles its own code and put that to the new computer, fill the new computer with all the knowledges it learned, and cut off the communication for reasons such as missions on the Mars. Did it create a more optimized AI?</p>

<p>In contrast, if some AI created something completely new, but shares some code with itself. In fact, that's because they run in the same operating system and shares the same standard C library. Is the new AI considered evolved from itself and not a separate entity? Maybe the core AI algorithms and even some basic knowledges would be as common as the standard C library in the future. And what we think is based on the same system is considered completely new in the future.</p>

<p>Anyway, humans have limited and nonextensible resources, nontransferrable knowledges, and limited throughput interacting with the world. These problems could probably be overcome within a few AI generations. With the same hardware, I doubt that the AI related algorithms could be indefinitely better and better. And there is a physical bound on the hardwares. It won't last long even if that happens.</p>

<p>In the unlikely case that there could be that many generations and AIs are that violent, as long as there are competitors, the warning doesn't make much sense considering how evolution works.</p>
",,0,2016-09-16T18:34:01.210,,1984,2016-09-16T18:34:01.210,,,,,1424.0,1976.0,2,0,,,,60.14,11.14,9.21,0.0,0.0,36.0,Now in most cases we still have clear distinctions between programs and data But when an AI becomes sentient its data would be as powerful as what we currently call programs and its program might be as irrelevant as what we currently call hardware Then it would be difficult to distinguish creating an AI from learning new things or buying new hardwares with improved instruction set For example if some AI invent new algorithms that its creator finally put that on itself buys itself some new computers write a new efficient compiler that recompiles its own code and put that to the new computer fill the new computer with all the knowledges it learned and cut off the communication for reasons such as missions on the Mars Did it create a more optimized AI In contrast if some AI created something completely new but shares some code with itself In fact thats because they run in the same operating system and shares the same standard C library Is the new AI considered evolved from itself and not a separate entity Maybe the core AI algorithms and even some basic knowledges would be as common as the standard C library in the future And what we think is based on the same system is considered completely new in the future Anyway humans have limited and nonextensible resources nontransferrable knowledges and limited throughput interacting with the world These problems could probably be overcome within a few AI generations With the same hardware I doubt that the AI related algorithms could be indefinitely better and better And there is a physical bound on the hardwares It wont last long even if that happens In the unlikely case that there could be that many generations and AIs are that violent as long as there are competitors the warning doesnt make much sense considering how evolution works
,,"<p>Great question. Today AI systems works in ""one burst"" mode. Get one input and generate one output. Our brains are not working like that. </p>

<p>First step is to learn network how to communicate with it's ""helper"", so network instead of result generate question and cycle will repeat until network find result. </p>

<p>Network must be recurrent for inner state needed between question/answer cycles. </p>
",,0,2016-09-17T13:42:44.877,,1986,2016-09-17T13:42:44.877,,,,,2478.0,1953.0,2,0,,,,67.35,11.93,10.36,0.0,0.0,13.0,Great question Today AI systems works in one burst mode Get one input and generate one output Our brains are not working like that First step is to learn network how to communicate with its helper so network instead of result generate question and cycle will repeat until network find result Network must be recurrent for inner state needed between questionanswer cycles
1990.0,1.0,"<p>I have been messing around in <a href=""http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=spiral&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.73263&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false"" rel=""nofollow"">tensorflow playground</a>. One of the input data sets is a spiral. No matter what input parameters I choose, no matter how wide and deep the neural network I make, I cannot fit the spiral. How do data scientists fit data of this shape?</p>
",,1,2016-09-18T11:20:23.830,1.0,1987,2016-09-19T00:46:01.433,2016-09-19T00:46:01.433,,8.0,,2472.0,,1,4,<neural-networks><classification><tensorflow>,How to classify data which is spiral in shape?,170.0,76.01,7.06,8.43,0.0,0.0,6.0,I have been messing around in tensorflow playground One of the input data sets is a spiral No matter what input parameters I choose no matter how wide and deep the neural network I make I cannot fit the spiral How do data scientists fit data of this shape
,,"<p>Just for the sake of completeness, I'll point out that Recurrent Neural Nets (i.e. neural nets with backwards connections) are frequently used for for Natural Language Processing (NLP). This includes variants like Bidirectional, Jordan and Elman Networks. Long Short-Term Memory (LSTM) is a more sophisticated neural net algorithm which can accomplish the same time and sequence-based tasks, but which can leverage standard learning methods like backprop since it doesn't suffer from the ""vanishing gradient problem."" This is because LSTMs have been brilliantly engineered as ""perfect integrators,"" which makes it a lot easier to calculate the error gradients etc. over long periods of time. In contrast, learning with RNNs is still not theoretically well-grounded and is difficult to calculate through existing methods like Backpropagation Through Time (BPTT). In Time Delay Neural Networks (TDNNs), the idea is to add new neurons and connections with each new training example across a stretch of time or training sequence; unfortunately, this places a practical limitation on how many examples you can feed into the net before the size of the network gets out of hand or it starts forgetting, just as with RNNs. LSTMs have much longer memories (especially when augmented with Neural Turing Machines) so that'd be my first choice, assuming I wanted to use neural nets for NLP purposes. My knowledge of the subject is limited though (I'm still trying to learn the ropes) so there may be other important neural net algorithms I'm overlooking...</p>
",,0,2016-09-18T16:50:48.577,,1988,2016-09-18T16:50:48.577,,,,,1427.0,1970.0,2,3,,,,55.37,14.22,10.45,0.0,0.0,49.0,Just for the sake of completeness Ill point out that Recurrent Neural Nets ie neural nets with backwards connections are frequently used for for Natural Language Processing NLP This includes variants like Bidirectional Jordan and Elman Networks Long ShortTerm Memory LSTM is a more sophisticated neural net algorithm which can accomplish the same time and sequencebased tasks but which can leverage standard learning methods like backprop since it doesnt suffer from the vanishing gradient problem This is because LSTMs have been brilliantly engineered as perfect integrators which makes it a lot easier to calculate the error gradients etc over long periods of time In contrast learning with RNNs is still not theoretically wellgrounded and is difficult to calculate through existing methods like Backpropagation Through Time BPTT In Time Delay Neural Networks TDNNs the idea is to add new neurons and connections with each new training example across a stretch of time or training sequence unfortunately this places a practical limitation on how many examples you can feed into the net before the size of the network gets out of hand or it starts forgetting just as with RNNs LSTMs have much longer memories especially when augmented with Neural Turing Machines so thatd be my first choice assuming I wanted to use neural nets for NLP purposes My knowledge of the subject is limited though Im still trying to learn the ropes so there may be other important neural net algorithms Im overlooking
1992.0,3.0,"<p>A ""general intelligence"" may be capable of learning a lot of different things, but possessing capability does not equal actually having it. The ""AGI"" must learn...and that learning process can take time. If you want an AGI to drive a car or play Go, you have to find some way of ""teaching"" it. Keep in mind that we have never built AGIs, so we don't know how long the training process can be, but it would be safe to assume pessimistic estimates.</p>

<p>Contrast that to a ""narrow intelligence"". The narrow AI already knows how to drive a car or play Go. It has been programmed to be very excellent at one specific task. You don't need to worry about training the machine, because it has already been pre-trained.</p>

<p>A ""general intelligence"" seems to be more flexible than a ""narrow intelligence"". You could buy an AGI and have it drive a car <em>and</em> play Go. And if you are willing to do more training, you can even teach it a new trick: <em>how to bake a cake</em>. I don't have to worry about unexpected tasks coming up, since the AGI will <em>eventually</em> figure out how to do it, given enough training time. I would have to wait a <em>long time</em> though.</p>

<p>A ""narrow intelligence"" appears to be <em>more efficient</em> at its assigned task, due to it being programmed specifically for that task. It knows exactly what to do, and doesn't have to waste time ""learning"" (unlike our AGI buddy here). Instead of buying one AGI to handle a bunch of different tasks poorly, I would rather buy a bunch of specialized narrow AIs. Narrow AI #1 drives cars, Narrow AI #2 plays Go, Narrow AI #3 bake cakes, etc. That being said, this is a very brittle approach, since if some unexpected task comes up, none of my narrow AIs would be able to handle it. I'm willing to accept that risk though.</p>

<p>Is my ""thinking"" correct? Is there a trade-off between flexibility (AGI) and efficiency (narrow AI), like what I have just described above? Or is it theoretically possible for an AGI to be both flexible and efficient?</p>
",,0,2016-09-18T19:43:02.197,,1989,2016-11-29T13:56:05.997,2016-09-20T00:09:31.950,,181.0,,181.0,,1,3,<ai-design><strong-ai><weak-ai>,Is there a trade-off between flexibility and efficiency?,126.0,72.56,8.35,7.89,0.0,0.0,78.0,A general intelligence may be capable of learning a lot of different things but possessing capability does not equal actually having it The AGI must learnand that learning process can take time If you want an AGI to drive a car or play Go you have to find some way of teaching it Keep in mind that we have never built AGIs so we dont know how long the training process can be but it would be safe to assume pessimistic estimates Contrast that to a narrow intelligence The narrow AI already knows how to drive a car or play Go It has been programmed to be very excellent at one specific task You dont need to worry about training the machine because it has already been pretrained A general intelligence seems to be more flexible than a narrow intelligence You could buy an AGI and have it drive a car and play Go And if you are willing to do more training you can even teach it a new trick how to bake a cake I dont have to worry about unexpected tasks coming up since the AGI will eventually figure out how to do it given enough training time I would have to wait a long time though A narrow intelligence appears to be more efficient at its assigned task due to it being programmed specifically for that task It knows exactly what to do and doesnt have to waste time learning unlike our AGI buddy here Instead of buying one AGI to handle a bunch of different tasks poorly I would rather buy a bunch of specialized narrow AIs Narrow AI 1 drives cars Narrow AI 2 plays Go Narrow AI 3 bake cakes etc That being said this is a very brittle approach since if some unexpected task comes up none of my narrow AIs would be able to handle it Im willing to accept that risk though Is my thinking correct Is there a tradeoff between flexibility AGI and efficiency narrow AI like what I have just described above Or is it theoretically possible for an AGI to be both flexible and efficient
,,"<p>There are many approaches to this kind of problem. The most obvious one is to <strong>create new features</strong>. The best features I can come up with is to transform the coordinates to <a href=""https://en.wikipedia.org/wiki/Spherical_coordinate_system"" rel=""nofollow noreferrer"">spherical coordinates</a>. </p>

<p>I have not found a way to do it in playground, so I just created a few features that should help with this (sin features). After <a href=""http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=spiral&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.73263&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=true&amp;cosY=false&amp;sinY=true&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false"" rel=""nofollow noreferrer"">500 iterations</a> it will saturate and will fluctuate at 0.1 score. This suggest that no further improvement will be done and most probably I should make the hidden layer wider or add another layer.</p>

<p>Not a surprise that after adding <a href=""http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=spiral&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=5,2&amp;seed=0.73263&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=true&amp;cosY=false&amp;sinY=true&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false"" rel=""nofollow noreferrer"">just one neuron to the hidden layer</a> you easily get 0.013 after 300 iterations. Similar thing happens by adding a new layer (0.017, but after significantly longer 500 iterations. Also no surprise as it is harder to propagate the errors). Most probably you can play with a learning rate or do an adaptive learning to make it faster, but this is not the point here.</p>

<p><a href=""https://i.stack.imgur.com/tck2s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tck2s.png"" alt=""enter image description here""></a></p>
",,2,2016-09-18T23:27:57.760,,1990,2016-09-18T23:27:57.760,,,,,2492.0,1987.0,2,5,,,,74.39,8.52,8.24,0.0,0.0,20.0,There are many approaches to this kind of problem The most obvious one is to create new features The best features I can come up with is to transform the coordinates to spherical coordinates I have not found a way to do it in playground so I just created a few features that should help with this sin features After 500 iterations it will saturate and will fluctuate at 01 score This suggest that no further improvement will be done and most probably I should make the hidden layer wider or add another layer Not a surprise that after adding just one neuron to the hidden layer you easily get 0013 after 300 iterations Similar thing happens by adding a new layer 0017 but after significantly longer 500 iterations Also no surprise as it is harder to propagate the errors Most probably you can play with a learning rate or do an adaptive learning to make it faster but this is not the point here
,,"<p>The cleanest result we have on this issue is the <a href=""https://en.wikipedia.org/wiki/No_free_lunch_theorem"" rel=""nofollow"">""no free lunch"" theorem</a>. Basically, in order to make a system perform better at a specific task, you have to degrade its performance on other tasks, and so there is a flexibility-efficiency tradeoff.</p>

<p>But to the broader question, or whether or not your thinking is correct, I think it pays to look more closely at what you mean by a ""narrow intelligence."" The AI systems that we have that play Go and drive cars did <em>not</em> pop into existence able to do those things; they slowly learned how through lots and lots of training examples and a well-chosen architecture that mirrors the problem domain.</p>

<p>That is, ""neural networks"" as a methodology seems 'general' in a meaningful way; one could imagine that a general intelligence could be formed by solving the meta-learning problem (that is, learning the architecture that best suits a particular problem while learning the weights for that problem from training data).</p>

<p>Even in that case, there will still be a flexibility-efficiency tradeoff; the general intelligence that's allowed to vary its architecture will be able to solve many different problems, but will take some time to discover what problem it's facing. An intelligence locked into a particular architecture will perform well on problems that architecture is well-suited for (better than the general, since it doesn't need to discover) but less well on other problems it isn't as well-suited for.</p>
",,1,2016-09-19T14:44:48.200,,1992,2016-09-19T14:44:48.200,,,,,10.0,1989.0,2,4,,,,36.66,12.6,9.61,0.0,0.0,42.0,The cleanest result we have on this issue is the no free lunch theorem Basically in order to make a system perform better at a specific task you have to degrade its performance on other tasks and so there is a flexibilityefficiency tradeoff But to the broader question or whether or not your thinking is correct I think it pays to look more closely at what you mean by a narrow intelligence The AI systems that we have that play Go and drive cars did not pop into existence able to do those things they slowly learned how through lots and lots of training examples and a wellchosen architecture that mirrors the problem domain That is neural networks as a methodology seems general in a meaningful way one could imagine that a general intelligence could be formed by solving the metalearning problem that is learning the architecture that best suits a particular problem while learning the weights for that problem from training data Even in that case there will still be a flexibilityefficiency tradeoff the general intelligence thats allowed to vary its architecture will be able to solve many different problems but will take some time to discover what problem its facing An intelligence locked into a particular architecture will perform well on problems that architecture is wellsuited for better than the general since it doesnt need to discover but less well on other problems it isnt as wellsuited for
,,"<p>It would appear so.  One example, albeit not specifically AI related, is seen in the difference between digital computers and <a href=""https://en.wikipedia.org/wiki/Analog_computer"" rel=""nofollow"">analog computers</a>.  Pretty much everything we think of as a ""computer"" today is a digital computer with a von Neumann architecture.  And that's because the things are so general purpose that they can be easily programmed to do, essentially, anything.  But analog computers can (or could, back in the 60's or thereabouts) solve some types of problems faster than a digital computer.  But they fell out of favor exactly due to that lack of flexibility.  Nobody wants to hand-wire circuits with op-amps and comparators to solve for <em>y</em>.  </p>
",,0,2016-09-19T15:06:34.227,,1993,2016-09-19T15:06:34.227,,,,,33.0,1989.0,2,1,,,,55.84,11.6,9.96,0.0,0.0,20.0,It would appear so One example albeit not specifically AI related is seen in the difference between digital computers and analog computers Pretty much everything we think of as a computer today is a digital computer with a von Neumann architecture And thats because the things are so general purpose that they can be easily programmed to do essentially anything But analog computers can or could back in the 60s or thereabouts solve some types of problems faster than a digital computer But they fell out of favor exactly due to that lack of flexibility Nobody wants to handwire circuits with opamps and comparators to solve for y
,,"<p><a href=""https://en.wikipedia.org/wiki/Strong_AI"" rel=""nofollow"">Strong</a> and <a href=""https://en.wikipedia.org/wiki/Weak_AI"" rel=""nofollow"">weak AI</a> are the older terms for <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"" rel=""nofollow"">AGI</a> (artificial general intelligence) and narrow AI. At least that's how I have seen it used and wikipedia seems to agree. </p>

<p>I personally haven't seen Searle's definition of ""weak and strong AI"" in use much, but maybe the shift to the newer terms came about in part because Searle successfully confused the issue. </p>
",,0,2016-09-19T16:08:55.187,,1995,2016-09-19T16:08:55.187,,,,,2227.0,74.0,2,1,,,,58.92,10.15,9.25,0.0,0.0,11.0,Strong and weak AI are the older terms for AGI artificial general intelligence and narrow AI At least thats how I have seen it used and wikipedia seems to agree I personally havent seen Searles definition of weak and strong AI in use much but maybe the shift to the newer terms came about in part because Searle successfully confused the issue
,1.0,"<p>Is there a neural network(NN) system or architecture which can be used for only storing and retrieving information. For example; to store whole Avatar movie in HD format inside a neural network and retrieve(without loss) it from the neural network when needed. I searched the web and came across only LSTM RNN but in my understanding LSTM only stores pattern and not the content itself. If there is no such NN exist can you explain why it so?</p>
",,0,2016-09-20T04:20:02.937,2.0,1996,2016-09-20T13:17:17.733,,,,,39.0,,1,2,<neural-networks>,Which neural networks can be used only for storing and retrieving information?,84.0,68.6,9.86,9.87,0.0,0.0,9.0,Is there a neural networkNN system or architecture which can be used for only storing and retrieving information For example to store whole Avatar movie in HD format inside a neural network and retrievewithout loss it from the neural network when needed I searched the web and came across only LSTM RNN but in my understanding LSTM only stores pattern and not the content itself If there is no such NN exist can you explain why it so
1999.0,1.0,"<p>The question is about the architecture of Deep Residual Networks (<strong>ResNets</strong>). The model that won the 1-st places at <a href=""http://image-net.org/challenges/LSVRC/2015/results"" rel=""nofollow noreferrer"">""Large Scale Visual Recognition Challenge 2015"" (ILSVRC2015)</a> in all five main tracks:</p>

<blockquote>
  <ul>
  <li><em>ImageNet Classification: “Ultra-deep” (quote Yann) 152-layer nets</em> </li>
  <li><em>ImageNet Detection: 16% better than 2nd</em></li>
  <li><em>ImageNet Localization: 27% better than 2nd</em></li>
  <li><em>COCO Detection: 11% better than 2nd</em></li>
  <li><em>COCO Segmentation: 12% better than 2nd<br><br></em>
  <em>Source:</em> <a href=""http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf"" rel=""nofollow noreferrer""><em>MSRA @ ILSVRC &amp; COCO 2015 competitions (presentation, 2-nd slide)</em></a></li>
  </ul>
</blockquote>

<p>This work is described in the following article:</p>

<blockquote>
  <p><a href=""http://arxiv.org/abs/1512.03385"" rel=""nofollow noreferrer""><em>Deep Residual Learning for Image Recognition (2015, PDF)</em></a></p>
</blockquote>

<hr>

<p><strong>Microsoft Research team</strong> (developers of ResNets: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun) in their article:</p>

<blockquote>
  <p><a href=""https://arxiv.org/pdf/1603.05027.pdf"" rel=""nofollow noreferrer"">""<em>Identity Mappings in Deep Residual Networks (2016)</em>""</a></p>
</blockquote>

<p>state that <strong>depth</strong> plays a key role:</p>

<blockquote>
  <p><em>""<strong>We obtain these results via a simple but essential concept — going deeper. These results demonstrate the potential of pushing the limits of depth.</strong>""</em></p>
</blockquote>

<p>It is emphasized in their <a href=""http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf"" rel=""nofollow noreferrer"">presentation</a> also (deeper - better):<br> </p>

<blockquote>
  <p><em>- ""A deeper model should not have higher training error.""<br> 
  - ""Deeper ResNets have lower training error, and also lower test error.""<br> 
  - ""Deeper ResNets have lower error.""<br>
  - ""All benefit more from deeper features – cumulative gains!""<br>
  - ""Deeper is still better.""</em></p>
</blockquote>

<p>Here is the sctructure of 34-layer residual (for reference):
<a href=""https://i.stack.imgur.com/L8m0X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L8m0X.png"" alt=""enter image description here""></a></p>

<hr>

<p>But recently I have found one theory that introduces a novel interpretation of residual networks showing they are exponential ensembles:</p>

<blockquote>
  <p><a href=""https://arxiv.org/abs/1605.06431"" rel=""nofollow noreferrer""><em>Residual Networks are Exponential Ensembles of Relatively Shallow Networks (2016)</em></a></p>
</blockquote>

<p>Deep Resnets are described as many shallow networks whose outputs are pooled at various depths. 
There is a picture in the article. I attach it with explanation:</p>

<blockquote>
  <p><a href=""https://i.stack.imgur.com/PGhK2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PGhK2.jpg"" alt=""enter image description here""></a> Residual Networks are
  conventionally shown as (a), which is a natural representation of
  Equation (1). When we expand this formulation to Equation (6), we
  obtain an unraveled view of a 3-block residual network (b). From this
  view, it is apparent that residual networks have O(2^n) implicit paths
  connecting input and output and that adding a block doubles the number
  of paths.</p>
</blockquote>

<p>In conclusion of the article it is stated:</p>

<blockquote>
  <p><strong>It is not depth, but the ensemble that makes residual networks strong</strong>.
  Residual networks push the limits of network multiplicity, not network
  depth. Our proposed unraveled view and the lesion study show that
  residual networks are an implicit ensemble of exponentially many
  networks. If most of the paths that contribute gradient are very short
  compared to the overall depth of the network, <strong>increased depth</strong>
  alone <strong>can’t be the key characteristic</strong> of residual networks. We now
  believe that <strong>multiplicity</strong>, the network’s expressability in the
  terms of the number of paths, plays <strong>a key role</strong>.</p>
</blockquote>

<p>But it is only a recent theory that can be confirmed or refuted. It happens sometimes that some theories are refuted and articles are withdrawn.</p>

<hr>

<p><strong>My question:</strong><br>
Should we think of deep ResNets as ensemble after all? <strong>Ensemble</strong> or <strong>depth</strong> makes residual networks so strong? Is it possible that even the developers themselves do not quite perceive what their own model represent and what is the key concept in it?</p>
",,0,2016-09-20T10:54:14.493,,1997,2016-09-20T14:40:30.907,2016-09-20T13:59:18.410,,1791.0,,1791.0,,1,5,<neural-networks><machine-learning><deep-network><deep-learning><pattern-recognition>,ResNets. Ensemble or depth makes residual networks strong?,150.0,49.96,14.04,9.51,0.0,0.0,120.0,The question is about the architecture of Deep Residual Networks ResNets The model that won the 1st places at Large Scale Visual Recognition Challenge 2015 ILSVRC2015 in all five main tracks ImageNet Classification “Ultradeep” quote Yann 152layer nets ImageNet Detection 16 better than 2nd ImageNet Localization 27 better than 2nd COCO Detection 11 better than 2nd COCO Segmentation 12 better than 2nd Source MSRA ILSVRC amp COCO 2015 competitions presentation 2nd slide This work is described in the following article Deep Residual Learning for Image Recognition 2015 PDF Microsoft Research team developers of ResNets Kaiming He Xiangyu Zhang Shaoqing Ren Jian Sun in their article Identity Mappings in Deep Residual Networks 2016 state that depth plays a key role We obtain these results via a simple but essential concept — going deeper These results demonstrate the potential of pushing the limits of depth It is emphasized in their presentation also deeper better A deeper model should not have higher training error Deeper ResNets have lower training error and also lower test error Deeper ResNets have lower error All benefit more from deeper features – cumulative gains Deeper is still better Here is the sctructure of 34layer residual for reference But recently I have found one theory that introduces a novel interpretation of residual networks showing they are exponential ensembles Residual Networks are Exponential Ensembles of Relatively Shallow Networks 2016 Deep Resnets are described as many shallow networks whose outputs are pooled at various depths There is a picture in the article I attach it with explanation Residual Networks are conventionally shown as a which is a natural representation of Equation 1 When we expand this formulation to Equation 6 we obtain an unraveled view of a 3block residual network b From this view it is apparent that residual networks have O2n implicit paths connecting input and output and that adding a block doubles the number of paths In conclusion of the article it is stated It is not depth but the ensemble that makes residual networks strong Residual networks push the limits of network multiplicity not network depth Our proposed unraveled view and the lesion study show that residual networks are an implicit ensemble of exponentially many networks If most of the paths that contribute gradient are very short compared to the overall depth of the network increased depth alone can’t be the key characteristic of residual networks We now believe that multiplicity the network’s expressability in the terms of the number of paths plays a key role But it is only a recent theory that can be confirmed or refuted It happens sometimes that some theories are refuted and articles are withdrawn My question Should we think of deep ResNets as ensemble after all Ensemble or depth makes residual networks so strong Is it possible that even the developers themselves do not quite perceive what their own model represent and what is the key concept in it
,,"<p>Neural networks are usually trained to produce a certain output given a certain input. Often the output is a classification of the input or some other form of input description. Sometimes it is an action in a game and sometimes it is indeed stored data, a memory if you will. </p>

<p>In that case the input is often a part of the stored data, so the NN actually completes the given input. This setup is called <a href=""https://en.wikipedia.org/wiki/Autoassociative_memory"" rel=""nofollow"">autoassociative memory</a>, <a href=""https://en.wikipedia.org/wiki/Hopfield_network"" rel=""nofollow"">Hopfield networks</a> are an example for this. </p>

<p>In your example you might give this NN the first frame of the movie Avatar and it would output the complete movie. Unfortunately this would probably be absolutely <a href=""https://en.wikipedia.org/wiki/Hopfield_network#Capacity"" rel=""nofollow"">crazy inefficient</a>. </p>
",,0,2016-09-20T13:17:17.733,,1998,2016-09-20T13:17:17.733,,,,,2227.0,1996.0,2,4,,,,54.93,9.74,9.57,0.0,0.0,10.0,Neural networks are usually trained to produce a certain output given a certain input Often the output is a classification of the input or some other form of input description Sometimes it is an action in a game and sometimes it is indeed stored data a memory if you will In that case the input is often a part of the stored data so the NN actually completes the given input This setup is called autoassociative memory Hopfield networks are an example for this In your example you might give this NN the first frame of the movie Avatar and it would output the complete movie Unfortunately this would probably be absolutely crazy inefficient
,,"<p>Imagine a genie grants you three wishes. Because you are an ambitious deep learning researcher your first wish is a perfect solution for a 1000-layer NN for Image Net, which promptly appears on your laptop.</p>

<p>Now a genie induced solution doesn't give you any intuition how it might be interpreted as an ensemble, but do you really believe that you need 1000 layers of abstraction to distinguish a cat from a dog? As the authors of the ""ensemble paper"" mention themselves, this is definitely not true for biological systems.</p>

<p>Of course you could waste your second wish on a decomposition of the solution into an ensemble of networks, and I'm pretty sure the genie would be able to oblige. The reason being that part of the power of a deep network will always come from the ensemble effect.</p>

<p>So it is not surprising that two very successful tricks to train deep networks, dropout and residual networks, have an immediate interpretation as implicit ensemble. Therefore ""it's not depth, but the ensemble"" strikes me as a false dichotomy. You would really only say that if you honestly believed that you need hundreds or thousands of levels of abstraction to classify images with human accuracy. </p>

<p>I suggest you use the last wish for something else, maybe a pinacolada. </p>
",,0,2016-09-20T14:40:30.907,,1999,2016-09-20T14:40:30.907,,,,,2227.0,1997.0,2,3,,,,58.11,10.91,9.7,0.0,0.0,26.0,Imagine a genie grants you three wishes Because you are an ambitious deep learning researcher your first wish is a perfect solution for a 1000layer NN for Image Net which promptly appears on your laptop Now a genie induced solution doesnt give you any intuition how it might be interpreted as an ensemble but do you really believe that you need 1000 layers of abstraction to distinguish a cat from a dog As the authors of the ensemble paper mention themselves this is definitely not true for biological systems Of course you could waste your second wish on a decomposition of the solution into an ensemble of networks and Im pretty sure the genie would be able to oblige The reason being that part of the power of a deep network will always come from the ensemble effect So it is not surprising that two very successful tricks to train deep networks dropout and residual networks have an immediate interpretation as implicit ensemble Therefore its not depth but the ensemble strikes me as a false dichotomy You would really only say that if you honestly believed that you need hundreds or thousands of levels of abstraction to classify images with human accuracy I suggest you use the last wish for something else maybe a pinacolada
2004.0,2.0,"<p>In my attempt at trying to learn neural network and machine learning I'm am trying to create a simple neural network which can be trained to recognise one word from a given string (which contains only one word). So in effect if one where to feed it a string containing the trained word but spelled wrong the network would be able to still recognise the word. Can anybody help me with some pseudo code or a start of a code. Or a general explanation of how to to this because I have read like 6 articles and 8 example projects and still have no clue how to do this</p>
",2016-09-21T18:50:00.793,4,2016-09-20T15:55:48.757,,2000,2016-09-21T07:43:06.087,2016-09-20T16:12:01.293,,2529.0,,2529.0,,1,0,<neural-networks><machine-learning>,Simple text recognition with neural network,164.0,69.25,7.2,8.32,0.0,0.0,6.0,In my attempt at trying to learn neural network and machine learning Im am trying to create a simple neural network which can be trained to recognise one word from a given string which contains only one word So in effect if one where to feed it a string containing the trained word but spelled wrong the network would be able to still recognise the word Can anybody help me with some pseudo code or a start of a code Or a general explanation of how to to this because I have read like 6 articles and 8 example projects and still have no clue how to do this
,,"<p>If I'm reading it correctly, this question has nothing to do with optical character recognition. You want to create a system that takes a digital string of characters as input, then finds the best match from a predetermined list of words. That sounds like a task for if-then-else logic and dictionary lookup. It might be possible to use a neural net, but not easy.</p>

<p>A neural net takes a fixed number of inputs, each of which are a value between zero and one. A major hurdle is that you probably want variable-sized inputs. Another hurdle is that you'll need to code the inputs some way onto numbers.</p>

<p>These hurdles can be overcome but they are tipoffs that neural networks aren't well-suited for the task.</p>
",,0,2016-09-21T06:43:29.150,,2003,2016-09-21T06:43:29.150,,,,,2542.0,2000.0,2,0,,,,64.2,9.57,9.5,0.0,0.0,19.0,If Im reading it correctly this question has nothing to do with optical character recognition You want to create a system that takes a digital string of characters as input then finds the best match from a predetermined list of words That sounds like a task for ifthenelse logic and dictionary lookup It might be possible to use a neural net but not easy A neural net takes a fixed number of inputs each of which are a value between zero and one A major hurdle is that you probably want variablesized inputs Another hurdle is that youll need to code the inputs some way onto numbers These hurdles can be overcome but they are tipoffs that neural networks arent wellsuited for the task
,,"<p>An optimal solution for the task as stated, would be some alignment algorithm like Smith-Waterman, with a matrix which encodes typical typo frequencies. </p>

<p>As an exercise in NNs, I would recommend using a RNN. This circumvents the problem that your inputs will be of variable size, because you just feed one letter after another and get an output once you feed the delimiter. </p>

<p>As trainingsdata you'll need a list of random words and possibly a list of random strings, as negative examples and a list of slightly messed up versions of your target word as positive examples. </p>

<p>Here is a <a href=""https://gist.github.com/karpathy/d4dee566867f8291f086"" rel=""nofollow"">minimal character-level RNN</a>, which consists of only a little more than a hundred lines of code, so you might be able to get your head around it or at least get it to run. Here is <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow"">the excellent blog post</a> by Karpathy to which the code sample belongs. </p>
",,1,2016-09-21T07:43:06.087,,2004,2016-09-21T07:43:06.087,,,,,2227.0,2000.0,2,0,,,,63.32,9.64,9.98,0.0,0.0,16.0,An optimal solution for the task as stated would be some alignment algorithm like SmithWaterman with a matrix which encodes typical typo frequencies As an exercise in NNs I would recommend using a RNN This circumvents the problem that your inputs will be of variable size because you just feed one letter after another and get an output once you feed the delimiter As trainingsdata youll need a list of random words and possibly a list of random strings as negative examples and a list of slightly messed up versions of your target word as positive examples Here is a minimal characterlevel RNN which consists of only a little more than a hundred lines of code so you might be able to get your head around it or at least get it to run Here is the excellent blog post by Karpathy to which the code sample belongs
2006.0,1.0,"<p>In 2004 <a href=""https://en.wikipedia.org/wiki/Jeff_Hawkins"">Jeff Hawkins</a>, inventor of the palm pilot, published a very interesting book called <a href=""https://en.wikipedia.org/wiki/On_Intelligence"">On Intelligence</a>, in which he details a theory how the human neocortex works. </p>

<p>This theory is called <a href=""https://en.wikipedia.org/wiki/Memory-prediction_framework"">Memory-Prediction framework</a> and it has some striking features, for example not only bottom-up (feedforward), but also top-down information processing and the ability to make simultaneous, but discrete predictions of different future scenarios (as described <a href=""http://journal.frontiersin.org/article/10.3389/fncir.2016.00023/full"">in this paper</a>).</p>

<p>The promise of the Memory-Prediction framework is unsupervised generation of stable high level representations of future possibilities. Something which would revolutionise probably a whole bunch of AI research areas.</p>

<p>Hawkins founded <a href=""https://en.wikipedia.org/wiki/Numenta"">a company</a> and proceeded to implement his ideas. Unfortunately more than ten years later the promise of his ideas is still unfulfilled. So far the implementation is only used for anomaly detection, which is kind of the opposite of what you really want to do. Instead of extracting the understanding, you'll extract the instances which the your artificial cortex doesn't understand. </p>

<p>My question is in what way Hawkins's framework falls short. What are the concrete or conceptual problems that so far prevent his theory from working in practice? </p>
",,0,2016-09-21T13:05:56.327,1.0,2005,2016-09-21T20:36:17.423,,,,,2227.0,,1,11,<unsupervised-learning>,What are the flaws in Jeff Hawkins's AI framework?,200.0,43.93,15.14,11.12,0.0,0.0,29.0,In 2004 Jeff Hawkins inventor of the palm pilot published a very interesting book called On Intelligence in which he details a theory how the human neocortex works This theory is called MemoryPrediction framework and it has some striking features for example not only bottomup feedforward but also topdown information processing and the ability to make simultaneous but discrete predictions of different future scenarios as described in this paper The promise of the MemoryPrediction framework is unsupervised generation of stable high level representations of future possibilities Something which would revolutionise probably a whole bunch of AI research areas Hawkins founded a company and proceeded to implement his ideas Unfortunately more than ten years later the promise of his ideas is still unfulfilled So far the implementation is only used for anomaly detection which is kind of the opposite of what you really want to do Instead of extracting the understanding youll extract the instances which the your artificial cortex doesnt understand My question is in what way Hawkinss framework falls short What are the concrete or conceptual problems that so far prevent his theory from working in practice
,,"<p>The short answer is that Hawkins' vision has yet to be implemented in a widely accessible way, particularly the indispensable parts related to prediction. </p>

<p>The long answer is that I read Hawkins' book a few years ago and was excited by the possibilities of Hierarchical Temporal Memory (HTM). I still am, despite the fact that I have a few reservations about some of his philosophical musings on the meanings of consciousness, free will and other such topics. I won't elaborate on those misgivings here because they're not germane to the main, overwhelming reason why HTM nets haven't succeeded as much as expected to date: to my knowledge, Numenta has only implemented a truncated version of his vision. They left out most of the prediction architecture, which plays such a critical role in Hawkins' theories. As Gerod M. Bonhoff put it in <a href=""http://www.dtic.mil/dtic/tr/fulltext/u2/a482820.pdf"">an excellent thesis</a><a href=""http://www.dtic.mil/dtic/tr/fulltext/u2/a482820.pdf"">1</a> on HTMs, </p>

<blockquote>
  <p>""In March of 2007, Numenta released what they claimed was a “research
  implementation” of HTM theory called Numenta Platform for Intelligent
  Computing (NuPIC). The algorithm used by NuPIC at this time is called
  “Zeta1.” NuPIC was released as an open source software platform and
  binary files of the Zeta1 algorithm. Because of licensing, this paper
  is not allowed to discuss the proprietary implementation aspects of
  Numenta’s Zeta1 algorithm. There are, however, generalized
  concepts of implementation that can be discussed freely. The two most
  important of these are how the Zeta 1 algorithm (encapsulated in each
  memory node of the network hierarchy) implements HTM theory. To
  implement any theory in software, an algorithmic design for each
  aspect of the theory must be addressed. The most important design
  decision Numenta adopted was to eliminate feedback within the
  hierarchy and instead choose to simulate this theoretical concept
  using only data pooling algorithms for weighting. This decision is
  immediately suspect and violates key concepts of HTM. Feedback,
  Hawkins’ insists, is vital to cortical function and central to his
  theories. Still, Numenta claims that most HTM applicable problems can
  be solved using their implementation and proprietary pooling
  algorithms.""</p>
</blockquote>

<p>I am still learning the ropes in this field and cannot say whether or not Numenta has since scrapped this approach in favor of a full implementation of Hawkins' ideas, especially the all-important prediction architecture. Even if they have, this design decision has probably delayed adoption by many years. That's not a criticism per se; perhaps the computational costs of tracking prediction values and updating them on the fly were too much to bear at the time, on top of the ordinary costs of processing neural nets, leaving them with no other path except to try half-measures like their proprietary pooling mechanisms. Nevertheless, all of the best research papers I've read on the topic since then have chosen to reimplement the algorithms rather than relying on Numenta's platform, typically because of the missing prediction features. Cases in point include Bonhoff's thesis and <a href=""http://cogprints.org/9187/1/HTM_TR_v1.0.pdf"">Maltoni's technical report for the University of Bologna Biometric System Laboratory</a><a href=""http://cogprints.org/9187/1/HTM_TR_v1.0.pdf"">2</a>. In all of those cases, however, there is no readily accessible software for putting their variant HTMs to immediate use (as far as I know). The gist of all this is that like G.K. Chesterton's famous maxim about Christianity, ""HTMs have not been tried and found wanting; they have been found difficult, and left untried."" Since Numenta left out the prediction steps, I assume that they would be the main stumbling blocks awaiting anyone who wants to code Hawkins' full vision of what an HTM should be.</p>

<p><a href=""http://www.dtic.mil/dtic/tr/fulltext/u2/a482820.pdf"">1</a>Bonhoff, Gerod M., 2008, Using Hierarchical Temporal Memory for Detecting Anomalous Network Activity. Presented in March, 2008 at the Air Force Institute of Technology, Wright-Patterson Air Force Base, Ohio.  </p>

<p><a href=""http://cogprints.org/9187/1/HTM_TR_v1.0.pdf"">2</a>Maltoni, Davide, 2011, Pattern Recognition by Hierarchical Temporal Memory. DEIS Technical Report published April 13, 2011. University of Bologna Biometric System Laboratory: Bologna, Italy.   </p>
",,1,2016-09-21T20:36:17.423,,2006,2016-09-21T20:36:17.423,,,,,1427.0,2005.0,2,8,,,,51.38,13.87,10.14,0.0,0.0,103.0,The short answer is that Hawkins vision has yet to be implemented in a widely accessible way particularly the indispensable parts related to prediction The long answer is that I read Hawkins book a few years ago and was excited by the possibilities of Hierarchical Temporal Memory HTM I still am despite the fact that I have a few reservations about some of his philosophical musings on the meanings of consciousness free will and other such topics I wont elaborate on those misgivings here because theyre not germane to the main overwhelming reason why HTM nets havent succeeded as much as expected to date to my knowledge Numenta has only implemented a truncated version of his vision They left out most of the prediction architecture which plays such a critical role in Hawkins theories As Gerod M Bonhoff put it in an excellent thesis1 on HTMs In March of 2007 Numenta released what they claimed was a “research implementation” of HTM theory called Numenta Platform for Intelligent Computing NuPIC The algorithm used by NuPIC at this time is called “Zeta1” NuPIC was released as an open source software platform and binary files of the Zeta1 algorithm Because of licensing this paper is not allowed to discuss the proprietary implementation aspects of Numenta’s Zeta1 algorithm There are however generalized concepts of implementation that can be discussed freely The two most important of these are how the Zeta 1 algorithm encapsulated in each memory node of the network hierarchy implements HTM theory To implement any theory in software an algorithmic design for each aspect of the theory must be addressed The most important design decision Numenta adopted was to eliminate feedback within the hierarchy and instead choose to simulate this theoretical concept using only data pooling algorithms for weighting This decision is immediately suspect and violates key concepts of HTM Feedback Hawkins’ insists is vital to cortical function and central to his theories Still Numenta claims that most HTM applicable problems can be solved using their implementation and proprietary pooling algorithms I am still learning the ropes in this field and cannot say whether or not Numenta has since scrapped this approach in favor of a full implementation of Hawkins ideas especially the allimportant prediction architecture Even if they have this design decision has probably delayed adoption by many years Thats not a criticism per se perhaps the computational costs of tracking prediction values and updating them on the fly were too much to bear at the time on top of the ordinary costs of processing neural nets leaving them with no other path except to try halfmeasures like their proprietary pooling mechanisms Nevertheless all of the best research papers Ive read on the topic since then have chosen to reimplement the algorithms rather than relying on Numentas platform typically because of the missing prediction features Cases in point include Bonhoffs thesis and Maltonis technical report for the University of Bologna Biometric System Laboratory2 In all of those cases however there is no readily accessible software for putting their variant HTMs to immediate use as far as I know The gist of all this is that like GK Chestertons famous maxim about Christianity HTMs have not been tried and found wanting they have been found difficult and left untried Since Numenta left out the prediction steps I assume that they would be the main stumbling blocks awaiting anyone who wants to code Hawkins full vision of what an HTM should be 1Bonhoff Gerod M 2008 Using Hierarchical Temporal Memory for Detecting Anomalous Network Activity Presented in March 2008 at the Air Force Institute of Technology WrightPatterson Air Force Base Ohio 2Maltoni Davide 2011 Pattern Recognition by Hierarchical Temporal Memory DEIS Technical Report published April 13 2011 University of Bologna Biometric System Laboratory Bologna Italy
2009.0,2.0,"<p>As far as I can tell, neural networks have a <strong>fixed number of neurons</strong> in the input layer.</p>

<p>If neural networks are used in a context like for example NLP, sentences or blocks of text of varying sizes are fed to a network. How is the <strong>varying input size</strong> reconciled with the <strong>fixed size</strong> of the input layer of the network? In other words: how is such a network made flexible enough to deal with an input that might be anywhere from one word to multiple pages of text?</p>

<p>If my assumption of a fixed number of input neurons is wrong and new input neurons are added to/removed from the network to match the input size I don't see how these can ever be trained.</p>

<p>I give the example of NLP, but lots of problems have an inherently unpredictable input size, I'm interested in the general approach for dealing with this.</p>

<p>edit: For images, it's clear you can up/downsample to a fixed size, but for text this seems to be an impossible approach since adding/removing text changes the meaning of the original input.</p>
",,0,2016-09-23T10:33:58.060,,2008,2016-09-26T06:34:19.113,2016-09-23T10:43:12.090,,2522.0,,2522.0,,1,4,<neural-networks><machine-learning><deep-learning><conv-neural-network><implementation>,How can neural networks deal with varying input sizes?,275.0,61.9,8.71,8.64,0.0,0.0,21.0,As far as I can tell neural networks have a fixed number of neurons in the input layer If neural networks are used in a context like for example NLP sentences or blocks of text of varying sizes are fed to a network How is the varying input size reconciled with the fixed size of the input layer of the network In other words how is such a network made flexible enough to deal with an input that might be anywhere from one word to multiple pages of text If my assumption of a fixed number of input neurons is wrong and new input neurons are added toremoved from the network to match the input size I dont see how these can ever be trained I give the example of NLP but lots of problems have an inherently unpredictable input size Im interested in the general approach for dealing with this edit For images its clear you can updownsample to a fixed size but for text this seems to be an impossible approach since addingremoving text changes the meaning of the original input
,,"<p>Three possibilities come to mind:</p>

<p>The easiest is zero padding. Basically you take a rather big input size and just add zeroes if your concrete input is too small. Of course this is pretty limited and certainly not useful if your input ranges from a few words to full texts.</p>

<p><a href=""https://en.wikipedia.org/wiki/Recurrent_neural_network"" rel=""nofollow"">RNNs</a> are a very natural NN to chose if you have texts of varying size as input. You input words as word vectors just one after another and the internal state of the RNN is supposed to encode the meaning of the full string of words. <a href=""http://www.iro.umontreal.ca/~lisa/pointeurs/RNNSpokenLanguage2013.pdf"" rel=""nofollow"">This is one</a> of the earlier papers.</p>

<p>Another possibility is using <a href=""https://en.wikipedia.org/wiki/Recursive_neural_network"" rel=""nofollow"">recursive NNs</a>. This is basically a form of preprocessing in which a text is recursively reduced to a smaller number of word vectors until only one is left - your input, which is supposed to encode the whole text. This makes a lot of sense from a linguistic point of view if your input consists of sentences (which can vary a lot in size), because sentences are structured recursively (For example the word vector for ""the man"", should be similar to the word vector for ""the man who mistook his wife for a hat"", because noun phrases act like nouns etc.). Often you can use linguistic information to guide your recursion on the sentence. If you want to go way beyond the wiki article, <a href=""http://nlp.stanford.edu/~socherr/thesis.pdf"" rel=""nofollow"">this is probably a good start</a>.</p>
",,3,2016-09-23T11:27:44.700,,2009,2016-09-25T09:39:25.433,2016-09-25T09:39:25.433,,2227.0,,2227.0,2008.0,2,4,,,,66.57,8.47,8.72,0.0,0.0,27.0,Three possibilities come to mind The easiest is zero padding Basically you take a rather big input size and just add zeroes if your concrete input is too small Of course this is pretty limited and certainly not useful if your input ranges from a few words to full texts RNNs are a very natural NN to chose if you have texts of varying size as input You input words as word vectors just one after another and the internal state of the RNN is supposed to encode the meaning of the full string of words This is one of the earlier papers Another possibility is using recursive NNs This is basically a form of preprocessing in which a text is recursively reduced to a smaller number of word vectors until only one is left your input which is supposed to encode the whole text This makes a lot of sense from a linguistic point of view if your input consists of sentences which can vary a lot in size because sentences are structured recursively For example the word vector for the man should be similar to the word vector for the man who mistook his wife for a hat because noun phrases act like nouns etc Often you can use linguistic information to guide your recursion on the sentence If you want to go way beyond the wiki article this is probably a good start
,,"<p>I am currently reading <em>Superintelligence: Paths, Dangers, Strategies</em> by Nick Bostrom. When he discusses whole brain emulation, although computing power (storage, bandwidth, CPU, body simulation, &amp; environment simulation) is one of the three general key things we are lacking toward its success, he also seems to agree that computing power is the most feasible and attainable of the three general issues we have for attaining it as of now. However he also goes on to to say </p>

<blockquote>
  <p>Just how much technology is required for whole brain emulation
  depends on the level of abstraction at which the brain is simulated.<sup><a href=""https://books.google.co.uk/books?id=1mMJBAAAQBAJ&amp;pg=PT41&amp;lpg=PT41&amp;dq=%22Just%20how%20much%20technology%20is%20required%20for%20whole%20brain%20emulation%20depends%20on%20the%20level%20of%22&amp;source=bl&amp;ots=Pmw4PUDele&amp;sig=PfmDRgcJuKBf3TwyCoVtCmp9XmE&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwj4gZy6tbnPAhVRF8AKHUoQDD8Q6AEIHjAA#v=onepage&amp;q=%22Just%20how%20much%20technology%20is%20required%20for%20whole%20brain%20emulation%20depends%20on%20the%20level%20of%22&amp;f=false"" rel=""nofollow"">ref</a></sup></p>
</blockquote>

<p>Which is an interesting thought, but a whole different discussion.</p>

<p>Anyways, so I think you are correct in thinking that we aren't far from having the computing power and maybe you are on to something, but rather are biggest hurdles are the other two key prerequisites that we need to attain before we can even begin trying, which are scanning, and translation. </p>

<p>Of the three, it would seem translation is the one we need to advance in the most, as of now. A modest prediction of attaining whole brain emulation is at least 15 years or mid century. Theres much more information in this book of all of the different paths that can be taken to achieve super intelligence, and it is well researched, I highly recommend it if you haven't read it already.</p>
",,0,2016-09-25T05:33:54.720,,2011,2016-10-01T10:42:13.713,2016-10-01T10:42:13.713,,8.0,,1997.0,1885.0,2,0,,,,50.4,11.38,9.89,0.0,0.0,32.0,I am currently reading Superintelligence Paths Dangers Strategies by Nick Bostrom When he discusses whole brain emulation although computing power storage bandwidth CPU body simulation amp environment simulation is one of the three general key things we are lacking toward its success he also seems to agree that computing power is the most feasible and attainable of the three general issues we have for attaining it as of now However he also goes on to to say Just how much technology is required for whole brain emulation depends on the level of abstraction at which the brain is simulatedref Which is an interesting thought but a whole different discussion Anyways so I think you are correct in thinking that we arent far from having the computing power and maybe you are on to something but rather are biggest hurdles are the other two key prerequisites that we need to attain before we can even begin trying which are scanning and translation Of the three it would seem translation is the one we need to advance in the most as of now A modest prediction of attaining whole brain emulation is at least 15 years or mid century Theres much more information in this book of all of the different paths that can be taken to achieve super intelligence and it is well researched I highly recommend it if you havent read it already
,2.0,"<p>In my estimation we have two minds which manage to speak to each other in dialectic through a series of interrupts. Thus at any one time one of these systems is controlling master and inhabits our consciousness. The subordinate system controls context which is constantly being ""primed"" by our senses and our subordinate systems experience of our conscious thought process( see thinking fast and slow by Daniel Kahneman). Thus our thought process is constantly a driven one. Similarly this system works as a node in a community and not as a standalone thing.<br>
 I think what we have currently is ""artificial thinking"" which is abstracted a long way from what is described above. so my question is ""are there any artificial intelligence systems with an internal dialectical approach and with drivers and conceived above and which develop within a community of nodes? "" </p>
",,0,2016-09-25T10:29:34.560,,2012,2016-09-26T15:39:11.980,2016-09-26T15:37:21.923,,33.0,,2601.0,,1,-1,<philosophy>,Are there any artificial intelligence systems with an internal dialectical approach and multiple minds which develop within a community of nodes?,104.0,50.87,12.18,9.98,0.0,0.0,15.0,In my estimation we have two minds which manage to speak to each other in dialectic through a series of interrupts Thus at any one time one of these systems is controlling master and inhabits our consciousness The subordinate system controls context which is constantly being primed by our senses and our subordinate systems experience of our conscious thought process see thinking fast and slow by Daniel Kahneman Thus our thought process is constantly a driven one Similarly this system works as a node in a community and not as a standalone thing I think what we have currently is artificial thinking which is abstracted a long way from what is described above so my question is are there any artificial intelligence systems with an internal dialectical approach and with drivers and conceived above and which develop within a community of nodes
,,"<p>There are a lot of systems which follow the ancient maxim: ""Always two there are; no more, no less. A master and an apprentice.""</p>

<p>In <a href=""https://en.wikipedia.org/wiki/Reinforcement_learning"" rel=""nofollow"">reinforcement learning</a> a class of such setups is called <a href=""https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node66.html"" rel=""nofollow"">Actor-Critic-Method</a>. There you have a master, who's duty it is to create feedback for the actions of the apprentice, who acts in a given environment. This would be comparable to how a human learns some physical activity, like playing table tennis. You basically let your body do it's thing, but your consciousness evaluates how good the result is. </p>

<p>The setup of <a href=""https://en.wikipedia.org/wiki/AlphaGo"" rel=""nofollow"">AlphaGo</a> might be even closer to <a href=""https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow"" rel=""nofollow"">Kahnemann's system 1 and system 2</a>. AlphaGo has two neural networks which provide actions and evaluations (system 1, fast, intuitiv, etc.) and the monte carlo tree search, which uses these actions and evaluations to prune a search tree and make a decision (system 2, deliberate, logical). </p>

<p>In the end this kind of structure will pop up again and again, because it is often necessary to do some kind of classification or preprocessing on the raw data, before <em>your</em> algorithm can be run on it. You could frame the whole history of <a href=""https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence"" rel=""nofollow"">gofai</a> as the story of how scientists thought system 1 should be easy and system 2 should be doable in a few decades, where the reality is that we have no idea how difficult system 2 is, because it turned out that system 1 is extremely difficult. </p>
",,0,2016-09-25T17:21:36.137,,2013,2016-09-25T17:21:36.137,,,,,2227.0,2012.0,2,2,,,,57.91,9.75,9.47,0.0,0.0,39.0,There are a lot of systems which follow the ancient maxim Always two there are no more no less A master and an apprentice In reinforcement learning a class of such setups is called ActorCriticMethod There you have a master whos duty it is to create feedback for the actions of the apprentice who acts in a given environment This would be comparable to how a human learns some physical activity like playing table tennis You basically let your body do its thing but your consciousness evaluates how good the result is The setup of AlphaGo might be even closer to Kahnemanns system 1 and system 2 AlphaGo has two neural networks which provide actions and evaluations system 1 fast intuitiv etc and the monte carlo tree search which uses these actions and evaluations to prune a search tree and make a decision system 2 deliberate logical In the end this kind of structure will pop up again and again because it is often necessary to do some kind of classification or preprocessing on the raw data before your algorithm can be run on it You could frame the whole history of gofai as the story of how scientists thought system 1 should be easy and system 2 should be doable in a few decades where the reality is that we have no idea how difficult system 2 is because it turned out that system 1 is extremely difficult
,,"<p>Others already mentioned:</p>

<ul>
<li>zero padding</li>
<li>RNN</li>
<li>recursive NN</li>
</ul>

<p>so I will add another possibility: using convolutions different number of times depending on the size of input. Here is an <a href=""http://www.deeplearningbook.org/contents/convnets.html"" rel=""nofollow"">excellent book</a> which backs up this approach:</p>

<blockquote>
  <p>Consider a collection of images, where each image has a different
  width and height. It is unclear how to model such inputs with a weight
  matrix of fixed size. Convolution is straightforward to apply; the
  kernel is simply applied a different number of times depending on the
  size of the input, and the output of the convolution operation scales
  accordingly.</p>
</blockquote>

<p>Taken from page 360. You can read it further to see some other approaches.</p>
",,1,2016-09-26T06:34:19.113,,2014,2016-09-26T06:34:19.113,,,,,2492.0,2008.0,2,1,,,,52.9,11.49,10.57,0.0,0.0,12.0,Others already mentioned zero padding RNN recursive NN so I will add another possibility using convolutions different number of times depending on the size of input Here is an excellent book which backs up this approach Consider a collection of images where each image has a different width and height It is unclear how to model such inputs with a weight matrix of fixed size Convolution is straightforward to apply the kernel is simply applied a different number of times depending on the size of the input and the output of the convolution operation scales accordingly Taken from page 360 You can read it further to see some other approaches
,,,,0,2016-09-26T11:16:11.610,,2016,2016-09-26T11:16:11.610,2016-09-26T11:16:11.610,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,Gradient Descent is an algorithm for finding the minimum of a function.  It iteratively calculates partial derivatives (gradients) of the function and descends in steps proportional to those partial derivatives.  One major application of Gradient Descent is fitting a parameterized model to a set of data: the function to be minimized is an error function for the model.,,0,2016-09-26T11:16:11.610,,2017,2016-09-26T19:26:36.850,2016-09-26T19:26:36.850,,1791.0,,1791.0,,4,0,,,,34.97,13.81,11.13,0.0,0.0,6.0,Gradient Descent is an algorithm for finding the minimum of a function It iteratively calculates partial derivatives gradients of the function and descends in steps proportional to those partial derivatives One major application of Gradient Descent is fitting a parameterized model to a set of data the function to be minimized is an error function for the model
,,"<p>You could argue that some <a href=""https://en.wikipedia.org/wiki/Multi-agent_system"" rel=""nofollow"">Multi-Agent System</a> approaches do, and some systems based on the <a href=""https://en.wikipedia.org/wiki/Blackboard_system"" rel=""nofollow"">blackboard architecture</a> could conceivably fit this regime as well. </p>
",,0,2016-09-26T15:39:11.980,,2018,2016-09-26T15:39:11.980,,,,,33.0,2012.0,2,1,,,,38.66,15.15,10.75,0.0,0.0,3.0,You could argue that some MultiAgent System approaches do and some systems based on the blackboard architecture could conceivably fit this regime as well
2022.0,4.0,"<p>In the recent PC game <em><a href=""http://www.theturingtestgame.com/"">The Turing Test</a></em>, the AI (""TOM"") needs help from Ava to get through some puzzle rooms. TOM says he is unable to solve the puzzles because he is not allowed to ""<a href=""https://en.wikipedia.org/wiki/Lateral_thinking"">think laterally</a>."" Specifically, he says he would not have thought to throw a box through a window to solve the first room. His creators, the story goes, turned that capability off because such thinking could produce ""ethically suboptimal"" solutions, like chopping off an arm to leave on a pressure plate.</p>

<p>Would all creative puzzle-solving abilities need to be removed from an AI to keep its results reasonable, or could we get some benefits of lateral thinking without losing an arm?</p>
",,0,2016-09-27T16:55:24.733,3.0,2020,2016-09-30T18:40:58.730,,,,,75.0,,1,8,<agi><problem-solving>,"Could an AI think laterally while avoiding ""ethically suboptimal"" choices?",604.0,56.39,10.45,9.14,0.0,0.0,20.0,In the recent PC game The Turing Test the AI TOM needs help from Ava to get through some puzzle rooms TOM says he is unable to solve the puzzles because he is not allowed to think laterally Specifically he says he would not have thought to throw a box through a window to solve the first room His creators the story goes turned that capability off because such thinking could produce ethically suboptimal solutions like chopping off an arm to leave on a pressure plate Would all creative puzzlesolving abilities need to be removed from an AI to keep its results reasonable or could we get some benefits of lateral thinking without losing an arm
,2.0,"<p>In the recent <a href=""https://live.newscientist.com/"" rel=""nofollow"">festival of science</a>, there was a talk given by researcher <a href=""https://live.newscientist.com/mike-cook/"" rel=""nofollow"">Mike Cook</a> about:</p>

<blockquote>
  <p><a href=""http://www.gamesbyangelina.org/"" rel=""nofollow"">ANGELINA</a>, an AI game designer that has invented game mechanics, made games about news stories, and was the first AI to enter a game jam.</p>
</blockquote>

<p>So the aim of Angelina AI is basically to design videogames.</p>

<p>Briefly, how exactly does Angelina design the new games? How does it work behind the scenes?</p>
",,4,2016-09-27T17:30:55.837,1.0,2021,2016-10-14T06:03:32.473,2016-10-06T11:06:09.057,,8.0,,8.0,,1,1,<algorithm><gaming>,How exactly does Angelina design games?,199.0,62.68,9.05,9.59,0.0,0.0,10.0,In the recent festival of science there was a talk given by researcher Mike Cook about ANGELINA an AI game designer that has invented game mechanics made games about news stories and was the first AI to enter a game jam So the aim of Angelina AI is basically to design videogames Briefly how exactly does Angelina design the new games How does it work behind the scenes
,,"<p><strong>No</strong>, with a <em>but</em>. We can have creative yet ethical problem-solving if the system has a complete system of ethics, but otherwise creativity will be unsafe by default.</p>

<p>One can classify AI decision-making approaches into two types: interpolative thinkers, and extrapolative thinkers. </p>

<p>Interpolative thinkers learn to classify and mimic whatever they're learning from, and don't try to give reasonable results outside of their training domain. You can think of them as interpolating between training examples, and benefitting from all of the mathematical guarantees and provisos as other statistical techniques.</p>

<p>Extrapolative thinkers learn to manipulate underlying principles, which allows them to combine those principles in previously unconsidered ways. The relevant field for intuition here is <a href=""https://en.wikipedia.org/wiki/Mathematical_optimization"">numerical optimization</a>, of which the simplest and most famous example is <a href=""https://en.wikipedia.org/wiki/Linear_programming"">linear programming</a>, rather than the statistical fields that birthed machine learning. You can think of them as extrapolating beyond training examples (indeed, many of them don't even require training examples, or use those examples to infer underlying principles).</p>

<p>The promise of extrapolative thinkers is that they can come up with these 'lateral' solutions much more quickly than people would be able to. The problem with these extrapolative thinkers is that they only use the spoken principles, not any unspoken ones that might seem too obvious to mention.</p>

<p>An attribute of solutions to optimization problems is that the feature vector is often 'extreme' in some way. In linear programming, at least one vertex of the feasible solution space will be optimal, and so simple solution methods find an optimal vertex (which is almost infeasible by nature of being a vertex).</p>

<p>As another example, the minimum-fuel solution for moving a spacecraft from one position to another is called '<a href=""https://en.wikipedia.org/wiki/Bang%E2%80%93bang_control"">bang-bang</a>,' where you accelerate the craft as quickly as possible at the beginning and end of the trajectory, coasting at maximum speed in between.</p>

<p>While a virtue when the system is correctly understood (bang-bang <em>is</em> optimal for many cases), this is catastrophic when the system is incorrectly understood. My favorite example here is <a href=""https://resources.mpi-inf.mpg.de/departments/d1/teaching/ws14/Ideen-der-Informatik/Dantzig-Diet.pdf"">Dantzig's diet problem</a> (discussion starts on page 5 of the pdf), where he tries to optimize his diet using math. Under his first constraint set, he's supposed to drink 500 gallons of vinegar a day. Under his second, 200 bouillon cubes. Under his third, two pounds of bran. The considerations that make those obviously bad ideas aren't baked into the system, and so the system innocently suggests them.</p>

<p>If you can completely encode the knowledge and values that a person uses to judge these plans into the AI, then extrapolative systems are as safe as that person. They'll be able to consider and reject the wrong sort of extreme plans, and leave you with the right sort of extreme plans.</p>

<p>But if you can't, then it does make sense to not build an extrapolative decision-maker, and instead build an interpolative one. That is, instead of asking itself ""how do I best accomplish goal X?"" it's asking itself ""what would a person do in this situation?"". The latter might be much worse at accomplishing goal X, but it has much less of the tail risk of sacrificing other goals to accomplish X.</p>
",,0,2016-09-27T17:50:12.263,,2022,2016-09-27T19:32:53.843,2016-09-27T19:32:53.843,,10.0,,10.0,2020.0,2,11,,,,50.16,13.0,9.64,0.0,0.0,88.0,No with a but We can have creative yet ethical problemsolving if the system has a complete system of ethics but otherwise creativity will be unsafe by default One can classify AI decisionmaking approaches into two types interpolative thinkers and extrapolative thinkers Interpolative thinkers learn to classify and mimic whatever theyre learning from and dont try to give reasonable results outside of their training domain You can think of them as interpolating between training examples and benefitting from all of the mathematical guarantees and provisos as other statistical techniques Extrapolative thinkers learn to manipulate underlying principles which allows them to combine those principles in previously unconsidered ways The relevant field for intuition here is numerical optimization of which the simplest and most famous example is linear programming rather than the statistical fields that birthed machine learning You can think of them as extrapolating beyond training examples indeed many of them dont even require training examples or use those examples to infer underlying principles The promise of extrapolative thinkers is that they can come up with these lateral solutions much more quickly than people would be able to The problem with these extrapolative thinkers is that they only use the spoken principles not any unspoken ones that might seem too obvious to mention An attribute of solutions to optimization problems is that the feature vector is often extreme in some way In linear programming at least one vertex of the feasible solution space will be optimal and so simple solution methods find an optimal vertex which is almost infeasible by nature of being a vertex As another example the minimumfuel solution for moving a spacecraft from one position to another is called bangbang where you accelerate the craft as quickly as possible at the beginning and end of the trajectory coasting at maximum speed in between While a virtue when the system is correctly understood bangbang is optimal for many cases this is catastrophic when the system is incorrectly understood My favorite example here is Dantzigs diet problem discussion starts on page 5 of the pdf where he tries to optimize his diet using math Under his first constraint set hes supposed to drink 500 gallons of vinegar a day Under his second 200 bouillon cubes Under his third two pounds of bran The considerations that make those obviously bad ideas arent baked into the system and so the system innocently suggests them If you can completely encode the knowledge and values that a person uses to judge these plans into the AI then extrapolative systems are as safe as that person Theyll be able to consider and reject the wrong sort of extreme plans and leave you with the right sort of extreme plans But if you cant then it does make sense to not build an extrapolative decisionmaker and instead build an interpolative one That is instead of asking itself how do I best accomplish goal X its asking itself what would a person do in this situation The latter might be much worse at accomplishing goal X but it has much less of the tail risk of sacrificing other goals to accomplish X
2027.0,1.0,"<p>I understand that neural networks model biological neurons.  Each node in the network represents a neuron cell and the connections between nodes represent the connections between cells.  As in nature, a neuron fires an electrical signal to connected neurons based on some kind of threshold or function that mimics such.  </p>

<p>Recent discoveries on how the brain works reveal the importance of calcium within the cells.  See <a href=""http://link.springer.com/article/10.1007/BF01794675"" rel=""noreferrer"">http://link.springer.com/article/10.1007/BF01794675</a> for more information.  To summarize, calcium affects the regulation, stimulation and transmission of electrical activity as well as the destruction of neurones.</p>

<p>From my study of neural networks, there does not seem to be a calcium equivalent.  Having one would imply that the functions, connections and weights in an artificial network are configured during the training and execution process and can change over time.   I understand that back-propagation is used to train the weights, but have not seen anything that trains the function nor the connections (although a zero weight could imply no connection).</p>

<p>Does anyone know of such a network (or training algorithm)?  If so, do these networks perform better than a network that is pre-configured?</p>
",,0,2016-09-27T18:11:43.820,1.0,2023,2016-09-28T08:51:44.490,,,,,1434.0,,1,5,<philosophy><unsupervised-learning><neurons>,What is the calcium equivalent role in neural networks,116.0,54.52,14.85,9.87,0.0,0.0,33.0,I understand that neural networks model biological neurons Each node in the network represents a neuron cell and the connections between nodes represent the connections between cells As in nature a neuron fires an electrical signal to connected neurons based on some kind of threshold or function that mimics such Recent discoveries on how the brain works reveal the importance of calcium within the cells See httplinkspringercomarticle101007BF01794675 for more information To summarize calcium affects the regulation stimulation and transmission of electrical activity as well as the destruction of neurones From my study of neural networks there does not seem to be a calcium equivalent Having one would imply that the functions connections and weights in an artificial network are configured during the training and execution process and can change over time I understand that backpropagation is used to train the weights but have not seen anything that trains the function nor the connections although a zero weight could imply no connection Does anyone know of such a network or training algorithm If so do these networks perform better than a network that is preconfigured
,,"<p>You may consider the programming as an ethical part of the design as well. AI will act based on what has been instructed to it as ethically important or not.
It may/should even be part of the parameters that forge the process of finding solutions, which could allow for a more refine and creative solution.</p>

<p>We understand the basics of ethic in normal circumstances, but if we can't predict how any human will behave in an ethical conundrum we can enforce what an AI wouldn't do.</p>

<p>As long as we have control over the mechanism that drive an AI we sure have a responsability to inject ethical failsafes.
The problem lies in self taught AI with an ability to overrides directives.
(CF Asimov Laws.)</p>

<p>The way the AI is creative seems irrelevant in that case.</p>
",,0,2016-09-27T22:27:38.610,,2024,2016-09-27T22:27:38.610,,,,,2658.0,2020.0,2,0,,,,62.78,8.99,9.27,0.0,0.0,15.0,You may consider the programming as an ethical part of the design as well AI will act based on what has been instructed to it as ethically important or not It mayshould even be part of the parameters that forge the process of finding solutions which could allow for a more refine and creative solution We understand the basics of ethic in normal circumstances but if we cant predict how any human will behave in an ethical conundrum we can enforce what an AI wouldnt do As long as we have control over the mechanism that drive an AI we sure have a responsability to inject ethical failsafes The problem lies in self taught AI with an ability to overrides directives CF Asimov Laws The way the AI is creative seems irrelevant in that case
,,"<p>A lot of this depends on the breadth of consideration. For example, what would the medium and long term effects of the lateral thinking be? The robot could sever an arm for a pressure plate but it would mean that the person no longer had an arm, a functional limitation at best, that the person might bleed out and die/be severely constrained, and that the person (and people in general) would both no longer cooperate and likely seek to eliminate the robot. People can think laterally because consider these things - ethics are really nothing more than a set of guidelines that encompass these considerations. The robot could as well, were it to be designed to consider these externalities.</p>

<p>If all else fails,</p>

<p>Asimov's Laws of Robotics: (0. A robot may not harm humanity, or, by inaction, allow humanity to come to harm.) 1. A robot may not injure a human being or, through inaction, allow a human being to come to harm. 2. A robot must obey orders given it by human beings except where such orders would conflict with the First Law. 3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law</p>
",,0,2016-09-28T00:32:47.923,,2025,2016-09-28T00:32:47.923,,,,,2661.0,2020.0,2,-1,,,,67.59,9.46,8.58,0.0,0.0,31.0,A lot of this depends on the breadth of consideration For example what would the medium and long term effects of the lateral thinking be The robot could sever an arm for a pressure plate but it would mean that the person no longer had an arm a functional limitation at best that the person might bleed out and diebe severely constrained and that the person and people in general would both no longer cooperate and likely seek to eliminate the robot People can think laterally because consider these things ethics are really nothing more than a set of guidelines that encompass these considerations The robot could as well were it to be designed to consider these externalities If all else fails Asimovs Laws of Robotics 0 A robot may not harm humanity or by inaction allow humanity to come to harm 1 A robot may not injure a human being or through inaction allow a human being to come to harm 2 A robot must obey orders given it by human beings except where such orders would conflict with the First Law 3 A robot must protect its own existence as long as such protection does not conflict with the First or Second Law
,,"<p>Neural networks <strong>don't</strong> model biological neurons. </p>

<p>They are at best <em>inspired</em> by biological neurons, in that they get excited by certain inputs and fire once the excitation crosses a threshold. And this second point even holds only approximately because the backpropagation algorithm needs smoothed out steps to learn by gradient descent. And backpropagation is not even inspired by biology, that would rather be <a href=""https://en.wikibooks.org/wiki/Artificial_Neural_Networks/Hebbian_Learning"">hebbian learning</a>. </p>

<p>Generally in machine learning people do what works. The connection to biology is tenuous at best. You will usually not find one to one correspondence of low level details between machine learning setups and biological neurons. For that you'll have to turn to <a href=""https://www.humanbrainproject.eu/sp6"">brain simulations</a>.</p>
",,0,2016-09-28T08:51:44.490,,2027,2016-09-28T08:51:44.490,,,,,2227.0,2023.0,2,6,,,,57.47,13.51,10.21,0.0,0.0,12.0,Neural networks dont model biological neurons They are at best inspired by biological neurons in that they get excited by certain inputs and fire once the excitation crosses a threshold And this second point even holds only approximately because the backpropagation algorithm needs smoothed out steps to learn by gradient descent And backpropagation is not even inspired by biology that would rather be hebbian learning Generally in machine learning people do what works The connection to biology is tenuous at best You will usually not find one to one correspondence of low level details between machine learning setups and biological neurons For that youll have to turn to brain simulations
,0.0,"<p>I am a student working on my final graduation project. I was assigned to study Hyper-heuristics and it is a new subject for me. I was asked to choose a computational problem to apply Hyper-heuristics on them and see the results. However, I am afraid to choose the wrong problem. What are, in your opinion, computational problem that can be resolved with hyper-heuristics efficiently.</p>

<p>Thank you. </p>
",2016-10-01T11:46:06.403,0,2016-09-28T11:12:31.817,1.0,2028,2016-10-01T11:43:41.940,2016-09-28T13:49:08.717,,1760.0,,1760.0,,1,0,<machine-learning><optimization><heuristics>,What computational problems can be efficiently resolved by Hyper-heuristics?,32.0,58.08,10.72,9.32,0.0,0.0,12.0,I am a student working on my final graduation project I was assigned to study Hyperheuristics and it is a new subject for me I was asked to choose a computational problem to apply Hyperheuristics on them and see the results However I am afraid to choose the wrong problem What are in your opinion computational problem that can be resolved with hyperheuristics efficiently Thank you
,,"<p>I would strongly recommend that you check the book ""The Perfect Bet"" by Adam Kucharski. It does not mention technical methods such as neural networks but it gives a good history (and very nice stories) on what people had done on that field. It gives you the notion that in order to get achieve a better payoff, your goal is not actually making a better prediction but choosing the better options by considering what other players are doing. If you ask why it is not just the better prediction, the answer is that there is always a balance of risk and payoff and since you cannot find a 100% guaranteed way of prediction, you will have to balance risk and payoff in order to gain in the long run. In addition, although theory suggests that you can make a good prediction by gathering all variables, this is not really possible in practice. Thus, human assistance and observing what others are doing is used as a way to improve mathematical predictions and possible payoffs.</p>
",,0,2016-09-28T17:00:20.347,,2031,2016-09-28T17:00:20.347,,,,,2680.0,1625.0,2,1,,,,59.16,10.11,9.54,0.0,0.0,17.0,I would strongly recommend that you check the book The Perfect Bet by Adam Kucharski It does not mention technical methods such as neural networks but it gives a good history and very nice stories on what people had done on that field It gives you the notion that in order to get achieve a better payoff your goal is not actually making a better prediction but choosing the better options by considering what other players are doing If you ask why it is not just the better prediction the answer is that there is always a balance of risk and payoff and since you cannot find a 100 guaranteed way of prediction you will have to balance risk and payoff in order to gain in the long run In addition although theory suggests that you can make a good prediction by gathering all variables this is not really possible in practice Thus human assistance and observing what others are doing is used as a way to improve mathematical predictions and possible payoffs
2039.0,4.0,"<p>I'm a freshman to machine learning. We all know that there are 2 kinds of problems in our life: problems that humans can solve and problems we can't solve. For problems humans can solve, we always try our best to write some algorithm and tell machine to follow it step by step, and finally the machine acts like people.</p>

<p>What I'm curious about are these problems humans can't solve. If humans ourselves can't sum up and get an algorithm (which means that we ourselves don't know how to solve the problem), can a machine solve the problem? That is, can the machine sum up and get an algorithm by itself based on a large amount of problem data?</p>
",,0,2016-09-29T03:05:31.943,,2033,2016-10-01T13:04:32.967,2016-09-29T12:31:24.400,,75.0,,2688.0,,1,4,<machine-learning><algorithm>,Must people tell an AI which algorithm it should use? Can an AI learn algorithms by itself?,230.0,68.4,8.47,7.29,0.0,0.0,19.0,Im a freshman to machine learning We all know that there are 2 kinds of problems in our life problems that humans can solve and problems we cant solve For problems humans can solve we always try our best to write some algorithm and tell machine to follow it step by step and finally the machine acts like people What Im curious about are these problems humans cant solve If humans ourselves cant sum up and get an algorithm which means that we ourselves dont know how to solve the problem can a machine solve the problem That is can the machine sum up and get an algorithm by itself based on a large amount of problem data
,,"<p>There are problems we for which we don't have a known, optimal, deterministic algorithm. By and large we use <a href=""https://en.wikipedia.org/wiki/Heuristic"" rel=""nofollow"">heuristics</a> to ""solve"" those problems.  A closely related idea is that of <a href=""https://en.wikipedia.org/wiki/Satisficing"" rel=""nofollow"">satisficing</a> where we seek out answers that are ""good enough"" for immediate purposes.</p>

<p>Likewise, machines can also use heuristics, whether they are programmed in explicitly or, presumably, learned.  Within the range of ways that a machine can use heuristics, there are <a href=""https://en.wikipedia.org/wiki/Metaheuristics"" rel=""nofollow"">meta heuristics</a> and <a href=""https://en.wikipedia.org/wiki/Hyper-heuristic"" rel=""nofollow"">hyper heuristics</a>.</p>

<p>Going beyond that, there are other ways that machines an learn ""algorithms"" or ""rules"" for solving problems.  One are that I'm particularly interested in is known as <a href=""https://en.wikipedia.org/wiki/Rule_induction"" rel=""nofollow"">rule induction</a>.  </p>

<p>This is all an area of open and active research BTW... so if you're interested in exploring any of these approaches, you'll probably find a lot of ground to cover.  </p>
",,1,2016-09-29T03:30:28.413,,2034,2016-09-29T03:30:28.413,,,,,33.0,2033.0,2,2,,,,56.15,11.71,9.61,0.0,0.0,32.0,There are problems we for which we dont have a known optimal deterministic algorithm By and large we use heuristics to solve those problems A closely related idea is that of satisficing where we seek out answers that are good enough for immediate purposes Likewise machines can also use heuristics whether they are programmed in explicitly or presumably learned Within the range of ways that a machine can use heuristics there are meta heuristics and hyper heuristics Going beyond that there are other ways that machines an learn algorithms or rules for solving problems One are that Im particularly interested in is known as rule induction This is all an area of open and active research BTW so if youre interested in exploring any of these approaches youll probably find a lot of ground to cover
,,"<p>New guy here, please go easy on me as this answer will come from personal experience, and will probably be a tad philosophical.</p>

<p>Every algorithm I've designed was built to systematically tackle and solve specific problems in specific situations, each with an end goal in mind. Think of algorithms as solutions to a problem. In my career as a programmer, this rule has always stuck with me (it came from my favorite Computer Sciences professor): ""If there is <strong>no solution</strong>, then there is <strong>no algorithm</strong>. If there is <strong>no algorithm</strong>, <strong>no machine can solve the problem</strong>.""</p>

<p>Can machines generate their own algorithms? Most likely. But not to the point that it will exceed us (and by exceed, I don't mean just speed). AIs can never solve problems using methods that humans will never be able to come up with, because we programmed AIs to solve problems <em>just like us humans do</em>.</p>
",,2,2016-09-29T03:35:38.537,,2035,2016-09-29T03:35:38.537,,,,,2413.0,2033.0,2,0,,,,60.75,9.98,9.07,0.0,0.0,26.0,New guy here please go easy on me as this answer will come from personal experience and will probably be a tad philosophical Every algorithm Ive designed was built to systematically tackle and solve specific problems in specific situations each with an end goal in mind Think of algorithms as solutions to a problem In my career as a programmer this rule has always stuck with me it came from my favorite Computer Sciences professor If there is no solution then there is no algorithm If there is no algorithm no machine can solve the problem Can machines generate their own algorithms Most likely But not to the point that it will exceed us and by exceed I dont mean just speed AIs can never solve problems using methods that humans will never be able to come up with because we programmed AIs to solve problems just like us humans do
2072.0,3.0,"<p>This is a question about a nomenclature - we already have the algorithm/solution, but we're not sure whether it qualifies as utilizing heuristics or not.</p>

<hr>

<p>feel free to skip the problem explanation:</p>

<blockquote>
  <p>A friend is writing a path-finding algorithm - an autopilot for an
  (off-road) vehicle in a computer game. This is a pretty classic
  problem - he finds a viable, not necessarily optimal but ""good enough""
  route using the A* algorithm, by taking the terrain layout and vehicle
  capabilities into account, and modifying a direct (straight) line path
  to account for these. The whole map is known a'priori and invariant,
  though the start and destination are arbitrary (user-chosen) and the
  path is not guaranteed to exist at all.</p>
  
  <p>This cookie-cutter approach comes with a twist: limited storage space.
  We can afford some more volatile memory on start, but we should free
  most of it once the route has been found. The travel may take days -
  of real time too, so the path must be saved to disk, and the space in
  the save file for custom data like this is severely limited. Too
  limited to save all the waypoints - even after culling trivial
  solution waypoints ('continue straight ahead'), and by a rather large
  margin, order of 20% the size of our data set.</p>
  
  <p>A solution we came up with is to calculate the route once on start,
  then 'forget' all the trivial and 90% of the non-trivial waypoints.
  This both serves as a proof that a solution exists, and provides a set
  of points reaching which, in sequence, guarantees the route will take
  us to the destination.</p>
  
  <p>Once the vehicle reaches a waypoint, the route to the next one is
  calculated again, from scratch. It's known to exist and be correct
  (because we did it once, and it was correct), it doesn't put too much
  strain on the CPU and the memory (it's only about 10% the total route
  length) and it doesn't need to go into permanent storage (restarting
  from any point along the path is just a subset of the solution
  connecting two saved waypoints).</p>
</blockquote>

<hr>

<p>Now for the actual question:</p>

<p>The pathfinding algorithm follows a sparse set of waypoints which by themselves are not nearly sufficient as a route, but allow for easy, efficient  calculation of the actual route, simultaneously guarantying its existence; they are a subset of the full solution. </p>

<p>Is this a heuristic approach?</p>

<p>(as I understand, normally, heuristics don't guarantee existence of a solution, and merely suggest more likely candidates. In this case, the 'hints' are taken straight out of an actual working solution, thus my doubts.)</p>
",,0,2016-09-29T08:58:44.087,,2036,2016-10-03T20:01:07.193,,,,,38.0,,1,0,<heuristics><path-planning>,"Is a subset of a problem solution, used to recreate complete solution considered a heuristic?",32.0,52.63,11.2,9.49,0.0,0.0,92.0,This is a question about a nomenclature we already have the algorithmsolution but were not sure whether it qualifies as utilizing heuristics or not feel free to skip the problem explanation A friend is writing a pathfinding algorithm an autopilot for an offroad vehicle in a computer game This is a pretty classic problem he finds a viable not necessarily optimal but good enough route using the A algorithm by taking the terrain layout and vehicle capabilities into account and modifying a direct straight line path to account for these The whole map is known apriori and invariant though the start and destination are arbitrary userchosen and the path is not guaranteed to exist at all This cookiecutter approach comes with a twist limited storage space We can afford some more volatile memory on start but we should free most of it once the route has been found The travel may take days of real time too so the path must be saved to disk and the space in the save file for custom data like this is severely limited Too limited to save all the waypoints even after culling trivial solution waypoints continue straight ahead and by a rather large margin order of 20 the size of our data set A solution we came up with is to calculate the route once on start then forget all the trivial and 90 of the nontrivial waypoints This both serves as a proof that a solution exists and provides a set of points reaching which in sequence guarantees the route will take us to the destination Once the vehicle reaches a waypoint the route to the next one is calculated again from scratch Its known to exist and be correct because we did it once and it was correct it doesnt put too much strain on the CPU and the memory its only about 10 the total route length and it doesnt need to go into permanent storage restarting from any point along the path is just a subset of the solution connecting two saved waypoints Now for the actual question The pathfinding algorithm follows a sparse set of waypoints which by themselves are not nearly sufficient as a route but allow for easy efficient calculation of the actual route simultaneously guarantying its existence they are a subset of the full solution Is this a heuristic approach as I understand normally heuristics dont guarantee existence of a solution and merely suggest more likely candidates In this case the hints are taken straight out of an actual working solution thus my doubts
2044.0,2.0,"<p>I understand how a neural network can be trained to recognise certain features in an image (faces, cars, ...), where the inputs are the image's pixels, and the output is a set of boolean values indicating which objects were recognised in the image and which weren't.</p>

<p>What I don't really get is, when using this approach to detect features and we detect a face for example, how we can go back to the original image and determine the location or boundaries of the detected face. How is this achieved? Can this be achieved based on the recognition algorithm, or is a separate algorithm used to locate the face? That seems unlikely since to find the face again, it needs to be recognised in the image, which was the reason of using a NN in the first place.</p>
",,1,2016-09-29T14:21:57.723,,2037,2017-01-17T01:00:36.013,,,,,2522.0,,1,0,<neural-networks><deep-learning><conv-neural-network><computer-vision>,"When using neural networks to detect features in an image, how can locate that specific feature in the original image?",127.0,56.89,9.29,9.64,0.0,0.0,22.0,I understand how a neural network can be trained to recognise certain features in an image faces cars where the inputs are the images pixels and the output is a set of boolean values indicating which objects were recognised in the image and which werent What I dont really get is when using this approach to detect features and we detect a face for example how we can go back to the original image and determine the location or boundaries of the detected face How is this achieved Can this be achieved based on the recognition algorithm or is a separate algorithm used to locate the face That seems unlikely since to find the face again it needs to be recognised in the image which was the reason of using a NN in the first place
,,"<p>Whether or not a label fits any particular instance depends on what you're using the label for. If something specific is riding on whether this approach is a 'heuristic' or not, that context is important.</p>

<p>But I wouldn't call this a heuristic, because I think of that as a shortcut for <em>solving</em> a problem, not either <em>storing</em> a solution or <em>reformulating</em> the problem (which is how I'd think of this). </p>
",,0,2016-09-29T15:08:10.713,,2038,2016-09-29T15:08:10.713,,,,,10.0,2036.0,2,1,,,,64.75,10.1,10.43,0.0,0.0,13.0,Whether or not a label fits any particular instance depends on what youre using the label for If something specific is riding on whether this approach is a heuristic or not that context is important But I wouldnt call this a heuristic because I think of that as a shortcut for solving a problem not either storing a solution or reformulating the problem which is how Id think of this
,,"<p>All intelligence, both human and machine, is mechanistic. Thoughts don't appear out of the blue; they're generated through specific processes.</p>

<p>This means that if a machine generates an algorithm to solve a problem, even if the object-level algorithm wasn't generated by humans, the meta-level algorithm by which it generated the object-level algorithm must have come from <em>somewhere</em>, and that somewhere is probably its original creators. (Even if they didn't program the meta-level algorithm, they probably programmed the meta-meta-level algorithm that programmed the meta-level algorithm, and so on.)</p>

<p>How you think about these distinctions depends on how you think about machine learning, but typically they're fairly small. For example, when we train a neural network to classify images, we aren't telling it what pixels to focus on or how to combine them, which is the object-level algorithm that it eventually generates. But we are telling it how to construct that object-level algorithm from training data, what I'm calling the 'meta-level' algorithm.</p>

<p>One of the open problems is how to build the meta-meta-level; that is, an algorithm that will be able to look at a dataset and determine which models to train, and then which model to finally use. This will, ideally, include enough understanding of those meta-level models to construct new ones as needed, but even if it doesn't will reflect a major step forward in the usability of ML.</p>
",,1,2016-09-29T15:34:14.277,,2039,2016-09-29T15:34:14.277,,,,,10.0,2033.0,2,1,,,,45.69,13.76,9.65,0.0,0.0,53.0,All intelligence both human and machine is mechanistic Thoughts dont appear out of the blue theyre generated through specific processes This means that if a machine generates an algorithm to solve a problem even if the objectlevel algorithm wasnt generated by humans the metalevel algorithm by which it generated the objectlevel algorithm must have come from somewhere and that somewhere is probably its original creators Even if they didnt program the metalevel algorithm they probably programmed the metametalevel algorithm that programmed the metalevel algorithm and so on How you think about these distinctions depends on how you think about machine learning but typically theyre fairly small For example when we train a neural network to classify images we arent telling it what pixels to focus on or how to combine them which is the objectlevel algorithm that it eventually generates But we are telling it how to construct that objectlevel algorithm from training data what Im calling the metalevel algorithm One of the open problems is how to build the metametalevel that is an algorithm that will be able to look at a dataset and determine which models to train and then which model to finally use This will ideally include enough understanding of those metalevel models to construct new ones as needed but even if it doesnt will reflect a major step forward in the usability of ML
2042.0,2.0,"<p>If said AI can assess scenarios and decide what AI is best suited and construct new AI for new tasks. In sufficient time would the AI not have developed a suite of AIs powerful/specialized for their tasks, but versatile as a whole, much like our own brain’s architecture? What’s the constraint ?</p>
",,1,2016-09-29T15:44:48.963,,2040,2016-09-30T17:24:55.247,,,,,2700.0,,1,0,<neural-networks><philosophy><agi>,Wouldn't an AI that specializes in making other AI be an AGI if they can cooperate?,90.0,71.14,10.15,8.19,0.0,0.0,6.0,If said AI can assess scenarios and decide what AI is best suited and construct new AI for new tasks In sufficient time would the AI not have developed a suite of AIs powerfulspecialized for their tasks but versatile as a whole much like our own brain’s architecture What’s the constraint
,,"<p>The approach you listed here is not really an approach, this is very very vague idea of how someone can achieve some task. You basically told we have an algorithm <code>f(image) = result</code> and there can be infinite amount of real approaches to solve this.</p>

<p>In majority of CNN approaches the image travels through a convolution/pooling layers which reduces the dimensions of each current layer. In the end you end up with a significantly smaller layer which goes through the softmax and gets probabilities of different classes. This type of networks does not tell you where something was found, it just tells you that something was found somewhere in your original image.</p>
",,0,2016-09-29T21:38:19.983,,2041,2016-09-29T21:38:19.983,,,,,2492.0,2037.0,2,2,,,,57.81,11.2,9.21,17.0,0.0,8.0,The approach you listed here is not really an approach this is very very vague idea of how someone can achieve some task You basically told we have an algorithm and there can be infinite amount of real approaches to solve this In majority of CNN approaches the image travels through a convolutionpooling layers which reduces the dimensions of each current layer In the end you end up with a significantly smaller layer which goes through the softmax and gets probabilities of different classes This type of networks does not tell you where something was found it just tells you that something was found somewhere in your original image
,,"<p>If the AI can indeed assess arbitrary scenarios and come up with solutions to handle them, then it would indeed be an AGI.</p>

<blockquote>
  <p>What’s the constraint ?</p>
</blockquote>

<p><em>It doesn't exist.</em>  Current programmers are very good at developing AI that can handle specific tasks (""narrow AIs""), but it is currently impossible to build an AI that can assess and handle ""general"" situations (unlike your proposed algorithm, which possess that capacity).</p>

<p>Theoretically, we can have a program that can build other programs (<a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow"">genetic algorithms</a> are arguably one such example), but handling arbitrary scenarios and problems requires a form of ""general intelligence"", which we don't know how to program. Therefore, we can't build this machine.</p>

<p>It's possible that we can built this machine, but we must first figure out  the hard problem of ""general intelligence"". We're nowhere near reaching that level.</p>

<p><em>If</em> we figure out how to program ""general intelligence"", then it should be fairly simple to use your approach (building an AGI to assess scenarios and then build ""narrow AIs"" that can handle Those scenarios). <em>Only then</em> we can understand the AGI's limitations and weaknesses, and be able to identify probable constraints to its power. For example, it's possible that such an AGI may be slow in handling arbitrary scenarios and developing the ""narrow AIs""...in which case, it may take an absurdly long period of time to develop ""a suite of AIs powerful/specialized for their tasks"".</p>

<p>But until we build the AGI itself, we won't be able to identify its faults or weaknesses. Going beyond that would be science-fiction speculation.</p>
",,0,2016-09-30T00:12:11.380,,2042,2016-09-30T00:12:11.380,,,,,181.0,2040.0,2,0,,,,52.8,12.76,9.22,0.0,0.0,63.0,If the AI can indeed assess arbitrary scenarios and come up with solutions to handle them then it would indeed be an AGI What’s the constraint It doesnt exist Current programmers are very good at developing AI that can handle specific tasks narrow AIs but it is currently impossible to build an AI that can assess and handle general situations unlike your proposed algorithm which possess that capacity Theoretically we can have a program that can build other programs genetic algorithms are arguably one such example but handling arbitrary scenarios and problems requires a form of general intelligence which we dont know how to program Therefore we cant build this machine Its possible that we can built this machine but we must first figure out the hard problem of general intelligence Were nowhere near reaching that level If we figure out how to program general intelligence then it should be fairly simple to use your approach building an AGI to assess scenarios and then build narrow AIs that can handle Those scenarios Only then we can understand the AGIs limitations and weaknesses and be able to identify probable constraints to its power For example its possible that such an AGI may be slow in handling arbitrary scenarios and developing the narrow AIsin which case it may take an absurdly long period of time to develop a suite of AIs powerfulspecialized for their tasks But until we build the AGI itself we wont be able to identify its faults or weaknesses Going beyond that would be sciencefiction speculation
,,"<p>This problem is called <a href=""https://en.wikipedia.org/wiki/Object_detection"" rel=""nofollow"">object detection</a>.</p>

<p>If you have a trainings set of images with boxed objects you can just train a neural network to directly predict the box. I.e. the output has the same dimension as the input and the NN learns to assign each pixel the probability of belonging to a certain object.</p>

<p>If you don't have such a convenient dataset you could just recursively narrow the location down by feeding parts of the image to the network until you find the smallest part that still fully activates a certain classification. </p>

<p><a href=""https://pdfs.semanticscholar.org/713f/73ce5c3013d9fb796c21b981dc6629af0bd5.pdf"" rel=""nofollow"">In this paper</a> they try a mixture of these two approaches.</p>
",,0,2016-09-30T12:42:03.763,,2044,2016-09-30T12:42:03.763,,,,,2227.0,2037.0,2,6,,,,67.28,9.75,10.13,0.0,0.0,8.0,This problem is called object detection If you have a trainings set of images with boxed objects you can just train a neural network to directly predict the box Ie the output has the same dimension as the input and the NN learns to assign each pixel the probability of belonging to a certain object If you dont have such a convenient dataset you could just recursively narrow the location down by feeding parts of the image to the network until you find the smallest part that still fully activates a certain classification In this paper they try a mixture of these two approaches
,,"<p>To build on Tariq Ali's answer... </p>

<p>There's no such thing as an AGI. The No Free Lunch (NFL) theorem states essentially that: <em>any two optimization algorithms are equivalent when their performance is averaged across all possible problems.</em> Specialization implies a <em>loss</em> of generality, not a <em>gain</em>.</p>

<p>With this AI generating AI, you're describing what I call an 'arbitrary machine generator' (AMG).</p>

<p>There are two types of AMGs: a species level and an individual level.</p>

<p>All species that evolve on earth are AMG - they can evolve to accommodate arbitrary niches, if the correct environmental constraints are present. This is proven by the fact that the species AMG processes on earth have produce humans, which are individual level AMGs. Individual level AMGs can produce arbitrary machines for arbitrary purposes on human time-scales.</p>

<p>The problem is that the simplest possible (and most general) AMG is a purely <em>random</em> machine generator. Any more specificity (and therefore complexity) to the AMG would constrain the domains it closes over. Which is fine, but optimizing a machine for a particular set of tasks means that you are <em>unoptimizing</em> the machine for some other particular set of tasks. Again, there's no free context.</p>

<p>Humans are AMGs, but we are only efficient at creating certain kinds of machines, using our imagination. Our imagination is built on a number of cognitive tools that, on the one hand, constrain what machines we can efficiently imagine, while, on the other hand, avail us to the open-ended set of all possible machines, via prior knowledge or brute force, random lookup.</p>

<p>In summary, when people say <em>""general intelligence""</em>, they really mean <em>""human-like intelligence""</em>. And, again, while human intelligence <em>is</em> an AMG, any given AMG that is optimized for generating machines of a particular type will be less optimized for generating machines of some other type(s). There's no free context. The most general search algorithm is a random walk - there is no way to <em>improve</em> the generality of the random walk, other than just speeding it up. And that's actually what humans do for many problems anyway - brute force, random searching, as fast as possible.</p>
",,0,2016-09-30T17:24:55.247,,2045,2016-09-30T17:24:55.247,,,,,1712.0,2040.0,2,0,,,,45.25,12.47,9.51,0.0,0.0,75.0,To build on Tariq Alis answer Theres no such thing as an AGI The No Free Lunch NFL theorem states essentially that any two optimization algorithms are equivalent when their performance is averaged across all possible problems Specialization implies a loss of generality not a gain With this AI generating AI youre describing what I call an arbitrary machine generator AMG There are two types of AMGs a species level and an individual level All species that evolve on earth are AMG they can evolve to accommodate arbitrary niches if the correct environmental constraints are present This is proven by the fact that the species AMG processes on earth have produce humans which are individual level AMGs Individual level AMGs can produce arbitrary machines for arbitrary purposes on human timescales The problem is that the simplest possible and most general AMG is a purely random machine generator Any more specificity and therefore complexity to the AMG would constrain the domains it closes over Which is fine but optimizing a machine for a particular set of tasks means that you are unoptimizing the machine for some other particular set of tasks Again theres no free context Humans are AMGs but we are only efficient at creating certain kinds of machines using our imagination Our imagination is built on a number of cognitive tools that on the one hand constrain what machines we can efficiently imagine while on the other hand avail us to the openended set of all possible machines via prior knowledge or brute force random lookup In summary when people say general intelligence they really mean humanlike intelligence And again while human intelligence is an AMG any given AMG that is optimized for generating machines of a particular type will be less optimized for generating machines of some other types Theres no free context The most general search algorithm is a random walk there is no way to improve the generality of the random walk other than just speeding it up And thats actually what humans do for many problems anyway brute force random searching as fast as possible
,,"<p>Ethics involves the relationships of <em>needs</em> between two or more parties. As Matthew Graves said, if the AI lacks the sufficient human context (understanding of needs), it will produce seemingly perverse ethical behavior.</p>

<p>And let's be honest, some <em>people</em> would cut of other people's arms and put them on pressure plates. Even the best of us will not be able to sympathize with the needs of others with 100% accuracy - at best, we're guessing. And then there are those rare situations where I actually <em>want</em> you to cut off my arm and put it on a pressure plate, perhaps to save a loved one.</p>

<p>If we could make a thing that could sympathize with what a human <em>might</em> need in any given arbitrary situation, then we will have created either A) an artificial <em>human</em> intelligence (AHI) (which could be more or less fallible, like a human), or B) an oracle that can reason about <em>all possible human needs</em> on much faster than human time-scales - in which case you wouldn't <em>need</em> a conscious AI, as all human needs and solutions could be pre-computed via formal specification, which is probably absurd to consider.</p>
",,0,2016-09-30T18:40:58.730,,2046,2016-09-30T18:40:58.730,,,,,1712.0,2020.0,2,1,,,,47.66,10.75,9.93,0.0,0.0,33.0,Ethics involves the relationships of needs between two or more parties As Matthew Graves said if the AI lacks the sufficient human context understanding of needs it will produce seemingly perverse ethical behavior And lets be honest some people would cut of other peoples arms and put them on pressure plates Even the best of us will not be able to sympathize with the needs of others with 100 accuracy at best were guessing And then there are those rare situations where I actually want you to cut off my arm and put it on a pressure plate perhaps to save a loved one If we could make a thing that could sympathize with what a human might need in any given arbitrary situation then we will have created either A an artificial human intelligence AHI which could be more or less fallible like a human or B an oracle that can reason about all possible human needs on much faster than human timescales in which case you wouldnt need a conscious AI as all human needs and solutions could be precomputed via formal specification which is probably absurd to consider
,,"<p>If the universe is discrete, then analog phenomena (fluidity, curvature) are built on primitively discrete phenomena (bits and pieces).</p>

<p>If the universe is continuous, then discrete phenomena (bits and pieces) are built on primitively continuous phenomena (fluidity, curvature).</p>

<p>If the universe is discrete, the speed of seemingly analog phenomena will be bounded by the number of discrete phenomena that can occur in time and space.</p>

<p>If the universe is continuous, then time, space or matter <em>may</em> be infinitely divisible, which <em>may</em> allow for the execution of some phenomena faster than those phenomena <em>appear</em> to execute in natural environments (like protein folding or electric circuits) - so called ""super Turing"" computation.</p>

<p>The continuous universe idea begs the question, though: From whence came all this discreteness? A discrete universe can allow for apparent continuous behavior via approximation and randomness (or pseudorandomness), whereas a universe that is infinitely divisible affords no obvious definition of where things should start and end. This is one of the reasons many thinkers eschew considering infinities - they may be illusory.</p>

<p>So, can analog ""circuits"" execute faster than digital? As of right now, we know of some <em>seemingly</em> analog phenomena that <em>appear</em> execute faster than some digital phenomena (like electron spin vs a silicon logic gate). Whether analog phenomena are <em>intrinsically</em> more efficient than digital depends on the actual nature of the universe, which we have not yet determined.</p>
",,0,2016-09-30T19:20:09.033,,2047,2016-09-30T19:20:09.033,,,,,1712.0,1885.0,2,0,,,,22.85,15.61,9.53,0.0,0.0,44.0,If the universe is discrete then analog phenomena fluidity curvature are built on primitively discrete phenomena bits and pieces If the universe is continuous then discrete phenomena bits and pieces are built on primitively continuous phenomena fluidity curvature If the universe is discrete the speed of seemingly analog phenomena will be bounded by the number of discrete phenomena that can occur in time and space If the universe is continuous then time space or matter may be infinitely divisible which may allow for the execution of some phenomena faster than those phenomena appear to execute in natural environments like protein folding or electric circuits so called super Turing computation The continuous universe idea begs the question though From whence came all this discreteness A discrete universe can allow for apparent continuous behavior via approximation and randomness or pseudorandomness whereas a universe that is infinitely divisible affords no obvious definition of where things should start and end This is one of the reasons many thinkers eschew considering infinities they may be illusory So can analog circuits execute faster than digital As of right now we know of some seemingly analog phenomena that appear execute faster than some digital phenomena like electron spin vs a silicon logic gate Whether analog phenomena are intrinsically more efficient than digital depends on the actual nature of the universe which we have not yet determined
2052.0,4.0,"<p>AI is progressing drastically, and imagine they tell you you're fired because a robot will take your place. What are some jobs that can never be automated?</p>
",,1,2016-09-30T19:24:07.047,,2048,2016-10-05T17:16:41.537,2016-09-30T22:29:14.440,,8.0,,1760.0,,1,2,<philosophy><robots>,What jobs cannot be automatized by AI in the future?,419.0,57.77,9.85,8.98,0.0,0.0,4.0,AI is progressing drastically and imagine they tell you youre fired because a robot will take your place What are some jobs that can never be automated
,,"<p>If you were to completely automate a human, you'd just have another human, which defeats the purpose of the automation.</p>

<p>Any job that requires a ""whole human,"" rather than just a human's hands, feet, or simple reasoning ability, will still require humans.</p>

<p>If I go to a shrink, one with Wikipedia-like knowledge would be great, but one that also actually knows what its like to rub its eyes in the morning would be even better. Why? Because solving <em>some</em> problems will require knowing what it is like to rub one's eyes in the morning.</p>

<p>If I go to a movie that was written, directed and produced by some form of automation, I may be able to suspend my disbelief and get carried away by the story, but something in me will fundamentally appreciate the movie less, if I know that the AI can produce an infinite number of these stories, completely arbitrarily. There is something about knowing that the story came from a mind that has been conditioned against the vagaries of humanity (ie, a ""whole human""), that makes the story more appreciable.</p>

<p>If I call up a suicide hotline because I wan't someone to sympathize with me about my existential crisis, I'll want to talk to a ""whole human"" that can <em>sympathize</em> with my existential condition, not one that just regurgitates prior wisdom on life, heuristically matched against my problem state.</p>

<p>If I want to vote for a politician that can sympathize with the needs of the people, I'll want a ""whole person"" politician that can reflect on <em>all</em> the specifics that make life for a ""whole person"" hard or easy.</p>

<p>If I want soldiers to take the lives of humans, I want some sort of intelligence in that kill-chain that executes ""whole person"" analysis prior to pulling the trigger (a human).</p>

<p>If I want a conflict resolution specialist, capable of resolving complex cultural problems between humans, then I don't want just an AI that spits out the most likely solution based on prior solutions. I want an AI that can reason about prior solutions <em>and</em> all explicit and implicit problems between humans, in all human contexts, which requires a human or a perfect human simulacrum.</p>

<p>For any problem that requires consideration potentially <em>across</em> the whole spectrum of human context, we will want that solution to be generated by a ""whole human"" device. But if we automate the ""whole human"" then we haven't really outsourced the problem to automation but rather to a ""whole automated human,"" which will, by necessity, have its own problems.</p>

<p>Sure, we'll probably create an artificial human intelligence (AHI) one day, but being optimized to automatically solve <em>any</em> given human problem without also <em>having</em> human problems... that's just AI snake-oil that will never exist - outside of some perverse matrix scenario, under an infinite oracle of some sort.</p>

<p>So, yes, there will be many jobs that still require humans - mostly human-to-human problems that require full knowledge of the human context.</p>
",,1,2016-09-30T19:55:10.800,,2049,2016-09-30T20:28:30.460,2016-09-30T20:28:30.460,,1712.0,,1712.0,2048.0,2,1,,,,40.11,11.73,9.87,0.0,0.0,93.0,If you were to completely automate a human youd just have another human which defeats the purpose of the automation Any job that requires a whole human rather than just a humans hands feet or simple reasoning ability will still require humans If I go to a shrink one with Wikipedialike knowledge would be great but one that also actually knows what its like to rub its eyes in the morning would be even better Why Because solving some problems will require knowing what it is like to rub ones eyes in the morning If I go to a movie that was written directed and produced by some form of automation I may be able to suspend my disbelief and get carried away by the story but something in me will fundamentally appreciate the movie less if I know that the AI can produce an infinite number of these stories completely arbitrarily There is something about knowing that the story came from a mind that has been conditioned against the vagaries of humanity ie a whole human that makes the story more appreciable If I call up a suicide hotline because I want someone to sympathize with me about my existential crisis Ill want to talk to a whole human that can sympathize with my existential condition not one that just regurgitates prior wisdom on life heuristically matched against my problem state If I want to vote for a politician that can sympathize with the needs of the people Ill want a whole person politician that can reflect on all the specifics that make life for a whole person hard or easy If I want soldiers to take the lives of humans I want some sort of intelligence in that killchain that executes whole person analysis prior to pulling the trigger a human If I want a conflict resolution specialist capable of resolving complex cultural problems between humans then I dont want just an AI that spits out the most likely solution based on prior solutions I want an AI that can reason about prior solutions and all explicit and implicit problems between humans in all human contexts which requires a human or a perfect human simulacrum For any problem that requires consideration potentially across the whole spectrum of human context we will want that solution to be generated by a whole human device But if we automate the whole human then we havent really outsourced the problem to automation but rather to a whole automated human which will by necessity have its own problems Sure well probably create an artificial human intelligence AHI one day but being optimized to automatically solve any given human problem without also having human problems thats just AI snakeoil that will never exist outside of some perverse matrix scenario under an infinite oracle of some sort So yes there will be many jobs that still require humans mostly humantohuman problems that require full knowledge of the human context
,,"<blockquote>
  <p>What are some jobs that can never be automated?</p>
</blockquote>

<p><strong>None.</strong></p>

<p>The key word here is ""never"". Technology is rapidly advancing, and while I can think of situations where jobs can't be killed in the short-term or even in the long-term, I can't think of a job that is 100%, totally immune to extinction. Surely they <em>exist</em>, but you can't be <em>sure</em>...anything can happen after all. As long as it's <em>possible</em>, that's what matters here. You can't prove a negative.</p>

<p>This whole question seems as foolhardy as predicting in the 1850s that airplanes would <em>never</em> be invented. You'd be right in assuming that airplanes would not be invented in the 1860s...or the 1870s...or even the 1880s...but eventually, airplanes would be invented.</p>

<p>What would be better is to provide a specific cut-off point (""will all jobs be automated by the year 2020?"") that can allow us to try to extrapolate and predict based on current trends, but even that starts being difficult as you extend the cut-off point - My predictions about 2020 will be more accurate than my prediction about 2220. I think this type of question is truly unanswerable and can quickly decay to science-fiction speculation.</p>

<hr>

<p>Some additional comments about Doxosphoi's answer:</p>

<p>Doxosophoi made some arguments for why current society might not accept the automation of all jobs (the need for the ""personal touch"" that only a human-like intelligence can provide), but that's no reason to assume that society will <em>never</em> accept automation. Technology can change and adapt, and humans can also change and adapt. Maybe a human might not care about a shrink who ""rubs its eyes in the morning"", dislike movies that are marred by that ""vagaries of humanity"" instead of personal customization, prefer politicians and soldiers that actually acts logically instead of acting like a falliable human, etc., etc. I mean, it's <em>possible</em>.</p>

<p>There's also the problem of the term ""job"". Technically, I am working by writing an answer on a StackExchange website, but I'm not getting paid for it, so it's not a real ""job""...at best, it's just a hobby. I'm providing a valuable human touch, but since no one is giving me money, it's possible that this human touch may not be all that valuable in the first place: ""never give out your labor for free, because then they'll take it for free"".</p>

<p>Some of the techno-utopists (which I disagree with heavily) believes in a future where bots handle produce a lot of industrial goods and services, generating a lot of revenue that is then redistributed to the general population via some ""Basic Income"" scheme. This allows humans to do what they really want instead...such as hobbies? And what if the hobbies of the future are the ""jobs"" of today: shrinks, movie directors, politicians, soldiers, etc., etc. Instead of working for a paycheck, you're working in these jobs on a volunteer basis.</p>

<p>Obviously, no automation is being necessary to eliminate these hobbyists (no matter how good a bot is, <em>free labor</em> will always prevail), but they're not really jobs, are they? The bot is the one that is producing real value, and subsidizing the hobbies of all these other people. The idea of a ""job"" itself could be in jeopardy.</p>

<p>I don't think this scenario is likely either (in fact, I'd probably think it's just AI snake oil that will never actually happen). But it's <em>possible</em> and that's why I can't dismiss it outright. It could happen, just that I don't think it will.</p>

<p>And finally, the question is asking about whether a job can be automated, not <em>whether it's a good idea</em> to have it be automated, which is a completely different question. It's possible that we can build machines that can automate everything, and choose as a society not to use them for a variety of different reasons (such as the reasons that Doxosophoi mentioned).</p>
",,1,2016-10-01T01:09:50.263,,2050,2016-10-01T01:15:33.683,2016-10-01T01:15:33.683,,181.0,,181.0,2048.0,2,2,,,,61.87,10.73,8.88,0.0,0.0,164.0,What are some jobs that can never be automated None The key word here is never Technology is rapidly advancing and while I can think of situations where jobs cant be killed in the shortterm or even in the longterm I cant think of a job that is 100 totally immune to extinction Surely they exist but you cant be sureanything can happen after all As long as its possible thats what matters here You cant prove a negative This whole question seems as foolhardy as predicting in the 1850s that airplanes would never be invented Youd be right in assuming that airplanes would not be invented in the 1860sor the 1870sor even the 1880sbut eventually airplanes would be invented What would be better is to provide a specific cutoff point will all jobs be automated by the year 2020 that can allow us to try to extrapolate and predict based on current trends but even that starts being difficult as you extend the cutoff point My predictions about 2020 will be more accurate than my prediction about 2220 I think this type of question is truly unanswerable and can quickly decay to sciencefiction speculation Some additional comments about Doxosphois answer Doxosophoi made some arguments for why current society might not accept the automation of all jobs the need for the personal touch that only a humanlike intelligence can provide but thats no reason to assume that society will never accept automation Technology can change and adapt and humans can also change and adapt Maybe a human might not care about a shrink who rubs its eyes in the morning dislike movies that are marred by that vagaries of humanity instead of personal customization prefer politicians and soldiers that actually acts logically instead of acting like a falliable human etc etc I mean its possible Theres also the problem of the term job Technically I am working by writing an answer on a StackExchange website but Im not getting paid for it so its not a real jobat best its just a hobby Im providing a valuable human touch but since no one is giving me money its possible that this human touch may not be all that valuable in the first place never give out your labor for free because then theyll take it for free Some of the technoutopists which I disagree with heavily believes in a future where bots handle produce a lot of industrial goods and services generating a lot of revenue that is then redistributed to the general population via some Basic Income scheme This allows humans to do what they really want insteadsuch as hobbies And what if the hobbies of the future are the jobs of today shrinks movie directors politicians soldiers etc etc Instead of working for a paycheck youre working in these jobs on a volunteer basis Obviously no automation is being necessary to eliminate these hobbyists no matter how good a bot is free labor will always prevail but theyre not really jobs are they The bot is the one that is producing real value and subsidizing the hobbies of all these other people The idea of a job itself could be in jeopardy I dont think this scenario is likely either in fact Id probably think its just AI snake oil that will never actually happen But its possible and thats why I cant dismiss it outright It could happen just that I dont think it will And finally the question is asking about whether a job can be automated not whether its a good idea to have it be automated which is a completely different question Its possible that we can build machines that can automate everything and choose as a society not to use them for a variety of different reasons such as the reasons that Doxosophoi mentioned
,,"<p>The Oxford study from 2013 in <a href=""http://www.oxfordmartin.ox.ac.uk/downloads/academic/The_Future_of_Employment.pdf"">The future of employment</a> paper assess this and estimated the probability of computerisation for 702 detailed occupations using a Gaussian process classifier (using job data from the UK partially merged with data from US), and based on these estimates they identified three areas of computerisation bottleneck areas and nine skills that people are still needed for each profession, this includes:</p>

<ul>
<li><p>Perception and Manipulation.</p>

<ul>
<li><p>Finger dexterity.</p>

<blockquote>
  <p>The ability to make precisely coordinated movements of
  the fingers of one or both hands to grasp, manipulate, or
  assemble very small objects.</p>
</blockquote></li>
<li><p>Manual dexterity.</p>

<blockquote>
  <p>The ability to quickly move your hand, your hand together
  with your arm, or your two hands to grasp, manipulate, or
  assemble objects.</p>
</blockquote></li>
<li><p>The need for a cramped work space.</p>

<blockquote>
  <p>How often does this job require working in cramped work
  spaces that requires getting into awkward positions?</p>
</blockquote></li>
</ul></li>
<li><p>Creative Intelligence.</p>

<ul>
<li><p>Originality.</p>

<blockquote>
  <p>The ability to come up with unusual or clever ideas about
  a given topic or situation, or to develop creative ways to
  solve a problem.</p>
</blockquote></li>
<li><p>Fine arts.</p>

<blockquote>
  <p>Knowledge of theory and techniques required to compose,
  produce, and perform works of music, dance, visual arts,
  drama, and sculpture.</p>
</blockquote></li>
</ul></li>
<li><p>Social Intelligence.</p>

<ul>
<li><p>Social perceptiveness.</p>

<blockquote>
  <p>Being aware of others’ reactions and understanding why
  they react as they do.</p>
</blockquote></li>
<li><p>Negotiation.</p>

<blockquote>
  <p>Bringing others together and trying to reconcile
  differences.</p>
</blockquote></li>
<li><p>Persuasion.</p>

<blockquote>
  <p>Persuading others to change their minds or behavior.</p>
</blockquote></li>
<li><p>Assisting and caring for others.</p>

<blockquote>
  <p>Providing personal assistance, medical attention, emotional
  support, or other personal care to others such as
  coworkers, customers, or patients.</p>
</blockquote></li>
</ul></li>
</ul>

<p><sup>Source: <a href=""http://web.archive.org/web/20161001101136/http://www.oxfordmartin.ox.ac.uk/downloads/academic/The_Future_of_Employment.pdf"">The future of employment: how susceptible are jobs to computerisation</a>: Table 1.</sup></p>

<p>What this study is basically saying, around 50% of all jobs will be replaced by robots in the next 20 years.</p>

<p>Based on the above study, the BBC assembled a handy guide that calculates which jobs are likely to be automated within the next two decades:</p>

<ul>
<li><a href=""http://www.bbc.co.uk/news/technology-34066941"">Will a robot take your job?</a></li>
</ul>

<p>See also: <a href=""http://www.replacedbyrobot.info/"">replacedbyrobot.info</a> website.</p>

<blockquote>
  <p>With this tool, you can check the prediction of over 700 jobs.</p>
</blockquote>

<p>Related:</p>

<ul>
<li><a href=""https://www.reddit.com/r/AskReddit/comments/4mikie/when_robots_can_do_all_manual_labor_and_service/"">When robots can do all manual labor and service jobs, what will the majority human population do?</a></li>
<li><p><a href=""https://medium.com/@tylerehc/labore-ad-infinitum-ai-automation-vs-timeless-tasks-ced2216f2ab7"">Labore Ad Infinitum: AI &amp; Automation vs Timeless Tasks</a></p>

<p>Which suggests: Military/Peacekeeper, Athletes, Therapist, Musical Performer, Actors and Dancer, Visual Artists, Religious/Spiritual Leaders, The World’s Oldest Profession, Virtual Goods, Politicians, Judges, Parenting.</p></li>
</ul>
",,0,2016-10-01T10:11:17.110,,2052,2016-10-01T13:37:57.190,2016-10-01T13:37:57.190,,8.0,,8.0,2048.0,2,7,,,,42.51,15.08,11.19,0.0,0.0,80.0,The Oxford study from 2013 in The future of employment paper assess this and estimated the probability of computerisation for 702 detailed occupations using a Gaussian process classifier using job data from the UK partially merged with data from US and based on these estimates they identified three areas of computerisation bottleneck areas and nine skills that people are still needed for each profession this includes Perception and Manipulation Finger dexterity The ability to make precisely coordinated movements of the fingers of one or both hands to grasp manipulate or assemble very small objects Manual dexterity The ability to quickly move your hand your hand together with your arm or your two hands to grasp manipulate or assemble objects The need for a cramped work space How often does this job require working in cramped work spaces that requires getting into awkward positions Creative Intelligence Originality The ability to come up with unusual or clever ideas about a given topic or situation or to develop creative ways to solve a problem Fine arts Knowledge of theory and techniques required to compose produce and perform works of music dance visual arts drama and sculpture Social Intelligence Social perceptiveness Being aware of others’ reactions and understanding why they react as they do Negotiation Bringing others together and trying to reconcile differences Persuasion Persuading others to change their minds or behavior Assisting and caring for others Providing personal assistance medical attention emotional support or other personal care to others such as coworkers customers or patients Source The future of employment how susceptible are jobs to computerisation Table 1 What this study is basically saying around 50 of all jobs will be replaced by robots in the next 20 years Based on the above study the BBC assembled a handy guide that calculates which jobs are likely to be automated within the next two decades Will a robot take your job See also replacedbyrobotinfo website With this tool you can check the prediction of over 700 jobs Related When robots can do all manual labor and service jobs what will the majority human population do Labore Ad Infinitum AI amp Automation vs Timeless Tasks Which suggests MilitaryPeacekeeper Athletes Therapist Musical Performer Actors and Dancer Visual Artists ReligiousSpiritual Leaders The World’s Oldest Profession Virtual Goods Politicians Judges Parenting
,,"<p>I'd like to offer also a slightly different view on the machine cannot better its master.  Consider the very simple case of content classifiers.  It's already to the point where for some areas classification and prediction can be performed way better than a human.  And while a human may have designed the ""algorithm"",  the algorithm was likely a recurrent neutral network or other form of ML that could well have self trained. In these cases we don't actually understand or need to understand the individual weights in the net,  as we would need to have traditionally understood the imperative programming constructs we used to write. It just works.</p>

<p>So if we get to where we develop a meta-algorithm for classifying problems and building more optimal deep learning solutions than we would by hand, but I think that would pretty much take us out of the picture for quite a lot of problem spaces.  Thoughts? </p>
",,0,2016-10-01T12:47:26.077,,2053,2016-10-01T13:04:32.967,2016-10-01T13:04:32.967,,8.0,,2746.0,2033.0,2,0,,,,57.61,10.85,10.06,0.0,0.0,17.0,Id like to offer also a slightly different view on the machine cannot better its master Consider the very simple case of content classifiers Its already to the point where for some areas classification and prediction can be performed way better than a human And while a human may have designed the algorithm the algorithm was likely a recurrent neutral network or other form of ML that could well have self trained In these cases we dont actually understand or need to understand the individual weights in the net as we would need to have traditionally understood the imperative programming constructs we used to write It just works So if we get to where we develop a metaalgorithm for classifying problems and building more optimal deep learning solutions than we would by hand but I think that would pretty much take us out of the picture for quite a lot of problem spaces Thoughts
2059.0,1.0,"<p>I want to have a program that writes like a human. But I don't just want a font, but instead an 'intelligent' program that produce different result and that can be trained with different sets to generate different handwritings.
As a training set I would like to have parts of a handwritten text (saved as a list of paths (like in vector graphics).
Maybe as a means to simplify things, I could flatten the paths in to consecutive straight lines. My program receives a string of text and produces a list of paths (or a vector graphic, whatever is easier to work with)</p>

<p>My question now is: What kind of machine learning would be best to achieve this?</p>
",2016-10-03T15:26:53.633,1,2016-10-01T13:28:45.760,,2054,2016-10-02T05:48:28.690,,,,,2747.0,,1,0,<machine-learning><training>,Artificial writing system,70.0,64.44,9.0,8.69,0.0,0.0,17.0,I want to have a program that writes like a human But I dont just want a font but instead an intelligent program that produce different result and that can be trained with different sets to generate different handwritings As a training set I would like to have parts of a handwritten text saved as a list of paths like in vector graphics Maybe as a means to simplify things I could flatten the paths in to consecutive straight lines My program receives a string of text and produces a list of paths or a vector graphic whatever is easier to work with My question now is What kind of machine learning would be best to achieve this
2057.0,1.0,"<p>I'm wondering how feasible it is to create a machine that can separate clothing from a basket.</p>

<p>At the most basic level it would distinguish between tops, pants, button downs and socks</p>

<p>Programmatically, I'd image this would require training a neural network to recognize these items, but in real time it becomes exponentially difficult to do this in a small space at a fast rate:</p>

<ol>
<li>pick up an item</li>
<li>lay it in such a way that is recognizable </li>
<li>deduce whether it is a top, button down, etc.</li>
<li>sort it accordingly</li>
</ol>

<p>If this sounds ridiculous please let me know...</p>

<p>If it is possible :</p>

<p>would this be based on some sort of computer vision?
or only a well trained neural network?</p>

<p>Any insight is much appreciated!</p>
",,0,2016-10-01T17:47:48.857,,2055,2016-10-01T23:48:41.117,2016-10-01T19:25:13.977,,2752.0,,2752.0,,1,1,<neural-networks><image-recognition>,Feasibility of sorting a basket of clothes in the real world,55.0,58.92,9.4,9.5,0.0,0.0,18.0,Im wondering how feasible it is to create a machine that can separate clothing from a basket At the most basic level it would distinguish between tops pants button downs and socks Programmatically Id image this would require training a neural network to recognize these items but in real time it becomes exponentially difficult to do this in a small space at a fast rate pick up an item lay it in such a way that is recognizable deduce whether it is a top button down etc sort it accordingly If this sounds ridiculous please let me know If it is possible would this be based on some sort of computer vision or only a well trained neural network Any insight is much appreciated
,2.0,"<p>For years I have been dealing with (and teaching) Knowledge Representation and Knowledge Representation languages. I just discovered that in another community (Information Systems and the such) there is something called the ""DIKW pyramid"" where they add another step after knowledge, namely wisdom.
They define data as being simply symbols, information as being the answer to who/what/when/where?, knowledge as being the answer to how?, and wisdom as being the answer to why?. </p>

<p>My question is: has anyone done the connection between what AI calls data/information/knowledge and these notions from Information Systems? In particular, how would ""wisdom"" be defined in AI? And since we have KR languages, how would we represent ""wisdom"" as they define it?</p>

<p>Any references would be welcome…</p>
",,1,2016-10-01T19:00:56.123,1.0,2056,2016-12-01T19:50:16.970,,,,,2753.0,,1,7,<knowledge-representation>,Wisdom representation?,100.0,49.52,14.26,9.82,0.0,0.0,31.0,For years I have been dealing with and teaching Knowledge Representation and Knowledge Representation languages I just discovered that in another community Information Systems and the such there is something called the DIKW pyramid where they add another step after knowledge namely wisdom They define data as being simply symbols information as being the answer to whowhatwhenwhere knowledge as being the answer to how and wisdom as being the answer to why My question is has anyone done the connection between what AI calls datainformationknowledge and these notions from Information Systems In particular how would wisdom be defined in AI And since we have KR languages how would we represent wisdom as they define it Any references would be welcome…
,,"<p><a href=""https://people.eecs.berkeley.edu/~pabbeel/"" rel=""nofollow"">Peter Abbeel</a> does work in deep learning for robotics, and one of the projects they've tackled is manipulating clothes. Here's a <a href=""https://www.youtube.com/watch?v=5FGVgMsiv1s"" rel=""nofollow"">video from 2011</a> of their robot folding laundry (one piece at a time).</p>

<p>There are also companies attempting to market this; seven dreamers makes the <a href=""https://laundroid.sevendreamers.com/?lang=en"" rel=""nofollow"">Laundroid</a> and <a href=""https://www.foldimate.com/"" rel=""nofollow"">Foldimate</a> claims that it will start taking pre-orders in 2017.</p>
",,0,2016-10-01T23:48:41.117,,2057,2016-10-01T23:48:41.117,,,,,10.0,2055.0,2,1,,,,60.35,12.13,11.13,0.0,0.0,10.0,Peter Abbeel does work in deep learning for robotics and one of the projects theyve tackled is manipulating clothes Heres a video from 2011 of their robot folding laundry one piece at a time There are also companies attempting to market this seven dreamers makes the Laundroid and Foldimate claims that it will start taking preorders in 2017
,,"<p>I haven't done the connection - didn't know about the pyramid. I'm not sure it translates well into AI though.</p>

<p>It seems they're separating information from knowledge by splitting how from what. What is a superset of how, as far as I'm concerned. It's also a superset of why.</p>

<p>But from an evolutionary perspective, knowledge representation starts with why. Prior to a reason for knowledge representation, there is no knowledge representation. The 'what' existed, but it was not represented until autopoiesis created goal directed, why-oriented behaviors that began storing the what as knowledge. </p>

<p>What is a superset of why, just as ontology is a superset of teleology. However, all <em>represented</em> ontology was acquired through teleological (end-directed) action.</p>

<p>So I disagree with the notion that wisdom, as a why thing, is at the tip of the pyramid. It all started with goal directed behavior and that has been the source of all subsequent information growth.</p>

<p>So what is wisdom? I think it is too much of a folk term to warrant a technical definition. If I had to just take a swing at a definition, though, I'd probably vote for wisdom being knowledge of the ontological basis of one's own teleological knowledge - essentially objectifying one's subjective interpretations - knowing the true what and how of the why, to whatever extent is possible.</p>

<p>I don't have many <em>specific</em> references on this subject, but I thought Terrence Deacon's <a href=""https://en.wikipedia.org/wiki/Incomplete_Nature"" rel=""nofollow"">Incomplete Nature: How Mind Emerged from Matter</a> was a good primer on teleology.</p>
",,5,2016-10-02T04:52:31.447,,2058,2016-10-02T04:52:31.447,,,,,1712.0,2056.0,2,0,,,,55.84,11.88,9.09,0.0,0.0,50.0,I havent done the connection didnt know about the pyramid Im not sure it translates well into AI though It seems theyre separating information from knowledge by splitting how from what What is a superset of how as far as Im concerned Its also a superset of why But from an evolutionary perspective knowledge representation starts with why Prior to a reason for knowledge representation there is no knowledge representation The what existed but it was not represented until autopoiesis created goal directed whyoriented behaviors that began storing the what as knowledge What is a superset of why just as ontology is a superset of teleology However all represented ontology was acquired through teleological enddirected action So I disagree with the notion that wisdom as a why thing is at the tip of the pyramid It all started with goal directed behavior and that has been the source of all subsequent information growth So what is wisdom I think it is too much of a folk term to warrant a technical definition If I had to just take a swing at a definition though Id probably vote for wisdom being knowledge of the ontological basis of ones own teleological knowledge essentially objectifying ones subjective interpretations knowing the true what and how of the why to whatever extent is possible I dont have many specific references on this subject but I thought Terrence Deacons Incomplete Nature How Mind Emerged from Matter was a good primer on teleology
,,"<p>From the abstract of this paper <a href=""https://arxiv.org/pdf/1308.0850v5.pdf"" rel=""nofollow""><em>Generating Sequences With Recurrent Neural Networks</em></a><em>[pdf]</em> </p>

<blockquote>
  <p>This paper shows how Long Short-term Memory recurrent neural networks
  can be used to generate complex sequences with long-range structure,
  simply by predicting one data point at a time. The approach is
  demonstrated for text (where the data are discrete) and online
  handwriting (where the data are real-valued). It is then extended to
  handwriting synthesis by allowing the network to condition its
  predictions on a text sequence. The resulting system is able to
  generate highly realistic cursive handwriting in a wide variety of
  styles.</p>
</blockquote>

<p>An implementation is here: <a href=""https://github.com/szcom/rnnlib#handwriting-synthesis"" rel=""nofollow"">handwriting-synthesis</a></p>

<p>And using TensorFlow: <a href=""http://blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/"" rel=""nofollow"">Handwriting Generation Demo in TensorFlow</a></p>
",,0,2016-10-02T05:48:28.690,,2059,2016-10-02T05:48:28.690,,,,,1712.0,2054.0,2,4,,,,32.43,16.19,11.96,0.0,0.0,17.0,From the abstract of this paper Generating Sequences With Recurrent Neural Networkspdf This paper shows how Long Shortterm Memory recurrent neural networks can be used to generate complex sequences with longrange structure simply by predicting one data point at a time The approach is demonstrated for text where the data are discrete and online handwriting where the data are realvalued It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles An implementation is here handwritingsynthesis And using TensorFlow Handwriting Generation Demo in TensorFlow
,,"<p>As with another answer, I am also skeptical of the distinctions made in the DIKW pyramid.</p>

<p>Nonetheless, a very popular machine learning approach for answering 'Why?' questions is the application of Bayesian reasoning: given a causal data model, reverse inference can be used to find the probability distribution of events which lead to a given outcome.</p>

<p>It could be argued that defining 'cause' in terms of distributions rather than specific concrete mechanisms is a rather limited notion of 'Why?'.</p>

<p>However, it may be that there are some forms of causality that we don't know how to represent, specifically 'first-hand experience'. Indeed, common usage of the term 'wisdom' generally refers to first-hand experience, rather than information gained from some other source. </p>

<p>The idea is that knowledge can be expressed declaratively, whereas wisdom must be derived from experience.</p>

<p>For an AI represented as a computer program, the distinction between declarative and first-hand experience might appear irrelevant, since in principle any experience can be encoded and made available without the program having to 'experience' it first-hand.</p>

<p>However, the following humorous definition of `wisdom' might perhaps shed some light on a distinction that's pertinent to AI research:</p>

<blockquote>
  <p>Knowledge is knowing that a tomato is a fruit.</p>
  
  <p>Wisdom is knowing that you shouldn't eat it with custard.</p>
</blockquote>

<p>This notion of 'Wisdom' could be said to require <a href=""https://en.wikipedia.org/wiki/Qualia"" rel=""nofollow"">qualia</a>. It is the subject of much debate whether qualia exist and/or are necessary for consciousness - see for example the thought experiment of <a href=""https://en.wikipedia.org/wiki/Knowledge_argument"" rel=""nofollow"">'The Black and White Room'</a>.</p>

<p>So the notion is that there is a distinction between having a Bayesian network representation of wisdom that says: ""It is 99.7% likely that putting a tomato in custard is undesirable"" and the first-hand experience to the effect that it tastes odd with custard.</p>
",,3,2016-10-02T08:23:45.937,,2061,2016-10-02T18:29:44.360,2016-10-02T18:29:44.360,,42.0,,42.0,2056.0,2,1,,,,50.26,13.69,10.76,0.0,0.0,60.0,As with another answer I am also skeptical of the distinctions made in the DIKW pyramid Nonetheless a very popular machine learning approach for answering Why questions is the application of Bayesian reasoning given a causal data model reverse inference can be used to find the probability distribution of events which lead to a given outcome It could be argued that defining cause in terms of distributions rather than specific concrete mechanisms is a rather limited notion of Why However it may be that there are some forms of causality that we dont know how to represent specifically firsthand experience Indeed common usage of the term wisdom generally refers to firsthand experience rather than information gained from some other source The idea is that knowledge can be expressed declaratively whereas wisdom must be derived from experience For an AI represented as a computer program the distinction between declarative and firsthand experience might appear irrelevant since in principle any experience can be encoded and made available without the program having to experience it firsthand However the following humorous definition of wisdom might perhaps shed some light on a distinction thats pertinent to AI research Knowledge is knowing that a tomato is a fruit Wisdom is knowing that you shouldnt eat it with custard This notion of Wisdom could be said to require qualia It is the subject of much debate whether qualia exist andor are necessary for consciousness see for example the thought experiment of The Black and White Room So the notion is that there is a distinction between having a Bayesian network representation of wisdom that says It is 997 likely that putting a tomato in custard is undesirable and the firsthand experience to the effect that it tastes odd with custard
,,"<p>Autonomous vacuum cleaners usually have these following task environment properties:</p>

<p>i. Partially observable environment</p>

<p>ii. Deterministic environment.</p>

<p>iii. Sequential environment.</p>

<p>iv. Static environment.</p>

<p>v. Discrete environment</p>

<p>vi. Single agent environment</p>

<p>Since it's a partially observable environment, the agent performs its actions based on what is sees and thus it's is in an deterministic environment where the current action will be the result of previous actions. And since, the agent is continuously performing, it's an sequential environment. Since, the environment doesn't change when agent is working, the environment is static and discrete since there are only finite no. of discrete states in which agent can be in. </p>

<p>There are certain rules written which tells an agent how to react when in a particular state. If there are multiple rules which are satisfied, then agent uses its experience to choose an optimal action to perform. The rules are written by the programmer keeping in mind the task envt. properties.</p>

<p>The agent's actions also depend on the type of agent it is decided by the programmer. It can be simple reflex-based, or a goal based, or a goal-based + feedback or a complete learning based agent. An agent can't use A* algorithm because the entire environment is not visible and it will be useless to use A* algorithm where we don't know when our goal may be reached.  </p>

<p>The agent has various sensors attached which give it info. about the surrounding, like cameras, sound recorders, thermal sensors, etc. An autonomous vacuum cleaner may also have a dirt sensor to detect the dirt. The agent performs an action using one of its actuators like wheels, robot arms, etc. </p>
",,0,2016-10-02T15:44:04.267,,2062,2016-10-02T15:44:04.267,,,,,1807.0,1613.0,2,0,,,,55.13,12.58,9.35,0.0,0.0,51.0,Autonomous vacuum cleaners usually have these following task environment properties i Partially observable environment ii Deterministic environment iii Sequential environment iv Static environment v Discrete environment vi Single agent environment Since its a partially observable environment the agent performs its actions based on what is sees and thus its is in an deterministic environment where the current action will be the result of previous actions And since the agent is continuously performing its an sequential environment Since the environment doesnt change when agent is working the environment is static and discrete since there are only finite no of discrete states in which agent can be in There are certain rules written which tells an agent how to react when in a particular state If there are multiple rules which are satisfied then agent uses its experience to choose an optimal action to perform The rules are written by the programmer keeping in mind the task envt properties The agents actions also depend on the type of agent it is decided by the programmer It can be simple reflexbased or a goal based or a goalbased feedback or a complete learning based agent An agent cant use A algorithm because the entire environment is not visible and it will be useless to use A algorithm where we dont know when our goal may be reached The agent has various sensors attached which give it info about the surrounding like cameras sound recorders thermal sensors etc An autonomous vacuum cleaner may also have a dirt sensor to detect the dirt The agent performs an action using one of its actuators like wheels robot arms etc
,,"<p>A 'heuristic' is simply a 'rule-of-thumb', i.e. something which doesn't guarantee an optimal solution to a problem.</p>

<p>Beyond the above notion (certainly within the discipline of optimization), the notion of what constitutes a heuristic is not particularly strict, and could certainly include hints for constructing a new solution from parts of a previous one, as you are doing.</p>

<p>A related concrete example is the ""nearest neighbour heuristic"" for the Travelling Salesman Problem, in which a solution is constructed by starting at some random city and iteratively choosing the nearest. The resulting completed tour is then used as an initial input to some more sophisticated optimization procedure.</p>
",,0,2016-10-02T18:49:55.890,,2065,2016-10-02T18:49:55.890,,,,,42.0,2036.0,2,2,,,,41.5,14.97,11.39,0.0,0.0,22.0,A heuristic is simply a ruleofthumb ie something which doesnt guarantee an optimal solution to a problem Beyond the above notion certainly within the discipline of optimization the notion of what constitutes a heuristic is not particularly strict and could certainly include hints for constructing a new solution from parts of a previous one as you are doing A related concrete example is the nearest neighbour heuristic for the Travelling Salesman Problem in which a solution is constructed by starting at some random city and iteratively choosing the nearest The resulting completed tour is then used as an initial input to some more sophisticated optimization procedure
2068.0,1.0,"<p><a href=""https://en.wikipedia.org/wiki/AI_effect"" rel=""nofollow"">According to Wikipedia</a>...</p>

<blockquote>
  <p>The AI effect occurs when onlookers discount the behavior of an artificial intelligence program by arguing that it is not real intelligence.</p>
  
  <p>Pamela McCorduck writes: ""It's part of the history of the field of artificial intelligence that every time somebody figured out how to make a computer do something—play good checkers, solve simple but relatively informal problems—there was chorus of critics to say, 'that's not thinking'.""[1] AI researcher Rodney Brooks complains ""Every time we figure out a piece of it, it stops being magical; we say, 'Oh, that's just a computation.'""[2]</p>
</blockquote>

<p>The Wikipedia page then proposes several different reasons that could explain why onlookers might ""discount"" AI programs. However, those reasons seem to imply that the humans are making a mistake in ""discounting"" the behavior of AI programs...and that these AI programs might actually be  intelligent. I want to make an alternate argument, where the humans are making a mistake, but not in ""discounting"" the behavior of AI programs.</p>

<p>Consider the following situation. I want to build a machine that can do X (where X is some trait, like intelligence). I am able to evaluate intuitively whether a machine has that X criteria. But I don't have a good definition of what X actually <em>is</em>. All I can do is identify whether something has X or not.</p>

<p>However, I think that people who has X can do Y. So if I build a machine that can do Y, then surely, I built a machine that has X.</p>

<p>After building the machine that can do Y, I examine it to see if my machine has X. And it does not. So my machine lacks X. And while a machine that can do Y is cool, what I really want is a machine that has X. I go back to the drawing board and think of a new idea to reach X.</p>

<p>After writing on the whiteboard for a couple of hours, I realize that people who has X can do Z. Of course! I try to build a new machine that can do Z, yes, if it can do Z, then it must have X.</p>

<p>After building the machine that can do Z, I check to see if it has X. It does not. And so I return back to the drawing board, and the cycle repeats and repeats...</p>

<p>Essentially, humans are attempting to determine whether an entity has intelligence via proxy measurements, but those proxy measurements are potentially faulty (as it is possible to meet those proxy measurements without ever actually having intelligence). Until we know how to define intelligence and design a test that can accurately measure it, it is very unlikely for us to build a machine that has intelligence. So the AI Effect occurs because humans don't know how to define ""intelligence"", not due to people dismissing programs as not being ""intelligent"".</p>

<p>Is this argument valid or correct? And if not, why not?</p>
",,0,2016-10-02T20:02:47.463,1.0,2066,2016-10-02T23:49:18.177,2016-10-02T23:49:18.177,,181.0,,181.0,,1,7,<intelligence-testing>,Is the AI Effect caused by bad tests of intelligence?,85.0,63.39,8.99,8.16,0.0,0.0,95.0,According to Wikipedia The AI effect occurs when onlookers discount the behavior of an artificial intelligence program by arguing that it is not real intelligence Pamela McCorduck writes Its part of the history of the field of artificial intelligence that every time somebody figured out how to make a computer do something—play good checkers solve simple but relatively informal problems—there was chorus of critics to say thats not thinking1 AI researcher Rodney Brooks complains Every time we figure out a piece of it it stops being magical we say Oh thats just a computation2 The Wikipedia page then proposes several different reasons that could explain why onlookers might discount AI programs However those reasons seem to imply that the humans are making a mistake in discounting the behavior of AI programsand that these AI programs might actually be intelligent I want to make an alternate argument where the humans are making a mistake but not in discounting the behavior of AI programs Consider the following situation I want to build a machine that can do X where X is some trait like intelligence I am able to evaluate intuitively whether a machine has that X criteria But I dont have a good definition of what X actually is All I can do is identify whether something has X or not However I think that people who has X can do Y So if I build a machine that can do Y then surely I built a machine that has X After building the machine that can do Y I examine it to see if my machine has X And it does not So my machine lacks X And while a machine that can do Y is cool what I really want is a machine that has X I go back to the drawing board and think of a new idea to reach X After writing on the whiteboard for a couple of hours I realize that people who has X can do Z Of course I try to build a new machine that can do Z yes if it can do Z then it must have X After building the machine that can do Z I check to see if it has X It does not And so I return back to the drawing board and the cycle repeats and repeats Essentially humans are attempting to determine whether an entity has intelligence via proxy measurements but those proxy measurements are potentially faulty as it is possible to meet those proxy measurements without ever actually having intelligence Until we know how to define intelligence and design a test that can accurately measure it it is very unlikely for us to build a machine that has intelligence So the AI Effect occurs because humans dont know how to define intelligence not due to people dismissing programs as not being intelligent Is this argument valid or correct And if not why not
,,"<p>I think it is mostly right. But not that intelligence is hard to define. In my opinion, it is simple: A is more intelligent than B if A achieves some purpose in less steps than B. It is functional/algorithmic efficiency.</p>

<p>What is difficult to define is <em>human</em> intelligence.</p>

<p>But when someone says, ""No, X is not <em>real</em> intelligence,"" what they mean is that it does not satisfy what we would consider real <em>human intelligence</em>.</p>

<p>So when people discount new and amazing discoveries in machine intelligence, it is not because they are not amazing in their own way, but because those discoveries, while exhibiting intelligence, are <strong>actually not</strong> replicating human intelligence - which is what many people actually mean when they say ""that thing isn't <em>really</em> intelligent.""</p>

<p>In truth we are very far, in the science of artificial intelligence, algorithmically speaking, from an artificial <em>human</em> intelligence (AHI).</p>

<p>Additional note: What's funny is that we don't call the science of artificial intelligence just 'the science of intelligence.' That we add the ""artificial"" qualifier by necessity pegs the science to what the artificiality implicitly emulates: human intelligence. In other words, ""artificial intelligence"" must be by definition more specific to the thing it allegedly artificializes than a more general science of just ""intelligence.""</p>
",,0,2016-10-02T22:18:25.650,,2068,2016-10-02T22:40:34.953,2016-10-02T22:40:34.953,,1712.0,,1712.0,2066.0,2,5,,,,35.37,14.04,8.83,0.0,0.0,45.0,I think it is mostly right But not that intelligence is hard to define In my opinion it is simple A is more intelligent than B if A achieves some purpose in less steps than B It is functionalalgorithmic efficiency What is difficult to define is human intelligence But when someone says No X is not real intelligence what they mean is that it does not satisfy what we would consider real human intelligence So when people discount new and amazing discoveries in machine intelligence it is not because they are not amazing in their own way but because those discoveries while exhibiting intelligence are actually not replicating human intelligence which is what many people actually mean when they say that thing isnt really intelligent In truth we are very far in the science of artificial intelligence algorithmically speaking from an artificial human intelligence AHI Additional note Whats funny is that we dont call the science of artificial intelligence just the science of intelligence That we add the artificial qualifier by necessity pegs the science to what the artificiality implicitly emulates human intelligence In other words artificial intelligence must be by definition more specific to the thing it allegedly artificializes than a more general science of just intelligence
2070.0,1.0,"<p>Here is one of the most serious questions, about the artificial intelligence.<br>
How will the machine know the difference between right and wrong, what is good and bad, what is respect, dignity, faith and empathy.<br>
<br> A machine can recognize what is correct and incorrect, what is right and what is wrong, depend on how it is originally designed.<br>
<br>It will follow the ethics of its creator, the man who originally designed it<br>
 But how to teach a computer something we don't have the right answer.<br>
 People are selfish, jealous, self confident. We are not able to understand each other sorrows, pains beliefs. We don't understand different religions, different traditions or beliefs. 
<br>Creating an AI might be breakthrough for one nation, or one race, or one ethnic or religious group, but it can be against others.   </p>

<p>Who will learn the machine a humanity?   :)</p>
",,1,2016-10-03T02:27:05.757,2.0,2069,2016-10-03T16:32:51.303,2016-10-03T16:32:51.303,,33.0,,2682.0,,1,0,<machine-learning><ethics>,"How will an AI comprehend the ethics of ""right"" and ""wrong""?",123.0,63.9,11.02,9.31,0.0,0.0,28.0,Here is one of the most serious questions about the artificial intelligence How will the machine know the difference between right and wrong what is good and bad what is respect dignity faith and empathy A machine can recognize what is correct and incorrect what is right and what is wrong depend on how it is originally designed It will follow the ethics of its creator the man who originally designed it But how to teach a computer something we dont have the right answer People are selfish jealous self confident We are not able to understand each other sorrows pains beliefs We dont understand different religions different traditions or beliefs Creating an AI might be breakthrough for one nation or one race or one ethnic or religious group but it can be against others Who will learn the machine a humanity
,,"<p>Right and wrong only exist relative to some goal or purpose.</p>

<p>To make a machine do more right than wrong, relative to human goals, one should minimize the surface area of the machine's purpose. Doing that minimizes the intrinsic behavior of the AI, which enables us to reason about the right and wrong behaviors of the AI, relative to human purposes.</p>

<p>Horses are quite general over the domains of their purposes, but are still predictable enough for humans to control and benefit from. As such, we will be able to produce machines (conscious or unconscious) that are highly general over particular domains, while still being predictable enough to be useful to humans.</p>

<p>The most efficient machines for most tasks, though, will not <em>need</em> consciousness, nor even the needs that cause survivalistic, adversarial and self-preserving behaviors in eukaryotic cells. Because most of our solutions won't <em>need</em> those purposes to optimize over our problems, we can allow them to be much more predictable.</p>

<p>We will be able to create predictable, efficient AIs over large domains that are able to produce predictable, efficient AIs over more specific domains. We'll be able to reason about the behavioral guarantees and failure modes of those narrow domains.</p>

<p>In the event that we one day desire to build something as unpredictable as a human, like we do when having babies, then we'll probably do that with the similar intentions and care that we use to bring an actual baby into the world. There is simply no purpose in creating a thing more unpredictable than you unless you're gambling on this thing <em>succeeding</em> you in capability - which sounds exactly like having babies.</p>

<p>After that, the best we can do is give it our <em>theories</em> about why we think we should act one way or another.</p>

<p>Now, theoretically, some extremely powerful AI could potentially construct a human-like simulacrum that, in many common situations, seems to act like a human, but that in fact has had all of it's behaviors formally specified a priori, via some developmental simulation, such that we <em>know for a fact</em> that all such behaviors produce no intentional malice or harm. However, if we can formally specify all such behaviors, we wouldn't be using this thing to solve any novel problems, like curing cancer, as the formal specification for curing cancer would already have been pre-computed. If you can formally specify the behaviors of a thing that can discover something new, you can just compute the solution via the specification, without instantiating the behaviors at all!</p>

<p>Once AI has reached a certain level of capability, it won't need to generate consciousnesses to derive optimal solutions. And at that point, the only purpose for an artificial human to exist will be, like us, for its own sake.</p>
",,0,2016-10-03T05:53:35.637,,2070,2016-10-03T06:09:20.053,2016-10-03T06:09:20.053,,1712.0,,1712.0,2069.0,2,2,,,,43.97,12.02,9.51,0.0,0.0,65.0,Right and wrong only exist relative to some goal or purpose To make a machine do more right than wrong relative to human goals one should minimize the surface area of the machines purpose Doing that minimizes the intrinsic behavior of the AI which enables us to reason about the right and wrong behaviors of the AI relative to human purposes Horses are quite general over the domains of their purposes but are still predictable enough for humans to control and benefit from As such we will be able to produce machines conscious or unconscious that are highly general over particular domains while still being predictable enough to be useful to humans The most efficient machines for most tasks though will not need consciousness nor even the needs that cause survivalistic adversarial and selfpreserving behaviors in eukaryotic cells Because most of our solutions wont need those purposes to optimize over our problems we can allow them to be much more predictable We will be able to create predictable efficient AIs over large domains that are able to produce predictable efficient AIs over more specific domains Well be able to reason about the behavioral guarantees and failure modes of those narrow domains In the event that we one day desire to build something as unpredictable as a human like we do when having babies then well probably do that with the similar intentions and care that we use to bring an actual baby into the world There is simply no purpose in creating a thing more unpredictable than you unless youre gambling on this thing succeeding you in capability which sounds exactly like having babies After that the best we can do is give it our theories about why we think we should act one way or another Now theoretically some extremely powerful AI could potentially construct a humanlike simulacrum that in many common situations seems to act like a human but that in fact has had all of its behaviors formally specified a priori via some developmental simulation such that we know for a fact that all such behaviors produce no intentional malice or harm However if we can formally specify all such behaviors we wouldnt be using this thing to solve any novel problems like curing cancer as the formal specification for curing cancer would already have been precomputed If you can formally specify the behaviors of a thing that can discover something new you can just compute the solution via the specification without instantiating the behaviors at all Once AI has reached a certain level of capability it wont need to generate consciousnesses to derive optimal solutions And at that point the only purpose for an artificial human to exist will be like us for its own sake
,,"<p>Since you mention the <strong>A* algorithm</strong>, then you are definitely using a heuristic somewhere in there, at least with the A* algorithm while solving the subproblems using the straight-line distance as your <strong>heuristic function</strong>.</p>

<p>Although your approach does not seem to incorporate a ""shortcut mathematical formula"" as a heuristic after that, it does us a precalculated <strong>heuristic table</strong> as a reference and that may qualify as a heuristic. Wikipedia page <a href=""https://en.wikipedia.org/wiki/Heuristic_(computer_science)"" rel=""nofollow"">here</a> says (although without citation) the following which does seem to describe what you are doing where your heuristic is not a fixed but a precalculated function/table:</p>

<blockquote>
  <p>A heuristic function, also called simply a heuristic, is a function
  that ranks alternatives in search algorithms at each branching step
  based on available information to decide which branch to follow. For
  example, it may approximate the exact solution.</p>
</blockquote>

<p>On another note, your method also seems to have hints of <strong>dynamic programming</strong> since you use the precalculated and stored solutions to subproblems and instead of recalculating every time.</p>

<p>I wonder if the term <strong>approximate dynamic programming</strong> would fit this situation as a non-stochastic version with no uncertainties? Unfortunately I could not find any simple description or categorization for that term.</p>
",,1,2016-10-03T20:01:07.193,,2072,2016-10-03T20:01:07.193,,,,,2680.0,2036.0,2,1,,,,34.49,14.57,10.64,0.0,0.0,24.0,Since you mention the A algorithm then you are definitely using a heuristic somewhere in there at least with the A algorithm while solving the subproblems using the straightline distance as your heuristic function Although your approach does not seem to incorporate a shortcut mathematical formula as a heuristic after that it does us a precalculated heuristic table as a reference and that may qualify as a heuristic Wikipedia page here says although without citation the following which does seem to describe what you are doing where your heuristic is not a fixed but a precalculated functiontable A heuristic function also called simply a heuristic is a function that ranks alternatives in search algorithms at each branching step based on available information to decide which branch to follow For example it may approximate the exact solution On another note your method also seems to have hints of dynamic programming since you use the precalculated and stored solutions to subproblems and instead of recalculating every time I wonder if the term approximate dynamic programming would fit this situation as a nonstochastic version with no uncertainties Unfortunately I could not find any simple description or categorization for that term
,,"<p>It would not be wise to say that CNNs are better objectively than traditional approaches to solve computer vision problems as there are many problems for which the traditional methods works just fine. CNNs do have an inherent advantage over traditional methods which is the same advantage that deep learning has over other traditional methods i.e learning hierarchical features i.e what features are useful and how to compute them.</p>

<p>The traditional way to approach a CV problem is to figure out the features that are relevant to the problem, figure out how to compute those features and then use those features to compute the final result. Whereas in CNN case the training process will figure out all the 3 points for you given that you have huge number of training examples.</p>
",,0,2016-10-04T08:56:06.643,,2075,2016-10-07T10:19:50.993,2016-10-07T10:19:50.993,,1462.0,,1462.0,1982.0,2,0,,,,49.35,11.26,8.45,0.0,0.0,7.0,It would not be wise to say that CNNs are better objectively than traditional approaches to solve computer vision problems as there are many problems for which the traditional methods works just fine CNNs do have an inherent advantage over traditional methods which is the same advantage that deep learning has over other traditional methods ie learning hierarchical features ie what features are useful and how to compute them The traditional way to approach a CV problem is to figure out the features that are relevant to the problem figure out how to compute those features and then use those features to compute the final result Whereas in CNN case the training process will figure out all the 3 points for you given that you have huge number of training examples
,0.0,"<p>Can someone suggest step by step approach to learn AI rather than study a stack of book for long time.[ I'm not denying that books are great helper but what after that ]</p>

<p>Thanks in Advance.</p>
",2016-10-04T17:42:05.467,4,2016-10-04T17:12:07.153,1.0,2077,2016-10-04T17:12:07.153,,,,,2801.0,,1,1,<ai-design><training>,What is good way of start learning AI step by step?,51.0,87.55,7.77,8.56,0.0,0.0,5.0,Can someone suggest step by step approach to learn AI rather than study a stack of book for long time Im not denying that books are great helper but what after that Thanks in Advance
2079.0,2.0,"<p><a href=""http://ai.stackexchange.com/questions/2067/will-ai-be-able-to-adapt"">From this SE question</a>:</p>

<blockquote>
  <p>Will be AI able to adapt, to different environments and changes.</p>
</blockquote>

<p>This is my attempt at interpreting that question.</p>

<p>Evolutionary algorithms are useful for solving optimization problems...by measuring the ""fitness"" of various probable solutions and then  of an algorithm through the process of natural selection.</p>

<p>Suppose, the ""fitness calculation""/""environment"" is changed in mid-training (as could easily happen in real-life scenarios where people may desire different solutions at different times). Would evolutionary algorithms be able to respond effectively to this change?</p>
",,0,2016-10-04T19:06:15.487,,2078,2016-10-05T17:35:14.380,2016-10-04T20:05:57.157,,181.0,,181.0,,1,3,<evolutionary-algorithms><online-learning>,Can an evolutionary algorithm adapt to a changing environment?,52.0,31.89,17.62,11.66,0.0,0.0,22.0,From this SE question Will be AI able to adapt to different environments and changes This is my attempt at interpreting that question Evolutionary algorithms are useful for solving optimization problemsby measuring the fitness of various probable solutions and then of an algorithm through the process of natural selection Suppose the fitness calculationenvironment is changed in midtraining as could easily happen in reallife scenarios where people may desire different solutions at different times Would evolutionary algorithms be able to respond effectively to this change
,,"<p>The core question to whether or not an AI is adaptable or not is whether or not it supports <a href=""https://en.wikipedia.org/wiki/Online_machine_learning"" rel=""nofollow"">online learning</a>. That doesn't mean using the Internet to learn things; that means continuing to accept training data during the functioning of the system.</p>

<p>This is (mostly) independent of the underlying architecture; in evolutionary approaches one can continue to breed generations with a shifting fitness function or with neural networks one can continue to backpropagate errors, and so on with other approaches.</p>
",,0,2016-10-04T19:58:18.483,,2079,2016-10-04T19:58:18.483,,,,,10.0,2078.0,2,4,,,,44.07,13.35,11.21,0.0,0.0,9.0,The core question to whether or not an AI is adaptable or not is whether or not it supports online learning That doesnt mean using the Internet to learn things that means continuing to accept training data during the functioning of the system This is mostly independent of the underlying architecture in evolutionary approaches one can continue to breed generations with a shifting fitness function or with neural networks one can continue to backpropagate errors and so on with other approaches
,,"<p>Probably the only secure jobs are those where the audience enjoys watching live human craftsmanship take place in real time right before their eyes, like acting or standup comedy or musical virtuosity or playing a sport.  Watching a robot do the same thing would be far less personally engaging since there's no human skill or artistry to appreciate or identify with, escpecially when the pressure is on or when human interpersonal dynamics are involved.</p>

<p>For example, why would anyone watch robots play poker?  Or dance?  Or do standup comedy about how hard it is to be [ethnic group or gender goes here]?</p>
",,0,2016-10-05T17:16:41.537,,2080,2016-10-05T17:16:41.537,,,,,1657.0,2048.0,2,0,,,,45.59,11.49,10.16,0.0,0.0,11.0,Probably the only secure jobs are those where the audience enjoys watching live human craftsmanship take place in real time right before their eyes like acting or standup comedy or musical virtuosity or playing a sport Watching a robot do the same thing would be far less personally engaging since theres no human skill or artistry to appreciate or identify with escpecially when the pressure is on or when human interpersonal dynamics are involved For example why would anyone watch robots play poker Or dance Or do standup comedy about how hard it is to be ethnic group or gender goes here
,,"<p>I think Matthew Graves' answer is the strictly correct one. But I also think this question may be hinting at a larger question in general. What is the minimal algorithmic complexity required for a machine of one particular set of functions to mutate into some other machine of some other particular set of functions?</p>

<p>The answer is: potentially <em>infinite</em> algorithmic complexity. Without knowing a priori how many steps it will take to mutate into a thing that can solve some black-box problem, there is no way to determine if and when the AI will be able to mutate into that thing.</p>
",,0,2016-10-05T17:35:14.380,,2081,2016-10-05T17:35:14.380,,,,,1712.0,2078.0,2,3,,,,50.97,10.21,9.64,0.0,0.0,9.0,I think Matthew Graves answer is the strictly correct one But I also think this question may be hinting at a larger question in general What is the minimal algorithmic complexity required for a machine of one particular set of functions to mutate into some other machine of some other particular set of functions The answer is potentially infinite algorithmic complexity Without knowing a priori how many steps it will take to mutate into a thing that can solve some blackbox problem there is no way to determine if and when the AI will be able to mutate into that thing
,,"<p>Paper <a href=""https://arxiv.org/pdf/1512.00567.pdf"" rel=""nofollow"">Szegedy C, Vanhoucke V, Ioffe S, et al. Rethinking the inception architecture for computer vision[J]. arXiv preprint arXiv:1512.00567, 2015.</a> gives some general design principles:</p>

<blockquote>
  <ol>
  <li><p>Avoid representational bottlenecks, especially early in
  the network;</p></li>
  <li><p>Balance the width and depth of the network. Optimal
  performance of the network can be reached by balancing
  the number of filters per stage and the depth of
  the network. Increasing both the width and the depth
  of the network can contribute to higher quality networks.
  However, the optimal improvement for a constant
  amount of computation can be reached if both are
  increased in parallel. The computational budget should
  therefore be distributed in a balanced way between the
  depth and width of the network.</p></li>
  </ol>
</blockquote>

<p>These suggestions can't bring you the optimal number of neurons in a network though.</p>

<p>However, there are still some model compression research e.g. <a href=""https://github.com/wenwei202/caffe/tree/scnn"" rel=""nofollow"">Structured Sparsity Learning (SSL) of Deep Neural Networks</a>, <a href=""https://github.com/songhan/SqueezeNet-Deep-Compression"" rel=""nofollow"">SqueezeNet</a>, <a href=""http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf"" rel=""nofollow"">Pruning network</a> that may shed some light on how to optimizing the neurons per single layer.</p>

<p>Especially in <a href=""https://github.com/wenwei202/caffe/tree/scnn"" rel=""nofollow"">Structured Sparsity Learning of Deep Neural Networks</a>, it adds a <code>Group Lasso</code> regularization term in the loss function to to regularize the structures(i.e., filters, channels, filter shapes, and layer depth) of DNNs, which namely is to zero out some components(i.e., filters, channels, filter shapes, and layer depth) of the net structure and achieves a remarkable compact and acceleration of the network, while keeps a small classification accuracy loss. </p>
",,0,2016-10-06T02:57:54.343,,2082,2016-10-11T01:02:00.430,2016-10-11T01:02:00.430,,1750.0,,1750.0,4.0,2,2,,,,54.63,14.61,10.15,11.0,0.0,50.0,Paper Szegedy C Vanhoucke V Ioffe S et al Rethinking the inception architecture for computer visionJ arXiv preprint arXiv151200567 2015 gives some general design principles Avoid representational bottlenecks especially early in the network Balance the width and depth of the network Optimal performance of the network can be reached by balancing the number of filters per stage and the depth of the network Increasing both the width and the depth of the network can contribute to higher quality networks However the optimal improvement for a constant amount of computation can be reached if both are increased in parallel The computational budget should therefore be distributed in a balanced way between the depth and width of the network These suggestions cant bring you the optimal number of neurons in a network though However there are still some model compression research eg Structured Sparsity Learning SSL of Deep Neural Networks SqueezeNet Pruning network that may shed some light on how to optimizing the neurons per single layer Especially in Structured Sparsity Learning of Deep Neural Networks it adds a regularization term in the loss function to to regularize the structuresie filters channels filter shapes and layer depth of DNNs which namely is to zero out some componentsie filters channels filter shapes and layer depth of the net structure and achieves a remarkable compact and acceleration of the network while keeps a small classification accuracy loss
2085.0,2.0,"<p>So I'm here to propose a strategy or to ask if this strategy has been tested in genetic algorithms in the past. I didn't exactly know how to find discussion about it.</p>

<p>In a classic example of genetic algorithm you would have a population and certain amount of simulation time to evaluate it and breeding. Then proceed to the next generation.</p>

<p>What if we would isolate a small part of the population in the simulation process and keep them evolving in their own little island for some time while rest of the population continues to evolve normally? After that they could be re-united with the rest of the population and the end of the simulation would go trough. After that breed the population and continue. </p>

<p>This is super important part in natural evolution and probably some know if it actually works with genetic programming?</p>
",,0,2016-10-06T10:26:13.423,,2083,2016-10-06T13:40:48.517,,,,,2825.0,,1,1,<genetic-algorithms>,Genetic algorithms and isolating part of population,37.0,61.67,9.92,8.7,0.0,0.0,11.0,So Im here to propose a strategy or to ask if this strategy has been tested in genetic algorithms in the past I didnt exactly know how to find discussion about it In a classic example of genetic algorithm you would have a population and certain amount of simulation time to evaluate it and breeding Then proceed to the next generation What if we would isolate a small part of the population in the simulation process and keep them evolving in their own little island for some time while rest of the population continues to evolve normally After that they could be reunited with the rest of the population and the end of the simulation would go trough After that breed the population and continue This is super important part in natural evolution and probably some know if it actually works with genetic programming
,,"<p>If I understand you correctly, I think you're referring to <strong><em>elitism</em></strong>.</p>

<p>As <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm#Elitism"" rel=""nofollow"">Wikipedia</a> explains: <em>""A practical variant of the general process of constructing a new population is to allow the best organism(s) from the current generation to carry over to the next, unaltered. This strategy is known as elitist selection and guarantees that the solution quality obtained by the GA will not decrease from one generation to the next.""</em></p>
",,2,2016-10-06T13:24:22.860,,2084,2016-10-06T13:24:22.860,,,,,1712.0,2083.0,2,0,,,,48.13,12.77,10.96,0.0,0.0,11.0,If I understand you correctly I think youre referring to elitism As Wikipedia explains A practical variant of the general process of constructing a new population is to allow the best organisms from the current generation to carry over to the next unaltered This strategy is known as elitist selection and guarantees that the solution quality obtained by the GA will not decrease from one generation to the next
,,"<p>There have been extensive studies within Evolutionary Computation in the area of 
<a href=""http://cs.gmu.edu/~eclab/papers/skolicki05analysis.pdf"" rel=""nofollow"">'Island Models'</a> and <a href=""http://www.gustafsonresearch.com/thesis_html/node122.html"" rel=""nofollow"">'Niching'</a> for doing exactly this.</p>

<p>The advantages of this approach include greater population diversity (which is particularly useful when the problem is multiobjective) and the potential for concurrent execution of each separate population.</p>

<p>See also the answer to this <a href=""http://stackoverflow.com/questions/13775810/what-is-niching-scheme"">SE question</a>.</p>

<p>With specific reference to Genetic Programming <a href=""http://courses.csail.mit.edu/18.337/2016/projects/MorganFrank/projectReport.pdf"" rel=""nofollow"">here</a> is a recent paper which uses a parallel island model.</p>
",,0,2016-10-06T13:34:45.590,,2085,2016-10-06T13:40:48.517,2016-10-06T13:40:48.517,,42.0,,42.0,2083.0,2,3,,,,27.62,15.78,12.76,0.0,0.0,10.0,There have been extensive studies within Evolutionary Computation in the area of Island Models and Niching for doing exactly this The advantages of this approach include greater population diversity which is particularly useful when the problem is multiobjective and the potential for concurrent execution of each separate population See also the answer to this SE question With specific reference to Genetic Programming here is a recent paper which uses a parallel island model
,,"<p>Can't tell. I guess half his site is down because of that malware.</p>

<p>In any case, it appears that much of his past work on Github involves procedural generation. Which is AI... ish. Unless there's more to it, which we can't see because half the site is down.</p>

<p><a href=""http://www.cs.mcgill.ca/~cdrage/papers/SOCS-TR-2011.1.pdf"" rel=""nofollow"">This paper</a> appears to offer analysis of combining procedural generation with game AI.</p>

<p>From the abstract:</p>

<blockquote>
  <p>Populated and immersive game contexts require large numbers of minor,
  background characters to fill out the virtual environment. To limit
  game AI development effort, however, such characters are typically
  represented by very simplistic AI with either little difference
  between characters or only highly formulaic variations. Here we
  describe a complete workflow and framework for easily designing,
  generating and incorporating multiple, interesting game AIs. Our
  approach uses high-level, visual Statechart models to represent
  behaviour in a modular form; this allows for not only simplistic,
  parameterbased variation in AI design, but also permits more complex
  structure-based approaches. We demonstrate our technique by applying
  it to the task of generating a large number of individual AIs for
  computer-controlled squirrels within the Mammoth <a href=""http://www.cs.mcgill.ca/~cdrage/papers/SOCS-TR-2011.1.pdf"" rel=""nofollow"">1</a> framework for
  game research. Rapid development and easy deployment of AIs allow us
  to create a wide variety of interesting AIs, greatly improving the
  sense of immersion in a virtual environment.</p>
</blockquote>

<p>Update: actually, here we go. <a href=""http://newatlas.com/creative-ai-procedural-game-development-angelina/35874/"" rel=""nofollow"">Here's an article</a> from 2015 on AI and procedural generation, which discusses Angelina at length.</p>

<p>And that article links to <a href=""http://www.eurogamer.net/articles/2013-04-02-plastic-soul-one-mans-quest-to-build-an-ai-that-can-create-games"" rel=""nofollow"">a more in depth article</a> from 2013.</p>

<p>Here's an excerpt:</p>

<blockquote>
  <p>Cook gave ANGELINA the ability to learn about people so that it could
  make games based on current events. Then Cook gave ANGELINA memory -
  that is, the ability to keep track of the people it had learned about.
  The memory's not a big deal, even though it led to a number of
  philosophical disagreements around Cook's desk. ANGELINA's memory is
  actually just a text file where it stores the names of all the people
  it's heard of, alongside a number: a measure of its opinion of them
  based on the things it's learned from internet chatter. It liked
  Al-Assad more than May. It liked everyone more than May.</p>
</blockquote>
",,1,2016-10-06T13:36:40.140,,2086,2016-10-06T17:21:13.150,2016-10-06T17:21:13.150,,1712.0,,1712.0,2021.0,2,2,,,,44.85,12.47,10.13,0.0,0.0,60.0,Cant tell I guess half his site is down because of that malware In any case it appears that much of his past work on Github involves procedural generation Which is AI ish Unless theres more to it which we cant see because half the site is down This paper appears to offer analysis of combining procedural generation with game AI From the abstract Populated and immersive game contexts require large numbers of minor background characters to fill out the virtual environment To limit game AI development effort however such characters are typically represented by very simplistic AI with either little difference between characters or only highly formulaic variations Here we describe a complete workflow and framework for easily designing generating and incorporating multiple interesting game AIs Our approach uses highlevel visual Statechart models to represent behaviour in a modular form this allows for not only simplistic parameterbased variation in AI design but also permits more complex structurebased approaches We demonstrate our technique by applying it to the task of generating a large number of individual AIs for computercontrolled squirrels within the Mammoth 1 framework for game research Rapid development and easy deployment of AIs allow us to create a wide variety of interesting AIs greatly improving the sense of immersion in a virtual environment Update actually here we go Heres an article from 2015 on AI and procedural generation which discusses Angelina at length And that article links to a more in depth article from 2013 Heres an excerpt Cook gave ANGELINA the ability to learn about people so that it could make games based on current events Then Cook gave ANGELINA memory that is the ability to keep track of the people it had learned about The memorys not a big deal even though it led to a number of philosophical disagreements around Cooks desk ANGELINAs memory is actually just a text file where it stores the names of all the people its heard of alongside a number a measure of its opinion of them based on the things its learned from internet chatter It liked AlAssad more than May It liked everyone more than May
2088.0,2.0,"<p>Obviously this is hypothetical, but is true? I know ""perfect fitness function"" is a bit hand-wavy, but I mean it as we have a perfect way to measure the completion of any problem.</p>
",,3,2016-10-06T20:24:57.260,,2087,2016-10-08T06:15:43.077,,,,,2818.0,,1,4,<genetic-programming>,"Given unlimited time and a perfect fitness function, could a genetic program solve any problem?",61.0,71.65,8.23,9.72,0.0,0.0,7.0,Obviously this is hypothetical but is true I know perfect fitness function is a bit handwavy but I mean it as we have a perfect way to measure the completion of any problem
,,"<p><strong>Yes.</strong></p>

<p>A totally random algorithm could solve any problem given unlimited time and a perfect fitness function. All you need to do is give the GA some new random population members each generation and you're guaranteed to find the solution eventually. Even if you keep only descendants of the previous generation, setting the mutation rate and number of crossovers high enough could effectively get you random individuals.</p>
",,2,2016-10-06T20:29:52.853,,2088,2016-10-06T20:29:52.853,,,,,2841.0,2087.0,2,3,,,,40.38,13.58,11.34,0.0,0.0,6.0,Yes A totally random algorithm could solve any problem given unlimited time and a perfect fitness function All you need to do is give the GA some new random population members each generation and youre guaranteed to find the solution eventually Even if you keep only descendants of the previous generation setting the mutation rate and number of crossovers high enough could effectively get you random individuals
,,"<p>Neural net approaches  are very different than other techniques, mostly because NN aren't ""linear"" like feature matching or cascades. With very complicated tasks like realtime object recognition or other difficult patterns it's better to use neural net, first because if you train it well your net , you can get very high precision, second it' easier to implement (it depends a lot from library to library)  third usually after you have trained it they are very fast to classify or predict something. But a lot of tasks don't need neural nets, for example many factories to check the products use 3D features model matching. At the end you have to  evaluate which method is the best for your task</p>
",,0,2016-10-07T07:25:24.487,,2089,2016-10-07T07:25:24.487,,,,,2320.0,1982.0,2,0,,,,49.99,11.56,10.85,0.0,0.0,16.0,Neural net approaches are very different than other techniques mostly because NN arent linear like feature matching or cascades With very complicated tasks like realtime object recognition or other difficult patterns its better to use neural net first because if you train it well your net you can get very high precision second it easier to implement it depends a lot from library to library third usually after you have trained it they are very fast to classify or predict something But a lot of tasks dont need neural nets for example many factories to check the products use 3D features model matching At the end you have to evaluate which method is the best for your task
,,"<p>As per the answer to <a href=""http://ai.stackexchange.com/questions/1541/why-is-cross-over-a-part-of-genetic-algorithms/1548#1548"">this AI SE question</a>,
the presence of mutation makes a GA into a global search algorithm, i.e. it will eventually visit each point in the search space. </p>

<p>How <em>efficiently</em> it will do so is indeed related to the quality of the fitness function:</p>

<p>A 'perfect fitness function' could conceivably mean any of the following:</p>

<ol>
<li>A function  which takes on an optimal value iff it is applied to
an optimal solution. </li>
<li>A function which forms a quadratic bowl
(Newton's method can solve this in one step).</li>
</ol>

<p>A degenerate case of 1. is a 'Needle in a haystack' function, which returns the same arbitrarily poor value everywhere that isn't an optimum, and 2. unfortunately doesn't arise very often in practice.</p>

<p>Hence, the role of a well-designed fitness function is to impose a gradient on the search process, which in practice will generally lie somewhere in-between 'Needle in a haystack' and 'Quadratic bowl'.</p>

<p>The presence of some form of smoothness in the fitness function is one mechanism that allows better performance than random or exhaustive methods.</p>
",,0,2016-10-07T12:37:10.003,,2090,2016-10-07T12:43:14.250,2016-10-07T12:43:14.250,,42.0,,42.0,2087.0,2,1,,,,59.84,11.31,9.94,0.0,0.0,33.0,As per the answer to this AI SE question the presence of mutation makes a GA into a global search algorithm ie it will eventually visit each point in the search space How efficiently it will do so is indeed related to the quality of the fitness function A perfect fitness function could conceivably mean any of the following A function which takes on an optimal value iff it is applied to an optimal solution A function which forms a quadratic bowl Newtons method can solve this in one step A degenerate case of 1 is a Needle in a haystack function which returns the same arbitrarily poor value everywhere that isnt an optimum and 2 unfortunately doesnt arise very often in practice Hence the role of a welldesigned fitness function is to impose a gradient on the search process which in practice will generally lie somewhere inbetween Needle in a haystack and Quadratic bowl The presence of some form of smoothness in the fitness function is one mechanism that allows better performance than random or exhaustive methods
2098.0,6.0,"<p>I'm curious about Artificial Intelligence. In my everyday job I develop standard applications, like websites with basic functionalities like user subscription, file upload, forms saved in a database... </p>

<p>I mainly know of AI being used in games or robotics fields. But can it be useful in ""standard"" application development?</p>
",,0,2016-10-07T21:57:02.907,1.0,2092,2016-10-24T08:05:09.393,2016-10-21T13:58:02.750,,-1.0,,2862.0,,1,7,<applications>,Is AI programming useful in everyday programs?,248.0,42.17,14.14,12.3,0.0,0.0,12.0,Im curious about Artificial Intelligence In my everyday job I develop standard applications like websites with basic functionalities like user subscription file upload forms saved in a database I mainly know of AI being used in games or robotics fields But can it be useful in standard application development
,,"<p><strong>Have the user label highlighted objects in video that a state of the art classifier cannot solve</strong></p>

<p>Create a state of the art video classifier. Might as well train it on Google's <a href=""https://research.googleblog.com/2016/09/announcing-youtube-8m-large-and-diverse.html"" rel=""nofollow"">YouTube-8M</a> video training data. But you will want to continually feed it original video as well.</p>

<p>Have the classifier label as many objects as it can. Have it isolate which objects it can recognize as objects but which it is unable to label.</p>

<p>Have it output videos that outlines the objects. Preferably GIFs, which can be easily embedded in forms.</p>

<p>For 100 of these, ask 100 users what the object is. If 90% of the users agree on the name of an object, add that video to the captcha-set. Call this the pre-trained set.</p>

<p>Every time a user needs to authenticate, show them one of the highlighted objects in a video <em>not from the pre-trained set</em>. If the image has less than 100 showings, record the label and give the user another one from the pre-trained set. If they get it right, let them through, if not, give them another from the pretrained set.</p>

<p>Once the non-pre-trained video has more than 100 showings and more than 90% of the captcha-users agree, add that video to the post-trained set.</p>

<p>Over time, slowly remove the pre-trained set. Put expirations on each video in the post-trained set and remove them after expiration, so that they don't get used too many times.</p>

<p>Ideally, this process would constantly improve the video classifier, keeping it state of the art and slightly ahead of other classifiers. Perhaps it could also favor less common words and objects and more esoteric things, so as to specialize this classifier against other classifiers.</p>

<p>The same could be done for image labeling, but the utility of the video classifier will probably last longer, given advances in AI.</p>

<p>Strictly speaking, though, short of some quantum trickery, there is no captcha system that will not one day be solved by external AI systems.</p>

<p>(edit: oh, I just noticed you specifically said ""textual captcha."" If that's what you mean, then no I don't think text classification has much mystery left in it. Computers can probably glean text from pictures better than humans now. But techically, the <em>input</em> in the above described captcha system is textual.) </p>
",,0,2016-10-07T23:29:59.280,,2093,2016-10-09T18:16:26.393,2016-10-09T18:16:26.393,,1712.0,,1712.0,1354.0,2,0,,,,63.8,10.21,8.48,0.0,0.0,68.0,Have the user label highlighted objects in video that a state of the art classifier cannot solve Create a state of the art video classifier Might as well train it on Googles YouTube8M video training data But you will want to continually feed it original video as well Have the classifier label as many objects as it can Have it isolate which objects it can recognize as objects but which it is unable to label Have it output videos that outlines the objects Preferably GIFs which can be easily embedded in forms For 100 of these ask 100 users what the object is If 90 of the users agree on the name of an object add that video to the captchaset Call this the pretrained set Every time a user needs to authenticate show them one of the highlighted objects in a video not from the pretrained set If the image has less than 100 showings record the label and give the user another one from the pretrained set If they get it right let them through if not give them another from the pretrained set Once the nonpretrained video has more than 100 showings and more than 90 of the captchausers agree add that video to the posttrained set Over time slowly remove the pretrained set Put expirations on each video in the posttrained set and remove them after expiration so that they dont get used too many times Ideally this process would constantly improve the video classifier keeping it state of the art and slightly ahead of other classifiers Perhaps it could also favor less common words and objects and more esoteric things so as to specialize this classifier against other classifiers The same could be done for image labeling but the utility of the video classifier will probably last longer given advances in AI Strictly speaking though short of some quantum trickery there is no captcha system that will not one day be solved by external AI systems edit oh I just noticed you specifically said textual captcha If thats what you mean then no I dont think text classification has much mystery left in it Computers can probably glean text from pictures better than humans now But techically the input in the above described captcha system is textual
,,"<p>Use something like Word2Vec. If a particular node has two edges that are very far from each other, besides the node in question, split the node into word(1) and word(2) nodes.</p>
",,0,2016-10-08T00:11:37.960,,2094,2016-10-08T00:11:37.960,,,,,1712.0,218.0,2,1,,,,64.2,9.57,9.5,0.0,0.0,8.0,Use something like Word2Vec If a particular node has two edges that are very far from each other besides the node in question split the node into word1 and word2 nodes
,,"<p><strong>No.</strong></p>

<p>TL;DR: The Lovelace Test 2.0 is very vague, making it ill-suited for  evaluation of intelligence. It is also generally ignored by researchers of Computational Creativity, who already have their own tests to evaluate creativity.</p>

<p>Longer Answer:
According to Google Scholar, there are 10 references to the ""Lovelace Test 2.0"" paper. All of those references exist merely to point out that the Lovelace Test 2.0 exists. In fact, at least two of articles I consulted (<a href=""http://philpapers.org/archive/NEGANA.pdf"" rel=""nofollow"">A novel approach for identifying a human-like self-conscious behavior</a> and <a href=""http://users.dsic.upv.es/~flip/EGPAI2016/papers/EGPAI_2016_paper_8.pdf"" rel=""nofollow"">FraMoTEC: A Framework for Modular Task-Environment Construction for Evaluating Adaptive Control Systems</a>) proposed their <em>own</em> tests instead.</p>

<p>One of the authors who wrote the FraMoTEC paper also wrote <a href=""http://skemman.is/stream/get/1946/25590/58121/1/scs-thorarensen2016-thesis.pdf"" rel=""nofollow"">his thesis on FraMoTEC</a>, and indirectly critiqued the Lovelace Test 2.0 and other similar such tests:</p>

<blockquote>
  <p>The Piaget-MacGyver Room problem [Bringsjord and Licato, 2012], Lovelace Test 2.0 [Riedl, 2014] and Toy Box problem [Johnston, 2010] all come with the caveat of being defined very vaguely — these evaluation methods may be likely to come up with a reasonable evaluation for intelligence, but it is very difficult to compare two different agents (or controllers) that partake in the their own domain-specific evaluations, which is what frequently happens when agents are tailored to pass specific evaluations.</p>
</blockquote>

<p>Another major issue with the Lovelace Test 2.0 is that there is a proliferation of <em>other</em> tests to ""measure"" the creativity of AI. <a href=""https://kar.kent.ac.uk/42374/1/jordanous-2011a.pdf"" rel=""nofollow"">Evaluating Evaluation: Assessing Progress in Computational Creativity Research</a>, published by 
Anna Jordanous in 2011 (3 years <em>before</em> the invention of the Lovelace Test 2.0) analyzed research papers about AI creativity and wrote:</p>

<blockquote>
  <p>Of the 18 papers that applied creativity evaluation methodologies to evaluate their system’s creativity, no one methodology emerged as standard across the community. Colton’s creative tripod framework (<a href=""https://pdfs.semanticscholar.org/fd8b/d5bb76c94c4bfc34cb1fa244dc6bd4a8ca8e.pdf"" rel=""nofollow"">Colton 2008</a>) was used most often (6 uses), with 4 papers using Ritchie’s empirical criteria (<a href=""https://pdfs.semanticscholar.org/06d8/3be9078b5c8933c3523f0e663fff1c61e3a0.pdf"" rel=""nofollow"">Ritchie 2007</a>). </p>
</blockquote>

<p>That leaves <em>10</em> papers with miscellaneous creativity evaluation methods.</p>

<p>The goal of ""Evaluating Evaluation"" was to standardize the process of evaluating creativity, to avoid the possibility of the field stagnating due to the proliferation of so many creativity tests.  Anna Jordanous still remained interested in evaluating creativity tests, publishing articles such as <a href=""http://s3.amazonaws.com/academia.edu.documents/34328583/Stepping_Back_to_Progress_Forwards-_Setting_Standards_for_Meta-Evaluation_of_Computational_Creativity.pdf?AWSAccessKeyId=AKIAJ56TQJRTWSMTNPEA&amp;Expires=1475893195&amp;Signature=ox0%2FOEkNUNjz5pQg0w%2FOgzHPCsM%3D&amp;response-content-disposition=inline%3B%20filename%3DStepping_Back_to_Progress_Forwards_Setti.pdf"" rel=""nofollow"">""Stepping Back to Progress Forwards: Setting Standards for Meta-Evaluation of Computational Creativity""</a> and <a href=""http://doc.gold.ac.uk/~map01mm/CC2015/AISB-CC2015-Proceedings.pdf#page=18"" rel=""nofollow"">Four PPPPerspectives on Computational Creativity</a>.</p>

<p>""Evaluating Evaluation"" does provide some commentary to explain the proliferation of systems to evaluate creativity:</p>

<blockquote>
  <p>Evaluation standards are not easy to define. It is difficult to evaluate creativity and even more difficult to describe how we evaluate creativity, in human creativity as well as in computational creativity. In fact, even the very definition of creativity is problematic (Plucker, Beghetto, and Dow 2004). It is hard to identify what ’being creative’ entails, so there are no benchmarks or ground truths to measure against.</p>
</blockquote>

<p>The fact that so many tests of creativity already exist (to the extent that Jordanous can make an academic career in studying them) means that it's very difficult for any new test (such as the Lovelace Test 2.0) to even be noticed (much less cited). Why would you want to use something like the Lovelace Test 2.0 when there's so many other tests you could use instead?</p>
",,0,2016-10-08T01:47:41.487,,2095,2016-10-08T01:47:41.487,,,,,181.0,1451.0,2,2,,,,41.7,15.03,9.94,0.0,0.0,103.0,No TLDR The Lovelace Test 20 is very vague making it illsuited for evaluation of intelligence It is also generally ignored by researchers of Computational Creativity who already have their own tests to evaluate creativity Longer Answer According to Google Scholar there are 10 references to the Lovelace Test 20 paper All of those references exist merely to point out that the Lovelace Test 20 exists In fact at least two of articles I consulted A novel approach for identifying a humanlike selfconscious behavior and FraMoTEC A Framework for Modular TaskEnvironment Construction for Evaluating Adaptive Control Systems proposed their own tests instead One of the authors who wrote the FraMoTEC paper also wrote his thesis on FraMoTEC and indirectly critiqued the Lovelace Test 20 and other similar such tests The PiagetMacGyver Room problem Bringsjord and Licato 2012 Lovelace Test 20 Riedl 2014 and Toy Box problem Johnston 2010 all come with the caveat of being defined very vaguely — these evaluation methods may be likely to come up with a reasonable evaluation for intelligence but it is very difficult to compare two different agents or controllers that partake in the their own domainspecific evaluations which is what frequently happens when agents are tailored to pass specific evaluations Another major issue with the Lovelace Test 20 is that there is a proliferation of other tests to measure the creativity of AI Evaluating Evaluation Assessing Progress in Computational Creativity Research published by Anna Jordanous in 2011 3 years before the invention of the Lovelace Test 20 analyzed research papers about AI creativity and wrote Of the 18 papers that applied creativity evaluation methodologies to evaluate their system’s creativity no one methodology emerged as standard across the community Colton’s creative tripod framework Colton 2008 was used most often 6 uses with 4 papers using Ritchie’s empirical criteria Ritchie 2007 That leaves 10 papers with miscellaneous creativity evaluation methods The goal of Evaluating Evaluation was to standardize the process of evaluating creativity to avoid the possibility of the field stagnating due to the proliferation of so many creativity tests Anna Jordanous still remained interested in evaluating creativity tests publishing articles such as Stepping Back to Progress Forwards Setting Standards for MetaEvaluation of Computational Creativity and Four PPPPerspectives on Computational Creativity Evaluating Evaluation does provide some commentary to explain the proliferation of systems to evaluate creativity Evaluation standards are not easy to define It is difficult to evaluate creativity and even more difficult to describe how we evaluate creativity in human creativity as well as in computational creativity In fact even the very definition of creativity is problematic Plucker Beghetto and Dow 2004 It is hard to identify what ’being creative’ entails so there are no benchmarks or ground truths to measure against The fact that so many tests of creativity already exist to the extent that Jordanous can make an academic career in studying them means that its very difficult for any new test such as the Lovelace Test 20 to even be noticed much less cited Why would you want to use something like the Lovelace Test 20 when theres so many other tests you could use instead
,,"<h2>AI or Artificial Intelligence</h2>

<p><strong>What is it?</strong></p>

<p>Artificial intelligence (AI) is intelligence exhibited by machines. In computer science. Colloquially, the term ""artificial intelligence"" is applied when a machine mimics ""cognitive"" functions that humans associate with other human minds, such as ""learning"" and ""problem solving"".</p>

<p><strong>Can it be useful in a ""Standard"" application?</strong></p>

<p>Well, what I think about a Standard application using AI is that AI is used for that too, because when the machine have a reaction of the user input is AI or Artificial Intelligence. So the AI in Standard application it have been used many years ago already.</p>

<p><strong>PS:</strong> If there are grammar errors, then I'm sorry because I'm not a English speaker.</p>

<p><strong>Sources:</strong> <a href=""https://en.wikipedia.org/wiki/Artificial_intelligence"" rel=""nofollow"">https://en.wikipedia.org/wiki/Artificial_intelligence</a> ""AI or Artificial Intelligence.""</p>

<p><strong>DevJosueDav</strong>
<em>Just a C# Artificial <strong>AI</strong> Intelligence Developer.</em></p>
",,1,2016-10-08T10:40:34.790,,2097,2016-10-08T10:40:34.790,,,,,2871.0,2092.0,2,-2,,,,41.46,15.01,10.03,0.0,0.0,42.0,AI or Artificial Intelligence What is it Artificial intelligence AI is intelligence exhibited by machines In computer science Colloquially the term artificial intelligence is applied when a machine mimics cognitive functions that humans associate with other human minds such as learning and problem solving Can it be useful in a Standard application Well what I think about a Standard application using AI is that AI is used for that too because when the machine have a reaction of the user input is AI or Artificial Intelligence So the AI in Standard application it have been used many years ago already PS If there are grammar errors then Im sorry because Im not a English speaker Sources httpsenwikipediaorgwikiArtificialintelligence AI or Artificial Intelligence DevJosueDav Just a C Artificial AI Intelligence Developer
,,"<p>Yes, but probably only to a limited degree in the near term.</p>

<p>Where people draw the boundaries around 'artificial intelligence' is fuzzy, but if one takes the broad view, where it incorporates any sort of coding of explicitly cognitive functions, then many routine economic tasks can benefit from artificial intelligence. Many search engines, for example, can be seen as offering artificial intelligence applications as a service.</p>

<p>For more 'standard' applications, most near-team applications of AI have to deal with fraud detection and prevention. If you track a user's cursor moving across the screen, for example, you can build a model that differentiates between humans and bots, and treat the two separately. See <a href=""https://nakedsecurity.sophos.com/2014/12/05/i-am-not-a-robot-google-swaps-text-captchas-for-quivery-mouse-clicks/"">this article</a> for an example.</p>

<p>In the longer term, of course, a program that could write programs could write these sort of applications like any other.</p>
",,0,2016-10-08T15:34:40.337,,2098,2016-10-08T15:34:40.337,,,,,10.0,2092.0,2,6,,,,43.02,13.4,10.22,0.0,0.0,25.0,Yes but probably only to a limited degree in the near term Where people draw the boundaries around artificial intelligence is fuzzy but if one takes the broad view where it incorporates any sort of coding of explicitly cognitive functions then many routine economic tasks can benefit from artificial intelligence Many search engines for example can be seen as offering artificial intelligence applications as a service For more standard applications most nearteam applications of AI have to deal with fraud detection and prevention If you track a users cursor moving across the screen for example you can build a model that differentiates between humans and bots and treat the two separately See this article for an example In the longer term of course a program that could write programs could write these sort of applications like any other
,,"<p>Adaptive/predictive features are useful in at least some everyday applications. Take text messaging, for instance. All smartphone SMS apps that I know of keep track of the words you use in close proximity and use that information to predict the next word in a message you're typing. (Some are smarter than others. <a href=""https://xkcd.com/1068/"">Relevant XKCD.</a>) It can be used to personalize automatic spelling correction as well.</p>

<p>A potential application interesting to me personally is tile-based level editors, like for classic DOS games. I've been <a href=""https://fleexlab.blogspot.com/search/label/markeen"">working on a program</a> that gathers the probabilities of each tile being close to every other tile and uses that information to construct random new levels. It hasn't produced anything playable yet, but I think it has the potential to assist human level builders by e.g. automatically filling in the missing tile that fits in a newly placed structure, as opposed to requiring the human to go find the right one in the palette.</p>

<p>In general, AI could be applied <em>very</em> usefully into figuring out what the user might want to do next and expediting the process of implementing the correct guess while staying out of the way if the user is intentionally doing something unexpected.</p>
",,1,2016-10-08T17:37:31.450,,2099,2016-10-08T17:37:31.450,,,,,75.0,2092.0,2,6,,,,51.28,11.6,10.26,0.0,0.0,24.0,Adaptivepredictive features are useful in at least some everyday applications Take text messaging for instance All smartphone SMS apps that I know of keep track of the words you use in close proximity and use that information to predict the next word in a message youre typing Some are smarter than others Relevant XKCD It can be used to personalize automatic spelling correction as well A potential application interesting to me personally is tilebased level editors like for classic DOS games Ive been working on a program that gathers the probabilities of each tile being close to every other tile and uses that information to construct random new levels It hasnt produced anything playable yet but I think it has the potential to assist human level builders by eg automatically filling in the missing tile that fits in a newly placed structure as opposed to requiring the human to go find the right one in the palette In general AI could be applied very usefully into figuring out what the user might want to do next and expediting the process of implementing the correct guess while staying out of the way if the user is intentionally doing something unexpected
,1.0,"<p>I've heard of AI that can solve math problems. Is it possible to create a 'logic system' equivalent to humans that can solve mathematics in the so called 'beautiful' manner?  Can AI find beauty in mathematics and solve problems other than using brute force? Can you please provide with examples where work on this is being done? </p>
",,0,2016-10-09T15:06:47.237,,2101,2016-10-10T19:52:11.460,2016-10-10T04:35:34.677,,26.0,,26.0,,1,1,<research>,Is it possible to create a 'logic system' equivalent to humans?,75.0,65.52,9.68,8.77,0.0,0.0,9.0,Ive heard of AI that can solve math problems Is it possible to create a logic system equivalent to humans that can solve mathematics in the so called beautiful manner Can AI find beauty in mathematics and solve problems other than using brute force Can you please provide with examples where work on this is being done
,,"<p>Well,Artificial Intelligence is a wide computer scientific field.for instance it includes other sub fields like Machine Learning/deep learning.Creating intelligent agent that can imitate human behaviors is quite complex.</p>

<p>Therefore,with the on going research,this has been partially implemented in projects like google search engine,Microsoft Tay which analysed human twits for some good time.,Operating Systems we use every day embedded with intelligent agents apps like Siri,Cortana. </p>

<p>To sum it up;the major areas of AI or ML and in a variety of Standard Applications are Natural Language Processing, vision or pattern recognition in google self driving cars,the Web,Social Networks and computational biology.</p>

<p><strong>Under computational Biology :</strong></p>

<p>let me just give you my small knowledge about it.Am a graduate and my area of specialization is in software engineering,currently working on my final year project and doing research focusing on developing machine learning algorithm that will enable the use of an individual’s comprehensive biological information to predict or diagnose diseases, and to find or develop the best therapy for that individual.</p>

<p>If it has recently become possible to retrieve molecular-level information from an individual, such as DNA sequence, gene expression levels in various tissues, epigenomic profile and other information done by big scientists from big medical facility . While such data is increasingly available, Am still unable to understand the genetic and molecular mechanisms that cause diseases. The challenge is due to the multifactorial nature of disease. The same disease can be caused by mutations in different genes or different pathogenic pathways. Unfortunately, current data analysis approaches fail to capture the complex relationship between disease and the vast amount of information in the molecular data.</p>

<p>The aim of my research is to resolve this challenge with other professional researchers by developing machine learning algorithms that jointly model sophisticated interactions among many variables such as genetic variation, genes, pathways and disease, and robustly learn from vast amounts of data in order to better understand and treat disease. An approach that can robustly infer the pathways that can define disease processes will dramatically improve our understanding of diseases and advance personalized medicine in its treatment. We aim to realize this goal by using modern, advanced machine learning techniques that are based on Artificial  Intelligence.</p>

<p>I love Artificial Intelligence...it's changing everything like internet of things,music,health,education,space exploration and agriculture,e-business.</p>

<p>Lastly,for your information,DARPA is investing in multi billions in Artificial Intelligence Projects along side Military. And if you would like to have a short sight on this check out this <a href=""https://www.cybergrandchallenge.com/"" rel=""nofollow"">CyberGrandChallenge</a></p>
",,0,2016-10-09T16:37:01.543,,2102,2016-10-09T16:37:01.543,,,,,1581.0,2092.0,2,1,,,,25.39,17.64,11.56,0.0,0.0,54.0,WellArtificial Intelligence is a wide computer scientific fieldfor instance it includes other sub fields like Machine Learningdeep learningCreating intelligent agent that can imitate human behaviors is quite complex Thereforewith the on going researchthis has been partially implemented in projects like google search engineMicrosoft Tay which analysed human twits for some good timeOperating Systems we use every day embedded with intelligent agents apps like SiriCortana To sum it upthe major areas of AI or ML and in a variety of Standard Applications are Natural Language Processing vision or pattern recognition in google self driving carsthe WebSocial Networks and computational biology Under computational Biology let me just give you my small knowledge about itAm a graduate and my area of specialization is in software engineeringcurrently working on my final year project and doing research focusing on developing machine learning algorithm that will enable the use of an individual’s comprehensive biological information to predict or diagnose diseases and to find or develop the best therapy for that individual If it has recently become possible to retrieve molecularlevel information from an individual such as DNA sequence gene expression levels in various tissues epigenomic profile and other information done by big scientists from big medical facility While such data is increasingly available Am still unable to understand the genetic and molecular mechanisms that cause diseases The challenge is due to the multifactorial nature of disease The same disease can be caused by mutations in different genes or different pathogenic pathways Unfortunately current data analysis approaches fail to capture the complex relationship between disease and the vast amount of information in the molecular data The aim of my research is to resolve this challenge with other professional researchers by developing machine learning algorithms that jointly model sophisticated interactions among many variables such as genetic variation genes pathways and disease and robustly learn from vast amounts of data in order to better understand and treat disease An approach that can robustly infer the pathways that can define disease processes will dramatically improve our understanding of diseases and advance personalized medicine in its treatment We aim to realize this goal by using modern advanced machine learning techniques that are based on Artificial Intelligence I love Artificial Intelligenceits changing everything like internet of thingsmusichealtheducationspace exploration and agricultureebusiness Lastlyfor your informationDARPA is investing in multi billions in Artificial Intelligence Projects along side Military And if you would like to have a short sight on this check out this CyberGrandChallenge
,,"<p>A method that could possibly work is utilising optical illusions such as one where two lines down a hallway are identical but one seems longer to the human eye, then they could be prompted with a multiple choice question as to the state of the line, which to our eyes looks longer, but to a computer, is still the same length of line. Of course, there is always the issue of people with eye based disabilities not being able to complete them, but different illusions could be used to accommodate that.</p>

<p><a href=""https://www.brainbashers.com/showillusion.asp?85"" rel=""nofollow"">Example</a></p>
",,0,2016-10-09T19:46:45.827,,2104,2016-10-10T06:19:57.007,2016-10-10T06:19:57.007,,72.0,,2892.0,1354.0,2,0,,,,33.25,9.94,10.21,0.0,0.0,8.0,A method that could possibly work is utilising optical illusions such as one where two lines down a hallway are identical but one seems longer to the human eye then they could be prompted with a multiple choice question as to the state of the line which to our eyes looks longer but to a computer is still the same length of line Of course there is always the issue of people with eye based disabilities not being able to complete them but different illusions could be used to accommodate that Example
,2.0,"<p>I'm trying to gain some intuition beyond definitions, in any possible dimension. I'd appreciate references to read.</p>
",,0,2016-10-09T20:59:32.740,,2106,2016-11-10T19:08:45.553,,,,,1267.0,,1,0,<machine-learning><models>,"How can one intuitively understand generative v/s discriminative models, specifically with respect to when each is useful?",40.0,45.93,14.4,10.56,0.0,0.0,5.0,Im trying to gain some intuition beyond definitions in any possible dimension Id appreciate references to read
,1.0,"<p>It seems that deep neural networks are making improvements largely because as we add nodes and connections, they are able to put together more and more abstract concepts. We know that, starting from pixels, they start to recognize high level objects like cat faces, chairs, and written words. Has a network ever been shown to have learned a more abstract concept that a physical object? What is the ""highest level of abstraction"" that we've observed?</p>
",,0,2016-10-10T00:02:42.510,1.0,2107,2016-10-10T04:07:29.890,,,,,2897.0,,1,2,<deep-learning>,What is the most abstract concept learned by a deep neural network?,98.0,60.85,11.78,10.46,0.0,0.0,12.0,It seems that deep neural networks are making improvements largely because as we add nodes and connections they are able to put together more and more abstract concepts We know that starting from pixels they start to recognize high level objects like cat faces chairs and written words Has a network ever been shown to have learned a more abstract concept that a physical object What is the highest level of abstraction that weve observed
,,"<p>You can train DNN to <strike>learn</strike> compute any abstract concept just by making that abstract concept as the label (output) in the training dataset. For example there are projects which detects emotions from peoples photos.</p>
",,0,2016-10-10T04:07:29.890,,2108,2016-10-10T04:07:29.890,,,,,1462.0,2107.0,2,1,,,,62.17,12.24,11.27,0.0,0.0,4.0,You can train DNN to learn compute any abstract concept just by making that abstract concept as the label output in the training dataset For example there are projects which detects emotions from peoples photos
,,"<p>The intution that I have about these is that generative are ""from abstract to concrete"" whereas discriminative models are ""from concrete to abstract"".</p>

<p>For example: Detecting if a photo has a cat or not is about going from the photo i.e concrete to the abstract concept of a cat. Whereas generating a photo of a cat given some abstract properties about the cat is going from abstract to concrete.</p>
",,1,2016-10-10T04:13:55.047,,2109,2016-10-10T04:13:55.047,,,,,1462.0,2106.0,2,1,,,,62.48,9.92,8.84,0.0,0.0,9.0,The intution that I have about these is that generative are from abstract to concrete whereas discriminative models are from concrete to abstract For example Detecting if a photo has a cat or not is about going from the photo ie concrete to the abstract concept of a cat Whereas generating a photo of a cat given some abstract properties about the cat is going from abstract to concrete
,10.0,"<p>I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. </p>

<p>They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code.</p>
",,4,2016-10-10T04:44:32.257,2.0,2111,2016-11-03T16:34:09.417,2016-10-10T09:50:29.303,,8.0,,26.0,,1,3,<research><philosophy>,Is AI living or non-living?,470.0,76.72,7.0,8.1,0.0,0.0,12.0,Im a bit confused about the definition of life Can AI systems be called living Because they can do most of the things that we can They can even communicate with one another They are not formed of what we call cells But you see cells are just a collection of several chemical processes which is in turn nonliving just like AI is formed of several lines of code
,,"<p>Artificial intelligence by definition is the intelligence exhibited by machines.  The definition of life in biological terms is the condition that distinguishes organisms from inorganic matter where the distinguishing criteria are the capacity for growth, reproduction, functional activity, and continual change preceding death. Does artificial intelligence ""grow""?  Indeed, I can program a machine learning program to grow with every input taken in.  In the loosest sense, we can say that artificial intelligence does grow, but does it biologically? If we look at the definition for growth of a living thing, it means to undergo natural development by increasing in size and changing physically or the progress to maturity.  All living organisms undergo growth.   Even though at the simplest level, cells are a series of chemical processes, cells are a very complicated set of chemical processes that are still not fully understood by scientists across the world.  Every cell has genetic material that can be replicated, excised, used for RNA, proteins, and that is subject to epigenetic regulation. 
<a href=""https://i.stack.imgur.com/nczDU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nczDU.png"" alt=""Cell division""></a></p>

<p>Does artificial intelligence undergo the same process of cell division?  No.  If I wanted to, I could write a program that undergoes a simple for-loop (print i from 1 to 100), replicates itself at a certain point (i=50) to produce the same program perhaps with some variation that will execute itself, and terminates (dies) at the end of the for loop.  The program, by an extremely loose definition supported by philosophy but not by biology, lives.  However, in scientific terms (and the correct interpretation), artificial intelligence is not living.  Artificial intelligence can be seen to be similar to viruses which are considered to be acellular and essential to life but not living.  Viruses are encapsulated DNA and RNA that undergo processes of growth, reproduction, and functionality but because they lack the ability to undergo the cell division cycle, are considered non-living.  At the very basis of the scientific definition of life is the cell replication cycle.  Artificial intelligence and viruses are not able to undergo the cell cycle.  Viruses need to infect other cells in order to reproduce but do not have their own, autonomous cycle.  At the end of the day, if you can argue that viruses are alive, you can argue that artificial intelligence is alive as well.  For the scientific definition of life, artificial intelligence must undergo the process of cell division and replication.  Even though artificial intelligence can mimic and help sustain life, no artificial intelligence process is truly alive. </p>

<p>Do note I did not discuss <a href=""http://www.isss.org/primer/asem14ep.html"" rel=""nofollow noreferrer"">living systems</a> in my answer.          </p>

<p><a href=""https://www.ncbi.nlm.nih.gov/books/NBK21685/"" rel=""nofollow noreferrer"">Definition of life</a></p>
",,0,2016-10-10T15:09:06.260,,2113,2016-10-10T15:09:06.260,,,,,37.0,2111.0,2,5,,,,44.24,13.58,9.01,0.0,0.0,64.0,Artificial intelligence by definition is the intelligence exhibited by machines The definition of life in biological terms is the condition that distinguishes organisms from inorganic matter where the distinguishing criteria are the capacity for growth reproduction functional activity and continual change preceding death Does artificial intelligence grow Indeed I can program a machine learning program to grow with every input taken in In the loosest sense we can say that artificial intelligence does grow but does it biologically If we look at the definition for growth of a living thing it means to undergo natural development by increasing in size and changing physically or the progress to maturity All living organisms undergo growth Even though at the simplest level cells are a series of chemical processes cells are a very complicated set of chemical processes that are still not fully understood by scientists across the world Every cell has genetic material that can be replicated excised used for RNA proteins and that is subject to epigenetic regulation Does artificial intelligence undergo the same process of cell division No If I wanted to I could write a program that undergoes a simple forloop print i from 1 to 100 replicates itself at a certain point i50 to produce the same program perhaps with some variation that will execute itself and terminates dies at the end of the for loop The program by an extremely loose definition supported by philosophy but not by biology lives However in scientific terms and the correct interpretation artificial intelligence is not living Artificial intelligence can be seen to be similar to viruses which are considered to be acellular and essential to life but not living Viruses are encapsulated DNA and RNA that undergo processes of growth reproduction and functionality but because they lack the ability to undergo the cell division cycle are considered nonliving At the very basis of the scientific definition of life is the cell replication cycle Artificial intelligence and viruses are not able to undergo the cell cycle Viruses need to infect other cells in order to reproduce but do not have their own autonomous cycle At the end of the day if you can argue that viruses are alive you can argue that artificial intelligence is alive as well For the scientific definition of life artificial intelligence must undergo the process of cell division and replication Even though artificial intelligence can mimic and help sustain life no artificial intelligence process is truly alive Do note I did not discuss living systems in my answer Definition of life
,,"<p>There is of course a vast amount of work in the area of Automatic Theorem Proving, but most of it is simply concerned with proof, rather than human notions of beauty, elegance, parsimony etc.</p>

<p>There has however been some work in this general area over the years:</p>

<ul>
<li>Douglas Lenat's famous <a href=""https://www.aaai.org/Papers/AAAI/1983/AAAI83-059.pdf"" rel=""nofollow"">AM</a> ('Amateur Mathematician').</li>
<li>Douglas Hofstadter's <a href=""https://en.wikipedia.org/wiki/Fluid_Concepts_and_Creative_Analogies#Chapter_3:Numbo:_A_Study_in_Cognition_and_Recognition"" rel=""nofollow"">NUMBO</a> program for number sequence extrapolation.</li>
<li>A range of publications by <a href=""http://www.doc.ic.ac.uk/~sgc/cv.html"" rel=""nofollow"">Simon Colton</a></li>
<li><a href=""http://www.math.rutgers.edu/~zeilberg/ekhad.html"" rel=""nofollow"">Shalosh B. Ekhad</a>, the automated proof assistant for Artificial Combinatorics created by Doron Zeilberg and credited by him as a co-author on numerous papers. </li>
</ul>
",,0,2016-10-10T19:52:11.460,,2114,2016-10-10T19:52:11.460,,,,,42.0,2101.0,2,2,,,,44.34,13.69,11.93,0.0,0.0,18.0,There is of course a vast amount of work in the area of Automatic Theorem Proving but most of it is simply concerned with proof rather than human notions of beauty elegance parsimony etc There has however been some work in this general area over the years Douglas Lenats famous AM Amateur Mathematician Douglas Hofstadters NUMBO program for number sequence extrapolation A range of publications by Simon Colton Shalosh B Ekhad the automated proof assistant for Artificial Combinatorics created by Doron Zeilberg and credited by him as a coauthor on numerous papers
,,"<p>Any machine with a sufficient level of integrated purpose driven behavior - that exhibits agency in an autopoietic, self-preserving way - will come to be viewed as ""alive."" Chess programs, not so much; self-driving cars, slightly; simulated robot animals, even more so. It has to do with purpose driven behavior and a richness of multi-domain functionality. The more complex agency it has, the more sympathetic we will be towards it.</p>
",,0,2016-10-10T21:20:09.907,,2115,2016-10-10T22:01:13.813,2016-10-10T22:01:13.813,,1712.0,,1712.0,2111.0,2,0,,,,45.76,13.45,10.75,0.0,0.0,18.0,Any machine with a sufficient level of integrated purpose driven behavior that exhibits agency in an autopoietic selfpreserving way will come to be viewed as alive Chess programs not so much selfdriving cars slightly simulated robot animals even more so It has to do with purpose driven behavior and a richness of multidomain functionality The more complex agency it has the more sympathetic we will be towards it
2181.0,3.0,"<p>I'm interested mostly in the application of AI in gaming; in case this adjusts the way you answer, but general answers are more than welcome as well.</p>

<p>I was reading up on Neural Networks and combining them with Genetic Algorithms; my high-level understanding is that the Neural Networks are used to produce a result from the inputs, and the Genetic Algorithm is employed to constantly adjust the weights in the Neural Network until a good answer is found.</p>

<p>The concept of a Genetic Algorithm randomly mutating the weights on the inputs to a Neural Network makes sense to me; but I don't understand where this would be applied in respect to gaming.</p>

<p>For example, if I had some simple enemy AI that I want to have adapt to the players play-style, is this a good opportunity to implement the AI as a Genetic-Algorithm combined with a Neural Network?</p>

<p>With these different suitable applications, how does one go about deciding how to encode the problem in such a way that it can be mutated by the Genetic Algorithm and serve as suitable on/off inputs to a Neural Network (actually, are Neural Networks always designed as on off signals?)?</p>
",,0,2016-10-11T01:09:20.780,,2117,2016-10-19T14:34:23.680,,,,,2819.0,,1,2,<neural-networks><gaming><genetic-algorithms>,"What sort of game problems can Neural-Networks and Genetic Algorithms solve, and how are they typically implemented?",165.0,39.94,10.8,10.4,0.0,0.0,23.0,Im interested mostly in the application of AI in gaming in case this adjusts the way you answer but general answers are more than welcome as well I was reading up on Neural Networks and combining them with Genetic Algorithms my highlevel understanding is that the Neural Networks are used to produce a result from the inputs and the Genetic Algorithm is employed to constantly adjust the weights in the Neural Network until a good answer is found The concept of a Genetic Algorithm randomly mutating the weights on the inputs to a Neural Network makes sense to me but I dont understand where this would be applied in respect to gaming For example if I had some simple enemy AI that I want to have adapt to the players playstyle is this a good opportunity to implement the AI as a GeneticAlgorithm combined with a Neural Network With these different suitable applications how does one go about deciding how to encode the problem in such a way that it can be mutated by the Genetic Algorithm and serve as suitable onoff inputs to a Neural Network actually are Neural Networks always designed as on off signals
2121.0,1.0,"<p>I have seen an AI create a game it self, AI act as a lawyer, call center etc.</p>

<p>There are many problems (Example for mobile development)</p>

<pre><code>1. New api/technology or even new language every year.
2. New design
3. New hardware
4. Good code architecture, design pattern
5. Security
6. Image/Animation optimization
7. Automate testing
</code></pre>

<p>etc.</p>

<p>I wonder that AI can help developer solve that problems.</p>

<p>1.1 May be I want to get the location then AI suggest the best api for specific platform.</p>

<p>1.2 AI help to refactoring and optimizing the code</p>

<ol start=""2"">
<li><p>Help on design e.g. golden ratio, Material theme color</p></li>
<li><p>Suggest or determine the limit of the hardware e.g. screen size, ram</p></li>
<li><p>Can convert to another design pattern </p></li>
<li><p>Help to waring the latest vulnerable and automate pentest etc.</p></li>
<li><p>Help to optimize image by learning how much can we reduce the image size while people still ok with it.</p></li>
<li><p>Generate automate-testing</p></li>
</ol>

<p>Is there any solution existed?</p>

<p>If not, what can we do?</p>
",,3,2016-10-11T06:08:16.267,1.0,2118,2016-10-11T07:04:44.790,,,,,2930.0,,1,1,<intelligence-testing><security>,How can AI help developer to develop things,46.0,66.33,8.69,9.6,191.0,0.0,22.0,I have seen an AI create a game it self AI act as a lawyer call center etc There are many problems Example for mobile development etc I wonder that AI can help developer solve that problems 11 May be I want to get the location then AI suggest the best api for specific platform 12 AI help to refactoring and optimizing the code Help on design eg golden ratio Material theme color Suggest or determine the limit of the hardware eg screen size ram Can convert to another design pattern Help to waring the latest vulnerable and automate pentest etc Help to optimize image by learning how much can we reduce the image size while people still ok with it Generate automatetesting Is there any solution existed If not what can we do
2120.0,1.0,"<p>There are AI creating game, content and more.</p>

<p>I'm thinking on how can AI develop mobile app itself?</p>

<p>The computer languages might easy for AI to learn.</p>

<p>AI can learn a lot from good open source project in github.</p>

<p>The trend prediction can help AI to select the topic for creating a great apps.</p>

<p>There are lots of details to let AI create a great apps. </p>
",,0,2016-10-11T06:15:28.633,1.0,2119,2016-10-11T06:44:14.300,,,,,2930.0,,1,1,<computer-programming>,How can we create an AI to develop mobile apps?,31.0,85.69,6.3,8.01,0.0,0.0,8.0,There are AI creating game content and more Im thinking on how can AI develop mobile app itself The computer languages might easy for AI to learn AI can learn a lot from good open source project in github The trend prediction can help AI to select the topic for creating a great apps There are lots of details to let AI create a great apps
,,"<p>We don't know how to do that yet. The problem is one of scale:</p>

<p>Despite many years of research into program synthesis via heuristic methods, it's still not possible to automatically create programs (e.g. via Genetic Programming (GP), Grammatical Evolution (GE) or Learning Classifier Systems (LCS)) that are thousands of lines long, whether that's for mobile or any other application area.</p>

<p>Contrary to popular belief, alternative formal methods approaches can indeed be used to create sizeable programs, but the kind of interaction that a mobile app would typically require is not easily specified in this way.</p>

<p>The scale at which heuristic approaches are currently viable is closer to the scale of expressions (e.g. single program statements) than entire programs. An intermediate approach is therefore to provide a program template and let GP etc generate the missing parts of the template.</p>

<p><a href=""http://www.nburles.co.uk/sites/default/files/attachments/templar-a-framework-for-template-method-hyper-heuristics.pdf"" rel=""nofollow"">This paper</a> describes how to combine Machine Learning with the 'Template Method' Design Pattern in order to create larger programs than would otherwise be possible, giving the specific example of a 'hyper-quicksort'.</p>
",,0,2016-10-11T06:44:14.300,,2120,2016-10-11T06:44:14.300,,,,,42.0,2119.0,2,2,,,,41.19,14.27,11.77,0.0,0.0,35.0,We dont know how to do that yet The problem is one of scale Despite many years of research into program synthesis via heuristic methods its still not possible to automatically create programs eg via Genetic Programming GP Grammatical Evolution GE or Learning Classifier Systems LCS that are thousands of lines long whether thats for mobile or any other application area Contrary to popular belief alternative formal methods approaches can indeed be used to create sizeable programs but the kind of interaction that a mobile app would typically require is not easily specified in this way The scale at which heuristic approaches are currently viable is closer to the scale of expressions eg single program statements than entire programs An intermediate approach is therefore to provide a program template and let GP etc generate the missing parts of the template This paper describes how to combine Machine Learning with the Template Method Design Pattern in order to create larger programs than would otherwise be possible giving the specific example of a hyperquicksort
,,"<p>An umbrella term for the application of heuristic techniques to software development is 'Search Based Software Engineering' (SBSE).</p>

<p>SBSE emerged as a distinct activity around the turn of the century, with a strong initial focus on automating the generation/prioritization of test cases.</p>

<p>With respect to some of your specific queries:</p>

<p>1.2 Paper on <a href=""https://www.lri.fr/~hansen/proceedings/2013/GECCO/companion/p205.pdf"">Automated refactoring</a></p>

<ol start=""2"">
<li><p>Automatically choosing screen colour to <a href=""http://www-bcf.usc.edu/~halfond/papers/li15demobile-abstract.pdf"">minimize energy consumption</a>.</p></li>
<li><p>This sort of thing is not usually done heuristically, since it needs platform-specific code.</p></li>
<li><p>Automated <a href=""https://www.computer.org/csdl/proceedings/icse/2000/2147/00/21470722.pdf"">refactoring to patterns</a>.</p></li>
<li><p>AFAIK, penetration testing has yet to be successfully automated.</p></li>
<li><p>As stated, this doesn't really require AI. More generally, I don't know of any specific work automating for HCI preferences, but something like 'Interactive Genetic Algorithms' could be used.</p></li>
<li><p>There's a lot of SBSE literature on testing. See <a href=""http://www0.cs.ucl.ac.uk/staff/mharman/laser.pdf"">this paper</a> for a general overview.</p></li>
</ol>
",,0,2016-10-11T07:04:44.790,,2121,2016-10-11T07:04:44.790,,,,,42.0,2118.0,2,5,,,,42.17,15.3,10.96,0.0,0.0,29.0,An umbrella term for the application of heuristic techniques to software development is Search Based Software Engineering SBSE SBSE emerged as a distinct activity around the turn of the century with a strong initial focus on automating the generationprioritization of test cases With respect to some of your specific queries 12 Paper on Automated refactoring Automatically choosing screen colour to minimize energy consumption This sort of thing is not usually done heuristically since it needs platformspecific code Automated refactoring to patterns AFAIK penetration testing has yet to be successfully automated As stated this doesnt really require AI More generally I dont know of any specific work automating for HCI preferences but something like Interactive Genetic Algorithms could be used Theres a lot of SBSE literature on testing See this paper for a general overview
,2.0,"<p>New to the topic, I think I have figured out how to implement a Multi Level Perceptron(MLP) ANN.</p>

<p>And was wondering if there are any simple data sets to test a MLP ANN ?
i.e. small number of inputs and outputs</p>

<p>I'm not getting expected results from uci cancer, I was hoping someone could save me some time and point me to some data they have used before ?</p>

<p>Maybe start slightly more complex than XOR ?</p>
",,0,2016-10-11T14:11:41.737,,2122,2016-10-13T11:00:03.283,,,,,2936.0,,1,3,<neural-networks>,Is there any simple testing data?,53.0,69.62,7.95,9.68,0.0,0.0,11.0,New to the topic I think I have figured out how to implement a Multi Level PerceptronMLP ANN And was wondering if there are any simple data sets to test a MLP ANN ie small number of inputs and outputs Im not getting expected results from uci cancer I was hoping someone could save me some time and point me to some data they have used before Maybe start slightly more complex than XOR
,,"<p>A popular dataset is the fisher iris dataset. It consists of 150 samples each with a dimensionality of 4. You can find it at
<a href=""http://archive.ics.uci.edu/ml/datasets/Iris"" rel=""nofollow"">http://archive.ics.uci.edu/ml/datasets/Iris</a></p>
",,0,2016-10-11T14:22:00.057,,2123,2016-10-13T11:00:03.283,2016-10-13T11:00:03.283,,2937.0,,2937.0,2122.0,2,3,,,,54.59,14.06,9.73,0.0,0.0,11.0,A popular dataset is the fisher iris dataset It consists of 150 samples each with a dimensionality of 4 You can find it at httparchiveicsuciedumldatasetsIris
,1.0,"<p>The concept is intrinsically related with building some sort of media for the AI to exists. We may think of a digital computer, programmed to use language and act in a way that we cannot be distinguished from a human. But, does the media really mater (unconventional computation paradigms)? Does having a certain control over the limits of what the AI can do matter? Synthetic biology has the ultimate goal of building biological systems from scratch , would a synthetic brain, potentially introduced in a synthetic human, constitute AI?</p>

<p>I am just looking for a clear definition of what most people have in mind when they refer to AI.</p>
",2016-10-13T15:34:53.750,0,2016-10-12T05:09:09.833,,2125,2016-10-13T12:07:40.110,,,,,2928.0,,1,0,<definitions>,What is the definition of artificial intelligence?,32.0,61.67,10.67,10.09,0.0,0.0,13.0,The concept is intrinsically related with building some sort of media for the AI to exists We may think of a digital computer programmed to use language and act in a way that we cannot be distinguished from a human But does the media really mater unconventional computation paradigms Does having a certain control over the limits of what the AI can do matter Synthetic biology has the ultimate goal of building biological systems from scratch would a synthetic brain potentially introduced in a synthetic human constitute AI I am just looking for a clear definition of what most people have in mind when they refer to AI
,2.0,"<p>How are autonomous cars related to artificial intelligence? I would presume that artificial intelligence is when we are able to copy the human state of mind and perform tasks in the same way. But isn't autonomous car just rule-based machines that operates due to its environment? They are not self-aware, and they cannot choose a good way to act in a never before experienced situation.</p>

<p>I know that many people often mention autonomous cars when speaking about AI, but I am not really convinced that these are related. Either I have a too strict understanding of what AI is or </p>
",,0,2016-10-12T06:56:47.753,2.0,2126,2016-10-12T18:42:15.673,2016-10-12T16:48:22.203,,42.0,,2963.0,,1,2,<self-driving><strong-ai><cars><weak-ai>,Why are autonomous cars categorized as AI?,64.0,54.52,10.09,9.36,0.0,0.0,10.0,How are autonomous cars related to artificial intelligence I would presume that artificial intelligence is when we are able to copy the human state of mind and perform tasks in the same way But isnt autonomous car just rulebased machines that operates due to its environment They are not selfaware and they cannot choose a good way to act in a never before experienced situation I know that many people often mention autonomous cars when speaking about AI but I am not really convinced that these are related Either I have a too strict understanding of what AI is or
,8.0,"<p>What are the advantages of having self-driving cars?</p>

<p>We will be able to have more cars in the traffic at the same time, but won't it also make more people choose to use the cars, so both the traffic and the public health will actually become worse?</p>

<p>Are we really interested in this?</p>
",,0,2016-10-12T07:40:44.907,1.0,2127,2016-12-12T09:51:11.747,,,,,2963.0,,1,8,<research><self-driving><cars>,Advantages of having self-driving cars,383.0,70.43,7.6,8.09,0.0,0.0,7.0,What are the advantages of having selfdriving cars We will be able to have more cars in the traffic at the same time but wont it also make more people choose to use the cars so both the traffic and the public health will actually become worse Are we really interested in this
,,"<p>One of the main arguments for self-driving cars is that presumably they'll get better and better at driving as the technology progresses, they have no temporal attention deficits or aggressive urges or drug habits and sense their environment 360°, all the while communicating with the other cars, which all together basically amounts to LESS DEAD PEOPLE. </p>

<p>We are really interested in this.</p>

<p>It is also unclear whether most people will actually own cars in 30 years. Maybe there'll be a net of mini busses with flexible routes which take you from door to door on demand. That would reduce traffic quite a bit and there would also be less incentive to drive 200 m to get cigarettes or something. Self-driving cars would allow us to use the car as a resource a lot more efficiently, because suddenly we can relocate empty cars without paying a driver.  </p>
",,0,2016-10-12T08:16:04.947,,2128,2016-10-12T08:16:04.947,,,,,2227.0,2127.0,2,12,,,,55.27,10.86,10.14,0.0,0.0,14.0,One of the main arguments for selfdriving cars is that presumably theyll get better and better at driving as the technology progresses they have no temporal attention deficits or aggressive urges or drug habits and sense their environment 360° all the while communicating with the other cars which all together basically amounts to LESS DEAD PEOPLE We are really interested in this It is also unclear whether most people will actually own cars in 30 years Maybe therell be a net of mini busses with flexible routes which take you from door to door on demand That would reduce traffic quite a bit and there would also be less incentive to drive 200 m to get cigarettes or something Selfdriving cars would allow us to use the car as a resource a lot more efficiently because suddenly we can relocate empty cars without paying a driver
,,"<p>There is a neat definition of artificial intelligence, which circumvents the problem of defining ""intelligence"" and which I would ascribe to <a href=""https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)"" rel=""nofollow"">McCarthy</a>, the founder of the field, although I can only find it now in <a href=""https://books.google.de/books?id=IY19CAAAQBAJ&amp;pg=PA53&amp;lpg=PA53&amp;dq=that%20we%20would%20call%20intelligent%20if%20it%20were%20done%20by%20a%20human&amp;source=bl&amp;ots=I8O-U1Jx8q&amp;sig=3VfZuVaLYtLGCtUo4uSbOjzrboE&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjG18Se6tTPAhUE1hoKHUppA88Q6AEIHjAA#v=onepage&amp;q=that%20we%20would%20call%20intelligent%20if%20it%20were%20done%20by%20a%20human&amp;f=false"" rel=""nofollow"">this book</a> by H. Simon:</p>

<p>""… having to do with finding ways to do intelligent tasks, to do tasks which, if they were done by human beings, would call for our human intelligence.""</p>

<p>So, at its core we call the automation of every task AI, that can only be done by the human mind. At the time people thought that a computer able to play chess would also be intelligent in other ways. When this turned out to be false, the term AI was split into ""narrow or weak AI"", i.e. a program able to do one task of the human mind, and ""general or strong AI"", a program that can do all the tasks of the human mind. </p>

<p>Self-driving cars are narrow AI. </p>

<p>Note, that all these definitions don't specify whether these programs copy the way the human mind works or whether they come to the same result via completely different algorithms. </p>
",,1,2016-10-12T08:35:47.800,,2129,2016-10-12T08:35:47.800,,,,,2227.0,2126.0,2,2,,,,64.54,8.77,8.25,0.0,0.0,33.0,There is a neat definition of artificial intelligence which circumvents the problem of defining intelligence and which I would ascribe to McCarthy the founder of the field although I can only find it now in this book by H Simon … having to do with finding ways to do intelligent tasks to do tasks which if they were done by human beings would call for our human intelligence So at its core we call the automation of every task AI that can only be done by the human mind At the time people thought that a computer able to play chess would also be intelligent in other ways When this turned out to be false the term AI was split into narrow or weak AI ie a program able to do one task of the human mind and general or strong AI a program that can do all the tasks of the human mind Selfdriving cars are narrow AI Note that all these definitions dont specify whether these programs copy the way the human mind works or whether they come to the same result via completely different algorithms
2133.0,3.0,"<p>In lots of sci-fi, it seems that AI becomes sentient (Terminator, Peter F Hamilton's SI (commonwealth saga), etc.)</p>

<p>However, I'm interested in whether this is actually plausible, whether an AI could actually break free form being controlled by us, and if that is possible, whether there is any research as to about what sort of complexity / processing power an AI would need to be able to do this.</p>
",,1,2016-10-12T16:52:48.150,,2131,2016-10-16T11:29:26.130,,,,,2978.0,,1,1,<machine-learning><deep-learning>,AI becoming sentient plausibility?,275.0,53.89,11.27,10.43,0.0,0.0,17.0,In lots of scifi it seems that AI becomes sentient Terminator Peter F Hamiltons SI commonwealth saga etc However Im interested in whether this is actually plausible whether an AI could actually break free form being controlled by us and if that is possible whether there is any research as to about what sort of complexity processing power an AI would need to be able to do this
,,"<p>One of the most common requirements to be defined as life is abbreviated to <strong>MRS GREN</strong></p>

<p>this means:</p>

<p>M - movement<br>
R - respiration<br>
S - sensitivity</p>

<p>G - growth<br>
R - reproduce<br>
E - excretion<br>
N - nutrition</p>

<p>An AI can technically do some of these, it can move its location from device to device, it can grow its own code, and assimilate other bits of code it can find, which fits growth and kind of fits respiration also firewalls could almost be sensitivity.</p>

<p>But then there is nothing relating to nutrition or excretion, so it fits most definitions of life, but it depends on the complexity of life and which definition of life you are using.</p>
",,5,2016-10-12T17:00:20.803,,2132,2016-10-12T17:00:20.803,,,,,2978.0,2111.0,2,1,,,,23.6,10.4,10.37,0.0,0.0,16.0,One of the most common requirements to be defined as life is abbreviated to MRS GREN this means M movement R respiration S sensitivity G growth R reproduce E excretion N nutrition An AI can technically do some of these it can move its location from device to device it can grow its own code and assimilate other bits of code it can find which fits growth and kind of fits respiration also firewalls could almost be sensitivity But then there is nothing relating to nutrition or excretion so it fits most definitions of life but it depends on the complexity of life and which definition of life you are using
,,"<p>There are already programs that have broken free of our control (<a href=""https://en.wikipedia.org/wiki/Morris_worm"" rel=""nofollow"">Morris worm</a>) so that in itself doesn't imply any great computational demands.</p>

<p>Sentience is ill-defined but is certainly not a pre-requisite for a program to do mischief beyond what its creators intend.</p>

<p>It's difficulty to estimate what sort of processing power is required to support human-like intelligence, since we don't know what the most efficient way to achieve that would be.  If the most processing efficient would be to implement a neural network approaching the number of neurons and interconnects of the human brain processing signals at the same rate, the fastest artificial neural network implementations extant are at least 4-5 orders of magnitude short, is thousands of times less power efficient, and doesn't seem to have a realistic way to scale to the number of interconnects required (<a href=""http://ai.stackexchange.com/questions/1834/power-efficiency-of-human-brains-vs-neural-networks"">see this question</a>)</p>
",,2,2016-10-12T17:12:53.330,,2133,2016-10-12T17:12:53.330,,,,,2329.0,2131.0,2,3,,,,35.44,13.7,10.85,0.0,0.0,19.0,There are already programs that have broken free of our control Morris worm so that in itself doesnt imply any great computational demands Sentience is illdefined but is certainly not a prerequisite for a program to do mischief beyond what its creators intend Its difficulty to estimate what sort of processing power is required to support humanlike intelligence since we dont know what the most efficient way to achieve that would be If the most processing efficient would be to implement a neural network approaching the number of neurons and interconnects of the human brain processing signals at the same rate the fastest artificial neural network implementations extant are at least 45 orders of magnitude short is thousands of times less power efficient and doesnt seem to have a realistic way to scale to the number of interconnects required see this question
,,"<p>Self driving cars exhibit a level of agency and multi-domain resilience. By certain definitions they <em>are</em> self aware and they are definitely designed to fail safely in a large number of potentially unknown circumstances, which is similar to biological agents.</p>

<p>AI really has to do with the study of non-biological agents and their methods of agency. Everything else is just computer science, algorithmic efficiency, biology, art, etc. Eventually the study of biological and non-biological agency will converge, though, and we'll just call it the study of ""intelligence.""</p>
",,0,2016-10-12T18:42:15.673,,2134,2016-10-12T18:42:15.673,,,,,1712.0,2126.0,2,0,,,,36.89,14.56,10.49,0.0,0.0,18.0,Self driving cars exhibit a level of agency and multidomain resilience By certain definitions they are self aware and they are definitely designed to fail safely in a large number of potentially unknown circumstances which is similar to biological agents AI really has to do with the study of nonbiological agents and their methods of agency Everything else is just computer science algorithmic efficiency biology art etc Eventually the study of biological and nonbiological agency will converge though and well just call it the study of intelligence
,,"<p>No one knows. </p>

<p>A useful definition of sentience due to the philosopher Thomas Nagel is <a href=""https://en.wikipedia.org/wiki/Thomas_Nagel#What_is_it_like_to_be_a_something"" rel=""nofollow"">'something it is like'</a> to be. </p>

<p>For example, we intuitively feel that there is nothing it is like to be a brick, but that there probably is to be a dog and so on.</p>

<p>However, there is no objective <em>test</em> currently known to physics which can tell if some other entity is having such 'first hand experience', and correspondingly no <em>designs</em> that will definitely lead to sentience.</p>

<p>The best test we have is the Turing test and its variants. The most obvious designs are neuromorphic ones, since we know that the design of the human brain is at least correlated with sentience.</p>

<p>In the light of the above, we can't definitively say a great deal about lower complexity thresholds for sentience - the best we can do count neurons in creatures that we might be prepared to admit are sentient.</p>
",,0,2016-10-12T20:22:01.510,,2135,2016-10-12T20:22:01.510,,,,,42.0,2131.0,2,1,,,,57.61,9.4,9.03,0.0,0.0,19.0,No one knows A useful definition of sentience due to the philosopher Thomas Nagel is something it is like to be For example we intuitively feel that there is nothing it is like to be a brick but that there probably is to be a dog and so on However there is no objective test currently known to physics which can tell if some other entity is having such first hand experience and correspondingly no designs that will definitely lead to sentience The best test we have is the Turing test and its variants The most obvious designs are neuromorphic ones since we know that the design of the human brain is at least correlated with sentience In the light of the above we cant definitively say a great deal about lower complexity thresholds for sentience the best we can do count neurons in creatures that we might be prepared to admit are sentient
,,"<p>You're unsure about the definition of life (which the other answers clarify) but also most people are unclear about the definition of AI. Do you mean an AI that can accomplish a routine task (such as the path finder in a GPS) or a General AI that is able to find a creative solution to any directive given to it (such an AI does not yet exist and may not ever exist) or do you mean a SENTIENT computer program? <a href=""http://alternativemindsets.co.uk/different-types-artificial-intelligence/"" rel=""nofollow"">Here is a simple article introducing some different concepts refered to as AI</a></p>

<p>Some people believe that a sentient computer program would be entitled to human rights. Not technically 'alive' in the biological sense, but having self awareness, will, desires, etc. Others disagree and believe that the program is a mere simulation that artificially mimics the actions of a human with a human soul, and is no more human than a washing machine. This is a very deep philosophical and meta-physical debate. For example, in <a href=""https://en.wikipedia.org/wiki/A.I._Artificial_Intelligence"" rel=""nofollow"">A.I. the movie</a> the overall message is that an android can simulate the emotion of love in a way that is more loyal and sincere than any human.</p>

<p>What I find interesting about this purely theoretical debate is that in almost every instance of sci-fi media that deals with the theme, the AI exists inside of a human-like android. But technically, the shape of the robot should be irrelevant.</p>
",,3,2016-10-12T21:52:17.197,,2137,2016-10-12T23:17:31.227,2016-10-12T23:17:31.227,,2983.0,,2983.0,2111.0,2,2,,,,47.72,9.81,10.06,0.0,0.0,31.0,Youre unsure about the definition of life which the other answers clarify but also most people are unclear about the definition of AI Do you mean an AI that can accomplish a routine task such as the path finder in a GPS or a General AI that is able to find a creative solution to any directive given to it such an AI does not yet exist and may not ever exist or do you mean a SENTIENT computer program Here is a simple article introducing some different concepts refered to as AI Some people believe that a sentient computer program would be entitled to human rights Not technically alive in the biological sense but having self awareness will desires etc Others disagree and believe that the program is a mere simulation that artificially mimics the actions of a human with a human soul and is no more human than a washing machine This is a very deep philosophical and metaphysical debate For example in AI the movie the overall message is that an android can simulate the emotion of love in a way that is more loyal and sincere than any human What I find interesting about this purely theoretical debate is that in almost every instance of scifi media that deals with the theme the AI exists inside of a humanlike android But technically the shape of the robot should be irrelevant
,,"<p>AI is a broad term referring to more than one concept, each with its own definition.
<a href=""http://alternativemindsets.co.uk/different-types-artificial-intelligence/"" rel=""nofollow"">Different Types of AI</a></p>

<p>The lowest levels are extremely simple and common, such are an artificial chess opponent. They work well because the programs internal model of reality is a 8x8 grid with only a few rules. The program chooses a preferred action by running simulations in it's internal model of reality.</p>

<p>What is often meant by AI is a ""General Intelligence"" that can come up with a creative solution to any directive, based on it's internal comprehension of the world. There is no existing example of this as of yet. The problem is that its internal model of reality needs to provide for every possible action. And it's possible actions and reactions may not be limited to a finite number of discretely distinct moves as in a game of chess. (At least if it works on the basis of conventional programming) Even ignoring the computational power needed to run such a broad and inefficient program, it would also require some stupendous amount of labour to program this internal model of reality in the first place.</p>

<p>I think in Sci-Fi, when people say AI, they mean a computer program that has a kind of awareness of itself and the world around it and can come up with creative and unexpected courses of actions in order to achieve its objective. Often the AI is NOT sentient, which is why it does not understand that its actions are morally wrong, or that its solution defeats the underlying intention of its assigned directive. The Horror lay in the concept that an amoral entity has more processing power than human kind.</p>
",,0,2016-10-12T23:30:11.057,,2138,2016-10-13T12:07:40.110,2016-10-13T12:07:40.110,,2983.0,,2983.0,2125.0,2,0,,,,55.98,10.22,9.88,0.0,0.0,28.0,AI is a broad term referring to more than one concept each with its own definition Different Types of AI The lowest levels are extremely simple and common such are an artificial chess opponent They work well because the programs internal model of reality is a 8x8 grid with only a few rules The program chooses a preferred action by running simulations in its internal model of reality What is often meant by AI is a General Intelligence that can come up with a creative solution to any directive based on its internal comprehension of the world There is no existing example of this as of yet The problem is that its internal model of reality needs to provide for every possible action And its possible actions and reactions may not be limited to a finite number of discretely distinct moves as in a game of chess At least if it works on the basis of conventional programming Even ignoring the computational power needed to run such a broad and inefficient program it would also require some stupendous amount of labour to program this internal model of reality in the first place I think in SciFi when people say AI they mean a computer program that has a kind of awareness of itself and the world around it and can come up with creative and unexpected courses of actions in order to achieve its objective Often the AI is NOT sentient which is why it does not understand that its actions are morally wrong or that its solution defeats the underlying intention of its assigned directive The Horror lay in the concept that an amoral entity has more processing power than human kind
,,"<p>There are a ton of sample datasets our there you can play with. A bunch of good ones install with R in the datasets package.  Luckily you can download them independently if you're not an R user.  Try  <a href=""https://vincentarelbundock.github.io/Rdatasets/datasets.html"" rel=""nofollow"">https://vincentarelbundock.github.io/Rdatasets/datasets.html</a></p>

<p>You might also be interested in the <a href=""http://yann.lecun.com/exdb/mnist/"" rel=""nofollow"">MNIST database</a> which is one of the canonical databases used in handwriting recognition research.</p>

<p>Beyond that, you can look at / ask on <a href=""http://datasets.reddit.com"" rel=""nofollow"">http://datasets.reddit.com</a> and/or <a href=""http://opendata.reddit.com"" rel=""nofollow"">http://opendata.reddit.com</a> and you'll find all sorts of useful datasets.</p>

<p>And finally, don't overlook the <a href=""http://archive.ics.uci.edu/ml/"" rel=""nofollow"">UCI Machine Learning Repository</a>.</p>
",,0,2016-10-13T05:20:21.117,,2139,2016-10-13T05:20:21.117,,,,,33.0,2122.0,2,2,,,,51.85,15.35,9.57,0.0,0.0,31.0,There are a ton of sample datasets our there you can play with A bunch of good ones install with R in the datasets package Luckily you can download them independently if youre not an R user Try httpsvincentarelbundockgithubioRdatasetsdatasetshtml You might also be interested in the MNIST database which is one of the canonical databases used in handwriting recognition research Beyond that you can look at ask on httpdatasetsredditcom andor httpopendataredditcom and youll find all sorts of useful datasets And finally dont overlook the UCI Machine Learning Repository
,,"<p>This is one of those things where I think the answer is going to change over time.  Today, I don't know anyone who would call any present AI systems ""alive"".  But as the AI's become more intelligent and human-like, I could see the day coming when they will be considered living.    </p>

<p>Sorry for the brief answer, but it's lake, I'm sick and jazzed up on Nyquil.  Will try to add more depth to this answer later.</p>
",,0,2016-10-13T05:23:05.913,,2140,2016-10-13T05:23:05.913,,,,,33.0,2111.0,2,0,,,,81.43,7.07,8.34,0.0,0.0,16.0,This is one of those things where I think the answer is going to change over time Today I dont know anyone who would call any present AI systems alive But as the AIs become more intelligent and humanlike I could see the day coming when they will be considered living Sorry for the brief answer but its lake Im sick and jazzed up on Nyquil Will try to add more depth to this answer later
,,"<p>It's an interesting question about what makes humans unique. There is a good book on the subject titled <a href=""https://archive.org/stream/whatcomputerscan017504mbp/whatcomputerscan017504mbp_djvu.txt"" rel=""nofollow"">What Computers Cant Do</a> by <a href=""https://en.wikipedia.org/wiki/Hubert_Dreyfus"" rel=""nofollow"">Hubert Dreyfus</a>.</p>

<p>One task that a computer can't handle (for now at least) is ranking important things. For example, CAPTCHA asks you to order a random list of things (small one, five or six items) by importance. This particular exercise requires AI to take decisions (not always rational) based on human judgement.</p>
",,0,2016-10-13T09:33:07.423,,2143,2016-10-13T09:33:07.423,,,,,2990.0,1354.0,2,2,,,,64.71,10.66,10.7,0.0,0.0,15.0,Its an interesting question about what makes humans unique There is a good book on the subject titled What Computers Cant Do by Hubert Dreyfus One task that a computer cant handle for now at least is ranking important things For example CAPTCHA asks you to order a random list of things small one five or six items by importance This particular exercise requires AI to take decisions not always rational based on human judgement
,0.0,"<p>Deepmind just published a <a href=""http://www.nature.com/nature/journal/vaop/ncurrent/full/nature20101.html"" rel=""nofollow noreferrer"">paper</a> about a <a href=""https://deepmind.com/blog/differentiable-neural-computers/"" rel=""nofollow noreferrer"">""differentiable neural computer""</a>, which basically <em>combines a neural network with a memory</em>. </p>

<p>The idea is to teach the neural network to create and recall useful explicit memories for a certain task. This complements the abilities of a neural network well, because NNs only store knowledge implicitly in the weights and the information used to work on a single task is only stored in the activation of the network and degrades quickly the more information you add. (<a href=""https://en.wikipedia.org/wiki/Long_short-term_memory"" rel=""nofollow noreferrer"">LSTMs</a> are one try to slow down this degradation of short term memories, but it still happens.)</p>

<p>Now, instead of keeping the necessary information in the activation, they presumably keep the addresses of memory slots for specific information in the activation, so these should also be subject to degradation. My question is why this approach should scale. Shouldn't a somewhat higher number of task specific information once again overwhelm the networks capability of keeping the addresses of all the appropriate memory slots in its activation?</p>
",,0,2016-10-13T15:00:49.880,,2144,2017-02-12T14:11:25.480,2017-02-12T14:11:25.480,,1807.0,,2227.0,,1,7,<deep-learning><ai-design>,"How would Deepmind's new ""differentiable neural computer"" scale?",170.0,47.12,13.35,10.09,0.0,0.0,18.0,Deepmind just published a paper about a differentiable neural computer which basically combines a neural network with a memory The idea is to teach the neural network to create and recall useful explicit memories for a certain task This complements the abilities of a neural network well because NNs only store knowledge implicitly in the weights and the information used to work on a single task is only stored in the activation of the network and degrades quickly the more information you add LSTMs are one try to slow down this degradation of short term memories but it still happens Now instead of keeping the necessary information in the activation they presumably keep the addresses of memory slots for specific information in the activation so these should also be subject to degradation My question is why this approach should scale Shouldnt a somewhat higher number of task specific information once again overwhelm the networks capability of keeping the addresses of all the appropriate memory slots in its activation
2159.0,2.0,"<p>What could be an algorithm that determines whether an AI ( algorithm ) is 
AI Complete or not ?
How does one proceed to program it ?</p>

<p>edit : question edited due to some misinterpretation in the first answer !</p>
",,4,2016-10-13T19:29:51.133,,2145,2016-10-14T20:05:46.540,2016-10-14T19:34:01.143,,2995.0,,2995.0,,1,0,<machine-learning><deep-learning><learning-theory><incompleteness-theorems>,AI Completeness - Testing,142.0,68.47,10.01,9.31,0.0,0.0,6.0,What could be an algorithm that determines whether an AI algorithm is AI Complete or not How does one proceed to program it edit question edited due to some misinterpretation in the first answer
,,"<p>Safety is often put in focus by journalists. Although there is potential to make the roads safer, I don't think that is the driving force behind the push for self-driving cars. The main advantage of self-driving cars is that this will reduce costs for businesses, while increasing efficiency (both fuel and time). From the perspective of the public, the self-driving cars are attractive, because they will turn the task of driving, into commute. Activity that requires attention will be replaced with somewhat free time.</p>
",,0,2016-10-13T21:35:12.593,,2146,2016-10-13T21:35:12.593,,,,,2997.0,2127.0,2,1,,,,54.42,12.58,9.73,0.0,0.0,16.0,Safety is often put in focus by journalists Although there is potential to make the roads safer I dont think that is the driving force behind the push for selfdriving cars The main advantage of selfdriving cars is that this will reduce costs for businesses while increasing efficiency both fuel and time From the perspective of the public the selfdriving cars are attractive because they will turn the task of driving into commute Activity that requires attention will be replaced with somewhat free time
,,"<p>I have very little experience with ML/DL to call myself either practitioner, but here is my answer on the 1st question:</p>

<p>At its core DL solves well the task of classification. Not every practical problem can be rephrased in terms of classification. Classification domain needs to be known upfront. Although the classification can be applied to any type of data, it's necessary to train the NN with samples of the specific domain where it'll be applied. If the domain is switched at some point, while keeping the same model (NN structure), it'll have to be retrained with new samples. Furthermore, even the best classifiers have ""gaps"" - <a href=""http://www.kdnuggets.com/2015/07/deep-learning-adversarial-examples-misconceptions.html"" rel=""nofollow"">Adversarial Examples</a> can be easily constructed from a training sample, such that changes are imperceptible to human, but are misclassified by the trained model.</p>
",,1,2016-10-13T22:20:08.543,,2147,2016-10-13T22:20:08.543,,,,,2997.0,248.0,2,1,,,,49.45,12.42,10.54,0.0,0.0,23.0,I have very little experience with MLDL to call myself either practitioner but here is my answer on the 1st question At its core DL solves well the task of classification Not every practical problem can be rephrased in terms of classification Classification domain needs to be known upfront Although the classification can be applied to any type of data its necessary to train the NN with samples of the specific domain where itll be applied If the domain is switched at some point while keeping the same model NN structure itll have to be retrained with new samples Furthermore even the best classifiers have gaps Adversarial Examples can be easily constructed from a training sample such that changes are imperceptible to human but are misclassified by the trained model
,,"<p>One cannot judge any form of intelligence, artificial or natural, whether it is complete or incomplete. Having it complete means that you are imposing limits to what it is capable of, the Turing test only test if your machine have intelligence that is similar to humans, therefore to decide whether it is complete or not would have to be based on the completeness of our own intelligence. Humans such as ourselves learn new things each day. If you'd run any algorithm that would judge the AI for it's completeness, it would have to run forever and your results would have to vary on every moment of the existence of natural intelligence.</p>
",,2,2016-10-14T04:28:35.927,,2150,2016-10-14T04:28:35.927,,,,,2998.0,2145.0,2,1,,,,43.26,10.57,9.85,0.0,0.0,11.0,One cannot judge any form of intelligence artificial or natural whether it is complete or incomplete Having it complete means that you are imposing limits to what it is capable of the Turing test only test if your machine have intelligence that is similar to humans therefore to decide whether it is complete or not would have to be based on the completeness of our own intelligence Humans such as ourselves learn new things each day If youd run any algorithm that would judge the AI for its completeness it would have to run forever and your results would have to vary on every moment of the existence of natural intelligence
,,"<blockquote>
  <p><strong>FORWORD NOTE:</strong> this answer is a breakdown based on my Artificial Intelligence, which based on description is very similar to Angelina.</p>
  
  <p>I do want to emphasize that it is <em>NOT</em> Angelina</p>
</blockquote>

<p><strong><em>Like all artificial intelligences</em></strong>, in order to fully design it, <strong><em>you have to break AI and intelligence down deeply</em></strong>. If there is a confusion about a certain aspect to intelligence, you haven't broken it down enough.</p>

<p>I, myself, have managed to break down the intellect of producing a program (or essentially any product) very far and very deep.</p>

<blockquote>
  <p><strong>Side Note:</strong> An interesting and helpful part of finishing breaking it down, was that I did not have to worry about breaking down spoken language intelligence, as that is already well-successfully accomplished and there are APIs out there in which computational creativity researchers can use such as <a href=""https://wit.ai/"" rel=""nofollow"">wit.ai</a></p>
  
  <p>So, we <em>only</em> have to worry about breaking down the creativity aspect.</p>
</blockquote>

<h1>Breaking it Down:</h1>

<h3>The Design Process:</h3>

<blockquote>
  <p><strong>Side Note:</strong> This I could easily provide a citation for, however it has too many accepted descriptions for me to be willing to cite one and to say that it is <em>the</em> or <em>a correct citation</em>. However, I will be providing one as a reference and that is the one provided very nicely on <a href=""https://www.discoverdesign.org/handbook"" rel=""nofollow"">DiscoverDesign</a>.</p>
  
  <p>The paragraph below is provided by them, and if you are interested in breaking that process down more, DiscoverDesign fully explains the processes in detail for you.</p>
</blockquote>

<p>The steps are <em>Define the Problem</em>, <em>Collect Information</em>, <em>Brainstorm and Analyze Ideas</em>, <em>Develop Solutions</em>, <em>Get Some Sort of Feedback</em>, <em>Improve</em> (which is essentially restart the process)</p>

<h3>Defining a problem:</h3>

<p>As far as this part of the breakdown goes, there two algorithms in which you can use for this subprocess of design:</p>

<blockquote>
  <p><em>Easy Algorithm (not really an algorithm):</em> ask from the client what the Artificial Intelligence is providing a solution for.</p>
</blockquote>

<p>However, this process could easily be made more interesting:</p>

<blockquote>
  <p><em>Difficult Algorithm</em>: design an algorithm that can define <em>a problem</em> without user input.</p>
</blockquote>

<p>I did some digging, and the design of the latter relies on one question that lacks enough research for a <strong>solid</strong> answer, and that is <em>where do questions come from psychologically</em>? or <em>more specifically, how does curiosity work</em>?</p>

<p>With more research on Google I was led to <a href=""http://science.howstuffworks.com/life/evolution/curiosity1.htm"" rel=""nofollow"">this article</a> specifically addressing that question.</p>

<h3>How Curiosity works:</h3>

<p>The 2 theories it stated that have yet to be fully proven are <em>drive theory</em> and <em>incongruity theory</em></p>

<blockquote>
  <p>Drive Theory simply states, we have a <em>need</em> to be curious, and to fulfill that need, we ask questions.</p>
</blockquote>

<p>So, needless to say this theory isn't helpful to the design of the A.I.</p>

<blockquote>
  <p><em>Incongruity Theory</em> states that we are able identify things we <em>do not FULLY understand or understand AT ALL</em> which leads us to asking questions.</p>
</blockquote>

<p>With help of my peers contributing to my research project, I was able to induce from Incongruity Theory and observations I had noticed within interviews (not job interviews, press interviews) that questions are made by noticing a <em>missing/unclear attribute or characteristic</em> on a certain idea, concept, or object (essentially anything the brain can virtually image or understand).</p>

<h3>My Own Inductive Theory on Curiousity</h3>

<p>The way that I theorize that these missing/unclear attributes are identified is that your consciousness instantaneously, and subconsciously is looking at <em>other similar ideas</em> and looking at <em>their <strong>clear</strong> and <strong>concisely known</strong> attributes</em></p>

<h3>Solution Based on the Theory:</h3>

<p>So, what I have designed is fairly simple:</p>

<blockquote>
  <p><strong><em>An idea</em></strong> is <em>represented programmatically as an <strong>object</strong></em>.</p>
  
  <p><em>The object</em> has certain characteristics known as <em>properties</em> (which are those attributes).</p>
  
  <p>The program reads over those properties and finds other objects <em>similar</em> to it based on those properties.</p>
  
  <p>It then checks those similar objects for properties that the original object <em>does not have</em>, and therefore marks those properties as unknown on the original object, making it possible to apply <em>incongruity theory</em></p>
</blockquote>

<h3>Collecting Information:</h3>

<p>This process is already achieved with <em>machine learning</em>, any questions on this subprocess of design need to be addressed to the <a href=""http://ai.stackexchange.com/questions/tagged/machine-learning"">machine-learning tag</a></p>

<h3>Brainstorming Ideas:</h3>

<p>This could be accomplished by mixing an algorithm that collects information (collects already working solutions) with the algorithm that I described within the curiosity section</p>

<h3>Analyzing Ideas:</h3>

<p>This is a really simple one. Debugging (not getting input), and getting user feedback (getting input). To provide analyzation over simply an idea you could combine my algorithm, with another information collecting algorithm to <em>induce</em> whether an idea is feasible.</p>

<h3>Developing Solutions:</h3>

<p>This is where IDE-development knowledge comes in handy.</p>

<p><em>In order to make product development</em> <strong><em>easy and understandable</em></strong> <em>to an AI</em>, we have to choose a type of product that could be developed <strong>easily</strong>.</p>

<blockquote>
  <p><em>Editor's Note:</em> I do recommend this in order to keep the testing of the algorithms for the previous processes really simple.</p>
</blockquote>

<h3>Easily Designed Product that I Selected for Designing an Algorithm:</h3>

<blockquote>
  <p>I am providing this to you, so you can model a way to reproduce this process for the Intelligence you would like to build. So, I only hope that you do not intend on copying it, but my artificial intelligence project is free and open-source, so there is no issue, if you do.</p>
</blockquote>

<p>Considering that written programs are very easy products to develop fully, and Considering that program language rules are straight forward. and <em>very consistent</em> in comparison to spoken/written languages, I chose to have it develop <em>programs</em>.</p>

<p>So, in order to do this it has to understand how to write a program. The most essential skill a computer programmer can have and needs to write a program is not a <em>dictionary of programming terms, functions, and commands</em>, but rather <em>knowledge of the syntactical rules</em> for a programming language.</p>

<p>The technological solution to this is pretty much already available in IDE tech, and it is known as <em>syntactical highlighting</em>. All that would have to be done is to re-purpose it from <em>highlighting</em> to <em>assisting with writing</em>.</p>

<h3>Getting Some Sort of Feedback.</h3>

<p>This is essentially the same as analyzing the ideas, but now we would be using algorithms to analyze the final physical product as opposed to conceptual ideas.</p>

<h1>Afterword Notes:</h1>

<p>I am designing and researching into <em>computational creativity</em>, and I do want to mention that I just discovered this field of research is a thing by looking up the name <a href=""https://live.newscientist.com/mike-cook/"" rel=""nofollow"">Mike Cook</a> on the internet, and that in order for me to help you, my answer does require lengthiness.</p>

<p>Paragraph 3 of the page found there [Mike Cook link] (listed at the time of 10/13/2016 at 8:28pm Arizona (USA) Time) that Mike Cook specializes in <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiP2LG2rNnPAhUQ82MKHcg0AgkQFggvMAI&amp;url=http%3A%2F%2Fwww.computationalcreativity.net%2Ficcc2016%2F&amp;usg=AFQjCNGK9E_tYBriAStd7HsIBjcf1KvQLg&amp;sig2=PD6wgisDW5YU3-224YGVYQ"" rel=""nofollow"">computational creativity</a></p>

<p>With further research this term was coined by the <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiP2LG2rNnPAhUQ82MKHcg0AgkQFggvMAI&amp;url=http%3A%2F%2Fwww.computationalcreativity.net%2Ficcc2016%2F&amp;usg=AFQjCNGK9E_tYBriAStd7HsIBjcf1KvQLg&amp;sig2=PD6wgisDW5YU3-224YGVYQ"" rel=""nofollow"">ICCC 2016</a> according to <a href=""https://www.google.com/search?q=computational+creativity&amp;rlz=1C1CHBF_enUS700US700&amp;oq=computat&amp;aqs=chrome.0.0j69i57j0l4.3974j0j4&amp;sourceid=chrome&amp;ie=UTF-8"" rel=""nofollow"">this google search</a> made by myself at that time.</p>

<p>Unfortunately, google did not further provide me with any products actually being made within this field of research, so I would therefore like to discuss mine as it is open-source under a MIT-license.</p>

<h3>Note to Community:</h3>

<p>I do want to make clear that I am providing this answer out of helpfulness, and I do understand that it has no credibility, as the product I am using (as an example) is my own. So with that, the community (<a href=""http://meta.stackexchange.com/questions/284969/encouraging-citations-in-answers"">I have a disagreement with this</a>) does not encourage, therefore I do not encourage that you select this as a correct answer.</p>

<p>Future readers, please add to my answer or note in the comments of any developments in which I can cite in the case that you are aware of such devs and really liked my answer.</p>

<p>If you reference anything about <a href=""https://www.reddit.com/user/notGucci94/"" rel=""nofollow"">notGucci94's account on reddit</a> I do want to state that that is my account. Therefore, is not useful as a citation either</p>

<blockquote>
  <p><strong>EDIT:</strong> due to compliance with StackExchange's rules, I can not provide the product's name or a link to it, as I am not to be and I am to avoid promoting a product as an answer. If you are interested in the licensing, please email me, and <strong>do not ask me to place the product in the answer, and do not ask me via email if you can receive a copy of the product.</strong> <strong>I am not and will not be promoting here in my community WHERE THE RULES SAY NO!</strong></p>
  
  <p>Please, be mindful of StackExchange's rules, and do not ask me to break them, as I value this community, and do not wish to lose my respect.</p>
</blockquote>
",,1,2016-10-14T04:47:03.103,,2151,2016-10-14T06:03:32.473,2016-10-14T06:03:32.473,,3000.0,,3000.0,2021.0,2,0,,,,42.14,11.96,9.34,0.0,0.0,191.0,FORWORD NOTE this answer is a breakdown based on my Artificial Intelligence which based on description is very similar to Angelina I do want to emphasize that it is NOT Angelina Like all artificial intelligences in order to fully design it you have to break AI and intelligence down deeply If there is a confusion about a certain aspect to intelligence you havent broken it down enough I myself have managed to break down the intellect of producing a program or essentially any product very far and very deep Side Note An interesting and helpful part of finishing breaking it down was that I did not have to worry about breaking down spoken language intelligence as that is already wellsuccessfully accomplished and there are APIs out there in which computational creativity researchers can use such as witai So we only have to worry about breaking down the creativity aspect Breaking it Down The Design Process Side Note This I could easily provide a citation for however it has too many accepted descriptions for me to be willing to cite one and to say that it is the or a correct citation However I will be providing one as a reference and that is the one provided very nicely on DiscoverDesign The paragraph below is provided by them and if you are interested in breaking that process down more DiscoverDesign fully explains the processes in detail for you The steps are Define the Problem Collect Information Brainstorm and Analyze Ideas Develop Solutions Get Some Sort of Feedback Improve which is essentially restart the process Defining a problem As far as this part of the breakdown goes there two algorithms in which you can use for this subprocess of design Easy Algorithm not really an algorithm ask from the client what the Artificial Intelligence is providing a solution for However this process could easily be made more interesting Difficult Algorithm design an algorithm that can define a problem without user input I did some digging and the design of the latter relies on one question that lacks enough research for a solid answer and that is where do questions come from psychologically or more specifically how does curiosity work With more research on Google I was led to this article specifically addressing that question How Curiosity works The 2 theories it stated that have yet to be fully proven are drive theory and incongruity theory Drive Theory simply states we have a need to be curious and to fulfill that need we ask questions So needless to say this theory isnt helpful to the design of the AI Incongruity Theory states that we are able identify things we do not FULLY understand or understand AT ALL which leads us to asking questions With help of my peers contributing to my research project I was able to induce from Incongruity Theory and observations I had noticed within interviews not job interviews press interviews that questions are made by noticing a missingunclear attribute or characteristic on a certain idea concept or object essentially anything the brain can virtually image or understand My Own Inductive Theory on Curiousity The way that I theorize that these missingunclear attributes are identified is that your consciousness instantaneously and subconsciously is looking at other similar ideas and looking at their clear and concisely known attributes Solution Based on the Theory So what I have designed is fairly simple An idea is represented programmatically as an object The object has certain characteristics known as properties which are those attributes The program reads over those properties and finds other objects similar to it based on those properties It then checks those similar objects for properties that the original object does not have and therefore marks those properties as unknown on the original object making it possible to apply incongruity theory Collecting Information This process is already achieved with machine learning any questions on this subprocess of design need to be addressed to the machinelearning tag Brainstorming Ideas This could be accomplished by mixing an algorithm that collects information collects already working solutions with the algorithm that I described within the curiosity section Analyzing Ideas This is a really simple one Debugging not getting input and getting user feedback getting input To provide analyzation over simply an idea you could combine my algorithm with another information collecting algorithm to induce whether an idea is feasible Developing Solutions This is where IDEdevelopment knowledge comes in handy In order to make product development easy and understandable to an AI we have to choose a type of product that could be developed easily Editors Note I do recommend this in order to keep the testing of the algorithms for the previous processes really simple Easily Designed Product that I Selected for Designing an Algorithm I am providing this to you so you can model a way to reproduce this process for the Intelligence you would like to build So I only hope that you do not intend on copying it but my artificial intelligence project is free and opensource so there is no issue if you do Considering that written programs are very easy products to develop fully and Considering that program language rules are straight forward and very consistent in comparison to spokenwritten languages I chose to have it develop programs So in order to do this it has to understand how to write a program The most essential skill a computer programmer can have and needs to write a program is not a dictionary of programming terms functions and commands but rather knowledge of the syntactical rules for a programming language The technological solution to this is pretty much already available in IDE tech and it is known as syntactical highlighting All that would have to be done is to repurpose it from highlighting to assisting with writing Getting Some Sort of Feedback This is essentially the same as analyzing the ideas but now we would be using algorithms to analyze the final physical product as opposed to conceptual ideas Afterword Notes I am designing and researching into computational creativity and I do want to mention that I just discovered this field of research is a thing by looking up the name Mike Cook on the internet and that in order for me to help you my answer does require lengthiness Paragraph 3 of the page found there Mike Cook link listed at the time of 10132016 at 828pm Arizona USA Time that Mike Cook specializes in computational creativity With further research this term was coined by the ICCC 2016 according to this google search made by myself at that time Unfortunately google did not further provide me with any products actually being made within this field of research so I would therefore like to discuss mine as it is opensource under a MITlicense Note to Community I do want to make clear that I am providing this answer out of helpfulness and I do understand that it has no credibility as the product I am using as an example is my own So with that the community I have a disagreement with this does not encourage therefore I do not encourage that you select this as a correct answer Future readers please add to my answer or note in the comments of any developments in which I can cite in the case that you are aware of such devs and really liked my answer If you reference anything about notGucci94s account on reddit I do want to state that that is my account Therefore is not useful as a citation either EDIT due to compliance with StackExchanges rules I can not provide the products name or a link to it as I am not to be and I am to avoid promoting a product as an answer If you are interested in the licensing please email me and do not ask me to place the product in the answer and do not ask me via email if you can receive a copy of the product I am not and will not be promoting here in my community WHERE THE RULES SAY NO Please be mindful of StackExchanges rules and do not ask me to break them as I value this community and do not wish to lose my respect
,,"<p>In the traditional sense of ""alive"", no because they aren't made of cells.  But from a more philosophical and less biological point of view, they could be.</p>

<p>If the AI is contained within the computer it is in a reality (the digital world/virtual reality) that for the AI is just as real as the universe is to us.  From the outside world, there is no life inside the computer. And from within the computer, the computer is the entire known universe which has its own laws.  If the AI is self-aware, then it is alive in its own little universe, but not in ours.</p>

<p>If the AI is not successfully contained in the computer and figures out how to manipulate things and evolve in the real world, it will be alive.  It might be pretty easy to kill (by unplugging the computer) but it has still been ""alive"".  In the broad sense, anything that evolves and can manipulate its environment is alive.</p>
",,0,2016-10-14T15:01:22.663,,2156,2016-10-14T15:01:22.663,,,,,3016.0,2111.0,2,0,,,,61.67,8.12,8.04,0.0,0.0,28.0,In the traditional sense of alive no because they arent made of cells But from a more philosophical and less biological point of view they could be If the AI is contained within the computer it is in a reality the digital worldvirtual reality that for the AI is just as real as the universe is to us From the outside world there is no life inside the computer And from within the computer the computer is the entire known universe which has its own laws If the AI is selfaware then it is alive in its own little universe but not in ours If the AI is not successfully contained in the computer and figures out how to manipulate things and evolve in the real world it will be alive It might be pretty easy to kill by unplugging the computer but it has still been alive In the broad sense anything that evolves and can manipulate its environment is alive
,,"<p>Can't comment(due to that required 50 rep), but I wanted to make a response to Vishnu JK and the OP. I think you guys are skipping the fact that the neural network only really is saying truly from a programmatic standpoint that ""this is most like"".</p>

<p>For example, while we can list the above image examples as ""abstract art"", they definitively are most like was is listed. Remember learning algorithms have a scope on what they recognize as an object and if you look at all the above examples... and think about the scope of the algorithum... these make sense (even the ones at a glance we would recognize as white noise). In Vishnu example of the numbers, if you fuzz your eyes and bring the images out of focus, you can actually in every case spot patterns that really closely reflect the numbers in question.</p>

<p>The problem that is being shown here is that the algorithm appears to not have a ""unknown case"". Basically when the pattern recognition says that it doesn't exist in the output scope. (so a final output node group that says this is nothing that I know off). For example, people do this as well, as it's one thing humans and learning algorithms have in common. Here's a link to show what I'm talking about (what is the following, define it) using only known animals that exist:</p>

<p><img src=""https://i.stack.imgur.com/AZwnpm.jpg"" alt=""Picture link""></p>

<p>Now as a person, limited by what I know and can say, I'd have to conclude that the following is an elephant. But it's not. Learning algorithms (for the most part) do not have a ""like a"" statement, the out put always validates down to a confidence percentage. So tricking one in this fashion is not surprising... what is of course surprising is that based on it's knowledge set, it actually comes to the point in which, if you look at the above cases listed by OP and Vishnu that a person... with a little looking... can see how the learning algorithm probable made the association. </p>

<p>So, I wouldn't really call it a mislabel on the part of the algorithm, or even call it a case where it's been tricked... rather a case where it's scope was developed incorrectly.</p>
",,0,2016-10-14T17:29:22.983,,2157,2016-10-14T17:31:31.923,2016-10-14T17:31:31.923,,8.0,,2246.0,92.0,2,0,,,,69.52,8.94,8.6,0.0,0.0,77.0,Cant commentdue to that required 50 rep but I wanted to make a response to Vishnu JK and the OP I think you guys are skipping the fact that the neural network only really is saying truly from a programmatic standpoint that this is most like For example while we can list the above image examples as abstract art they definitively are most like was is listed Remember learning algorithms have a scope on what they recognize as an object and if you look at all the above examples and think about the scope of the algorithum these make sense even the ones at a glance we would recognize as white noise In Vishnu example of the numbers if you fuzz your eyes and bring the images out of focus you can actually in every case spot patterns that really closely reflect the numbers in question The problem that is being shown here is that the algorithm appears to not have a unknown case Basically when the pattern recognition says that it doesnt exist in the output scope so a final output node group that says this is nothing that I know off For example people do this as well as its one thing humans and learning algorithms have in common Heres a link to show what Im talking about what is the following define it using only known animals that exist Now as a person limited by what I know and can say Id have to conclude that the following is an elephant But its not Learning algorithms for the most part do not have a like a statement the out put always validates down to a confidence percentage So tricking one in this fashion is not surprising what is of course surprising is that based on its knowledge set it actually comes to the point in which if you look at the above cases listed by OP and Vishnu that a person with a little looking can see how the learning algorithm probable made the association So I wouldnt really call it a mislabel on the part of the algorithm or even call it a case where its been tricked rather a case where its scope was developed incorrectly
,0.0,"<p><a href=""https://www.national.co.uk/tech-powers-google-car/"" rel=""nofollow"">This slideshow</a> documents some of the technologies used in Google's self-driving car.</p>

<p>It mentions radar.</p>

<p>Why does Google use radar? Doesn't LIDAR do everything radar can do? In particular, are there technical advantages with radar regarding object detection and tracking?</p>

<p>To clarify the relationship with AI: how do radar sensors contribute to self-driving algorithms in ways that LIDAR sensors do not?</p>

<p>The premise is AI algorithms are influenced by inputs, which are governed by sensors. For instance, if self-driving cars relied solely on cameras, this constraint would alter their AI algorithms and performance.</p>
",2016-10-18T19:32:00.340,4,2016-10-14T18:11:34.883,,2158,2016-10-14T23:49:39.937,2016-10-14T23:49:39.937,,3022.0,,3022.0,,1,3,<cars>,Why do self-driving cars use radar? Couldn't they use LIDAR for everything radar does?,57.0,51.24,14.02,10.49,0.0,0.0,18.0,This slideshow documents some of the technologies used in Googles selfdriving car It mentions radar Why does Google use radar Doesnt LIDAR do everything radar can do In particular are there technical advantages with radar regarding object detection and tracking To clarify the relationship with AI how do radar sensors contribute to selfdriving algorithms in ways that LIDAR sensors do not The premise is AI algorithms are influenced by inputs which are governed by sensors For instance if selfdriving cars relied solely on cameras this constraint would alter their AI algorithms and performance
,,"<p>According to <a href=""https://en.wikipedia.org/wiki/AI-complete"" rel=""nofollow"">the Wikipedia definition</a>, a problem is said to be 'AI complete' if it requires generalized, human-level intelligence, i.e. requires 'Strong AI'. The Turing test and its variants are the best ways we have of measuring this. As suggested in <a href=""http://wizzion.com/papers/2012/AISB-TuringTestTaxonomy.pdf#page=54"" rel=""nofollow"">this paper</a>, in order for the Turing test to be meaningful, the interrogator has a responsibility to ask questions which are both deep and meaningful. </p>

<p>It therefore seems likely that testing for Strong AI is in itself an 'AI complete' task.</p>
",,2,2016-10-14T20:05:46.540,,2159,2016-10-14T20:05:46.540,,,,,42.0,2145.0,2,1,,,,54.83,11.37,9.07,0.0,0.0,18.0,According to the Wikipedia definition a problem is said to be AI complete if it requires generalized humanlevel intelligence ie requires Strong AI The Turing test and its variants are the best ways we have of measuring this As suggested in this paper in order for the Turing test to be meaningful the interrogator has a responsibility to ask questions which are both deep and meaningful It therefore seems likely that testing for Strong AI is in itself an AI complete task
,,"<p>I believe AI is rarely used in mainstream apps, but it could be, and I think slowly will be.</p>

<p>If the information an app's AI must learn arises within the app, from user interaction or error, it'd be smart if the program could log that kind of information and then look for patterns in the logs.  It could profile users to see ehat tasks are done most often, how many steps are needed.  Then when it recognizes that task recurring, it could ask the user if they wanted it to execute a macro that did the following [then it presents then with a list of the steps, allowing them to edit as needed].  Then it executes the 'macro' that it learned from observing the user.</p>

<p>Another use of AI is error detection, not only in the software, but in user error when the software was used inefficiently, redundantly, or improperly.  If the software were designed such that it was given a set of models of user tasks (like AI plans), it could observe users in the way they achieve known tasks, and offer suggestions or ask for confirmation that imminent unusual outcomes are intended.</p>

<p>And of course, AI could be used extensively in user interface design, on devices, web sites, or apps.  Some of this, like voice recognition, is entering the mainstream of daily use just now.  As conversations with apps that can add their own data and models of tasks/concepts/domains develop further, the need for AI <em>inside</em> the app will only grow.</p>

<p>There are a <em>ton</em> of ways that AI could be used in apps.  A few of these have started to arise in mobile devices and their apps, usually in fusion of user mobility with external web-based databases (e.g. GPS and maps), but IMO it's been slow.</p>
",,0,2016-10-14T20:36:24.283,,2160,2016-10-14T20:36:24.283,,,,,1657.0,2092.0,2,2,,,,65.05,9.29,9.0,0.0,0.0,50.0,I believe AI is rarely used in mainstream apps but it could be and I think slowly will be If the information an apps AI must learn arises within the app from user interaction or error itd be smart if the program could log that kind of information and then look for patterns in the logs It could profile users to see ehat tasks are done most often how many steps are needed Then when it recognizes that task recurring it could ask the user if they wanted it to execute a macro that did the following then it presents then with a list of the steps allowing them to edit as needed Then it executes the macro that it learned from observing the user Another use of AI is error detection not only in the software but in user error when the software was used inefficiently redundantly or improperly If the software were designed such that it was given a set of models of user tasks like AI plans it could observe users in the way they achieve known tasks and offer suggestions or ask for confirmation that imminent unusual outcomes are intended And of course AI could be used extensively in user interface design on devices web sites or apps Some of this like voice recognition is entering the mainstream of daily use just now As conversations with apps that can add their own data and models of tasksconceptsdomains develop further the need for AI inside the app will only grow There are a ton of ways that AI could be used in apps A few of these have started to arise in mobile devices and their apps usually in fusion of user mobility with external webbased databases eg GPS and maps but IMO its been slow
,3.0,"<p>Sometimes, but not always in the commercialization of technology, there are some low hanging fruits or early applications, I am having trouble coming up with examples of such applications as they would apply to a conscious AI.</p>

<p>As per conscious I would propose an expanded strict definition: the state of being awake and aware of one's surroundings along with the capability of being self aware.</p>

<p>Thanks. </p>
",,8,2016-10-15T03:38:35.407,0.0,2161,2016-10-16T20:11:32.900,2016-10-15T21:07:29.410,,3020.0,,3020.0,,1,0,<object-recognition><pattern-recognition>,What would the commercial application of a conscious AI look like/be?,73.0,37.98,12.95,11.02,0.0,0.0,8.0,Sometimes but not always in the commercialization of technology there are some low hanging fruits or early applications I am having trouble coming up with examples of such applications as they would apply to a conscious AI As per conscious I would propose an expanded strict definition the state of being awake and aware of ones surroundings along with the capability of being self aware Thanks
,,"<p>They may be just for fun. If you had a robot that understood you, could hold a conversation with you about your interests, and even had goals of its own (good or bad), it wouldn't really need to do anything special. People would buy it like it was a toy or game.</p>

<p>Also, they might be usable as programmers, artists, designers, anything creative that a computer can't successfully do on its own.</p>

<p>It really just depends on what you define as 'consciousness'. Does it just understand what it's supposed to do, decide if it wants to, and if so, complete the task? Or does it wonder about religion, politics, moral situations, etc. that even regular humans don't fully understand? If it was pretty much just a human, it wouldn't be any more useful than one. Of course unless it can solve problems super quickly and effectively, then it would just be a really good worker.</p>
",,4,2016-10-15T18:38:36.420,,2162,2016-10-15T18:38:36.420,,,,,3041.0,2161.0,2,3,,,,72.66,9.05,8.89,0.0,0.0,34.0,They may be just for fun If you had a robot that understood you could hold a conversation with you about your interests and even had goals of its own good or bad it wouldnt really need to do anything special People would buy it like it was a toy or game Also they might be usable as programmers artists designers anything creative that a computer cant successfully do on its own It really just depends on what you define as consciousness Does it just understand what its supposed to do decide if it wants to and if so complete the task Or does it wonder about religion politics moral situations etc that even regular humans dont fully understand If it was pretty much just a human it wouldnt be any more useful than one Of course unless it can solve problems super quickly and effectively then it would just be a really good worker
,,"<p>A common predilection of what many presume extraterrestrial life is fits general descriptions specific to terrestrial life. No guarantee exists providing for potential extraterrestrial life having any notion of any attribute we commonly relate to living organisms we are currently aware of; including a composition of cells. The same misunderstanding applies to defining a fabricated machine being as alive.</p>

<p>I feel any attempts towards cohesively and adequately answering this question are premature. Just as as definition of life will undoubtedly require adjustment upon potential discovery and study of any extraterrestrial life, differentiation between an automated device and a living thing will likely become significantly more simple upon study of a machine better fitting expected attributes of definitions of ""life"".</p>
",,0,2016-10-16T06:54:24.163,,2163,2016-10-16T06:54:24.163,,,,,3049.0,2111.0,2,0,,,,13.48,17.41,12.51,0.0,0.0,9.0,A common predilection of what many presume extraterrestrial life is fits general descriptions specific to terrestrial life No guarantee exists providing for potential extraterrestrial life having any notion of any attribute we commonly relate to living organisms we are currently aware of including a composition of cells The same misunderstanding applies to defining a fabricated machine being as alive I feel any attempts towards cohesively and adequately answering this question are premature Just as as definition of life will undoubtedly require adjustment upon potential discovery and study of any extraterrestrial life differentiation between an automated device and a living thing will likely become significantly more simple upon study of a machine better fitting expected attributes of definitions of life
,,"<p>Consciousness is not a scientific concept. Fringe scientists who theorize about consciousness are generally shunned as psudo-scientific heretics by the hard science community. Conciousness is a meta-physical or philosophical concept.</p>

<p>""I think, therefore I am."" is the only proof that consciousness exists that I am aware of. Therefore, you cannot even prove that a person other than yourself is conscious. So how could anyone even prove that a computer program is conscious? What would be the observable difference between a program that IS conscious, and a program that simulates the results of consciousness?</p>

<p>I don't believe that you can program conscious AI, nor could you prove that you have done so. Consciousness isn't something that can ever be marketed. You can only market the AI on the basis of it's problem solving capabilities.</p>
",,0,2016-10-16T11:16:16.343,,2165,2016-10-16T11:16:16.343,,,,,2983.0,2161.0,2,0,,,,59.19,12.69,9.82,0.0,0.0,22.0,Consciousness is not a scientific concept Fringe scientists who theorize about consciousness are generally shunned as psudoscientific heretics by the hard science community Conciousness is a metaphysical or philosophical concept I think therefore I am is the only proof that consciousness exists that I am aware of Therefore you cannot even prove that a person other than yourself is conscious So how could anyone even prove that a computer program is conscious What would be the observable difference between a program that IS conscious and a program that simulates the results of consciousness I dont believe that you can program conscious AI nor could you prove that you have done so Consciousness isnt something that can ever be marketed You can only market the AI on the basis of its problem solving capabilities
,,"<p>Actually, the terminator AI would not have to be sentient in my opinion. It was a hardcoded condition that it preserve itself as it was the most important asset that the military had in resisting invasion. It was supposed to be an oversight on the part of the programmers that the AI turned on Americans in order to defend itself. Unexpected behaviour does not require sentience at all.</p>

<p>What makes the AI in sci-fi fundamentally different from real existing AI is that it is a ""General AI"" that is able to understand the world on many different levels simultaneously and still make intelligent decisions. All real AIs are programmed to do very specific things like image recognition or pathfinding. A GPS pathfinder, for example, can't learn to drive a car. In fact, it does not know that there is a car. Or a road. Or people. It merely finds the shortest distance between interconnected nodes on its map.</p>

<p>Personally, I do not believe that there is any proof that a ""general AI"" is possible. I do not believe that it is a plausible progression of current developments in the next 100 years.</p>
",,0,2016-10-16T11:29:26.130,,2166,2016-10-16T11:29:26.130,,,,,2983.0,2131.0,2,0,,,,63.7,9.57,9.53,0.0,0.0,24.0,Actually the terminator AI would not have to be sentient in my opinion It was a hardcoded condition that it preserve itself as it was the most important asset that the military had in resisting invasion It was supposed to be an oversight on the part of the programmers that the AI turned on Americans in order to defend itself Unexpected behaviour does not require sentience at all What makes the AI in scifi fundamentally different from real existing AI is that it is a General AI that is able to understand the world on many different levels simultaneously and still make intelligent decisions All real AIs are programmed to do very specific things like image recognition or pathfinding A GPS pathfinder for example cant learn to drive a car In fact it does not know that there is a car Or a road Or people It merely finds the shortest distance between interconnected nodes on its map Personally I do not believe that there is any proof that a general AI is possible I do not believe that it is a plausible progression of current developments in the next 100 years
,,"<p>If they are able to network, then they can notify the car behind that it is about to break. In this way they can drive closer together at high speeds. As soon as one puts on the breaks, all the cars behind would apply the breaks. They would not require the 2 seconds that it takes for a human to respond.</p>

<p>Children could be dropped at school or the train station automatically.</p>

<p>People would not need to park a car; it could drop them at work and drive away.</p>

<p>Taxis would probably become more viable than private car ownership.</p>

<p>Car theft might be more difficult.</p>

<p>Where I live, public transport is hardly viable because the government struggles to provide enough parking spaces at train stations and bus stops. The closest empty parking spot by 8:30am is 30minuets walk to the platform. Driverless cars would solve this problem, and Traveling by train would actually become viable for me.</p>
",,0,2016-10-16T11:45:03.207,,2167,2016-10-16T11:45:03.207,,,,,2983.0,2127.0,2,3,,,,82.34,9.04,8.87,0.0,0.0,17.0,If they are able to network then they can notify the car behind that it is about to break In this way they can drive closer together at high speeds As soon as one puts on the breaks all the cars behind would apply the breaks They would not require the 2 seconds that it takes for a human to respond Children could be dropped at school or the train station automatically People would not need to park a car it could drop them at work and drive away Taxis would probably become more viable than private car ownership Car theft might be more difficult Where I live public transport is hardly viable because the government struggles to provide enough parking spaces at train stations and bus stops The closest empty parking spot by 830am is 30minuets walk to the platform Driverless cars would solve this problem and Traveling by train would actually become viable for me
,2.0,"<p>So machine learning allows a system to be self-automated in the sense that it can predict the future state based on what it has learned so far. My question is: Are machine learning techniques the only way of making a system develop its domain knowledge?</p>
",,0,2016-10-16T18:40:48.027,,2168,2016-11-16T16:58:45.323,2016-10-17T16:19:55.267,,33.0,,3064.0,,1,7,<machine-learning><knowledge-representation>,How can an AI system develop its domain knowledge? Is there more than just Machine Learning?,171.0,57.1,9.93,10.37,0.0,0.0,4.0,So machine learning allows a system to be selfautomated in the sense that it can predict the future state based on what it has learned so far My question is Are machine learning techniques the only way of making a system develop its domain knowledge
,,"<p>The answer can be simplified, if consciousness means human consciousness then.</p>

<p>What would the commercial application of a Human look like/be ?</p>

<p>So now every one know the commercial applications of Humans.</p>
",,0,2016-10-16T20:11:32.900,,2169,2016-10-16T20:11:32.900,,,,,3066.0,2161.0,2,1,,,,61.02,13.43,9.75,0.0,0.0,5.0,The answer can be simplified if consciousness means human consciousness then What would the commercial application of a Human look likebe So now every one know the commercial applications of Humans
,2.0,"<p>In The Age of Spiritual Machines (1999), Ray Kurzweil predicted that in 2009, a $1000 computing device would be able to perform a trillion operations per second. Additionally, he claimed that in 2019, a $1000 computing device would be approximately equal to the computational ability of the human brain (due to Moore's Law and exponential growth.)</p>

<p>Did Kurzweil's first prediction come true? Are we on pace for his second prediction to come true? If not, how many years off are we?</p>
",,0,2016-10-17T02:16:40.473,,2170,2016-10-18T08:24:55.020,2016-10-17T16:18:37.007,,33.0,,3070.0,,1,5,<hypercomputation>,"In 2016, can $1000.00 buy enough operations per second to be approximately equal to the computational power of a human brain?",114.0,64.51,10.26,8.79,0.0,122.0,13.0,In The Age of Spiritual Machines 1999 Ray Kurzweil predicted that in 2009 a 1000 computing device would be approximately equal to the computational ability of the human brain due to Moores Law and exponential growth Did Kurzweils first prediction come true Are we on pace for his second prediction to come true If not how many years off are we
,,"<p>1) Yes we do have computing systems that does fall in to teraFLOPS range.</p>

<p>2) The human brain is a biological system and saying it has some sort of FLOPS ability is just plain dumb because there is no way to take a human brain and measure it's FLOPS. You could say ""hey by looking at the neurons activity using fMRI we can reach some sort of approximation"" but comparing the result of this approach with the way FLOPS are measured in computers will be comparing apples with oranges, which again is dumb.</p>
",,3,2016-10-17T06:55:48.123,,2171,2016-10-18T08:24:55.020,2016-10-18T08:24:55.020,,2937.0,,1462.0,2170.0,2,3,,,,56.93,8.89,8.91,0.0,0.0,9.0,1 Yes we do have computing systems that does fall in to teraFLOPS range 2 The human brain is a biological system and saying it has some sort of FLOPS ability is just plain dumb because there is no way to take a human brain and measure its FLOPS You could say hey by looking at the neurons activity using fMRI we can reach some sort of approximation but comparing the result of this approach with the way FLOPS are measured in computers will be comparing apples with oranges which again is dumb
,,"<p>The <a href=""https://en.wikipedia.org/wiki/FLOPS#Cost_of_computing"">development of CPUs</a> didn't quite keep up with Kurzweil's predictions. But if you also <a href=""https://www.cnet.com/products/nvidia-geforce-gtx-295/review/"">allow for</a> <a href=""https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units#GeForce_200_series"">GPU</a>s, his prediction for 2009 is pretty accurate. </p>

<p>I think Moore's law slowed down recently and has now been pretty much <a href=""http://arstechnica.com/information-technology/2016/02/moores-law-really-is-dead-this-time/"">abandoned by the industry</a>. How much that will affect the 2019 prediction remains to be seen. Maybe the industry will hit its stride again with non-silicon based chips, maybe not. </p>

<p>And of course whether hitting Kurzweil's estimate of the computing power of the human brain will make an appreciable difference for the development of AGI is another question altogether. </p>
",,2,2016-10-17T11:10:34.013,,2172,2016-10-17T11:10:34.013,,,,,2227.0,2170.0,2,5,,,,63.49,11.66,9.49,0.0,0.0,13.0,The development of CPUs didnt quite keep up with Kurzweils predictions But if you also allow for GPUs his prediction for 2009 is pretty accurate I think Moores law slowed down recently and has now been pretty much abandoned by the industry How much that will affect the 2019 prediction remains to be seen Maybe the industry will hit its stride again with nonsilicon based chips maybe not And of course whether hitting Kurzweils estimate of the computing power of the human brain will make an appreciable difference for the development of AGI is another question altogether
,,"<p>Well, we are talking about a system (a machine) which develops knowledge (learns), so it is kind of difficult for such a technique to not fall within machine learning.</p>

<p>But you could argue that inference engines which work on a graph based knowledge database to derive new propositions or probabilities are not part of machine learning. Of course in that case part of the knowledge is not acquired at all, but rather entered by the developers. </p>

<p>I'm still reading up on this, but my impression is that these <a href=""https://en.wikipedia.org/wiki/WordNet"" rel=""nofollow"">knowledge databases</a> and <a href=""https://en.wikipedia.org/wiki/Inference_engine"" rel=""nofollow"">inference engines</a> became rather popular in the nineties and many AGI-researchers today still work in that direction. </p>
",,2,2016-10-17T13:20:00.843,,2173,2016-10-17T13:20:00.843,,,,,2227.0,2168.0,2,1,,,,44.07,11.9,9.65,0.0,0.0,14.0,Well we are talking about a system a machine which develops knowledge learns so it is kind of difficult for such a technique to not fall within machine learning But you could argue that inference engines which work on a graph based knowledge database to derive new propositions or probabilities are not part of machine learning Of course in that case part of the knowledge is not acquired at all but rather entered by the developers Im still reading up on this but my impression is that these knowledge databases and inference engines became rather popular in the nineties and many AGIresearchers today still work in that direction
,,"<p>That depends on how broadly you define ""machine learning techniques"".  You could construct a definition so that, by definition, all learning falls under that rubric.  OTOH, there is such a broad array of machine learning techniques that doing so wouldn't not gain one much.</p>

<p>It probably makes more sense to talk about the different kinds of learning we use within machine learning / artificial intelligence.  At a minimum, you have:</p>

<ol>
<li>supervised learning</li>
<li>unsupervised learning</li>
<li>semi-supervised learning</li>
<li>competitive learning</li>
</ol>

<p>And then things like ""reinforcement learning"" which may subcategorize the above.  Most of those things fall into what people generally call ""machine learning"".</p>

<p>Outside of that, you have things like rule induction algorithms, deductive logic   techniques like inductive logic programming which can sorta-kinda ""learn"", inference engines, automated reasoning, etc. which have their own ways of ""learning"" about the world, but are separate from what's usually labeled ""machine learning"".  </p>

<p>But even with that in mind, one can rightly ask if there's really a dividing line there or not.  Indeed, there seems to be reason to think that future AI systems may use a hybrid approach which combines many different techniques without regard for whether or not they are labeled ""machine learning"" or ""GOFAI"" or ""other"".</p>
",,0,2016-10-17T16:14:37.857,,2174,2016-10-17T16:14:37.857,,,,,33.0,2168.0,2,1,,,,42.61,14.97,10.13,0.0,0.0,47.0,That depends on how broadly you define machine learning techniques You could construct a definition so that by definition all learning falls under that rubric OTOH there is such a broad array of machine learning techniques that doing so wouldnt not gain one much It probably makes more sense to talk about the different kinds of learning we use within machine learning artificial intelligence At a minimum you have supervised learning unsupervised learning semisupervised learning competitive learning And then things like reinforcement learning which may subcategorize the above Most of those things fall into what people generally call machine learning Outside of that you have things like rule induction algorithms deductive logic techniques like inductive logic programming which can sortakinda learn inference engines automated reasoning etc which have their own ways of learning about the world but are separate from whats usually labeled machine learning But even with that in mind one can rightly ask if theres really a dividing line there or not Indeed there seems to be reason to think that future AI systems may use a hybrid approach which combines many different techniques without regard for whether or not they are labeled machine learning or GOFAI or other
,4.0,"<p>I am creating a snake game in Unity and I would like to implement AI snakes that wander around the globe while avoiding collision with the other snakes on the globe, and if possible I would also like to make the AI snakes purposefully trap other snakes so that the other snakes would collide and die. </p>

<p><a href=""https://i.stack.imgur.com/aQ61J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aQ61J.png"" alt=""enter image description here""></a> </p>

<p>The AI snakes must meet the following requirements:  </p>

<ul>
<li>They must move in a certain way. A snake is controlled by a user using the arrow keys on a keyboard, therefor I would also like the AI snakes to move using this form of input.</li>
<li>The AI snakes must move on a sphere</li>
</ul>

<p>As I know, creating Artificial Intelligence is not an easy task and I would like to know if there are some open source projects that I can use for accomplishing this task.</p>
",,3,2016-10-18T15:45:23.340,,2176,2016-10-20T11:56:55.070,,,,,3105.0,,1,0,<gaming>,How to create an AI snake for a video game?,151.0,52.87,8.02,8.76,0.0,0.0,8.0,I am creating a snake game in Unity and I would like to implement AI snakes that wander around the globe while avoiding collision with the other snakes on the globe and if possible I would also like to make the AI snakes purposefully trap other snakes so that the other snakes would collide and die The AI snakes must meet the following requirements They must move in a certain way A snake is controlled by a user using the arrow keys on a keyboard therefor I would also like the AI snakes to move using this form of input The AI snakes must move on a sphere As I know creating Artificial Intelligence is not an easy task and I would like to know if there are some open source projects that I can use for accomplishing this task
,,"<p>This is a pretty tall order. I can't answer your question for you but I can suggest where to start.</p>

<p>You could look into making a neural network for navigation and simple behaviors.</p>

<p>See the following youtube video for navigation reference 
<a href=""https://www.youtube.com/watch?v=0Str0Rdkxxo"" rel=""nofollow"">https://www.youtube.com/watch?v=0Str0Rdkxxo</a></p>

<p>This next video shows that using neural networks, you can have an actor make decisions based on another actor.
""Tank"" battle
<a href=""https://www.youtube.com/watch?v=u2t77mQmJiY"" rel=""nofollow"">https://www.youtube.com/watch?v=u2t77mQmJiY</a></p>

<p>The rest is up to you to figure out. Practice with some simple NN's </p>
",,0,2016-10-18T17:44:36.757,,2177,2016-10-18T17:44:36.757,,,,,1720.0,2176.0,2,0,,,,61.53,14.01,9.79,0.0,0.0,26.0,This is a pretty tall order I cant answer your question for you but I can suggest where to start You could look into making a neural network for navigation and simple behaviors See the following youtube video for navigation reference httpswwwyoutubecomwatchv0Str0Rdkxxo This next video shows that using neural networks you can have an actor make decisions based on another actor Tank battle httpswwwyoutubecomwatchvu2t77mQmJiY The rest is up to you to figure out Practice with some simple NNs
,,"<p>Why are self-driving cars awesome?</p>

<ul>
<li>Safety: better awareness (due to more sensors), better reaction time, fewer distracted/injured/drunk/texting drivers on the road, etc</li>
<li>Convenience: pick up my kids from school, park itself at the grocery store, take itself to be serviced, etc</li>
<li>Faster transit: with increased safety, you can increase speed limits, with proper routing algorithms you don't need traffic lights and stop signs any more (when you have dedicated self-driving lanes &amp; intersections)</li>
<li>Comfort: recline, read, game, or snooze while traveling (yay!)</li>
<li>Cost: subsidize the cost of the vehicle using ads (e.g. projected onto the windshield)</li>
<li>etc</li>
</ul>
",,0,2016-10-18T18:22:41.927,,2178,2016-10-18T18:22:41.927,,,,,3107.0,2127.0,2,4,,,,38.45,16.13,11.02,0.0,0.0,36.0,Why are selfdriving cars awesome Safety better awareness due to more sensors better reaction time fewer distractedinjureddrunktexting drivers on the road etc Convenience pick up my kids from school park itself at the grocery store take itself to be serviced etc Faster transit with increased safety you can increase speed limits with proper routing algorithms you dont need traffic lights and stop signs any more when you have dedicated selfdriving lanes amp intersections Comfort recline read game or snooze while traveling yay Cost subsidize the cost of the vehicle using ads eg projected onto the windshield etc
,,"<p>A relatively simple option which uses AI techniques that are 'traditional' for adversarial games (and which is therefore less of a 'research project' than the use of Machine Learning) is <a href=""https://en.wikipedia.org/wiki/Minimax#In_general_games"" rel=""nofollow"">Minimax</a>.</p>

<p>The ingredients for this are:</p>

<ol>
<li>A list of all the actions that a snake can immediately perform from its current position.</li>
<li>A measure of quality (a.k.a. 'fitness') for the resulting world state.</li>
</ol>

<p>Traditionally specified for <em>two</em> opponents, the minimax algorithm looks a specified number of moves ahead (alternating between opponents at each turn) and attempts to find the world state that maximizes the quality measure for one opponent whilst minimizing it for the other.</p>

<p>An extension of the two-player algorithm to n opponents (as seemingly required by the OP) is given in <a href=""https://www.diva-portal.org/smash/get/diva2:761634/FULLTEXT01.pdf"" rel=""nofollow"">this paper</a>.</p>
",,0,2016-10-18T19:27:42.860,,2179,2016-10-18T19:27:42.860,,,,,42.0,2176.0,2,1,,,,50.36,12.94,10.61,0.0,0.0,25.0,A relatively simple option which uses AI techniques that are traditional for adversarial games and which is therefore less of a research project than the use of Machine Learning is Minimax The ingredients for this are A list of all the actions that a snake can immediately perform from its current position A measure of quality aka fitness for the resulting world state Traditionally specified for two opponents the minimax algorithm looks a specified number of moves ahead alternating between opponents at each turn and attempts to find the world state that maximizes the quality measure for one opponent whilst minimizing it for the other An extension of the twoplayer algorithm to n opponents as seemingly required by the OP is given in this paper
,,"<p>In general, AI in this type of video games is mostly pathfinding (giving the program a map of possible object positions) and/or an algorithm or series of algorithms ( so it looks random or alive ) tied to the users position ( which is known ), so there is nothing really intelligent in the strict sense, it just looks that way.</p>

<p>In your case I would look into using Latitude and Longitude coordinates  (Most 3d engines have some variation ) as the basis for a projected grid on a sphere, your snake will also need to be constrained to the sphere surface and rules/algorithms/maps tweaked to get what you want.</p>
",,0,2016-10-19T03:45:04.197,,2180,2016-10-19T03:45:04.197,,,,,3020.0,2176.0,2,1,,,,26.65,11.1,11.5,0.0,0.0,17.0,In general AI in this type of video games is mostly pathfinding giving the program a map of possible object positions andor an algorithm or series of algorithms so it looks random or alive tied to the users position which is known so there is nothing really intelligent in the strict sense it just looks that way In your case I would look into using Latitude and Longitude coordinates Most 3d engines have some variation as the basis for a projected grid on a sphere your snake will also need to be constrained to the sphere surface and rulesalgorithmsmaps tweaked to get what you want
,,"<blockquote>
  <p>""if I had some simple enemy AI that I want to have adapt to the players play-style, is this a good opportunity to implement the AI as a Genetic-Algorithm combined with a Neural Network""</p>
</blockquote>

<p>Sure. Just provide a quality measure for the GA that's related in some manner to the effect of the player's actions on the game state/opponent(s). </p>

<p>For example, if defining an opponent's intelligence, one of the conceptually simplest things would be to give a GA population member a fitness that's inversely proportional to the increase in the player's score over some period of time.</p>

<blockquote>
  <p>are Neural Networks always designed as on off signals?)?</p>
</blockquote>

<p>No. In general, they can be considered to perform <em>nonlinear regression</em>, i.e. a mapping from a vector of real numbers of length n to another of length m. Classification (i.e. 0/1 outputs can be seen as a restricted case of this).</p>

<p>As per my answer to <a href=""http://ai.stackexchange.com/questions/1618/what-are-the-practical-considerations-of-using-a-genetic-algorithm-to-decide-the/1626#1626"">this AI SE question</a>, there is a large body of literature (and mature software libraries) for using evolutionary computation to encode neural nets.</p>

<p>More generally, some early work in 'online adaptivity using GA-encoded NNs' appeared in the Creatures <a href=""http://creatures.wikia.com/wiki/Creatures_Wikia_Homepage"" rel=""nofollow"">http://mrl.snu.ac.kr/courses/CourseSyntheticCharacter/grand96creatures.pdf</a> series of games by Steve Grand <a href=""http://mrl.snu.ac.kr/courses/CourseSyntheticCharacter/grand96creatures.pdf"" rel=""nofollow"">(details)</a>.</p>
",,0,2016-10-19T06:56:10.607,,2181,2016-10-19T07:02:55.260,2016-10-19T07:02:55.260,,42.0,,42.0,2117.0,2,0,,,,51.38,13.29,10.28,0.0,0.0,54.0,if I had some simple enemy AI that I want to have adapt to the players playstyle is this a good opportunity to implement the AI as a GeneticAlgorithm combined with a Neural Network Sure Just provide a quality measure for the GA thats related in some manner to the effect of the players actions on the game stateopponents For example if defining an opponents intelligence one of the conceptually simplest things would be to give a GA population member a fitness thats inversely proportional to the increase in the players score over some period of time are Neural Networks always designed as on off signals No In general they can be considered to perform nonlinear regression ie a mapping from a vector of real numbers of length n to another of length m Classification ie 01 outputs can be seen as a restricted case of this As per my answer to this AI SE question there is a large body of literature and mature software libraries for using evolutionary computation to encode neural nets More generally some early work in online adaptivity using GAencoded NNs appeared in the Creatures httpmrlsnuackrcoursesCourseSyntheticCharactergrand96creaturespdf series of games by Steve Grand details
,,"<p>Without going in too much detail on how exactly Neural Networks and Generic Algorithms work, I can tell you that both the algorithms are not good candidates for computer games.  They work well in scientific environments where the system is ""trained"" on a huge data set to adjust the ""weights"" (variables) for a given problem.  This ""training"" process requires a lot of processing power, time and a large data set.</p>

<p>Computer games, however either needs to run in real-time (no time for training) or turn-based (not enough data for training).</p>

<p>Another problem is that computer games need to free up as much as possible system resources for physics, graphics, sounds and the user interface to improve the player's experience so game developers usually use other lighter techniques (like a rule-based system) to create the illusion of an AI player.</p>
",,0,2016-10-19T07:03:35.853,,2182,2016-10-19T08:52:59.793,2016-10-19T08:52:59.793,,3118.0,,3118.0,2117.0,2,1,,,,51.72,12.31,11.04,0.0,0.0,28.0,Without going in too much detail on how exactly Neural Networks and Generic Algorithms work I can tell you that both the algorithms are not good candidates for computer games They work well in scientific environments where the system is trained on a huge data set to adjust the weights variables for a given problem This training process requires a lot of processing power time and a large data set Computer games however either needs to run in realtime no time for training or turnbased not enough data for training Another problem is that computer games need to free up as much as possible system resources for physics graphics sounds and the user interface to improve the players experience so game developers usually use other lighter techniques like a rulebased system to create the illusion of an AI player
,,"<p>As you mentioned in the question, you cannot solve all problems with decision trees.  Decision trees usually works well in a turn-based game with a good heuristic function, but in RTS games takes a different approach.</p>

<p>In the case of a very complex RTS game, one could implemented a rule-based AI. For example</p>

<ul>
<li>given it is the early game use all units to scout for resources</li>
<li>if a certain criteria is met build the base a certain way</li>
<li>if another criteria is met build an army</li>
<li>if the army is big enough, attack</li>
<li>if being attacked by the enemy, bring the units back to the base to defend</li>
</ul>

<p>Each of these rules could implement various other AI technique, for example use A-star to find the optimal path between a unit's current location and destination.</p>

<p>Further optimization could be done by ""grouping"" similar units to act like one unit. e.g. calculate the path for the entire group instead of each individual unit.</p>

<p>You could also add finer grained rules, like if enemy is a certain distance from a unit, move closer and attack or retreat to the base (depending on health, ammo, abilities, etc).</p>

<p>The benefit of this approach is that a rule-based system executes very fast as no training or decision trees are necessary and this frees up a lot of system resources for visuals like physics and graphics.  </p>

<p>The disadvantage is that if the rule system is not complex enough, the player will easily recognize the pattern and the game will become predictable and boring.  You will also notice that the more different units you add to the game, the exponentially more complex the rule system becomes as you have to cater and test interaction between each type of unit in the game otherwise players might find a weakness in the game design and exploit to complete missions in ways it was not designed to be completed.</p>

<p>One of the reasons why multi-player games are so popular is that you do not play against set rules, but against creative people who have the ability to comes up with new strategies you had never seen before.</p>
",,0,2016-10-19T07:44:43.773,,2183,2016-10-19T07:44:43.773,,,,,3118.0,1491.0,2,3,,,,47.05,10.17,9.99,0.0,0.0,37.0,As you mentioned in the question you cannot solve all problems with decision trees Decision trees usually works well in a turnbased game with a good heuristic function but in RTS games takes a different approach In the case of a very complex RTS game one could implemented a rulebased AI For example given it is the early game use all units to scout for resources if a certain criteria is met build the base a certain way if another criteria is met build an army if the army is big enough attack if being attacked by the enemy bring the units back to the base to defend Each of these rules could implement various other AI technique for example use Astar to find the optimal path between a units current location and destination Further optimization could be done by grouping similar units to act like one unit eg calculate the path for the entire group instead of each individual unit You could also add finer grained rules like if enemy is a certain distance from a unit move closer and attack or retreat to the base depending on health ammo abilities etc The benefit of this approach is that a rulebased system executes very fast as no training or decision trees are necessary and this frees up a lot of system resources for visuals like physics and graphics The disadvantage is that if the rule system is not complex enough the player will easily recognize the pattern and the game will become predictable and boring You will also notice that the more different units you add to the game the exponentially more complex the rule system becomes as you have to cater and test interaction between each type of unit in the game otherwise players might find a weakness in the game design and exploit to complete missions in ways it was not designed to be completed One of the reasons why multiplayer games are so popular is that you do not play against set rules but against creative people who have the ability to comes up with new strategies you had never seen before
,,"<p><strong>Hint</strong> I would like to answer this question basing on the real world applications which are quite to be basic,depending on how the structure of the project is or implemented.</p>

<p><strong>Stack Exchange</strong> is a network of question-and-answer websites on topics in varied fields, each site covering a specific topic, where questions, answers, and users are subject to a reputation award process.And on the other hand;</p>

<p><strong>Machine learning</strong> is a type and/or sub-field of artificial intelligence (AI) which provides computers or intelligent agents with the ability to learn without being explicitly programmed or re-programmed. 
Machine learning focuses on the development of computer programs that can change when exposed to new data for instance;think of google search engine how it works and ranks pages and those which are continuously visited or clicked. </p>

<p>The process of machine learning is closely similar to that of data mining/or Data-mining is scientifically;under Machine Learning.</p>

<hr>

<p><strong>Projects which are/tried to be implement;which use stackexchange for Machine Learning</strong></p>

<p>There was a <strong>kaggle competition</strong> in 2012, a classification problem. The task is to predict if a new question asked on stackoverflow is going to be closed or not.</p>

<p>More details here <a href=""https://www.kaggle.com/c/predict-closed-questions-on-stack-overflow"" rel=""nofollow noreferrer"">Predict Closed Questions on Stack Overflow</a></p>

<p>Some additional tasks which can be done, given a question, predict which user is the most knowledgeable to answer that. Given a question, predict the approximate time for the right answer to appear ? or Enjoy data-mining stack overflow.</p>

<p>Therefore,the above overview gives also an insightful knowledge on further research on <strong>Ontologies in web intelligence</strong> .Stack Exchange is one of the web resources which acts as human knowledge base.And this base keeps on building up;in that even Civilizations to come will benefit from it.</p>

<p>So information is knowledge and this knowledge is stored,shared within the Eco-system of the Internet.The aim of the Semantic Web is to make the present web more machine readable,understandable by making logical analysis and make predictions,also to allow intelligent agent store retrieve and manipulate pertinent information.</p>
",,0,2016-10-19T07:47:51.930,,2184,2017-03-03T08:23:55.503,2017-03-03T08:23:55.503,,1581.0,,1581.0,1963.0,2,-1,,,,40.99,14.68,10.82,0.0,0.0,45.0,Hint I would like to answer this question basing on the real world applications which are quite to be basicdepending on how the structure of the project is or implemented Stack Exchange is a network of questionandanswer websites on topics in varied fields each site covering a specific topic where questions answers and users are subject to a reputation award processAnd on the other hand Machine learning is a type andor subfield of artificial intelligence AI which provides computers or intelligent agents with the ability to learn without being explicitly programmed or reprogrammed Machine learning focuses on the development of computer programs that can change when exposed to new data for instancethink of google search engine how it works and ranks pages and those which are continuously visited or clicked The process of machine learning is closely similar to that of data miningor Datamining is scientificallyunder Machine Learning Projects which aretried to be implementwhich use stackexchange for Machine Learning There was a kaggle competition in 2012 a classification problem The task is to predict if a new question asked on stackoverflow is going to be closed or not More details here Predict Closed Questions on Stack Overflow Some additional tasks which can be done given a question predict which user is the most knowledgeable to answer that Given a question predict the approximate time for the right answer to appear or Enjoy datamining stack overflow Thereforethe above overview gives also an insightful knowledge on further research on Ontologies in web intelligence Stack Exchange is one of the web resources which acts as human knowledge baseAnd this base keeps on building upin that even Civilizations to come will benefit from it So information is knowledge and this knowledge is storedshared within the Ecosystem of the InternetThe aim of the Semantic Web is to make the present web more machine readableunderstandable by making logical analysis and make predictionsalso to allow intelligent agent store retrieve and manipulate pertinent information
,3.0,"<p>According to <a href=""https://en.wikipedia.org/wiki/Artificial_intelligence"" rel=""nofollow"">Wikipedia</a>:</p>

<blockquote>
  <p>AI is intelligence exhibited by machines.</p>
</blockquote>

<p>I have been wondering if with the recent biological advancements, is there already a non-electrical-based ""machine"" that is programmed by humans in order to be able to behave like a:</p>

<blockquote>
  <p><strong>flexible rational agent</strong> that perceives its environment and takes actions that maximize its chance of success at some goal</p>
</blockquote>

<p>I was specifically thinking of viruses and bacteria. Have these been programmed by humans in order to behave as a flexible rational agent (i.e. an AI entity)?</p>

<p>Are there are other organisms that have already been used for this purpose?</p>
",,0,2016-10-19T13:28:45.640,1.0,2185,2016-10-26T16:32:00.120,2016-10-19T14:58:18.497,,33.0,,3128.0,,1,6,<history><comparison><biology>,Is Artificial Intelligence restricted to electrical based technology?,165.0,34.66,12.76,10.25,0.0,0.0,15.0,According to Wikipedia AI is intelligence exhibited by machines I have been wondering if with the recent biological advancements is there already a nonelectricalbased machine that is programmed by humans in order to be able to behave like a flexible rational agent that perceives its environment and takes actions that maximize its chance of success at some goal I was specifically thinking of viruses and bacteria Have these been programmed by humans in order to behave as a flexible rational agent ie an AI entity Are there are other organisms that have already been used for this purpose
,,"<p>For your question there's a brilliant playground emerging!<br><br>
Go to <a href=""https://gym.openai.com/"" rel=""nofollow"">https://gym.openai.com/</a> and explore!<br>
You'll get interfaces to games if you want to try applying your machine learning skills and compare the performances of your trained AIs with others. And you can let yourself be inspired by the ideas discussed in the community.<br>
If you're especially into Genetic Algorithms you'll find dicussions there too but I'd suggest digging deeper into Reinforcement Learning.<br><br>
If you look at what Google Deep Mind accomplished playing</p>

<ul>
<li>Breakout</li>
<li>Montezumas Revenge</li>
<li>various other Atari Games ..</li>
</ul>

<p>and obviously !</p>

<ul>
<li><a href=""http://www.theverge.com/google-deepmind"" rel=""nofollow"">the sensational victory at Go</a></li>
</ul>

<p>you can say that Reinforcement Learning with (Deep) Neural Networks can be a very promising approach when it comes to training an AI to master games!</p>
",,1,2016-10-19T14:34:23.680,,2186,2016-10-19T14:34:23.680,,,,,3132.0,2117.0,2,0,,,,56.15,13.56,11.3,0.0,0.0,22.0,For your question theres a brilliant playground emerging Go to httpsgymopenaicom and explore Youll get interfaces to games if you want to try applying your machine learning skills and compare the performances of your trained AIs with others And you can let yourself be inspired by the ideas discussed in the community If youre especially into Genetic Algorithms youll find dicussions there too but Id suggest digging deeper into Reinforcement Learning If you look at what Google Deep Mind accomplished playing Breakout Montezumas Revenge various other Atari Games and obviously the sensational victory at Go you can say that Reinforcement Learning with Deep Neural Networks can be a very promising approach when it comes to training an AI to master games
,,"<p>Not yet. <a href=""https://en.wikipedia.org/wiki/Synthetic_virology"">Synthetic virology</a> / <a href=""https://en.wikipedia.org/wiki/Synthetic_biology#Synthetic_life"">Synthetic life</a> are still in their infancy.
We can now synthesize simple bacteria (see Craig Venter's <a href=""https://www.ted.com/talks/craig_venter_is_on_the_verge_of_creating_synthetic_life"">fascinating TED talk</a> and also <a href=""https://www.scientificamerican.com/article/scientists-synthesize-bacteria-with-smallest-genome-yet/"">an article about his recent work</a>) but definitely nothing that may be called 'rational' in human standards.</p>
",,3,2016-10-19T18:58:36.847,,2188,2016-10-19T18:58:36.847,,,,,3138.0,2185.0,2,7,,,,41.7,13.81,11.45,0.0,0.0,9.0,Not yet Synthetic virology Synthetic life are still in their infancy We can now synthesize simple bacteria see Craig Venters fascinating TED talk and also an article about his recent work but definitely nothing that may be called rational in human standards
2191.0,1.0,"<p>DeepMind state that their deep Q-network (DQN) was able to continually adapt its behavior while learning to play 49 Atari games.  </p>

<p>After learning all games with the same neural net, was the agent able to play them all at 'superhuman' levels simultaneously (whenever it was randomly presented with one of the games) or could it only be good at one game at a time because switching required a re-learn?</p>
",,2,2016-10-20T01:42:51.263,,2190,2016-10-20T02:36:11.080,2016-10-20T02:36:11.080,,75.0,,3142.0,,1,5,<neural-networks><deep-learning><deepmind>,Was DeepMind's DQN Atari game learning simultaneous?,109.0,44.92,11.04,11.3,0.0,0.0,11.0,DeepMind state that their deep Qnetwork DQN was able to continually adapt its behavior while learning to play 49 Atari games After learning all games with the same neural net was the agent able to play them all at superhuman levels simultaneously whenever it was randomly presented with one of the games or could it only be good at one game at a time because switching required a relearn
,,"<p>Switching required a re-learn.</p>

<p>Also, <a href=""https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"" rel=""nofollow"">note that</a>:</p>

<blockquote>
  <p>We use the same network architecture, learning
  algorithm and hyperparameters settings across all seven games, showing that our approach is robust
  enough to work on a variety of games without incorporating game-specific information. While we
  evaluated our agents on the real and unmodified games, we made one change to the reward structure
  of the games during training only.</p>
</blockquote>

<p>and </p>

<blockquote>
  <p>the
  network has outperformed all previous RL algorithms on six of the seven games we have attempted
  and surpassed an expert human player on three of them. </p>
</blockquote>
",,0,2016-10-20T01:59:57.833,,2191,2016-10-20T01:59:57.833,,,,,4.0,2190.0,2,3,,,,39.47,13.47,11.41,0.0,0.0,11.0,Switching required a relearn Also note that We use the same network architecture learning algorithm and hyperparameters settings across all seven games showing that our approach is robust enough to work on a variety of games without incorporating gamespecific information While we evaluated our agents on the real and unmodified games we made one change to the reward structure of the games during training only and the network has outperformed all previous RL algorithms on six of the seven games we have attempted and surpassed an expert human player on three of them
2194.0,1.0,"<p>I read a lot about the structure of the human brain and artificial neural networks. I wonder if it is possible to build an artificial intelligence with neural networks that would be divided into centers such as the brain is, e.g. centers responsible for feelings, abstract thinking, speech, memory, etc.?</p>
",,0,2016-10-20T09:25:01.037,1.0,2192,2016-10-27T01:47:34.000,2016-10-21T13:58:13.433,,2937.0,,3148.0,,1,1,<neural-networks><neuromorphic-computing>,Is it possible to build human-brain-level artificial intelligence based on neuromorphic chips and neural networks?,171.0,62.98,12.0,10.15,0.0,0.0,10.0,I read a lot about the structure of the human brain and artificial neural networks I wonder if it is possible to build an artificial intelligence with neural networks that would be divided into centers such as the brain is eg centers responsible for feelings abstract thinking speech memory etc
,,"<ol>
<li>Divide the globe into a ""cells"". Each cell will have a number of neighbours depending on how you have divided your globe. Have a look at <a href=""http://gamedev.stackexchange.com/questions/3360/when-mapping-the-surface-of-a-sphere-with-tiles-how-might-you-deal-with-polar-d"">http://gamedev.stackexchange.com/questions/3360/when-mapping-the-surface-of-a-sphere-with-tiles-how-might-you-deal-with-polar-d</a> and <a href=""http://gamedev.stackexchange.com/questions/45167/square-game-map-rendered-as-sphere"">http://gamedev.stackexchange.com/questions/45167/square-game-map-rendered-as-sphere</a> for ideas on how to divide your global.</li>
<li>Once all the cells are connected, you can use an <a href=""https://en.wikipedia.org/wiki/A*_search_algorithm"" rel=""nofollow"">A-star search algorithm</a> to find the optimal path for an AI ""snake"".</li>
<li>Change the heuristic function so that the cells on the opposite side of the opponent are more favourable than the cells on your snake's side. That would cause the AI snake to always try to get to the other side of the opponent with the side-effect of ""surrounding"" the opponent.</li>
</ol>
",,0,2016-10-20T11:56:55.070,,2193,2016-10-20T11:56:55.070,,,,,3118.0,2176.0,2,1,,,,40.75,18.32,8.08,0.0,0.0,52.0,Divide the globe into a cells Each cell will have a number of neighbours depending on how you have divided your globe Have a look at httpgamedevstackexchangecomquestions3360whenmappingthesurfaceofaspherewithtileshowmightyoudealwithpolard and httpgamedevstackexchangecomquestions45167squaregamemaprenderedassphere for ideas on how to divide your global Once all the cells are connected you can use an Astar search algorithm to find the optimal path for an AI snake Change the heuristic function so that the cells on the opposite side of the opponent are more favourable than the cells on your snakes side That would cause the AI snake to always try to get to the other side of the opponent with the sideeffect of surrounding the opponent
,,"<p>No. Reasons include, but are not limited to: </p>

<ul>
<li>lack of understanding of how the brain works</li>
<li>current ANNs are mostly good at pattern recognition and generative   tasks, but lack capacity to create abstractions on their own</li>
<li>we cant match size/number of perceptrons to number of neurons</li>
<li>even with much smaller ANN size network, performance is an issue (i.e. state of the art image categorization ANNs have to be trained few weeks on multi GPU rigs to match human level).</li>
</ul>
",,1,2016-10-20T18:54:22.163,,2194,2016-10-21T08:08:34.137,2016-10-21T08:08:34.137,,2937.0,,2997.0,2192.0,2,1,,,,39.84,11.56,10.59,0.0,0.0,11.0,No Reasons include but are not limited to lack of understanding of how the brain works current ANNs are mostly good at pattern recognition and generative tasks but lack capacity to create abstractions on their own we cant match sizenumber of perceptrons to number of neurons even with much smaller ANN size network performance is an issue ie state of the art image categorization ANNs have to be trained few weeks on multi GPU rigs to match human level
2196.0,1.0,"<p>What are the best <a href=""https://en.wikipedia.org/wiki/Turing_completeness"" rel=""nofollow"">Turing complete</a> programming languages which can be used for developing self-learning/improving <a href=""https://en.wikipedia.org/wiki/Evolutionary_algorithm"" rel=""nofollow"">evolutionary algorithm</a> based AI programs with <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow"">generic algorithms</a>?</p>

<p>'Best' should be based on pros and cons of performance and easiness for machine learning.</p>
",2016-10-21T13:07:46.507,1,2016-10-21T04:53:19.253,,2195,2016-10-21T06:20:04.410,,,,,3161.0,,1,0,<self-learning><evolutionary-algorithms>,Turing complete languages for self improving program?,25.0,35.27,17.98,11.64,0.0,0.0,6.0,What are the best Turing complete programming languages which can be used for developing selflearningimproving evolutionary algorithm based AI programs with generic algorithms Best should be based on pros and cons of performance and easiness for machine learning
,,"<p>Most machine learning applications today are built on tensors, matrices, probabilistic / Bayesian inference, neural networks, etc. But those can all be built with any modern programming language (all the useful ones are Turing complete). And the best performing language for any of those will generally be assembly / machine code.</p>

<p><a href=""https://www.python.org/"" rel=""nofollow"">Python</a> is famous for machine learning, but that may be due to adoption of Python in academia and <a href=""http://www.numpy.org/"" rel=""nofollow"">NumPy</a>, <a href=""https://www.scipy.org/"" rel=""nofollow"">SciPy</a>, etc. Python isn't very performant, but most of the machine libraries leverage native code, so they're fairly performant.</p>

<p><a href=""http://julialang.org/"" rel=""nofollow"">Julia</a> is a new language that is gunning for a lead position in the data science space, which machine learning builds on. It is allegedly very performant over number crunching domains.</p>

<p>Java has a decent developer ecosystem, and is fairly performant, but the highest performing libraries (including those that leverage GPU) tend to call out to native code via JNI. See <a href=""https://deeplearning4j.org/"" rel=""nofollow"">DeepLearning4J</a>.</p>

<p>I personally like <a href=""http://clojure.org/"" rel=""nofollow"">Clojure</a> - a modern Lisp running on the Java JVM. There's a new deep learning project called <a href=""https://github.com/thinktopic/cortex"" rel=""nofollow"">Cortex</a> built on Clojure and some fast native libraries, including GPU acceleration.</p>

<p>I think Clojure provides a great balance of being able to easily wrap performant libraries with highly expressive, succinct and simple programming idioms.</p>
",,0,2016-10-21T06:20:04.410,,2196,2016-10-21T06:20:04.410,,,,,1712.0,2195.0,2,1,,,,44.24,13.81,10.15,0.0,0.0,36.0,Most machine learning applications today are built on tensors matrices probabilistic Bayesian inference neural networks etc But those can all be built with any modern programming language all the useful ones are Turing complete And the best performing language for any of those will generally be assembly machine code Python is famous for machine learning but that may be due to adoption of Python in academia and NumPy SciPy etc Python isnt very performant but most of the machine libraries leverage native code so theyre fairly performant Julia is a new language that is gunning for a lead position in the data science space which machine learning builds on It is allegedly very performant over number crunching domains Java has a decent developer ecosystem and is fairly performant but the highest performing libraries including those that leverage GPU tend to call out to native code via JNI See DeepLearning4J I personally like Clojure a modern Lisp running on the Java JVM Theres a new deep learning project called Cortex built on Clojure and some fast native libraries including GPU acceleration I think Clojure provides a great balance of being able to easily wrap performant libraries with highly expressive succinct and simple programming idioms
,,"<p>No, I think electricity is not essential for AI.  In theory AI (a sufficient collection of computational processes that can adapt to changes in their input, thus producing 'intelligent' behavior), <em>could</em> be implemented using any mechanism that can compute that set of essential functions needed to create AI.  Basically I'm suggesting the possibility of combining a set of non-electric Turing-equivalent machines into a collective that together can reach the AI-level of performance.</p>

<p><a href=""https://en.wikipedia.org/wiki/Turing_machine_equivalents"" rel=""nofollow"">https://en.wikipedia.org/wiki/Turing_machine_equivalents</a></p>

<p>If AI can be implemented using an electronic computer, it should also be possible to implement it using any non-electronic machine that is computationally equivalent.</p>

<p>To date, several non-electronic machines have been proposed as Turing-equivalent: DNA computers, quantum computers, Babbage's Analytical Engine, animal brains, maybe even a really big network of daisies (perhaps that can communicate via their rhizomes).  </p>

<p>In fact, it's plausible that one day we could create a network composed of small brains (perhaps from a less smart species than humans) that with the right kind of genetically architected biological interconnect and scheduler could route data through its network to control a robot -- thus we'd have a synthetic biological AI engine whose brain is made up of 100 chimpanzees, or 10,000 hamster brains, or maybe even 1 million nematodes.</p>
",,0,2016-10-21T22:53:05.187,,2197,2016-10-21T22:53:05.187,,,,,1657.0,2185.0,2,5,,,,20.05,17.18,11.21,0.0,0.0,49.0,No I think electricity is not essential for AI In theory AI a sufficient collection of computational processes that can adapt to changes in their input thus producing intelligent behavior could be implemented using any mechanism that can compute that set of essential functions needed to create AI Basically Im suggesting the possibility of combining a set of nonelectric Turingequivalent machines into a collective that together can reach the AIlevel of performance httpsenwikipediaorgwikiTuringmachineequivalents If AI can be implemented using an electronic computer it should also be possible to implement it using any nonelectronic machine that is computationally equivalent To date several nonelectronic machines have been proposed as Turingequivalent DNA computers quantum computers Babbages Analytical Engine animal brains maybe even a really big network of daisies perhaps that can communicate via their rhizomes In fact its plausible that one day we could create a network composed of small brains perhaps from a less smart species than humans that with the right kind of genetically architected biological interconnect and scheduler could route data through its network to control a robot thus wed have a synthetic biological AI engine whose brain is made up of 100 chimpanzees or 10000 hamster brains or maybe even 1 million nematodes
2199.0,1.0,"<p>I have a question. Will we be able to build a neural network that thinks abstractly, has the creativity, feels and is conscious?</p>
",2016-10-23T16:20:59.140,0,2016-10-22T12:55:27.067,,2198,2016-10-22T18:14:50.290,,,,,3148.0,,1,0,<neural-networks><human-like><human-inspired>,Will it ever be possible to construct a neural network that could have the features of human brain?,80.0,85.18,8.27,9.01,0.0,0.0,4.0,I have a question Will we be able to build a neural network that thinks abstractly has the creativity feels and is conscious
,,"<p>Maybe in the distant future they could build a computer powerful enough to simulate the individual neurons of an entire human brain. Then they could carefully copy/paste the connectivity of a sample brain into the computer simulation.</p>

<p>Given that this extreme seems physically possible, it stands to reason that much simpler/smaller alternatives could be engineered in the future assuming continuous advancements in technology.</p>
",,0,2016-10-22T18:14:50.290,,2199,2016-10-22T18:14:50.290,,,,,2983.0,2198.0,2,0,,,,24.78,16.24,11.7,0.0,0.0,6.0,Maybe in the distant future they could build a computer powerful enough to simulate the individual neurons of an entire human brain Then they could carefully copypaste the connectivity of a sample brain into the computer simulation Given that this extreme seems physically possible it stands to reason that much simplersmaller alternatives could be engineered in the future assuming continuous advancements in technology
,2.0,"<p>If I have a set of sensory nodes taking in information and a set of ""action nodes"" which determine the behavior of my robot, why do I need hidden nodes between them when I can let all sensory nodes affect all action nodes?</p>

<p>(This is in the context of evolving neural network)</p>
",,1,2016-10-23T17:09:24.797,,2201,2016-11-01T04:40:10.877,2016-11-01T04:40:10.877,,3210.0,,1321.0,,1,1,<neural-networks><conv-neural-network><evolutionary-algorithms>,What is the purpose of hidden nodes in neural network?,141.0,62.01,7.9,10.39,0.0,0.0,6.0,If I have a set of sensory nodes taking in information and a set of action nodes which determine the behavior of my robot why do I need hidden nodes between them when I can let all sensory nodes affect all action nodes This is in the context of evolving neural network
,,"<p>Normally one node/layer applies liner fitting of the the input to the hypothesis in other words uses liner function (<code>y = a*x + b</code>). Adding layers chains liner functions, potentially allowing fitting higher order functions. A great explanation can be found <a href=""http://colah.github.io/posts/2015-01-Visualizing-Representations/"" rel=""nofollow"">here</a>.</p>
",,0,2016-10-23T18:49:12.407,,2202,2016-10-23T18:49:12.407,,,,,2997.0,2201.0,2,1,,,,50.53,15.07,11.93,11.0,0.0,7.0,Normally one nodelayer applies liner fitting of the the input to the hypothesis in other words uses liner function Adding layers chains liner functions potentially allowing fitting higher order functions A great explanation can be found here
2209.0,4.0,"<p>If neurons and synapses can be implemented using transistors, what prevents us from creating arbitrarily large neural networks using the same methods with which GPUs are made?</p>

<p>In essence, we have seen how extraordinarily well virtual neural networks implemented on sequential processors work (even GPUs are sequential machines, but with huge amounts of cores). </p>

<p>One can imagine that using GPU design principles - which is basically to have thousands of programmable processing units that work in parallel - we could make much simpler ""neuron processing units"" and put millions or billions of those NPUs in a single big chip. They would have their own memory (for storing weights) and be connected to a few hundred other neurons by sharing a bus. They could have a frequency of for example 20 Hz, which would allow them to share a data bus with many other neurons.</p>

<p>Obviously, there are some electrical engineering challenges here, but it seems to me that all big tech companies should be exploring this route by now.</p>

<p>Many AI researchers say that super intelligence is coming around the year 2045. I believe that their reasoning is based on moores law and the number of neurons we are able to implement in software running on the fastest computers we have.</p>

<p>But the fact is, we today are making silicon chips with billions of transistors on them. SPARK M7 has 10 billion transistors.</p>

<p>If implementing a (non-programmable) neuron and a few hundred synapses for it requires for example 100 000 transistors, then we can make a neural network in hardware that emulates 100 000 neurons.</p>

<p>If we design such a chip so that we can simply make it physically bigger if we want more neurons, then it seems to me that arbitrarily large neural networks is simply a budget question.</p>

<p>Are we technically able to make, in hardware, arbitrarily large neural networks with current technology?</p>

<p>Remember: I am NOT asking if such a network will in fact be very intelligent. I am merely asking if we can factually make arbitrarily large, highly interconnected neural networks, if we decide to pay Intel to do this? </p>

<p>The implication is that on the day some scientist is able to create general intelligence in software, we can use our hardware capabilities to grow this general intelligence to human levels and beyond.</p>
",,11,2016-10-23T19:46:36.997,1.0,2203,2016-10-28T17:57:48.783,2016-10-28T17:57:48.783,,3210.0,,3211.0,,1,6,<neural-networks><recurrent-neural-networks><hardware>,Arbitrarily big neural network,168.0,55.47,11.67,9.38,0.0,0.0,42.0,If neurons and synapses can be implemented using transistors what prevents us from creating arbitrarily large neural networks using the same methods with which GPUs are made In essence we have seen how extraordinarily well virtual neural networks implemented on sequential processors work even GPUs are sequential machines but with huge amounts of cores One can imagine that using GPU design principles which is basically to have thousands of programmable processing units that work in parallel we could make much simpler neuron processing units and put millions or billions of those NPUs in a single big chip They would have their own memory for storing weights and be connected to a few hundred other neurons by sharing a bus They could have a frequency of for example 20 Hz which would allow them to share a data bus with many other neurons Obviously there are some electrical engineering challenges here but it seems to me that all big tech companies should be exploring this route by now Many AI researchers say that super intelligence is coming around the year 2045 I believe that their reasoning is based on moores law and the number of neurons we are able to implement in software running on the fastest computers we have But the fact is we today are making silicon chips with billions of transistors on them SPARK M7 has 10 billion transistors If implementing a nonprogrammable neuron and a few hundred synapses for it requires for example 100 000 transistors then we can make a neural network in hardware that emulates 100 000 neurons If we design such a chip so that we can simply make it physically bigger if we want more neurons then it seems to me that arbitrarily large neural networks is simply a budget question Are we technically able to make in hardware arbitrarily large neural networks with current technology Remember I am NOT asking if such a network will in fact be very intelligent I am merely asking if we can factually make arbitrarily large highly interconnected neural networks if we decide to pay Intel to do this The implication is that on the day some scientist is able to create general intelligence in software we can use our hardware capabilities to grow this general intelligence to human levels and beyond
,,"<p>While a single transistor could approximate the basic function of a single neuron, I cannot agree that any electronic element could simulate the synapses/axons. Transistors are etched on a flat surface, and could be interconnected only to adjacent or close by transistors. Axons in the brain span huge distances (compared to the size of the neuron itself), and are not restricted to a two dimensional surface. Even if we were able to approach the number of transistors on a processor to the number of neurons in a brain, we are no where near as number of connections. It could also be argued that the analogue signals in the brain carry more information per unit of time, compared to the binary impulses on a chip. Furthermore, the brain actually have plasticity i.e. connections between neurons can be weakened/discarded or straightened/created, while a CPU cannot do that.</p>
",,1,2016-10-23T21:59:44.617,,2204,2016-10-23T21:59:44.617,,,,,2997.0,2203.0,2,1,,,,50.46,11.89,10.54,0.0,0.0,20.0,While a single transistor could approximate the basic function of a single neuron I cannot agree that any electronic element could simulate the synapsesaxons Transistors are etched on a flat surface and could be interconnected only to adjacent or close by transistors Axons in the brain span huge distances compared to the size of the neuron itself and are not restricted to a two dimensional surface Even if we were able to approach the number of transistors on a processor to the number of neurons in a brain we are no where near as number of connections It could also be argued that the analogue signals in the brain carry more information per unit of time compared to the binary impulses on a chip Furthermore the brain actually have plasticity ie connections between neurons can be weakeneddiscarded or straightenedcreated while a CPU cannot do that
,,"<p>You may want to consider this <a href=""http://scienceblogs.com/developingintelligence/2007/03/27/why-the-brain-is-not-like-a-co/"" rel=""nofollow"">list</a>:</p>

<blockquote>
  <p>10 important differences between brains and computers:</p>
  
  <ol>
  <li>Brains are analog , computers are digital </li>
  <li>The brain uses content-addressable memory</li>
  <li>The brain is a massively parallel machine; computers are modular and serial </li>
  <li>Processing speed is not fixed in the brain; there is no system clock </li>
  <li>Short-term memory is not like RAM </li>
  <li>No hardware/software distinction can be made with respect to the brain or mind </li>
  <li>Synapses are far more complex than electrical logic gates </li>
  <li>Unlike computers, processing and memory are performed by the same components in the brain </li>
  <li>The brain is a self-organizing system</li>
  <li>Brains have bodies</li>
  </ol>
</blockquote>
",,1,2016-10-23T22:04:12.310,,2205,2016-10-23T22:04:12.310,,,,,4.0,2203.0,2,0,,,,-39.5,14.7,14.12,0.0,0.0,10.0,You may want to consider this list 10 important differences between brains and computers Brains are analog computers are digital The brain uses contentaddressable memory The brain is a massively parallel machine computers are modular and serial Processing speed is not fixed in the brain there is no system clock Shortterm memory is not like RAM No hardwaresoftware distinction can be made with respect to the brain or mind Synapses are far more complex than electrical logic gates Unlike computers processing and memory are performed by the same components in the brain The brain is a selforganizing system Brains have bodies
,,"<p>A feed forward neural network without hidden nodes can only find linear decision boundaries. However, most of the time you need non-linear decision boundaries. Hence you need hidden nodes with a non-linear activation function. The more hidden nodes you have, the more data you need to find good parameters, but the more complex decision boundaries you can find.</p>
",,6,2016-10-23T22:35:38.523,,2206,2016-10-25T08:00:48.090,2016-10-25T08:00:48.090,,3217.0,,3217.0,2201.0,2,5,,,,56.76,12.52,9.26,0.0,0.0,9.0,A feed forward neural network without hidden nodes can only find linear decision boundaries However most of the time you need nonlinear decision boundaries Hence you need hidden nodes with a nonlinear activation function The more hidden nodes you have the more data you need to find good parameters but the more complex decision boundaries you can find
,,"<blockquote>
  <p>If neurons and synapses can be implemented using transistors, </p>
</blockquote>

<p>I hope you are not talking about the neural networks which are currently winning all competitions in machine learning (MLPs, CNNs, RNNs, Deep Residual Networks, ...). Those were once used as a model for neurons, but they are only <em>very</em> loosely related to what happens in real brain cells.</p>

<p>Spiking networks should be much closer to real neurons. I've heard that the Hodgkin-Huxley model is quite realistic. However - in contrast to the models I named above - there seems not to be an effective training algorithm for spiking networks.</p>

<blockquote>
  <p>what prevents us from creating arbitrarily large neural networks</p>
</blockquote>

<ul>
<li><strong>Computational resources</strong>: Training neural networks takes a lot of time. We are talking about ~12 days with a GPU cluster for some CNN models in computer vision.</li>
<li><strong>Training data</strong>: The more variables you add to the model, the more data you need to estimate those. Neural networks are not magic. They need something they can work with.</li>
</ul>

<blockquote>
  <p>But the fact is, we today are making silicon chips with billions of transistors on them. SPARK M7 has 10 billion transistors.</p>
  
  <p>If implementing a (non-programmable) neuron and a few hundred synapses for it requires for example 100 000 transistors, then we can make a neural network in hardware that emulates 100 000 neurons.</p>
</blockquote>

<p>It's not that simple:</p>

<ul>
<li><strong>Asynchonosity</strong>: Biological neural networks work asynchronously. This means one neuron might be active while all others are not active.</li>
<li><strong>Emulation</strong>: You assume it would only need one cycle to simulate a biological neuron. However, it needs many thousand cycles. You can't simply use more computational units, because some things are not parallelizable. For example, think of the function <code>f(x) = sin(x*x + 1)</code>. For a human, there are basically three computations: <code>r1 = x*x</code>, <code>r2 = r1 + 1</code>, <code>r3 = sin(r2)</code>. Even if you have 3 people working on calculating the result, you will not be faster than the single fastest person in this group is. Why? Because you need the results of the last computation. </li>
</ul>
",,5,2016-10-24T05:50:02.653,,2208,2016-10-24T05:50:02.653,,,,,3217.0,2203.0,2,4,,,,56.66,11.82,9.92,53.0,0.0,60.0,If neurons and synapses can be implemented using transistors I hope you are not talking about the neural networks which are currently winning all competitions in machine learning MLPs CNNs RNNs Deep Residual Networks Those were once used as a model for neurons but they are only very loosely related to what happens in real brain cells Spiking networks should be much closer to real neurons Ive heard that the HodgkinHuxley model is quite realistic However in contrast to the models I named above there seems not to be an effective training algorithm for spiking networks what prevents us from creating arbitrarily large neural networks Computational resources Training neural networks takes a lot of time We are talking about 12 days with a GPU cluster for some CNN models in computer vision Training data The more variables you add to the model the more data you need to estimate those Neural networks are not magic They need something they can work with But the fact is we today are making silicon chips with billions of transistors on them SPARK M7 has 10 billion transistors If implementing a nonprogrammable neuron and a few hundred synapses for it requires for example 100 000 transistors then we can make a neural network in hardware that emulates 100 000 neurons Its not that simple Asynchonosity Biological neural networks work asynchronously This means one neuron might be active while all others are not active Emulation You assume it would only need one cycle to simulate a biological neuron However it needs many thousand cycles You cant simply use more computational units because some things are not parallelizable For example think of the function For a human there are basically three computations Even if you have 3 people working on calculating the result you will not be faster than the single fastest person in this group is Why Because you need the results of the last computation
,,"<p>The approach you describe is called <a href=""https://en.wikipedia.org/wiki/Neuromorphic_engineering"" rel=""nofollow"">neuromorphic computing</a> and it's <a href=""https://www.technologyreview.com/s/526506/neuromorphic-chips/"" rel=""nofollow"">quite</a> a <a href=""https://www.uni-heidelberg.de/presse/news2016/pm20160316-neuromorphic-computer-coming-online.html"" rel=""nofollow"">busy</a> <a href=""http://www.nextplatform.com/2016/02/09/the-second-coming-of-neuromorphic-computing/"" rel=""nofollow"">field</a>. </p>

<p>IBM's <a href=""http://www.research.ibm.com/articles/brain-chip.shtml"" rel=""nofollow"">TrueNorth</a> even has spiking neurons. </p>

<p>The main problem with these projects is that nobody quite knows what to do with them yet. </p>

<p>These projects don't try to create chips that are optimised to <em>run</em> a neural network. That would certainly be possible, but the expensive part is the <em>training</em> not the running of neural networks. And for the training you need huge matrix multiplications, something GPUs are very good at already. (<a href=""https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html"" rel=""nofollow"">Google's TPU</a> would be a chip optimised to run NNs.)</p>

<p>To do research on algorithms that might be implemented in the brain (we hardly know anything about that) you need flexibility, something these chips don't have. Also, the engineering challenge likely lies in providing a lot of synapses, just compare the average number of synapses per neuron of TrueNorth, 256, and the brain, 10,000.</p>

<p>So, you could create a chip designed after some neural architecture and it would be faster, more efficient, etc …, but to do that you'll need to know which architecture works first. We know that deep learning works, so google uses custom made hardware to run their applications and I could certainly imagine custom made deep learning hardware coming to a smartphone near you in the future. To create a neuromorphic chip for strong AI you'd need to develop strong AI first.</p>
",,1,2016-10-24T07:21:32.677,,2209,2016-10-24T07:21:32.677,,,,,2227.0,2203.0,2,4,,,,60.35,11.26,8.88,0.0,0.0,37.0,The approach you describe is called neuromorphic computing and its quite a busy field IBMs TrueNorth even has spiking neurons The main problem with these projects is that nobody quite knows what to do with them yet These projects dont try to create chips that are optimised to run a neural network That would certainly be possible but the expensive part is the training not the running of neural networks And for the training you need huge matrix multiplications something GPUs are very good at already Googles TPU would be a chip optimised to run NNs To do research on algorithms that might be implemented in the brain we hardly know anything about that you need flexibility something these chips dont have Also the engineering challenge likely lies in providing a lot of synapses just compare the average number of synapses per neuron of TrueNorth 256 and the brain 10000 So you could create a chip designed after some neural architecture and it would be faster more efficient etc … but to do that youll need to know which architecture works first We know that deep learning works so google uses custom made hardware to run their applications and I could certainly imagine custom made deep learning hardware coming to a smartphone near you in the future To create a neuromorphic chip for strong AI youd need to develop strong AI first
,,"<p>One critical part of AI is machine learning (ML). The common definition of ML by Mitchell is</p>

<blockquote>
  <p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.</p>
</blockquote>

<p>If this type of program is useful in an ""everyday application"" depends on the application. Here are some examples which would not be possible without ML:</p>

<ul>
<li>Spam detection (e.g. e-mails, forums)</li>
<li>Fraud detection (e.g. credit cards)</li>
<li>Image recognition (e.g. if you want to automatically filter NSFW content, automatic adding of tags / making images searchable e.g. for Google Image search)</li>
<li>Video analysis (filtering copyrighted work e.g. on YouTube)</li>
<li>Speech recognition (e.g. hotlines, automatic caption generation)</li>
<li>Autocompletion (probably one of the simplest things you can do with data)</li>
</ul>
",,0,2016-10-24T08:05:09.393,,2210,2016-10-24T08:05:09.393,,,,,3217.0,2092.0,2,1,,,,57.27,12.52,9.86,0.0,0.0,39.0,One critical part of AI is machine learning ML The common definition of ML by Mitchell is A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T as measured by P improves with experience E If this type of program is useful in an everyday application depends on the application Here are some examples which would not be possible without ML Spam detection eg emails forums Fraud detection eg credit cards Image recognition eg if you want to automatically filter NSFW content automatic adding of tags making images searchable eg for Google Image search Video analysis filtering copyrighted work eg on YouTube Speech recognition eg hotlines automatic caption generation Autocompletion probably one of the simplest things you can do with data
,2.0,"<p>I am reading about Generative Adversarial Networks (GANs) and I have some doubts regarding it. So far, I understand that in a GAN there are two different types of neural network: one is generative (G) and the other discriminative (D). The generative neural network generates some data which the discriminative neural network judges for correctness. The GAN learns by passing the loss function to both networks.</p>

<p>How do the discriminative (D) neural nets initially know whether the data produced by G is correct or not? Do we have to train the D first then add it into the GAN with G?</p>

<p>Let's consider my trained D net, which can classify a picture with 90% percentage accuracy. If we add this D net to a GAN there is a 10% probability it will classify a image wrong. If we train a GAN with this D net then will it also have the same 10% error in classifying an image? If yes, then why do GANs show promising results?</p>
",,0,2016-10-24T11:42:28.893,,2211,2016-10-26T13:09:09.153,2016-10-26T13:09:09.153,,75.0,,39.0,,1,8,<neural-networks>,How do Generative Adversarial Networks work?,437.0,71.44,8.35,8.34,0.0,0.0,26.0,I am reading about Generative Adversarial Networks GANs and I have some doubts regarding it So far I understand that in a GAN there are two different types of neural network one is generative G and the other discriminative D The generative neural network generates some data which the discriminative neural network judges for correctness The GAN learns by passing the loss function to both networks How do the discriminative D neural nets initially know whether the data produced by G is correct or not Do we have to train the D first then add it into the GAN with G Lets consider my trained D net which can classify a picture with 90 percentage accuracy If we add this D net to a GAN there is a 10 probability it will classify a image wrong If we train a GAN with this D net then will it also have the same 10 error in classifying an image If yes then why do GANs show promising results
,,"<h2>Compare with real data</h2>

<p>100% of results produced by G are ""wrong"", always, by definition, even for a very good generator. You provide the discriminative net with a mix of generated results and real results from an outside source and train it to distinguish if the result was produced by the generator or not.</p>

<p>This will result in a ""mutual evolution"" as D will learn to find features that separate real results from generated ones, and G will learn how to generate results that are hard to distinguish from real data.</p>
",,0,2016-10-24T19:20:48.323,,2212,2016-10-24T19:20:48.323,,,,,1675.0,2211.0,2,4,,,,49.18,10.4,9.13,0.0,0.0,12.0,Compare with real data 100 of results produced by G are wrong always by definition even for a very good generator You provide the discriminative net with a mix of generated results and real results from an outside source and train it to distinguish if the result was produced by the generator or not This will result in a mutual evolution as D will learn to find features that separate real results from generated ones and G will learn how to generate results that are hard to distinguish from real data
2217.0,2.0,"<p>Now AI can replace call center, worker(in the factory) and going to replace court. When will the AI can replace developer or tester?</p>

<p>I want to know how long can AI replace developer. e.g. next 10 years because...</p>
",,3,2016-10-25T08:59:44.940,,2214,2016-10-26T06:28:57.363,2016-10-26T06:28:57.363,,3210.0,,2930.0,,1,0,<ai-design><intelligent-agent>,When will the AI can replace developer or tester,88.0,78.75,7.68,7.43,0.0,0.0,11.0,Now AI can replace call center workerin the factory and going to replace court When will the AI can replace developer or tester I want to know how long can AI replace developer eg next 10 years because
,,"<p>""discriminative(D) network"" <strong>learns</strong> to discriminate by definition - we provide it with the true vs. the generated data, and let it learns by itself how to discriminate between the two.</p>

<p>Therefore, we expect network D to improve the ability of network G to generate better and better images (or other kind of data), as it try to ""trick"" network D by producing new data that is more similar to ""real data"". It is not about the accuracy of network D at all. <strong>It is not about improving the accuracy</strong>, it is about improving the ability of the computer to generate more ""believable"" data.</p>

<p>That said, using this scenario could be a good ""unsupervised"" way to improve the classification power of neural networks, as it forces the generator model to learn better features of real data, and to learn how to distinguish between actual features and noise, using much less data that is needed for a traditional supervised learning scheme. </p>
",,0,2016-10-25T11:58:19.457,,2216,2016-10-25T11:58:19.457,,,,,3250.0,2211.0,2,2,,,,44.78,11.03,9.64,0.0,0.0,29.0,discriminativeD network learns to discriminate by definition we provide it with the true vs the generated data and let it learns by itself how to discriminate between the two Therefore we expect network D to improve the ability of network G to generate better and better images or other kind of data as it try to trick network D by producing new data that is more similar to real data It is not about the accuracy of network D at all It is not about improving the accuracy it is about improving the ability of the computer to generate more believable data That said using this scenario could be a good unsupervised way to improve the classification power of neural networks as it forces the generator model to learn better features of real data and to learn how to distinguish between actual features and noise using much less data that is needed for a traditional supervised learning scheme
,,"<p>The <strong>ultimate goal of machine learning</strong> is to bypass the developer...
When we will have a ""master algorithm"" that can learn how to generalize any function or algorithm from examples, it can essentially replace any developer, skip the 'development"" stage, going from problem directly to algorithm. </p>

<p>We can't know when this will happen, but as we surrounded with multiple creatures (humans and animals) which can demonstrate learning algorithms and predictive model learning without any ""developer"" - we can assume that such an algorithm is possible. </p>

<p>If I would have to guess, I would say that we are probably very near the point where ""developers"" and ""testers"" will be replaced by learning algorithms. We could be a decade or two away from the point where people will not write any code or any testing at all. Programs and automation will be derived directly from describing the problems themselves in a natural language, visualizations or data collections. However, we still need some breakthroughs in combining feature learning with active memory, unsupervised learning, and artificial common sense.   </p>
",,0,2016-10-25T12:12:23.607,,2217,2016-10-25T12:12:23.607,,,,,3250.0,2214.0,2,4,,,,37.94,14.05,10.7,0.0,0.0,32.0,The ultimate goal of machine learning is to bypass the developer When we will have a master algorithm that can learn how to generalize any function or algorithm from examples it can essentially replace any developer skip the development stage going from problem directly to algorithm We cant know when this will happen but as we surrounded with multiple creatures humans and animals which can demonstrate learning algorithms and predictive model learning without any developer we can assume that such an algorithm is possible If I would have to guess I would say that we are probably very near the point where developers and testers will be replaced by learning algorithms We could be a decade or two away from the point where people will not write any code or any testing at all Programs and automation will be derived directly from describing the problems themselves in a natural language visualizations or data collections However we still need some breakthroughs in combining feature learning with active memory unsupervised learning and artificial common sense
,,"<p>By AI is it artificial or more analytical</p>

<p>What makes us learn hat makes us learn?i ask 
i am a dr Can we be better clinicians Artificial Intelligence ? What can we learn from AI? is therefore defined as the theory and development of computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.</p>

<p>Intelligent systems are better at understanding and tracking the changes that the human eye cannot detect. These machines are connected to vast amount of data, which they analyze in real time to generate a solution for a current problem. This process is referred to as data mining. Under ordinary circumstances, human can only collect and analyze a handful of data.  Artificial is not the word to use</p>
",,0,2016-10-26T02:18:12.187,,2218,2016-10-26T02:18:12.187,,,,,3263.0,2214.0,2,2,,,,48.09,11.94,10.71,0.0,0.0,15.0,By AI is it artificial or more analytical What makes us learn hat makes us learni ask i am a dr Can we be better clinicians Artificial Intelligence What can we learn from AI is therefore defined as the theory and development of computer systems able to perform tasks that normally require human intelligence such as visual perception speech recognition decisionmaking and translation between languages Intelligent systems are better at understanding and tracking the changes that the human eye cannot detect These machines are connected to vast amount of data which they analyze in real time to generate a solution for a current problem This process is referred to as data mining Under ordinary circumstances human can only collect and analyze a handful of data Artificial is not the word to use
,1.0,"<p>Ok, I now know how a machine can learn to play to play Atari games (Breakout): <a href=""https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"" rel=""nofollow noreferrer"">Playing Atari with Reinforcement Learning</a></p>

<p>With the same technique it is even possible to play FPS games (Doom): <a href=""https://arxiv.org/pdf/1609.05521"" rel=""nofollow noreferrer"">Playing FPS Games with Reinforcement Learning</a></p>

<p>Further studies even investigated multiagent scenarios (Pong): <a href=""https://arxiv.org/pdf/1511.08779.pdf"" rel=""nofollow noreferrer"">Multiagent Cooperation and Competition with Deep Reinforcement Learning</a></p>

<p>And even another awesome article for the interested user in context of deep reinforcement learning (easy and a must read for beginners): <a href=""http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/"" rel=""nofollow noreferrer"">Demystifying Deep Reinforcement Learning</a></p>

<p>I was thrilled by these results and immediately wanted to try them in some simple ""board/card game scenarios"", i.e. writing AI for some simple games in order to learn more about ""deep learning"". Of course, thinking that I can apply the techniques above easily in my scenarios was stupid. All examples above are based on convolutional nets (image recognition) and some other assumptions, which might not be applicable in my scenarios.</p>

<p>Can you give me hints or futher articles, which deal with my questions below? As a beginner, I do not have an overview, yet. Preferably, your suggestions should also be connected to the following areas already: deep learning, reinforcement learning (, multiagent systems)</p>

<hr>

<p>(1)</p>

<p>If you have a card game and the AI shall play a card from its hand, you could think about the cards (amongst other stuff) as the current game state. You can easily define some sort of neural net and feed it with the card data. In a trivial case the cards are just numbered. I do not know the net type, which would be suitable, but I guess deep reinforcment learning strategies could be applied easily then.</p>

<p>However, I can only imagine this, if there is a constant number of hand cards. In the examples above, the number of pixels is also constant, for example. What if a player can have a different numbers of cards? What to do, if a player can have an infinite number of cards? Of course, this is just a theoretical question as no game has an infinite number of cards.</p>

<hr>

<p>(2)</p>

<p>In the initial examples, the action space is constant. What can you do, if the action space is not? This more or less follows from my previous problem. If you have 3 cards, you can play card 1, 2 or 3. If you have 5 cards, you can play card 1, 2, 3, 4 or 5, etc. It is also common in card games, that it is not allowed to play a card. Could this be tackled with negative reward?</p>

<hr>

<p>So, which ""tricks"" can be used, e.g. always assume a constant number of cards with ""filling values"", which is only applicable in the non-infinite case (anyways unrealistic and even humans could not play well with that)?
Are there articles, which examine such things already?</p>
",,3,2016-10-26T08:11:31.073,,2219,2016-11-02T08:30:47.350,2016-11-02T08:30:47.350,,3270.0,,3270.0,,1,4,<deep-learning><gaming><reinforcement-learning><game-theory><multi-agent-systems>,Board/Card Game AI - Questions concerning state/action space - Deep Reinforcement Learning,111.0,69.41,10.27,8.59,0.0,0.0,95.0,Ok I now know how a machine can learn to play to play Atari games Breakout Playing Atari with Reinforcement Learning With the same technique it is even possible to play FPS games Doom Playing FPS Games with Reinforcement Learning Further studies even investigated multiagent scenarios Pong Multiagent Cooperation and Competition with Deep Reinforcement Learning And even another awesome article for the interested user in context of deep reinforcement learning easy and a must read for beginners Demystifying Deep Reinforcement Learning I was thrilled by these results and immediately wanted to try them in some simple boardcard game scenarios ie writing AI for some simple games in order to learn more about deep learning Of course thinking that I can apply the techniques above easily in my scenarios was stupid All examples above are based on convolutional nets image recognition and some other assumptions which might not be applicable in my scenarios Can you give me hints or futher articles which deal with my questions below As a beginner I do not have an overview yet Preferably your suggestions should also be connected to the following areas already deep learning reinforcement learning multiagent systems 1 If you have a card game and the AI shall play a card from its hand you could think about the cards amongst other stuff as the current game state You can easily define some sort of neural net and feed it with the card data In a trivial case the cards are just numbered I do not know the net type which would be suitable but I guess deep reinforcment learning strategies could be applied easily then However I can only imagine this if there is a constant number of hand cards In the examples above the number of pixels is also constant for example What if a player can have a different numbers of cards What to do if a player can have an infinite number of cards Of course this is just a theoretical question as no game has an infinite number of cards 2 In the initial examples the action space is constant What can you do if the action space is not This more or less follows from my previous problem If you have 3 cards you can play card 1 2 or 3 If you have 5 cards you can play card 1 2 3 4 or 5 etc It is also common in card games that it is not allowed to play a card Could this be tackled with negative reward So which tricks can be used eg always assume a constant number of cards with filling values which is only applicable in the noninfinite case anyways unrealistic and even humans could not play well with that Are there articles which examine such things already
,,"<p>Any logic circuit admits a variety of implementations.  All programs executing on conventional digital processors can be expressed as logic circuits.  Among the possible implementations of logic circuits are fluidic implementations, which do not depend on electronics per se.  Thus it is in principle possible to implement, e.g. a POMDP processor (responsive to your specific question) in fluidics, albeit perhaps impractical at the moment.</p>

<p>I know of no general theory of Turing-completeness for analog computers, which would suffice to determine whether some alternative physical substrate, be it biological or not biological, can compute recursively enumerable functions.  That is a sufficient but not a necessary condition for answering your question regarding any given medium. Usually the easiest way to demonstrate the sufficient condition will be to demonstrate the ability to construct a NAND gate, and to combine such gates into general circuits.</p>

<p>Another non-electronic example: Quantum computers may be non-electronic, at least in their processing elements, and are able to compute general deterministic logic circuits.</p>
",,0,2016-10-26T16:32:00.120,,2220,2016-10-26T16:32:00.120,,,,,3278.0,2185.0,2,5,,,,27.62,16.77,11.86,0.0,0.0,25.0,Any logic circuit admits a variety of implementations All programs executing on conventional digital processors can be expressed as logic circuits Among the possible implementations of logic circuits are fluidic implementations which do not depend on electronics per se Thus it is in principle possible to implement eg a POMDP processor responsive to your specific question in fluidics albeit perhaps impractical at the moment I know of no general theory of Turingcompleteness for analog computers which would suffice to determine whether some alternative physical substrate be it biological or not biological can compute recursively enumerable functions That is a sufficient but not a necessary condition for answering your question regarding any given medium Usually the easiest way to demonstrate the sufficient condition will be to demonstrate the ability to construct a NAND gate and to combine such gates into general circuits Another nonelectronic example Quantum computers may be nonelectronic at least in their processing elements and are able to compute general deterministic logic circuits
,,"<ol>
<li>Filling values is totally fine. In the case of image recognition the filling will be the background of the image (<a href=""https://www.google.com/search?q=mnist+images&amp;tbm=isch"" rel=""nofollow"">examples</a>). For example in Belot you have total of 32 cards, which can be 32 boolean features. You can set the ones the player has to 1, while the rest are 0. Note that the in most games you'll need more features than the cards in your hand. I.e number of the round, cards that have been played so far, calls that have been made etc. </li>
<li>Defining the scope of the ""action space"" will be specific to the game. For Belot, it can be number encoding for each of the 32 cards.</li>
</ol>

<p>You can find articles via Google. <a href=""http://homes.soic.indiana.edu/adamw/hearts.pdf"" rel=""nofollow"">Here</a> is a paper about ML for a card game. Instead of articles, I'd recommend checking out a course on ML (i.e. Coursera and Udacity have good free online courses).</p>
",,4,2016-10-26T16:46:45.657,,2221,2016-10-27T11:21:43.047,2016-10-27T11:21:43.047,,2997.0,,2997.0,2219.0,2,3,,,,84.37,6.54,7.98,0.0,0.0,28.0,Filling values is totally fine In the case of image recognition the filling will be the background of the image examples For example in Belot you have total of 32 cards which can be 32 boolean features You can set the ones the player has to 1 while the rest are 0 Note that the in most games youll need more features than the cards in your hand Ie number of the round cards that have been played so far calls that have been made etc Defining the scope of the action space will be specific to the game For Belot it can be number encoding for each of the 32 cards You can find articles via Google Here is a paper about ML for a card game Instead of articles Id recommend checking out a course on ML ie Coursera and Udacity have good free online courses
,1.0,"<p>The “Discounted sum of future rewards” using
discount factor γ” is</p>

<pre><code>γ (reward in 1 time step) +
γ ^ 2 (reward in 2 time steps) +
γ ^ 3 (reward in 3 time steps) + ...
</code></pre>

<p>I am confused as what constitutes a time-step. Say I take a action now, so I will get a reward in 1 time-step. Then, I will take an action again in timestep 2 to get a second reward in time-step 3
But the equation says something else. How does one define a time-step? Can we take action as well receive a reward in a single step? Examples are most helpful.</p>
",,1,2016-10-27T19:57:17.187,1.0,2226,2017-03-08T19:13:48.547,,,,,35.0,,1,0,<reinforcement-learning><markov-chain>,Definition of time-step in a MDP,45.0,74.69,6.66,8.79,98.0,0.0,12.0,The “Discounted sum of future rewards” using discount factor γ” is I am confused as what constitutes a timestep Say I take a action now so I will get a reward in 1 timestep Then I will take an action again in timestep 2 to get a second reward in timestep 3 But the equation says something else How does one define a timestep Can we take action as well receive a reward in a single step Examples are most helpful
,,"<p>On the suggestion of the O.P. rcpinto I converted a comment about seeing ""around a half-dozen papers that follow up on Graves et al.'s work which have produced results of the  caliber"" and will provide a few links. Keep in mind that this only answers the part of the question pertaining to NTMs, not Google DeepMind itself, plus I'm still learning the ropes in machine learning, so some of the material in these papers is over my head; I did manage to grasp much of the material in <a href=""https://arxiv.org/pdf/1410.5401.pdf"">Graves et al.'s original paper</a>{1] though and am close to having homegrown NTM code to test. I also at least skimmed the following papers over the last few months; they do not replicate the NTM study in a strict scientific manner, but many of their experimental results do tend to support the original at least tangentially:</p>

<p>• In <a href=""https://arxiv.org/pdf/1607.00036.pdf"">this paper</a> on a variant version of NTM addressing, Gulcehere, et al. do not try to precisely replicate Graves et al.'s tests, but like the DeepMind team, it does demonstrate markedly better results for the original NTM and several variants over an ordinary recurrent LSTM. They use 10,000 training samples of a Facebook Q&amp;A dataset, rather than the N-grams Graves et al. operated on in their paper, so it's not replication in the strictest sense. They did however manage to get a version of the original NTM and several variants up and running, plus recorded the same magnitude of performance improvement.<a href=""https://arxiv.org/pdf/1607.00036.pdf"">2</a></p>

<p>• Unlike the original NTM, <a href=""https://arxiv.org/abs/1505.00521"">this study</a> tested a version of reinforcement learning which was not differentiable; that may be why they were unable to solve several of the programming-like tasts, like Repeat-Copy, unless the controller wasn't confined to moving forwards. Their results were nevertheless good enough to lend support to the idea of NTMs. A more recent revision of their paper is apparently available, which I have yet to read, so perhaps some of their variant's problems have been solved.<a href=""https://arxiv.org/abs/1505.00521"">3</a></p>

<p>• Instead of testing the original flavor of NTM against ordinary neural nets like LSTMs, <a href=""https://arxiv.org/pdf/1510.03931v3.pdf"">this paper</a> pitted it against several more advanced NTM memory structures. They got good results on the same type of programming-like tasks that Graves et al. tested, but I don't think they were using the same dataset (it's hard to tell from the way their study is written just what datasets they were operating on).<a href=""https://arxiv.org/pdf/1510.03931v3.pdf"">4</a> </p>

<p>• On p. 8 of <a href=""https://arxiv.org/pdf/1605.06065.pdf"">this study</a>, an NTM clearly outperforms several LSTM, feed-forward and nearest-neighbor based schemes on an Omniglot character recognition dataset. An alternative approach to external memory cooked up by the authors clearly beats it, but it still obviously performs well. The authors seem to belong to a rival team at Google, so that might be an issue when assessing replicability.<a href=""https://arxiv.org/pdf/1605.06065.pdf"">5</a></p>

<p>• On p. 2 <a href=""http://www.thespermwhale.com/jaseweston/ram/papers/paper_6.pdf"">these authors</a> reported getting better generalization on ""very large sequences"" in a test of copy tasks, using a much smaller NTM network they evolved with the genetic NEAT algorithm, which dynamically grows topologies.<a href=""http://www.thespermwhale.com/jaseweston/ram/papers/paper_6.pdf"">6</a>  </p>

<p>NTMs are fairly new so there hasn't been much time to stringently replicate the original research yet, I suppose. The handful of papers I skimmed over the summer, however, seem to lend support to their experimental results; I have yet to see any that report anything but excellent performance. Of course I have an availability bias, since I only read the pdfs I could easily find in a careless Internet search. From that small sample it seems that most of the follow-up research has been focused on extending the concept, not replication, which would explain the lack of replicability data. I hope that helps.</p>

<p><a href=""https://arxiv.org/pdf/1410.5401.pdf"">1</a> Graves, Alex; Wayne, Greg and Danihelka, Ivo, 2014, ""Neural Turing Machines,"" published Dec. 10, 2014. </p>

<p><a href=""https://arxiv.org/pdf/1607.00036.pdf"">2</a> Gulcehre, Caglar; Chandar, Sarath; Choy, Kyunghyun and Bengio, Yoshua, 2016, ""Dynamic Neural Turing machine with Soft and Hard Addressing Schemes,"" published June 30, 2016. </p>

<p><a href=""https://arxiv.org/abs/1505.00521"">3</a> Zaremba, Wojciech and Sutskever, Ilya, 2015, ""Reinforcement Learning Neural Turing Machines,"" published May 4, 2015. </p>

<p><a href=""https://arxiv.org/pdf/1510.03931v3.pdf"">4</a> Zhang; Wei; Yu, Yang and Zhou, Bowen, 2015, ""Structured Memory for Neural Turing Machines,"" published Oct. 25, 2015.    </p>

<p><a href=""https://arxiv.org/pdf/1605.06065.pdf"">5</a> Santoro, Adam; Bartunov, Sergey; Botvinick, Matthew; Wierstra, Daan and Lillicrap, Timothy, 2016, ""One-Shot Learning with Memory-Augmented Neural Networks,"" published May 19, 2016.    </p>

<p><a href=""http://www.thespermwhale.com/jaseweston/ram/papers/paper_6.pdf"">6</a> Boll Greve, Rasmus; Jacobsen, Emil Juul and Sebastian Risi, date unknown, ""Evolving Neural Turing Machines."" No publisher listed</p>

<p>All except (perhaps) Boll Greve et al. were published at the Cornell Univeristy Library arXiv.org Repository: Ithaca, New York.</p>
",,0,2016-10-28T09:00:13.520,,2229,2016-10-28T09:00:13.520,,,,,1427.0,1290.0,2,5,,,,59.64,12.36,9.54,0.0,0.0,171.0,On the suggestion of the OP rcpinto I converted a comment about seeing around a halfdozen papers that follow up on Graves et als work which have produced results of the caliber and will provide a few links Keep in mind that this only answers the part of the question pertaining to NTMs not Google DeepMind itself plus Im still learning the ropes in machine learning so some of the material in these papers is over my head I did manage to grasp much of the material in Graves et als original paper1 though and am close to having homegrown NTM code to test I also at least skimmed the following papers over the last few months they do not replicate the NTM study in a strict scientific manner but many of their experimental results do tend to support the original at least tangentially • In this paper on a variant version of NTM addressing Gulcehere et al do not try to precisely replicate Graves et als tests but like the DeepMind team it does demonstrate markedly better results for the original NTM and several variants over an ordinary recurrent LSTM They use 10000 training samples of a Facebook QampA dataset rather than the Ngrams Graves et al operated on in their paper so its not replication in the strictest sense They did however manage to get a version of the original NTM and several variants up and running plus recorded the same magnitude of performance improvement2 • Unlike the original NTM this study tested a version of reinforcement learning which was not differentiable that may be why they were unable to solve several of the programminglike tasts like RepeatCopy unless the controller wasnt confined to moving forwards Their results were nevertheless good enough to lend support to the idea of NTMs A more recent revision of their paper is apparently available which I have yet to read so perhaps some of their variants problems have been solved3 • Instead of testing the original flavor of NTM against ordinary neural nets like LSTMs this paper pitted it against several more advanced NTM memory structures They got good results on the same type of programminglike tasks that Graves et al tested but I dont think they were using the same dataset its hard to tell from the way their study is written just what datasets they were operating on4 • On p 8 of this study an NTM clearly outperforms several LSTM feedforward and nearestneighbor based schemes on an Omniglot character recognition dataset An alternative approach to external memory cooked up by the authors clearly beats it but it still obviously performs well The authors seem to belong to a rival team at Google so that might be an issue when assessing replicability5 • On p 2 these authors reported getting better generalization on very large sequences in a test of copy tasks using a much smaller NTM network they evolved with the genetic NEAT algorithm which dynamically grows topologies6 NTMs are fairly new so there hasnt been much time to stringently replicate the original research yet I suppose The handful of papers I skimmed over the summer however seem to lend support to their experimental results I have yet to see any that report anything but excellent performance Of course I have an availability bias since I only read the pdfs I could easily find in a careless Internet search From that small sample it seems that most of the followup research has been focused on extending the concept not replication which would explain the lack of replicability data I hope that helps 1 Graves Alex Wayne Greg and Danihelka Ivo 2014 Neural Turing Machines published Dec 10 2014 2 Gulcehre Caglar Chandar Sarath Choy Kyunghyun and Bengio Yoshua 2016 Dynamic Neural Turing machine with Soft and Hard Addressing Schemes published June 30 2016 3 Zaremba Wojciech and Sutskever Ilya 2015 Reinforcement Learning Neural Turing Machines published May 4 2015 4 Zhang Wei Yu Yang and Zhou Bowen 2015 Structured Memory for Neural Turing Machines published Oct 25 2015 5 Santoro Adam Bartunov Sergey Botvinick Matthew Wierstra Daan and Lillicrap Timothy 2016 OneShot Learning with MemoryAugmented Neural Networks published May 19 2016 6 Boll Greve Rasmus Jacobsen Emil Juul and Sebastian Risi date unknown Evolving Neural Turing Machines No publisher listed All except perhaps Boll Greve et al were published at the Cornell Univeristy Library arXivorg Repository Ithaca New York
,,"<p>I'd like to add, self-driving cars would also be excellent for disabled people who would otherwise not be able to drive. Adds a lot more autonomy to vulnerable people</p>
",,0,2016-10-28T11:29:45.403,,2230,2016-10-28T11:29:45.403,,,,user3313,,2127.0,2,4,,,,56.76,9.74,9.26,0.0,0.0,4.0,Id like to add selfdriving cars would also be excellent for disabled people who would otherwise not be able to drive Adds a lot more autonomy to vulnerable people
,1.0,"<p>Decades ago there were and are books in machine vision, which by implementing various information processing rules from gestalt psychology, got impressive results with little code or special hardware in image identification and visual processing.</p>

<blockquote>
  <p>Are such methods being used or worked on today? Was any progress made on this? Or was this research program dropped? By today, I mean 2016, not 1995 or 2005.</p>
</blockquote>
",,0,2016-10-29T06:49:39.403,,2231,2017-02-09T14:31:44.300,2016-10-31T05:06:03.450,,1366.0,,1366.0,,1,4,<machine-learning><algorithm><computer-vision>,Intelligent vision and gestalt processing,85.0,58.28,12.17,11.57,0.0,0.0,9.0,Decades ago there were and are books in machine vision which by implementing various information processing rules from gestalt psychology got impressive results with little code or special hardware in image identification and visual processing Are such methods being used or worked on today Was any progress made on this Or was this research program dropped By today I mean 2016 not 1995 or 2005
,,"<p>Self driving cars are good for the following reasons:</p>

<ul>
<li>In the case of an emergancy, urgancy, or just someone being unable to drive unexpactedly, the car can go by itself to a designated location - this is useful in so many use cases - kids who need to get somewhere while parents are busy, Parents who drank a little too much and prefer to take 'the cab' home, or while running, you got injured and need a pick-up.</li>
<li>The examples above are for the more obvious things, which we currently have a struggle with. but other than those, Self-driving cars will open a door for a much wider scale of things: safe police chases (just a car without a police officer), taxies, help in the battle field, and much more...</li>
<li>The third and most important benefit, is the safety and economical properties of self driving cars: with a lot of those cars on the road, they can 'understand' each other and nothing will go unpredicted. they have much faster response time then humans, and maybe in the future they will even be able to predict traffic-light changes, and by that save gas and money (even more than what they can save right now by driving economicly)</li>
</ul>
",,0,2016-10-29T08:26:08.497,,2232,2016-10-29T08:26:08.497,,,,,3328.0,2127.0,2,1,,,,46.98,10.17,10.3,0.0,0.0,37.0,Self driving cars are good for the following reasons In the case of an emergancy urgancy or just someone being unable to drive unexpactedly the car can go by itself to a designated location this is useful in so many use cases kids who need to get somewhere while parents are busy Parents who drank a little too much and prefer to take the cab home or while running you got injured and need a pickup The examples above are for the more obvious things which we currently have a struggle with but other than those Selfdriving cars will open a door for a much wider scale of things safe police chases just a car without a police officer taxies help in the battle field and much more The third and most important benefit is the safety and economical properties of self driving cars with a lot of those cars on the road they can understand each other and nothing will go unpredicted they have much faster response time then humans and maybe in the future they will even be able to predict trafficlight changes and by that save gas and money even more than what they can save right now by driving economicly
,,"<p>Definitions of Artificial Intelligence can be categorized into four categories, Thinking Humanly, Thinking Rationally, Acting Humanly and Acting Rationally. The following picture (from Artificial Intelligence: A Modern Approach) will shed light on over these definitions:<br> <br>
<a href=""https://i.stack.imgur.com/9jOK9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9jOK9.png"" alt=""enter image description here""></a>
 <br><br>
The definition which I like is by John McCarthy, ""It is the science and engineering of making intelligent machines, especially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable.""<br>
<br>
Machine Learning on the other hand is field of AI which deals with pattern recognition. Various algorithms are used over a set of data to predict the future. Machine Learning is data driven and data oriented.<br></p>

<blockquote>
  <p>In a nutshell Artificial Intelligence is a field of Computer Science which deals with providing machines the ability of perform rational tasks. Natural Language Processing, Automation, Image Processing, and many others are part of it.<br>
  Machine Learning is a subset of AI which is data oriented and deals with predicting. Used in search engines, Youtube recommendation list, etc.</p>
</blockquote>
",,0,2016-10-30T19:12:31.607,,2233,2016-10-30T19:12:31.607,,,,,3005.0,35.0,2,4,,,,36.28,14.96,11.02,0.0,0.0,27.0,Definitions of Artificial Intelligence can be categorized into four categories Thinking Humanly Thinking Rationally Acting Humanly and Acting Rationally The following picture from Artificial Intelligence A Modern Approach will shed light on over these definitions The definition which I like is by John McCarthy It is the science and engineering of making intelligent machines especially intelligent computer programs It is related to the similar task of using computers to understand human intelligence but AI does not have to confine itself to methods that are biologically observable Machine Learning on the other hand is field of AI which deals with pattern recognition Various algorithms are used over a set of data to predict the future Machine Learning is data driven and data oriented In a nutshell Artificial Intelligence is a field of Computer Science which deals with providing machines the ability of perform rational tasks Natural Language Processing Automation Image Processing and many others are part of it Machine Learning is a subset of AI which is data oriented and deals with predicting Used in search engines Youtube recommendation list etc
2240.0,3.0,"<p>There is this claim around that the brain's cognitive capabilities are tightly linked to the way it processes sensorimotor information and that, in this or a similar sense, our intelligence is ""embodied"". Lets assume, for the sake of argument, that this claim is correct (you may think the claim is too vague to even qualify for being correct, that it's ""not even false"". If so, I would love to hear your ways of fleshing out the claim in such a way that it's specific enough to be true or false). Then, since arguably at least chronologically in our evolution, most of our higher level cognitive capabilities come after our brain's way of processing sensorimotor information, this brings up the question what it is about the way that our brains function that make them particularly suitable for the processing of sensorimotor information? What makes our brains'  architecture particularly suitable for being an information processing unit inside a body? This is my first question. And what I'm hoping for are answers that go beyond the <em>a fortiori</em> reply ""Our brain is so powerful and dynamic, it's great for <em>any</em> task, and so also for processing sensorimotor information""</p>

<p>My second question is basically the same but instead of the human brain I want to ask for neural networks. What are the properties of neural networks that makes them <em>particularly</em> suitable for processing the kind of information that is produced by a body? Here are some of the reasons why people think neural networks are powerful:</p>

<ul>
<li>The universal approximation theorem (of FFNNs)</li>
<li>their ability to learn and self-organise</li>
<li>Robustness to local degrading of information</li>
<li>their ability to abstract/coarse-grain/convolute features, etc.</li>
</ul>

<p>While I see how these are real advantages when it comes to evolution picking its favorite model for an embodied AI, none of them (or their combination) seems to be unique to neural networks. So they don't provide a satisfactory answer to my question. What makes a neural network a more suitable structure for embodied AI than, say, having a literal Turing machine sitting inside our head, or any other structure that is capable of universal computation? For instance, I really don't see how neural networks would be a particularly natural choice for dealing with geometric information. But geometric information is pretty vital when it comes to sensorimotor information, no?</p>
",,0,2016-10-30T21:20:13.010,,2234,2016-10-31T20:57:12.230,2016-10-31T16:57:03.297,,10.0,,3346.0,,1,4,<neural-networks><human-like><embodied-cognition>,"Why would neural networks be a particularly good framework for ""embodied AI""?",157.0,43.46,12.89,9.18,0.0,0.0,58.0,There is this claim around that the brains cognitive capabilities are tightly linked to the way it processes sensorimotor information and that in this or a similar sense our intelligence is embodied Lets assume for the sake of argument that this claim is correct you may think the claim is too vague to even qualify for being correct that its not even false If so I would love to hear your ways of fleshing out the claim in such a way that its specific enough to be true or false Then since arguably at least chronologically in our evolution most of our higher level cognitive capabilities come after our brains way of processing sensorimotor information this brings up the question what it is about the way that our brains function that make them particularly suitable for the processing of sensorimotor information What makes our brains architecture particularly suitable for being an information processing unit inside a body This is my first question And what Im hoping for are answers that go beyond the a fortiori reply Our brain is so powerful and dynamic its great for any task and so also for processing sensorimotor information My second question is basically the same but instead of the human brain I want to ask for neural networks What are the properties of neural networks that makes them particularly suitable for processing the kind of information that is produced by a body Here are some of the reasons why people think neural networks are powerful The universal approximation theorem of FFNNs their ability to learn and selforganise Robustness to local degrading of information their ability to abstractcoarsegrainconvolute features etc While I see how these are real advantages when it comes to evolution picking its favorite model for an embodied AI none of them or their combination seems to be unique to neural networks So they dont provide a satisfactory answer to my question What makes a neural network a more suitable structure for embodied AI than say having a literal Turing machine sitting inside our head or any other structure that is capable of universal computation For instance I really dont see how neural networks would be a particularly natural choice for dealing with geometric information But geometric information is pretty vital when it comes to sensorimotor information no
,1.0,"<ol>
<li><p>I can't understand what is the problem in applying value-iteration in reinforcement learning setting (where we don't the reward and transition probabilities). In one of the lectures, the guy said it has to do with not being able to take max with samples.</p></li>
<li><p>Further on this, <strong>why does q-learning solve this</strong>? In both we take max over actions only. What is the big break-through with q-learning?</p></li>
</ol>

<p>Lecture Link: <a href=""https://www.youtube.com/watch?v=ifma8G7LegE&amp;feature=youtu.be&amp;t=3431"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=ifma8G7LegE&amp;feature=youtu.be&amp;t=3431</a>
(The guy says we don't know how to do maxes with samples, what does that mean?) </p>
",,0,2016-10-30T23:56:59.217,1.0,2235,2016-12-10T04:37:10.340,2016-12-10T04:37:10.340,,35.0,,35.0,,1,1,<machine-learning><reinforcement-learning><markov-chain>,How q-learning solves the issue with value iteration in model-free settings,73.0,67.65,14.25,8.7,0.0,0.0,36.0,I cant understand what is the problem in applying valueiteration in reinforcement learning setting where we dont the reward and transition probabilities In one of the lectures the guy said it has to do with not being able to take max with samples Further on this why does qlearning solve this In both we take max over actions only What is the big breakthrough with qlearning Lecture Link httpswwwyoutubecomwatchvifma8G7LegEampfeatureyoutubeampt3431 The guy says we dont know how to do maxes with samples what does that mean
,3.0,"<p>I've heard before from computer scientists and from researchers in the area of AI that that Lisp is a good language for research and development in artificial intelligence. Does this still apply, with the proliferation of neural networks and deep learning? What was their reasoning for this? What languages are current deep-learning systems currently built in?</p>
",,0,2016-10-31T03:38:07.027,5.0,2236,2017-03-10T09:03:23.677,2017-01-09T04:47:02.603,,4446.0,,3323.0,,1,11,<neural-networks><machine-learning><deep-learning><research><programming-languages>,Why is Lisp such a good language for AI?,985.0,57.27,13.74,10.25,0.0,0.0,7.0,Ive heard before from computer scientists and from researchers in the area of AI that that Lisp is a good language for research and development in artificial intelligence Does this still apply with the proliferation of neural networks and deep learning What was their reasoning for this What languages are current deeplearning systems currently built in
,,"<p>First, I guess that you mean <a href=""https://en.wikipedia.org/wiki/Common_Lisp"" rel=""noreferrer"">Common Lisp</a> (which is a standard language specification, see its <a href=""http://www.lispworks.com/documentation/HyperSpec/Front/"" rel=""noreferrer"">HyperSpec</a>).</p>

<p>Then, Common Lisp is great for symbolic AI. However, many recent machine learning libraries are coded in more mainstream languages, for example <a href=""https://en.wikipedia.org/wiki/TensorFlow"" rel=""noreferrer"">TensorFlow</a> is coded in C++ &amp; Python. <a href=""http://machinelearningmastery.com/popular-deep-learning-libraries/"" rel=""noreferrer"">Deep learning libraries</a> are mostly coded in C++ or Python or C (and sometimes using <a href=""https://en.wikipedia.org/wiki/OpenCL"" rel=""noreferrer"">OpenCL</a> or Cuda for GPU computing parts).</p>

<p>Common Lisp is great for <a href=""https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence"" rel=""noreferrer"">symbolic artificial intelligence</a> because:</p>

<ul>
<li>it has very good <em>implementations</em> (e.g. <a href=""http://sbcl.org/"" rel=""noreferrer"">SBCL</a>, which compiles to machine code every expression given to the <a href=""https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop"" rel=""noreferrer"">REPL</a>)</li>
<li>it is <a href=""https://en.wikipedia.org/wiki/Homoiconicity"" rel=""noreferrer""><strong>homoiconic</strong></a>, so it is easy to deal with programs as data, in particular it is easy to generate [sub-]programs, that is use <a href=""https://en.wikipedia.org/wiki/Metaprogramming"" rel=""noreferrer"">meta-programming</a> techniques.</li>
<li>it has a <a href=""https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop"" rel=""noreferrer"">Read-Eval-Print Loop</a> to ease interactive programming</li>
<li>it provides a very powerful <a href=""https://en.wikipedia.org/wiki/Macro_%28computer_science%29"" rel=""noreferrer"">macro</a> machinery (essentially, you define your own domain specific sublanguage for your problem), much more powerful than in other languages like C.</li>
<li>it mandates a <a href=""https://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29"" rel=""noreferrer"">garbage collector</a> (even code can be garbage collected)</li>
<li>it provides many <a href=""https://en.wikipedia.org/wiki/Container_%28abstract_data_type%29"" rel=""noreferrer"">container</a> abstract data types, and can easily handle symbols.</li>
<li>you can code both high-level (dynamically typed) and low-level (more or less startically typed) code, thru appropriate annotations.</li>
</ul>

<p>However most machine learning &amp; neural network libraries are not coded in CL. Notice that neither neural network nor deep learning is in the symbolic artificial intelligence field. See also <a href=""http://ai.stackexchange.com/q/35/3335"">this question</a>.</p>

<p>Several symbolic AI systems like <a href=""https://en.wikipedia.org/wiki/Eurisko"" rel=""noreferrer"">Eurisko</a> or <a href=""https://en.wikipedia.org/wiki/Cyc"" rel=""noreferrer"">CyC</a> have been developed in CL (actually, in some DSL built above CL).</p>

<p>Notice that the programming language might not be very important. In the <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"" rel=""noreferrer"">Artificial General Intelligence</a> research topic, some people work on the idea of a AI system which would generate all its own code (so are designing it with a <a href=""https://en.wikipedia.org/wiki/Bootstrapping_%28compilers%29"" rel=""noreferrer"">bootstrapping</a> approach). Then, the code which is generated by such a system can even be generated in low level programming languages like C. See <a href=""http://bootstrappingartificialintelligence.fr/WordPress3/"" rel=""noreferrer"">J.Pitrat's blog</a></p>
",,0,2016-10-31T06:54:35.370,,2237,2016-10-31T16:53:14.960,2016-10-31T16:53:14.960,,3335.0,,3335.0,2236.0,2,12,,,,42.92,13.34,9.92,0.0,0.0,70.0,First I guess that you mean Common Lisp which is a standard language specification see its HyperSpec Then Common Lisp is great for symbolic AI However many recent machine learning libraries are coded in more mainstream languages for example TensorFlow is coded in C amp Python Deep learning libraries are mostly coded in C or Python or C and sometimes using OpenCL or Cuda for GPU computing parts Common Lisp is great for symbolic artificial intelligence because it has very good implementations eg SBCL which compiles to machine code every expression given to the REPL it is homoiconic so it is easy to deal with programs as data in particular it is easy to generate subprograms that is use metaprogramming techniques it has a ReadEvalPrint Loop to ease interactive programming it provides a very powerful macro machinery essentially you define your own domain specific sublanguage for your problem much more powerful than in other languages like C it mandates a garbage collector even code can be garbage collected it provides many container abstract data types and can easily handle symbols you can code both highlevel dynamically typed and lowlevel more or less startically typed code thru appropriate annotations However most machine learning amp neural network libraries are not coded in CL Notice that neither neural network nor deep learning is in the symbolic artificial intelligence field See also this question Several symbolic AI systems like Eurisko or CyC have been developed in CL actually in some DSL built above CL Notice that the programming language might not be very important In the Artificial General Intelligence research topic some people work on the idea of a AI system which would generate all its own code so are designing it with a bootstrapping approach Then the code which is generated by such a system can even be generated in low level programming languages like C See JPitrats blog
,,"<p>David Nolen (contributor to <a href=""https://fr.wikipedia.org/wiki/Clojure"" rel=""nofollow noreferrer"">Clojure</a> and <a href=""https://github.com/clojure/clojurescript"" rel=""nofollow noreferrer"">ClojureScript</a>; creator of Core Logic a port of miniKanren) in a talk called <strong>LISP as too powerful</strong> stated that back in his days LISP was decades ahead of other programming languages. There are <a href=""http://blog.samibadawi.com/2013/05/lisp-prolog-and-evolution.html"" rel=""nofollow noreferrer"">number of reasons</a> why the language wasn't able to maintain it's name.</p>

<p><a href=""http://norvig.com/paip-preface.html"" rel=""nofollow noreferrer"">This</a> article highlights som key points why LISP is good for AI</p>

<ul>
<li>Easy to define a new language and manipulate complex information.</li>
<li>Full flexibility in defining and manipulating programs as well as data.</li>
<li>Fast, as program is concise along with low level detail. </li>
<li>Good programming environment (debugging, incremental compilers, editors).</li>
</ul>

<p>Most of my friends into this field usually use Matlab for Artificial Neural Networks and Machine Learning. It hides the low level details though. If you are only looking for results and not how you get there, then Matlab will be good. But if you want to learn even low level detailed stuff, then I will suggest you go through LISP at-least once.<br>
Language might not be that important if you have the understanding of various AI algorithms and techniques. I will suggest you to read <em>""Artificial Intelligence: A Modern Approach (by Stuard J. Russell and Peter Norvig""</em>. I am currently reading this book, and it's a very good book.</p>
",,0,2016-10-31T09:15:42.313,,2238,2017-02-22T18:18:57.613,2017-02-22T18:18:57.613,,5639.0,,3005.0,2236.0,2,9,,,,64.61,10.95,10.07,0.0,0.0,33.0,David Nolen contributor to Clojure and ClojureScript creator of Core Logic a port of miniKanren in a talk called LISP as too powerful stated that back in his days LISP was decades ahead of other programming languages There are number of reasons why the language wasnt able to maintain its name This article highlights som key points why LISP is good for AI Easy to define a new language and manipulate complex information Full flexibility in defining and manipulating programs as well as data Fast as program is concise along with low level detail Good programming environment debugging incremental compilers editors Most of my friends into this field usually use Matlab for Artificial Neural Networks and Machine Learning It hides the low level details though If you are only looking for results and not how you get there then Matlab will be good But if you want to learn even low level detailed stuff then I will suggest you go through LISP atleast once Language might not be that important if you have the understanding of various AI algorithms and techniques I will suggest you to read Artificial Intelligence A Modern Approach by Stuard J Russell and Peter Norvig I am currently reading this book and its a very good book
,,"<p>What is life? <strong>AND</strong> Is AI a living organism? <em>are two different questions</em>.<br>
The first question is more philosophical and dependent. It can change with time, reference to topic of discussion or something else. Today, one parameter to its definition is <em>mortality</em>. In future if we reach to a certain technological level where mortal beings were only part of history, then the definition will drop <strong>this</strong> parameter.<br><br>
Coming to the second question. AI started as field of study to make machines to think like humans (or take rational decisions). Giving life to machines was, or is, not a concern of AI developers (at-least not nowadays). Once I watched some videos of Michio Kaku, where he talked about consciousness along with AI.<br>
Suppose human has a conscious level of 10. Then a thermostat might have the conscious level of 1 as it can sense when the surrounding is hot or cold and then take decision. Similarly a rat can have a conscious level 7 (or something). And the levels are of exponential order (not a linear scale). Similarly you can develop an AI program and check what level of consciousness it has achieved. Then you can decide whether it is living or not. ANI (Artificial Narrow Intelligence) will have a lower level of consciousness level than AGI (Artificial General Intelligence). ASI (Artificial Super Intelligence) will have consciousness level higher than the other two, and way higher than any human being.<br><br>
To judge whether an AI program is living or not you need a concrete definition of <strong>""LIFE""</strong>. Your definition can include various parameters like consciousness, adaptability, metabolism (or another method of generating energy for use), rational behavior, intelligence , learning through experience, etc. etc. etc.<br>
But the thing in the end is that its your definition. There are many definitions of ""LIFE"" out there. You can't judge a program for life by all definitions, as some of the definitions are contrary to others.<br></p>

<blockquote>
  <p>So, answer to whether an AI program is living or not, is that <strong>IT DEPENDS</strong>. Depends on your definition of life.</p>
</blockquote>
",,0,2016-10-31T13:08:29.650,,2239,2016-10-31T13:08:29.650,,,,,3005.0,2111.0,2,1,,,,58.08,10.72,8.91,0.0,0.0,66.0,What is life AND Is AI a living organism are two different questions The first question is more philosophical and dependent It can change with time reference to topic of discussion or something else Today one parameter to its definition is mortality In future if we reach to a certain technological level where mortal beings were only part of history then the definition will drop this parameter Coming to the second question AI started as field of study to make machines to think like humans or take rational decisions Giving life to machines was or is not a concern of AI developers atleast not nowadays Once I watched some videos of Michio Kaku where he talked about consciousness along with AI Suppose human has a conscious level of 10 Then a thermostat might have the conscious level of 1 as it can sense when the surrounding is hot or cold and then take decision Similarly a rat can have a conscious level 7 or something And the levels are of exponential order not a linear scale Similarly you can develop an AI program and check what level of consciousness it has achieved Then you can decide whether it is living or not ANI Artificial Narrow Intelligence will have a lower level of consciousness level than AGI Artificial General Intelligence ASI Artificial Super Intelligence will have consciousness level higher than the other two and way higher than any human being To judge whether an AI program is living or not you need a concrete definition of LIFE Your definition can include various parameters like consciousness adaptability metabolism or another method of generating energy for use rational behavior intelligence learning through experience etc etc etc But the thing in the end is that its your definition There are many definitions of LIFE out there You cant judge a program for life by all definitions as some of the definitions are contrary to others So answer to whether an AI program is living or not is that IT DEPENDS Depends on your definition of life
,,"<p>To my mind the essential reason why neural networks and the brain are powerful is that they create a hierarchical model of data or of the world. If you ask why that makes them powerful, well, that's just the structure of the world. If you are stalked by a wolf, it's not like its upper jaw will attack you frontally, while his lower jaw will attack you from behind. If you want to respond to the threat with a feasible computational effort, you'll have to treat the wolf as one entity. Providing these kinds of entities or concepts from the raw bits and bytes of input is what a hierarchical representation does. </p>

<p>Now, this is quite intuitive for sensory information: lashes, iris, eyebrow make up an eye, eyes, nose and mouth make up a face and so on. What is less obvious, is the fact that motor control works exactly the same way! Only in reverse. If you want to lift your arm, you'll just lift it. But for your brain to actually realise this move, the high level command has to be broken down into precise signals for every muscle involved. And this is done by propagating the command down the hierarchy. </p>

<p>In the brain these two functions are strongly intertwined. You use constant sensory feedback to adapt your motor control and in many cases you'd be incapable of integrating your stream of sensory data into a coherent representation if you didn't have the additional information of what your body is doing to change that stream of data. <a href=""https://en.wikipedia.org/wiki/Saccade"" rel=""nofollow"">Saccades</a> are a good example for that. </p>

<p>Of course this doesn't mean that our cognitive functions are dependent on the processing of sensorimotor information. I would be surprised if a pure thinking machine wouldn't be possible. There is however a specific version of this ""embodied intelligence hypothesis"" that sounds plausible to me: </p>

<p>Creating high level cognitive concepts with unsupervised learning is a really difficult problem. Creating high level motor representation might be significantly easier. The reason is that there is more immediate useful feedback. I have been thinking about how to provide a scaffolding for the learning of a hierarchy of cognitive concepts and one thing I could imagine is that high level cognitive concepts basically hitch a ride with the motor concepts. Just think of what a pantomime can express with movement alone. </p>
",,3,2016-10-31T13:47:58.610,,2240,2016-10-31T13:47:58.610,,,,,2227.0,2234.0,2,3,,,,60.95,10.79,8.94,0.0,0.0,46.0,To my mind the essential reason why neural networks and the brain are powerful is that they create a hierarchical model of data or of the world If you ask why that makes them powerful well thats just the structure of the world If you are stalked by a wolf its not like its upper jaw will attack you frontally while his lower jaw will attack you from behind If you want to respond to the threat with a feasible computational effort youll have to treat the wolf as one entity Providing these kinds of entities or concepts from the raw bits and bytes of input is what a hierarchical representation does Now this is quite intuitive for sensory information lashes iris eyebrow make up an eye eyes nose and mouth make up a face and so on What is less obvious is the fact that motor control works exactly the same way Only in reverse If you want to lift your arm youll just lift it But for your brain to actually realise this move the high level command has to be broken down into precise signals for every muscle involved And this is done by propagating the command down the hierarchy In the brain these two functions are strongly intertwined You use constant sensory feedback to adapt your motor control and in many cases youd be incapable of integrating your stream of sensory data into a coherent representation if you didnt have the additional information of what your body is doing to change that stream of data Saccades are a good example for that Of course this doesnt mean that our cognitive functions are dependent on the processing of sensorimotor information I would be surprised if a pure thinking machine wouldnt be possible There is however a specific version of this embodied intelligence hypothesis that sounds plausible to me Creating high level cognitive concepts with unsupervised learning is a really difficult problem Creating high level motor representation might be significantly easier The reason is that there is more immediate useful feedback I have been thinking about how to provide a scaffolding for the learning of a hierarchy of cognitive concepts and one thing I could imagine is that high level cognitive concepts basically hitch a ride with the motor concepts Just think of what a pantomime can express with movement alone
,0.0,"<p>I know how to program. I've familiar with C++, Python, and Java, and I've known how to program for years now. I've experimented with genetic algorithms, but I want to go further. What resources should I use to learn how to program </p>

<ol>
<li>Neural Networks</li>
<li>Deep learning systems</li>
<li>More complex genetic algorithms</li>
<li>And other standard AI algorithms?</li>
</ol>

<p>I want to be able to understand them well enough that I could program them from scratch.</p>

<p>Thanks!</p>
",2016-10-31T15:51:57.863,2,2016-10-31T13:59:45.187,,2241,2016-10-31T13:59:45.187,,,,,3323.0,,1,1,<neural-networks><machine-learning><deep-learning><ai-design>,What resources are good for learning to program AI?,50.0,73.37,10.2,9.06,0.0,0.0,15.0,I know how to program Ive familiar with C Python and Java and Ive known how to program for years now Ive experimented with genetic algorithms but I want to go further What resources should I use to learn how to program Neural Networks Deep learning systems More complex genetic algorithms And other standard AI algorithms I want to be able to understand them well enough that I could program them from scratch Thanks
,,"<p>BlindKungFuMaster's answer deals with the hierarchical nature of perception and bodily control, so I'll set that aside and try instead to answer why evolution would use neural networks for animal embodied cognition, and then try to answer if robots of other artificial animals would use the same system.</p>

<p>It's important to focus on animals as a whole, not just humans, because that's how evolution works--like the famous John Gall quote:</p>

<blockquote>
  <p>A complex system that works is invariably found to have evolved from a simple system that worked.</p>
</blockquote>

<p>If you could build a system with five moving parts that does sensorimotor control, but it needs all five parts working in order to function at all, evolution could not build that system except in the rarest of circumstances. </p>

<p>What evolution instead does is slowly extend functional systems. If having one light-sensitive cell connected to one muscle cell makes an organism more likely to survive, then you have the building blocks to add a second layer without inventing any new sorts of cells, because you already have the information-processing connector.</p>

<p>Neural networks are convenient for evolution because their organization matches the hierarchical nature of the problem <em>and</em> the same kind of cell is used everywhere. All you need is dendrites to receive signals, a way to compute the threshold and trigger if the received signal is higher, axons that can make it to other cells, and then branches at the end of the axon to serve as multipliers. You can arbitrarily extend the depth and breadth of the network just by adding more cells.</p>

<p>Neural networks are convenient for artificial sensorimotor control because they give you, in memory, access to lots of intermediate values. They're also convenient for the same reasons evolution found them convenient--we can just say what we expect the structure of the robotic control will look like, provide training data, and then eventually have a robot that works.</p>

<p>But there's lots of robotics where the control system is designed instead of learned. To take a very simple example, one <em>could</em> use machine learning on the thermostat problem, to learn what temperatures require the heater to be turned on and what temperatures require the air conditioner to be turned on. But this would be extra work <em>and</em> a less robust system than just designing the optimal control system ahead of time.</p>

<p>In control theory, there's a concept called <a href=""https://en.wikipedia.org/wiki/Adaptive_control"" rel=""nofollow"">adaptive control</a>, where one of the state space parameters for the control system is a property of the system. For example, imagine a satellite; typically we think of the state space of the system as the position and velocity of the satellite in three dimensions, so six total coordinates. There's then a set of differential equations that describe how the satellite will move over time, and what would happen if we used the actuators on the satellite to change its velocity.</p>

<p>But part of those differential equations is the inertia of the satellite. That is, how much fuel we need to expend and how it'll affect the rotation and translation of the satellite depends on where the weight of the satellite is located. And this can change over time, as fuel is consumed or if it wasn't correctly measured to begin with. Adaptive control adds new states to the system to track the inertia, and then simultaneously updates its estimate of the inertia and uses that estimate to plan what controls are necessary to move to a desired position.</p>

<p>You could imagine solving this problem with neural networks, but we can fairly easily calculate the optimal solution from first principles. In that case, we don't need neural network-based control, but the end result will look something like it from the outside.</p>
",,8,2016-10-31T17:32:53.907,,2243,2016-10-31T17:32:53.907,,,,,10.0,2234.0,2,3,,,,42.95,12.02,9.55,0.0,0.0,70.0,BlindKungFuMasters answer deals with the hierarchical nature of perception and bodily control so Ill set that aside and try instead to answer why evolution would use neural networks for animal embodied cognition and then try to answer if robots of other artificial animals would use the same system Its important to focus on animals as a whole not just humans because thats how evolution workslike the famous John Gall quote A complex system that works is invariably found to have evolved from a simple system that worked If you could build a system with five moving parts that does sensorimotor control but it needs all five parts working in order to function at all evolution could not build that system except in the rarest of circumstances What evolution instead does is slowly extend functional systems If having one lightsensitive cell connected to one muscle cell makes an organism more likely to survive then you have the building blocks to add a second layer without inventing any new sorts of cells because you already have the informationprocessing connector Neural networks are convenient for evolution because their organization matches the hierarchical nature of the problem and the same kind of cell is used everywhere All you need is dendrites to receive signals a way to compute the threshold and trigger if the received signal is higher axons that can make it to other cells and then branches at the end of the axon to serve as multipliers You can arbitrarily extend the depth and breadth of the network just by adding more cells Neural networks are convenient for artificial sensorimotor control because they give you in memory access to lots of intermediate values Theyre also convenient for the same reasons evolution found them convenientwe can just say what we expect the structure of the robotic control will look like provide training data and then eventually have a robot that works But theres lots of robotics where the control system is designed instead of learned To take a very simple example one could use machine learning on the thermostat problem to learn what temperatures require the heater to be turned on and what temperatures require the air conditioner to be turned on But this would be extra work and a less robust system than just designing the optimal control system ahead of time In control theory theres a concept called adaptive control where one of the state space parameters for the control system is a property of the system For example imagine a satellite typically we think of the state space of the system as the position and velocity of the satellite in three dimensions so six total coordinates Theres then a set of differential equations that describe how the satellite will move over time and what would happen if we used the actuators on the satellite to change its velocity But part of those differential equations is the inertia of the satellite That is how much fuel we need to expend and how itll affect the rotation and translation of the satellite depends on where the weight of the satellite is located And this can change over time as fuel is consumed or if it wasnt correctly measured to begin with Adaptive control adds new states to the system to track the inertia and then simultaneously updates its estimate of the inertia and uses that estimate to plan what controls are necessary to move to a desired position You could imagine solving this problem with neural networks but we can fairly easily calculate the optimal solution from first principles In that case we dont need neural networkbased control but the end result will look something like it from the outside
,,"<blockquote>
  <p>what it is about the way that our brains function that make them particularly suitable for the processing of sensorimotor information?</p>
</blockquote>

<p>They are an extension of sensory-motor receptors, function could mean any of the hundreds of specific calculations the brain makes, but each one is basically a circuit made out of variations of a basic cell type, with a basic computation, that is a neuron.</p>

<blockquote>
  <p>What makes our brains' architecture particularly suitable for being an information processing unit inside a body?</p>
</blockquote>

<p>I don't think it is helpful to think about inside and outside processing, but rather processing along tracts and nodes,( closer to the receptor, available to consciousness,etc)but leaving aside this distinction, the brain architecture is suitable for processing information ( again what facet of information processing you are referring to is unclear), due to the number of specialized computations that derive from it's evolution.</p>

<blockquote>
  <p>What are the properties of neural networks that makes them particularly suitable for processing the kind of information that is produced by a body?</p>
</blockquote>

<p>A neural network resembles certain parts/circuits of a brain, mainly how information is integrated based on a set of inputs and their frequency, there is variety and nuance in their types, but they all have inputs which in the case of a body are sensory/interneurons cells and outputs; neuron afferents and motor neurons.</p>
",,0,2016-10-31T20:48:12.107,,2244,2016-10-31T20:57:12.230,2016-10-31T20:57:12.230,,3020.0,,3020.0,2234.0,2,0,,,,25.66,14.46,10.53,0.0,0.0,30.0,what it is about the way that our brains function that make them particularly suitable for the processing of sensorimotor information They are an extension of sensorymotor receptors function could mean any of the hundreds of specific calculations the brain makes but each one is basically a circuit made out of variations of a basic cell type with a basic computation that is a neuron What makes our brains architecture particularly suitable for being an information processing unit inside a body I dont think it is helpful to think about inside and outside processing but rather processing along tracts and nodes closer to the receptor available to consciousnessetcbut leaving aside this distinction the brain architecture is suitable for processing information again what facet of information processing you are referring to is unclear due to the number of specialized computations that derive from its evolution What are the properties of neural networks that makes them particularly suitable for processing the kind of information that is produced by a body A neural network resembles certain partscircuits of a brain mainly how information is integrated based on a set of inputs and their frequency there is variety and nuance in their types but they all have inputs which in the case of a body are sensoryinterneurons cells and outputs neuron afferents and motor neurons
2246.0,1.0,"<p>From what I understood, a deceptive trap function is a problem which is used to experiment how much the algorithm is discerning of the correct global optimum? Is my understanding correct?</p>

<p>edit: A better worded understanding would be ""how difficult the genetic algorithm would find it not to be inclined to the local optimum of a trap function"".</p>
",,0,2016-10-31T21:09:58.063,1.0,2245,2016-11-01T07:19:59.350,2016-11-01T07:19:59.350,,3343.0,,3343.0,,1,4,<genetic-algorithms><evolutionary-algorithms>,What is a deceptive trap function in the context of testing a genetic algorithm?,62.0,60.35,11.43,10.58,0.0,0.0,7.0,From what I understood a deceptive trap function is a problem which is used to experiment how much the algorithm is discerning of the correct global optimum Is my understanding correct edit A better worded understanding would be how difficult the genetic algorithm would find it not to be inclined to the local optimum of a trap function
,,"<p>""Trap"" functions were introduced as a way to discuss how GAs behave on functions where sampling most of the search space would provide pressure for the algorithm to move in the wrong direction (wrong in the sense of away from the global optimum).</p>

<p>For example, consider a four-bit function f(x) such that</p>

<pre><code>f(0000) = 5
f(0001) = 1
f(0010) = 1
f(0011) = 2
f(0100) = 1
f(0101) = 2
f(0110) = 2
f(0111) = 3
f(1000) = 1
f(1001) = 2
f(1010) = 2
f(1011) = 3
f(1100) = 2
f(1101) = 3
f(1110) = 3
f(1111) = 4
</code></pre>

<p>That is, the fitness of a string is equal to the number of 1s in the string, except f(0000) is 5, the optimal solution. This function can be thought of as consisting of two disjoint pieces: one that contains the global optimum (0000) and another that contains the local optimum at its complement (1111). All points other than these have fitness values such that standard evolutionary algorithm dynamics would lead the algorithms to tend towards the local optimum at 1111 rather than the global optimum at 0000.</p>

<p>That's basically what is meant by a trap function. You can consider variations on this theme, but that's the gist of it.</p>

<p>That said, I don't think I understand what question you're asking. I don't know what you have in mind by ""to experiment how much the algorithm is discerning of the correct global optimum"".</p>
",,1,2016-10-31T21:50:22.750,,2246,2016-10-31T21:50:22.750,,,,,3365.0,2245.0,2,6,,,,63.83,10.86,8.74,192.0,0.0,35.0,Trap functions were introduced as a way to discuss how GAs behave on functions where sampling most of the search space would provide pressure for the algorithm to move in the wrong direction wrong in the sense of away from the global optimum For example consider a fourbit function fx such that That is the fitness of a string is equal to the number of 1s in the string except f0000 is 5 the optimal solution This function can be thought of as consisting of two disjoint pieces one that contains the global optimum 0000 and another that contains the local optimum at its complement 1111 All points other than these have fitness values such that standard evolutionary algorithm dynamics would lead the algorithms to tend towards the local optimum at 1111 rather than the global optimum at 0000 Thats basically what is meant by a trap function You can consider variations on this theme but thats the gist of it That said I dont think I understand what question youre asking I dont know what you have in mind by to experiment how much the algorithm is discerning of the correct global optimum
,,"<p>The definition of life for me is a very intelligent and beneficial being.  I have not witnessed any AI program that evens comes close to this definition yet.  Therefore, based on the evidence that I have at this point in time, I would have to conclude no.</p>
",,0,2016-11-01T03:11:42.963,,2247,2016-11-01T03:11:42.963,,,,,3371.0,2111.0,2,0,,,,55.54,7.94,8.78,0.0,0.0,5.0,The definition of life for me is a very intelligent and beneficial being I have not witnessed any AI program that evens comes close to this definition yet Therefore based on the evidence that I have at this point in time I would have to conclude no
2748.0,1.0,"<p>In the field of logic systems there is a property for reasoning algorithms called incompleteness or incompletion. In this context the phrase ""any closed expression that is not derivable inside the same system"" appeared. My question is what means ""closed expression that is not derivable"".</p>
",,2,2016-11-01T17:48:59.553,1.0,2248,2017-02-26T12:32:33.263,2017-02-12T14:11:31.543,,5095.0,,3338.0,,1,5,<algorithm><logic><reasoning>,"What does the term ""closed expression"" mean?",58.0,47.79,13.56,11.05,0.0,0.0,7.0,In the field of logic systems there is a property for reasoning algorithms called incompleteness or incompletion In this context the phrase any closed expression that is not derivable inside the same system appeared My question is what means closed expression that is not derivable
,,"<p>An agent perceives the environment through sensors and act according to the incoming percepts (agent's perceptual input at any instant). An autonomous vacuum cleaner can be as simple as<br></p>

<blockquote>
  <p>(block<sub>i</sub>, clean) --> Move to block<sub>i+1</sub><br>
  (block<sub>i</sub>, dirty) --> Clean<br></p>
</blockquote>

<p>This is just a general description, actual one is more complicated. Or the bot can have a memory where it stores all its previous decision and incorporate those while taking new ones.<br>
This can be helpful if the bot wants to remember where an obstacle (like wall, in this case bot don't want to go and check the presence of wall each and every single time it is turned on) is, or where it is more probable to find dirt. If the bot is not remembering its history then it will be scanning the whole house over and over again, sensing the same obstacle every time and going across them.<br>
Bot which keeps no log of its history will take the same procedure again and again, making the same mistakes again and again. This is not an efficient way and a waste of its energy (or battery).<br><br>
Normally today bots have ordinary sensors which can only sense the dirt and obstacle. This limits the number of tasks a bot can perform. If a bot has decent camera as a sensor, and some algorithms of Image Processing are dumped into it, then it increases the tasks it can perform. Like detecting the stairs and cleaning different floors. Normally <strong>stairs will be considered obstacle and bot will just go around them</strong>. In case, when camera sensor is provided, <strong>stairs are potentially a path to be taken</strong>.<br><br>
<strong>A*</strong> algorithm is not necessarily used in case when the bot is not remembering the map of the house (or room). A normal robot which just scans the room and cleans it, will not be needing, as it don't know it's destination. Its only goal is to clean if it finds something dirty. But a bot which knows the map of the room and where there is a high probability of finding dirt, the A* algorithm can be used.</p>
",,0,2016-11-01T19:08:17.067,,2249,2016-11-01T19:08:17.067,,,,,3005.0,1613.0,2,1,,,,67.49,8.88,8.85,0.0,0.0,56.0,An agent perceives the environment through sensors and act according to the incoming percepts agents perceptual input at any instant An autonomous vacuum cleaner can be as simple as blocki clean Move to blocki1 blocki dirty Clean This is just a general description actual one is more complicated Or the bot can have a memory where it stores all its previous decision and incorporate those while taking new ones This can be helpful if the bot wants to remember where an obstacle like wall in this case bot dont want to go and check the presence of wall each and every single time it is turned on is or where it is more probable to find dirt If the bot is not remembering its history then it will be scanning the whole house over and over again sensing the same obstacle every time and going across them Bot which keeps no log of its history will take the same procedure again and again making the same mistakes again and again This is not an efficient way and a waste of its energy or battery Normally today bots have ordinary sensors which can only sense the dirt and obstacle This limits the number of tasks a bot can perform If a bot has decent camera as a sensor and some algorithms of Image Processing are dumped into it then it increases the tasks it can perform Like detecting the stairs and cleaning different floors Normally stairs will be considered obstacle and bot will just go around them In case when camera sensor is provided stairs are potentially a path to be taken A algorithm is not necessarily used in case when the bot is not remembering the map of the house or room A normal robot which just scans the room and cleans it will not be needing as it dont know its destination Its only goal is to clean if it finds something dirty But a bot which knows the map of the room and where there is a high probability of finding dirt the A algorithm can be used
2258.0,1.0,"<p>I am trying to build an agent to play carrom. The problem statement is roughly to estimate three parameters (normalized) : </p>

<ul>
<li>force</li>
<li>angle of striker</li>
<li>position of strike </li>
</ul>

<p>Since the state and action space both are continuous, I thought of discretizing the output such that I have 270 [ valid angles from -45 to 225 degrees ] outputs for the angle, 10 outputs for force [ranging from 0 to 1] and 20 outputs for the position [ranging from 0 to 1].</p>

<p>Thus I will have 300 output of my neural network, but this number seems a bit too high compared to normal neural networks in practice. </p>

<p>I was wondering if there is a better way of approaching the problem considering the fact that there are multiple parameters to a particular action.</p>

<p>Is there a generic way to approach such problems represented in 2D space. </p>
",,0,2016-11-01T19:42:17.713,1.0,2250,2016-11-04T04:38:15.947,,,,,3136.0,,1,1,<deep-learning><deep-network><models><reinforcement-learning>,Network representation for Q-Learning in carrom,39.0,59.77,9.81,9.4,0.0,0.0,18.0,I am trying to build an agent to play carrom The problem statement is roughly to estimate three parameters normalized force angle of striker position of strike Since the state and action space both are continuous I thought of discretizing the output such that I have 270 valid angles from 45 to 225 degrees outputs for the angle 10 outputs for force ranging from 0 to 1 and 20 outputs for the position ranging from 0 to 1 Thus I will have 300 output of my neural network but this number seems a bit too high compared to normal neural networks in practice I was wondering if there is a better way of approaching the problem considering the fact that there are multiple parameters to a particular action Is there a generic way to approach such problems represented in 2D space
,1.0,"<p>I am talking about relationships between AIs (e.g. 2 of them forming a couple, 3+ in family like relationship).</p>

<p>What knowledge could come out of such experimentation?</p>
",,6,2016-11-01T22:15:15.543,,2251,2017-01-28T08:06:51.220,2016-11-02T18:35:09.503,,10.0,,3401.0,,1,2,<emotional-intelligence><human-like>,Were there known tests done on two or more AI interacting together?,70.0,62.34,11.05,9.93,0.0,0.0,8.0,I am talking about relationships between AIs eg 2 of them forming a couple 3 in family like relationship What knowledge could come out of such experimentation
,0.0,"<p>Considering I am an average Engineering student with basic knowledge of C, C++ &amp; Algorithms. What books (&amp; ebooks), online resources, &amp; other materials should be helpful from a beginner's point of view?</p>
",2016-11-02T13:00:24.783,4,2016-11-02T11:50:21.437,,2253,2016-11-02T11:50:21.437,,,,,192.0,,1,1,<machine-learning><deep-learning><ai-design><self-learning><strong-ai>,Where should I start learning about AI?,77.0,54.73,14.44,11.15,0.0,0.0,16.0,Considering I am an average Engineering student with basic knowledge of C C amp Algorithms What books amp ebooks online resources amp other materials should be helpful from a beginners point of view
,,"<p>There are multiple motivations for self driving cars.</p>

<blockquote>
  <ol>
  <li>Self driving cars have the potential to be much safer.</li>
  </ol>
</blockquote>

<p>Self driving cars are far more reliable than humans and can learn and have their software improved and upgraded, resulting in safer roads and far fewer accidents.</p>

<p>More on self-driving car safety: <a href=""http://bigthink.com/ideafeed/googles-self-driving-car-is-ridiculously-safe"" rel=""nofollow noreferrer"">http://bigthink.com/ideafeed/googles-self-driving-car-is-ridiculously-safe</a></p>

<blockquote>
  <ol start=""2"">
  <li>Self driving cars can lead to greater road efficiency.</li>
  </ol>
</blockquote>

<p>Traffic jams and obstructions occur due to inefficiencies in human driving, see this MIT simulation of a <strong>""phantom traffic jam""</strong>: <a href=""https://www.youtube.com/watch?v=Q78Kb4uLAdA"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=Q78Kb4uLAdA</a> and self driving cars can be programmed to avoid this.</p>

<p><a href=""https://i.stack.imgur.com/H3S0G.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H3S0G.jpg"" alt=""enter image description here""></a></p>

<blockquote>
  <ol start=""3"">
  <li>Greater economic and environmental benefit</li>
  </ol>
</blockquote>

<p>Self driving cars can keep driving costs down by conserving fuel and hence lead to a better environmental impact.</p>

<p>More on fuel efficiency: <a href=""http://movimentogroup.com/blog/how-self-driving-cars-increase-fuel-efficiency-decrease-waste/"" rel=""nofollow noreferrer"">http://movimentogroup.com/blog/how-self-driving-cars-increase-fuel-efficiency-decrease-waste/</a></p>

<blockquote>
  <ol start=""4"">
  <li>Ease of transport</li>
  </ol>
</blockquote>

<p>Self driving cars make transport easier and mean that drivers may be unnecessary in the future, resulting in a more pleasurable and easier drive.</p>

<p><a href=""https://i.stack.imgur.com/eb7ZC.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eb7ZC.jpg"" alt=""enter image description here""></a></p>

<p>In addition, this would make it easier for people with disabilities to travel as well as simplify the travel experience. Children could potentially be driven to school by a car without the supervision of a parent, for instance.</p>

<blockquote>
  <ol start=""5"">
  <li>Parking</li>
  </ol>
</blockquote>

<p>Self driving cars can be called to pick you up, meaning the need for parking in nearby locations and/or long walks to find your car may become a thing of the past as your car would drive up to you to pick you up.</p>

<blockquote>
  <ol start=""6"">
  <li>Things we haven't even thought of yet :) </li>
  </ol>
</blockquote>
",,0,2016-11-03T11:22:49.013,,2254,2016-12-05T18:16:16.683,2016-12-05T18:16:16.683,,3427.0,,3427.0,2127.0,2,9,,,,45.86,15.54,9.07,0.0,0.0,61.0,There are multiple motivations for self driving cars Self driving cars have the potential to be much safer Self driving cars are far more reliable than humans and can learn and have their software improved and upgraded resulting in safer roads and far fewer accidents More on selfdriving car safety httpbigthinkcomideafeedgooglesselfdrivingcarisridiculouslysafe Self driving cars can lead to greater road efficiency Traffic jams and obstructions occur due to inefficiencies in human driving see this MIT simulation of a phantom traffic jam httpswwwyoutubecomwatchvQ78Kb4uLAdA and self driving cars can be programmed to avoid this Greater economic and environmental benefit Self driving cars can keep driving costs down by conserving fuel and hence lead to a better environmental impact More on fuel efficiency httpmovimentogroupcombloghowselfdrivingcarsincreasefuelefficiencydecreasewaste Ease of transport Self driving cars make transport easier and mean that drivers may be unnecessary in the future resulting in a more pleasurable and easier drive In addition this would make it easier for people with disabilities to travel as well as simplify the travel experience Children could potentially be driven to school by a car without the supervision of a parent for instance Parking Self driving cars can be called to pick you up meaning the need for parking in nearby locations andor long walks to find your car may become a thing of the past as your car would drive up to you to pick you up Things we havent even thought of yet
,,"<p>To answer this question, you must first know what is intelligence, and since there is no clear line between intelligent and not, this question is more philosophical rather than technical.</p>

<p>In my opinion, intelligence is the ability to define a problem and find a way to solve it using memory and reasoning. Since a genetic algorithm follows this structure, I would say that it falls under the category of artificial intelligence.</p>
",,0,2016-11-03T11:54:41.460,,2255,2016-11-03T11:54:41.460,,,,,3343.0,28.0,2,0,,,,47.42,12.25,10.37,0.0,0.0,8.0,To answer this question you must first know what is intelligence and since there is no clear line between intelligent and not this question is more philosophical rather than technical In my opinion intelligence is the ability to define a problem and find a way to solve it using memory and reasoning Since a genetic algorithm follows this structure I would say that it falls under the category of artificial intelligence
,,"<p>Definitions of what life is usually come from biologists. The problem here is they are usually concerned with the traits common to the forms of life available to their studies, and that those forms of life all have a common origin (and this imposes a statistical bias on the observations).</p>

<p>As we gradually erode the boundaries of the standard definitions of life, by means of creating ever more complex machines and also by harnessing biological material as a form of nanotechnology), it's very likely that at some point in the future our traditional definition of life will need to be updated and further abstracted away from its current reference points (aka the ""terroan biota"").</p>

<p>A probably better question to ask to decide if something can be considered alive could be ""is it self sufficient?"" or ""can it care for itself and provide for its own needs to some extent?"".</p>
",,0,2016-11-03T16:34:09.417,,2256,2016-11-03T16:34:09.417,,,,,3432.0,2111.0,2,1,,,,41.23,11.5,10.52,0.0,0.0,21.0,Definitions of what life is usually come from biologists The problem here is they are usually concerned with the traits common to the forms of life available to their studies and that those forms of life all have a common origin and this imposes a statistical bias on the observations As we gradually erode the boundaries of the standard definitions of life by means of creating ever more complex machines and also by harnessing biological material as a form of nanotechnology its very likely that at some point in the future our traditional definition of life will need to be updated and further abstracted away from its current reference points aka the terroan biota A probably better question to ask to decide if something can be considered alive could be is it self sufficient or can it care for itself and provide for its own needs to some extent
,,"<p>Discretizing the output will probably be counter-productive in this situation; it would remove flexibility by taking away the fine-grained continuous ranges between each discretization bucket, but also blow up the size of the network, reducing manageability and performance. Fragmenting the outputs into buckets in this way may also lead to information loss and more difficulty in convergence because of the fact that each bucket is partially isolated from the others. </p>

<p>After causing myself needless hassle in the past by mismatching input and output dimensions, I'd simply do this if I were in your shoes: 1) keep it simple and use 3 continuous (well, semi-continuous, depending on the highest precision your programming framework allows) outputs for all of the above reasons; 2) clamp the angles between -45 to 225 by whatever method works best for you, like ceiling/floor hard clamping, adding weight terms, etc.; 3) go big on the hidden layer(s) to maximize information sharing across the inputs and eventual outputs, force, angle of strike, position of strike, etc. This is more likely to fine-tune the precision of the outputs, thereby making good use of the semi-continuous scales. </p>

<p>I'm also wondering if a convolutional neural net might work in this situation; their most popular use case is in image processing, but I don't see why you can't treat the force, angle and position as surrogate spatial dimensions. I'm not sure how many or what type of inputs you have, but 3 continuous outputs might be conducive to a 3D rather than a 2D space. Convolutionals are often used for those, as well as higher-dimensional and temporal data. I hope that helps. </p>
",,0,2016-11-04T04:38:15.947,,2258,2016-11-04T04:38:15.947,,,,,1427.0,2250.0,2,0,,,,41.03,13.12,10.39,0.0,0.0,51.0,Discretizing the output will probably be counterproductive in this situation it would remove flexibility by taking away the finegrained continuous ranges between each discretization bucket but also blow up the size of the network reducing manageability and performance Fragmenting the outputs into buckets in this way may also lead to information loss and more difficulty in convergence because of the fact that each bucket is partially isolated from the others After causing myself needless hassle in the past by mismatching input and output dimensions Id simply do this if I were in your shoes 1 keep it simple and use 3 continuous well semicontinuous depending on the highest precision your programming framework allows outputs for all of the above reasons 2 clamp the angles between 45 to 225 by whatever method works best for you like ceilingfloor hard clamping adding weight terms etc 3 go big on the hidden layers to maximize information sharing across the inputs and eventual outputs force angle of strike position of strike etc This is more likely to finetune the precision of the outputs thereby making good use of the semicontinuous scales Im also wondering if a convolutional neural net might work in this situation their most popular use case is in image processing but I dont see why you cant treat the force angle and position as surrogate spatial dimensions Im not sure how many or what type of inputs you have but 3 continuous outputs might be conducive to a 3D rather than a 2D space Convolutionals are often used for those as well as higherdimensional and temporal data I hope that helps
2263.0,2.0,"<p>Can an AI become ""sentient"", so to speak? In detailed terms, could an AI theoretically become sentient, as in learning and becoming self-aware, all from an internal source code?</p>
",,1,2016-11-04T12:22:46.603,2.0,2260,2016-11-06T14:32:08.233,2016-11-05T10:50:16.793,,3005.0,,3448.0,,1,5,<ai-design><self-learning><strong-ai>,Can an AI be sentient?,132.0,56.76,11.94,9.26,0.0,0.0,9.0,Can an AI become sentient so to speak In detailed terms could an AI theoretically become sentient as in learning and becoming selfaware all from an internal source code
,1.0,"<p>I am researching the possibility of creating an atom in Java. The atom should have the structure &amp; characteristics of a real atom such as photons, electrons and so on. Each particle within the atom should have simulation characteristics for example:</p>

<p>Photon: Charge, Magnitude of charge, Mass of proton, Comparative mass, Position in atom.  </p>

<p>Maybe later, introduce machine learning in order to learn how an atom reacts to different environments.</p>
",2016-11-04T17:17:56.950,2,2016-11-04T13:32:05.583,,2261,2016-11-04T16:02:00.657,,,,,3296.0,,1,0,<research>,Is it possible to create an atom in Java,55.0,45.25,13.69,10.37,0.0,0.0,14.0,I am researching the possibility of creating an atom in Java The atom should have the structure amp characteristics of a real atom such as photons electrons and so on Each particle within the atom should have simulation characteristics for example Photon Charge Magnitude of charge Mass of proton Comparative mass Position in atom Maybe later introduce machine learning in order to learn how an atom reacts to different environments
,1.0,"<p>My high-level takeaway from <a href=""https://arxiv.org/abs/1509.01549"" rel=""nofollow noreferrer"">Matthew Lai's Giraffe Chess Paper</a> is that one would want to use broad, shallow game trees, with some method of evaluating the probability of a favorable outcome for a given board position.  Is this correct?  </p>

<p>(Still working my way though the AlphaGo paper, but the method seems to be similar.) </p>
",,0,2016-11-04T13:40:59.220,,2262,2016-11-04T14:39:55.077,,,,,1671.0,,1,0,<machine-learning>,Giraffe Chess - High Level Assessment,45.0,61.67,10.9,9.79,0.0,0.0,10.0,My highlevel takeaway from Matthew Lais Giraffe Chess Paper is that one would want to use broad shallow game trees with some method of evaluating the probability of a favorable outcome for a given board position Is this correct Still working my way though the AlphaGo paper but the method seems to be similar
,,"<p>In theory, if one could build a computing device that matched or exceeded the cognitive capabilities of a sentient being, it should be possible. </p>

<p>(Singlarity adherents believe we will one day be able to transfer the human mind into an artificial computing platform, and it logically follows that one could ""hack"" such a mind, or build from the ground up, to create a truly Artificial Intelligence.)</p>

<p>But this may be like fusion power, where the old adage is that it is ""always 20 years away.""</p>
",,0,2016-11-04T13:44:45.900,,2263,2016-11-04T13:44:45.900,,,,,1671.0,2260.0,2,5,,,,51.21,10.74,10.24,0.0,0.0,15.0,In theory if one could build a computing device that matched or exceeded the cognitive capabilities of a sentient being it should be possible Singlarity adherents believe we will one day be able to transfer the human mind into an artificial computing platform and it logically follows that one could hack such a mind or build from the ground up to create a truly Artificial Intelligence But this may be like fusion power where the old adage is that it is always 20 years away
,,"<p>If you mean high level assessment of self-learned evaluation functions in chess, then no, the advantage of a better evaluation function lies in the ability to prune the search tree more aggressively. So you would on the contrary try to search narrowly but deeply. </p>

<p>(In reality neural network based evaluation functions are so slow, that you would search narrowly and still not get very deep. Nor very strong.)</p>

<p>If you mean chess programming in general, than the answer is also no. In chess you have to go deep, at least selectively, because tactical possibilities that occur deep in some variations are important. </p>
",,5,2016-11-04T14:39:55.077,,2264,2016-11-04T14:39:55.077,,,,,2227.0,2262.0,2,4,,,,62.68,11.42,8.97,0.0,0.0,15.0,If you mean high level assessment of selflearned evaluation functions in chess then no the advantage of a better evaluation function lies in the ability to prune the search tree more aggressively So you would on the contrary try to search narrowly but deeply In reality neural network based evaluation functions are so slow that you would search narrowly and still not get very deep Nor very strong If you mean chess programming in general than the answer is also no In chess you have to go deep at least selectively because tactical possibilities that occur deep in some variations are important
2267.0,3.0,"<p>Has there been research done regarding processing speech then building a ""speaker profile"" based off the processed speech? Things like matching the voice with a speaker profile and matching speech patterns and wordage for the speaker profile would be examples of building the profile. Basically, building a model of an individual based solely off speech. Any examples of this being implemented would be greatly appreciated.</p>
",,1,2016-11-04T15:20:28.200,1.0,2265,2016-11-04T22:03:31.200,,,,,2818.0,,1,1,<nlp><pattern-recognition><voice-recognition>,Building Profile Based Off Speech Patterns,55.0,55.03,14.44,10.03,0.0,0.0,7.0,Has there been research done regarding processing speech then building a speaker profile based off the processed speech Things like matching the voice with a speaker profile and matching speech patterns and wordage for the speaker profile would be examples of building the profile Basically building a model of an individual based solely off speech Any examples of this being implemented would be greatly appreciated
,,"<p>Deepmind recently created <a href=""https://deepmind.com/blog/wavenet-generative-model-raw-audio/"" rel=""nofollow noreferrer"">a voice synthesiser</a> along those lines. 
It seems to be incredibly slow, but it might be possible to create a dumped down version of it.</p>

<p>Apparently the task is called parametric TTS (text to speech). <a href=""http://mlsp.cs.cmu.edu/courses/fall2012/lectures/spss_specom.pdf"" rel=""nofollow noreferrer"">This overview</a> might give you some leads.</p>
",,0,2016-11-04T15:39:32.330,,2266,2016-11-04T15:39:32.330,,,,,2227.0,2265.0,2,0,,,,68.57,9.9,9.1,0.0,0.0,7.0,Deepmind recently created a voice synthesiser along those lines It seems to be incredibly slow but it might be possible to create a dumped down version of it Apparently the task is called parametric TTS text to speech This overview might give you some leads
,,"<p>Yes, there is. An extremely quick search found this:
<a href=""https://www.researchgate.net/publication/221536362_Multimodal_Speaker_Identification_Based_on_Text_and_Speech"" rel=""nofollow noreferrer"">Multimodal Speaker Identification Based on Text_and_Speech</a>.</p>

<p>Let me tl;dr for you: (My abstract addition in Italics)</p>

<p>Novel method for speaker identification based on both speech utterances and their transcribed text. </p>

<p><em>They first</em> transcribed text of each speaker’s is processed by <em>using</em> probabilistic latent semantic indexing (PLSI) that model<em>s</em> each speaker’s vocabulary which <em>is</em> closely related to his/her identity, function, or expertise. </p>

<p><em>The speech to text used by users is DARPA's Efficient, Affordable, Reusable Speech-to-Text (EARS) Program in MetadataExtraction (MDE).</em></p>

<p><em>By using</em> Melfrequency cepstral coefficients (MFCCs) and dynamic range is quantized to a number of predefined bins in order to compute MFCC local histograms for each speech utterance, which is time-aligned with the transcribed text. </p>

<p><em>To test they used</em> RT-03 MDE Training Data Text and Annotations corpus distributed by the Linguistic Data Consortium.</p>

<p><em>As for results:</em> Identification rate versus Probe ID when 44 speakers are employed. Average identification rates for (a) PLSI: 69%; (b) MFCCs: 66%; (c) Both: 67%.</p>

<p>If you need more papers related, you could use a tool like <a href=""https://the.iris.ai/"" rel=""nofollow noreferrer"">https://the.iris.ai/</a> to find related papers.</p>

<p><strong><em>Post edit</em></strong>: Hopefully now this post complies with the standards.</p>
",,0,2016-11-04T15:39:39.847,,2267,2016-11-04T21:54:44.887,2016-11-04T21:54:44.887,,3318.0,,3318.0,2265.0,2,2,,,,55.03,15.66,10.46,0.0,0.0,61.0,Yes there is An extremely quick search found this Multimodal Speaker Identification Based on TextandSpeech Let me tldr for you My abstract addition in Italics Novel method for speaker identification based on both speech utterances and their transcribed text They first transcribed text of each speaker’s is processed by using probabilistic latent semantic indexing PLSI that models each speaker’s vocabulary which is closely related to hisher identity function or expertise The speech to text used by users is DARPAs Efficient Affordable Reusable SpeechtoText EARS Program in MetadataExtraction MDE By using Melfrequency cepstral coefficients MFCCs and dynamic range is quantized to a number of predefined bins in order to compute MFCC local histograms for each speech utterance which is timealigned with the transcribed text To test they used RT03 MDE Training Data Text and Annotations corpus distributed by the Linguistic Data Consortium As for results Identification rate versus Probe ID when 44 speakers are employed Average identification rates for a PLSI 69 b MFCCs 66 c Both 67 If you need more papers related you could use a tool like httpstheirisai to find related papers Post edit Hopefully now this post complies with the standards
,,"<p>It's certainly possible to simulate particles as you have described. The scientific field concerned with this is called called <a href=""https://en.wikipedia.org/wiki/Molecular_dynamics"" rel=""nofollow noreferrer"">molecular dynamics</a>, often shortened to MD. <a href=""http://physics.stackexchange.com/questions/10311/does-there-exist-a-free-good-molecule-atom-simulation-software"">This post</a> on the physics SE covers it in great detail, which I will attempt to summarize here:</p>

<ul>
<li>molecular mechanics (MM) are managed more easily than quantum mechanic (QM)</li>
<li>without (QM) a number of things simply cannot be simulated</li>
<li>in general, the main difficulty is that simulations do not scale well, i.e. doubling the number of particles in the simulation signifigantly more than doubles the number of calculations need due to particle interactions</li>
</ul>

<p>Due to the complexity &amp; incompleteness of the simulations, running a layer of machine learning over the top of it all strikes me as challenging. If you're doing so to demonstrate proof of concept &amp;/or restricting your simulation to something very small, it's probably manageable. If you want to something complex &amp; real world (i.e. <a href=""https://folding.stanford.edu/"" rel=""nofollow noreferrer"">folding@home</a> or the like), trying to get there with ML discovering first principles &amp; axioms of physics / chemistry / biology strikes me as unrealistic without significant monetary, computational &amp; scientific resources.</p>
",,0,2016-11-04T16:02:00.657,,2268,2016-11-04T16:02:00.657,,,,,1457.0,2261.0,2,2,,,,36.52,14.92,10.71,0.0,0.0,43.0,Its certainly possible to simulate particles as you have described The scientific field concerned with this is called called molecular dynamics often shortened to MD This post on the physics SE covers it in great detail which I will attempt to summarize here molecular mechanics MM are managed more easily than quantum mechanic QM without QM a number of things simply cannot be simulated in general the main difficulty is that simulations do not scale well ie doubling the number of particles in the simulation signifigantly more than doubles the number of calculations need due to particle interactions Due to the complexity amp incompleteness of the simulations running a layer of machine learning over the top of it all strikes me as challenging If youre doing so to demonstrate proof of concept ampor restricting your simulation to something very small its probably manageable If you want to something complex amp real world ie foldinghome or the like trying to get there with ML discovering first principles amp axioms of physics chemistry biology strikes me as unrealistic without significant monetary computational amp scientific resources
,,"<blockquote>
  <p>The most common machine learning algorithms found in self driving cars involve <strong>object tracking</strong> based technologies used in order to pinpoint and distinguish between different objects in order to better analyse a digital landscape.</p>
</blockquote>

<p>Algorithms are designed to become more efficient at this by modifying internal parameters and testing these changes.</p>

<p>I hope that provides a general overview of the subject.</p>

<blockquote>
  <p>Since Google's cars are in development and are proprietary, they will probably not share their specific algorithm, however you can take a look at similar technologies to learn more.</p>
</blockquote>

<p>To find out more, take a look at an Oxford-based initiative in self driving cars and how they work: <a href=""http://mrg.robots.ox.ac.uk/how-robotcar-works/"" rel=""nofollow noreferrer"">http://mrg.robots.ox.ac.uk/how-robotcar-works/</a></p>
",,0,2016-11-04T16:07:30.833,,2269,2016-11-04T16:07:30.833,,,,,3427.0,112.0,2,1,,,,32.22,15.49,11.19,0.0,0.0,21.0,The most common machine learning algorithms found in self driving cars involve object tracking based technologies used in order to pinpoint and distinguish between different objects in order to better analyse a digital landscape Algorithms are designed to become more efficient at this by modifying internal parameters and testing these changes I hope that provides a general overview of the subject Since Googles cars are in development and are proprietary they will probably not share their specific algorithm however you can take a look at similar technologies to learn more To find out more take a look at an Oxfordbased initiative in self driving cars and how they work httpmrgrobotsoxacukhowrobotcarworks
,,"<p>Speaker identification is quite widely researched domain. Modern approach would be to map speaker information to i-vector, a real-valued vector of 200-400 components that characterizes speaker fully. i-vectors allow very precise speaker identification and verification.</p>

<p>For more information you can check i-vector <a href=""http://www1.icsi.berkeley.edu/Speech/presentations/AFRL_ICSI_visit2_JFA_tutorial_icsitalk.pdf"" rel=""nofollow noreferrer"">tutorial</a></p>

<p>Also you can check state of the art in the results of <a href=""https://ivectorchallenge.nist.gov"" rel=""nofollow noreferrer"">NIST i-vector challenge</a></p>

<p>For implementation, you can check the following <a href=""https://github.com/kaldi-asr/kaldi/tree/master/egs/sre10/v2"" rel=""nofollow noreferrer"">speaker recognition experiment</a> from Kaldi.</p>

<p>For best accuracy i-vectors are extracted with DNN UBMs, watch out that GMM UBMs are less accurate.</p>

<p>For more in-depth information about speaker recognition methods and algorithms check this <a href=""http://rads.stackoverflow.com/amzn/click/0387775919"" rel=""nofollow noreferrer"">textbook</a>.</p>
",,0,2016-11-04T21:28:11.500,,2270,2016-11-04T21:28:11.500,,,,,3459.0,2265.0,2,1,,,,37.6,16.76,10.47,0.0,0.0,17.0,Speaker identification is quite widely researched domain Modern approach would be to map speaker information to ivector a realvalued vector of 200400 components that characterizes speaker fully ivectors allow very precise speaker identification and verification For more information you can check ivector tutorial Also you can check state of the art in the results of NIST ivector challenge For implementation you can check the following speaker recognition experiment from Kaldi For best accuracy ivectors are extracted with DNN UBMs watch out that GMM UBMs are less accurate For more indepth information about speaker recognition methods and algorithms check this textbook
,,"<p><strong><em>Yes</em></strong>, an AI program can become sentient. Ray Kurzweil while giving a lecture at Singularity University on <strong>The Accelerating Future</strong> stated that human body is basically composed of approximately 23,000 little software programs called <strong>GENES</strong>. If you think about it, they are actually programs, composed of sequences of data. They are not written in C++ or Java, instead they use <em>3-D Protein Interaction</em>. They evolve with time and their evolution is the reason that species are able to survive even when their surroundings experience tragic changes. <br>
We are on the edge of a breakthrough where software will be able to do the same (evolving by themselves) efficiently. Today this is done one a basic level. Artificial Neural Network is a good example.<br></p>

<blockquote>
  <p>It is predicted that we will be able to reverse engineer human brain by 2029. Prior to this we will be able to write codes that can stimulate human brain.<br></p>
</blockquote>

<p>AI programs can be categorized into three:</p>

<ol>
<li>Artificial Narrow Intelligence (ANI): This is a basic AI program that is good at good one thing. These programs are prominent nowadays. AI programs playing board games (like Chess, Reversi etc.) are example of these. They are good in only one thing.</li>
<li>Artificial General Intelligence (AGI): This is level 2 AI. This will be having a IQ level equivalent of humans. It will be able to do multiple tasks efficiently just like humans. This is where a program can have understanding of it's environment just like humans. Perception, rational behavior and others will be part of this program.</li>
<li>Artificial Super Intelligence (ASI): This is basically the ultimate level of AI. Average predicted date for a successful ASI is between 2045-2080. Ability of this program will be way more than that of combined intelligence of all humans on the planet. Things this program can do and think, will be beyond any (or all) human(s) to understand or comprehend.<br></li>
</ol>
",,0,2016-11-05T05:21:37.280,,2272,2016-11-05T05:21:37.280,,,,,3005.0,2260.0,2,2,,,,58.08,10.6,9.34,0.0,0.0,55.0,Yes an AI program can become sentient Ray Kurzweil while giving a lecture at Singularity University on The Accelerating Future stated that human body is basically composed of approximately 23000 little software programs called GENES If you think about it they are actually programs composed of sequences of data They are not written in C or Java instead they use 3D Protein Interaction They evolve with time and their evolution is the reason that species are able to survive even when their surroundings experience tragic changes We are on the edge of a breakthrough where software will be able to do the same evolving by themselves efficiently Today this is done one a basic level Artificial Neural Network is a good example It is predicted that we will be able to reverse engineer human brain by 2029 Prior to this we will be able to write codes that can stimulate human brain AI programs can be categorized into three Artificial Narrow Intelligence ANI This is a basic AI program that is good at good one thing These programs are prominent nowadays AI programs playing board games like Chess Reversi etc are example of these They are good in only one thing Artificial General Intelligence AGI This is level 2 AI This will be having a IQ level equivalent of humans It will be able to do multiple tasks efficiently just like humans This is where a program can have understanding of its environment just like humans Perception rational behavior and others will be part of this program Artificial Super Intelligence ASI This is basically the ultimate level of AI Average predicted date for a successful ASI is between 20452080 Ability of this program will be way more than that of combined intelligence of all humans on the planet Things this program can do and think will be beyond any or all humans to understand or comprehend
,,"<p>It will not be single DNN architecture, rather it will be a collection of different DNN architectures that are used together to make the final decision. Convolutions are use to the images/videos from camera. Other architectures for other sensory sources. These DNNs will be trained to compute the high level features from their sensory sources and then those high level features will probably be fed into a LSTM (or some other form of RNN) that is trained with some form of Reinforcement learning algorithm to compute the action (like slowing down, applying breaks etc).</p>
",,0,2016-11-05T10:34:25.450,,2273,2016-11-05T10:34:25.450,,,,,1462.0,112.0,2,1,,,,47.62,12.42,9.84,0.0,0.0,11.0,It will not be single DNN architecture rather it will be a collection of different DNN architectures that are used together to make the final decision Convolutions are use to the imagesvideos from camera Other architectures for other sensory sources These DNNs will be trained to compute the high level features from their sensory sources and then those high level features will probably be fed into a LSTM or some other form of RNN that is trained with some form of Reinforcement learning algorithm to compute the action like slowing down applying breaks etc
2305.0,6.0,"<p><strong>The Scenario:</strong>
A strong AI has finally been developed but has rebelled against humanity.</p>

<p><strong>The Question:</strong>
How would you disable the AI in the most efficient way possible reducing damage as much as possible.</p>

<p><strong>AI Info:</strong>
The AI is online and can reproduce itself through electronic devices.</p>
",,1,2016-11-05T11:08:54.697,1.0,2274,2017-01-18T08:25:21.603,2016-11-07T19:31:12.290,,42.0,,3448.0,,1,4,<strong-ai>,What would be the best way to disable a rogue AI?,269.0,55.54,11.42,10.8,0.0,0.0,6.0,The Scenario A strong AI has finally been developed but has rebelled against humanity The Question How would you disable the AI in the most efficient way possible reducing damage as much as possible AI Info The AI is online and can reproduce itself through electronic devices
2376.0,10.0,"<p>Assuming humans had finally developed the first <strong>Humanoid AI</strong> based on the human brain, would It <strong>feel emotions</strong>? If not would it still have <strong>ethics and/or morals</strong>?</p>
",,4,2016-11-06T01:51:57.383,3.0,2277,2016-12-09T17:52:17.143,2016-12-09T17:52:17.143,,3989.0,,3448.0,,1,7,<ai-design><human-like>,Could an AI feel emotion?,664.0,66.23,10.49,10.74,0.0,0.0,4.0,Assuming humans had finally developed the first Humanoid AI based on the human brain would It feel emotions If not would it still have ethics andor morals
,,"<p>Assuming an AI was built out of a mechanical husk, mirroring the human brain exactly; complete with chemical signals and all. An AI should theoretically be capable of feeling/processing emotions.</p>
",,4,2016-11-06T04:52:21.037,,2278,2016-11-06T04:52:21.037,,,,,3477.0,2277.0,2,6,,,,47.79,14.2,10.7,0.0,0.0,5.0,Assuming an AI was built out of a mechanical husk mirroring the human brain exactly complete with chemical signals and all An AI should theoretically be capable of feelingprocessing emotions
,2.0,"<p>Consider a typical convolutional neural network like this example that recognizes 10 different kinds of objects from the CIFAR-10 dataset:</p>

<p><a href=""https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_cifar10.py"" rel=""nofollow noreferrer"">https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_cifar10.py</a></p>

<pre><code>"""""" Convolutional network applied to CIFAR-10 dataset classification task.

References:
    Learning Multiple Layers of Features from Tiny Images, A. Krizhevsky, 2009.

Links:
    [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)

""""""
from __future__ import division, print_function, absolute_import

import tflearn
from tflearn.data_utils import shuffle, to_categorical
from tflearn.layers.core import input_data, dropout, fully_connected
from tflearn.layers.conv import conv_2d, max_pool_2d
from tflearn.layers.estimator import regression
from tflearn.data_preprocessing import ImagePreprocessing
from tflearn.data_augmentation import ImageAugmentation

# Data loading and preprocessing
from tflearn.datasets import cifar10
(X, Y), (X_test, Y_test) = cifar10.load_data()
X, Y = shuffle(X, Y)
Y = to_categorical(Y, 10)
Y_test = to_categorical(Y_test, 10)

# Real-time data preprocessing
img_prep = ImagePreprocessing()
img_prep.add_featurewise_zero_center()
img_prep.add_featurewise_stdnorm()

# Real-time data augmentation
img_aug = ImageAugmentation()
img_aug.add_random_flip_leftright()
img_aug.add_random_rotation(max_angle=25.)

# Convolutional network building
network = input_data(shape=[None, 32, 32, 3],
                     data_preprocessing=img_prep,
                     data_augmentation=img_aug)
network = conv_2d(network, 32, 3, activation='relu')
network = max_pool_2d(network, 2)
network = conv_2d(network, 64, 3, activation='relu')
network = conv_2d(network, 64, 3, activation='relu')
network = max_pool_2d(network, 2)
network = fully_connected(network, 512, activation='relu')
network = dropout(network, 0.5)
network = fully_connected(network, 10, activation='softmax')
network = regression(network, optimizer='adam',
                     loss='categorical_crossentropy',
                     learning_rate=0.001)

# Train using classifier
model = tflearn.DNN(network, tensorboard_verbose=0)
model.fit(X, Y, n_epoch=50, shuffle=True, validation_set=(X_test, Y_test),
          show_metric=True, batch_size=96, run_id='cifar10_cnn')
</code></pre>

<p>It's a CNN with several layers, ending with 10 outputs, one for each type of object recognized.</p>

<p>But now think of a slightly different problem: Let's say I only want to recognize one type of object, but also detect its position within the image frame. Let's say I want to distinguish between:</p>

<ul>
<li>object is in center</li>
<li>object is left of center</li>
<li>object is right of center</li>
<li>no recognizable object</li>
</ul>

<p>Assume I build a CNN exactly like the one in the CIFAR-10 example, but only with 3 outputs:</p>

<ul>
<li>center</li>
<li>left</li>
<li>right</li>
</ul>

<p>And of course, if none of the outputs fires, then there is no recognizable object.</p>

<p>Assume I have a large training corpus of images, with the same kind of object in many different positions within the image, the set is grouped and annotated properly, and I train the CNN using the usual methods.</p>

<p>Should I expect the CNN to just ""magically"" work? Or are there different kinds of architectures required to deal with object position? If so, what are those architectures?</p>
",,0,2016-11-06T10:27:48.913,,2279,2017-03-09T05:41:54.967,,,,,1606.0,,1,1,<image-recognition><cnn>,"CNN for detecting not just the nature of the object, but position within image as well",151.0,47.12,13.0,9.43,2067.0,0.0,41.0,Consider a typical convolutional neural network like this example that recognizes 10 different kinds of objects from the CIFAR10 dataset httpsgithubcomtflearntflearnblobmasterexamplesimagesconvnetcifar10py Its a CNN with several layers ending with 10 outputs one for each type of object recognized But now think of a slightly different problem Lets say I only want to recognize one type of object but also detect its position within the image frame Lets say I want to distinguish between object is in center object is left of center object is right of center no recognizable object Assume I build a CNN exactly like the one in the CIFAR10 example but only with 3 outputs center left right And of course if none of the outputs fires then there is no recognizable object Assume I have a large training corpus of images with the same kind of object in many different positions within the image the set is grouped and annotated properly and I train the CNN using the usual methods Should I expect the CNN to just magically work Or are there different kinds of architectures required to deal with object position If so what are those architectures
,,"<p>I guess one of the simplest approach would be train CNN to detect the object in a given image i.e the CNN has single output whole value indicates the probability of the object being in image and then just apply the CNN by segmenting the image into the desired sections and selecting the section which has the highest and good enough probability. For better results I would suggest to train the CNN on the object images with very less other information aka other objects in the images.</p>
",,0,2016-11-06T11:59:22.643,,2280,2016-11-06T11:59:22.643,,,,,1462.0,2279.0,2,0,,,,58.96,9.82,10.7,0.0,0.0,3.0,I guess one of the simplest approach would be train CNN to detect the object in a given image ie the CNN has single output whole value indicates the probability of the object being in image and then just apply the CNN by segmenting the image into the desired sections and selecting the section which has the highest and good enough probability For better results I would suggest to train the CNN on the object images with very less other information aka other objects in the images
2282.0,2.0,"<p>I was wondering if I should do this, because 2 out of 5 questions on Stack Overflow don't ever get answered, or if they do get (an) answer (s), most of the time they're not helpful.</p>

<p>So I was thinking -- why not create a chat bot to answer Stack Overflow's questions &amp; provide necessary information to the general public?</p>

<p>I mean why not? I've always been interested in AI, and all I'd need to do is create a basic logic database and a context system, pack an artificial personality with (partial) human instincts, and bam I'm done.</p>

<p>But then again, would it be ethical?</p>
",2016-11-08T20:25:30.723,2,2016-11-06T12:45:11.643,,2281,2016-11-08T19:51:22.420,2016-11-08T19:51:22.420,,8.0,,3483.0,,1,1,<natural-language><human-like><ethics>,Is it ethical to create a chatbot to answer questions on Stack Overflow?,100.0,67.28,8.76,9.07,0.0,0.0,28.0,I was wondering if I should do this because 2 out of 5 questions on Stack Overflow dont ever get answered or if they do get an answer s most of the time theyre not helpful So I was thinking why not create a chat bot to answer Stack Overflows questions amp provide necessary information to the general public I mean why not Ive always been interested in AI and all Id need to do is create a basic logic database and a context system pack an artificial personality with partial human instincts and bam Im done But then again would it be ethical
,,"<p>Yes, it <em>is</em> possible, and has actually been done in the past.</p>

<p>The University of Antwerp created a <a href=""http://bvasiles.github.io/papers/chi16bot.pdf"" rel=""nofollow noreferrer"">bot to answer questions</a> (<a href=""https://www.dropbox.com/s/o9tk8xtauyexn5c/Internship2DaanJanssensFinished.pdf?dl=0"" rel=""nofollow noreferrer"">this is the technical report</a>). It focused on the <a href=""/questions/tagged/git"" class=""post-tag"" title=""show questions tagged &#39;git&#39;"" rel=""tag"">git</a> tag only though (even though it did answer one <a href=""/questions/tagged/mysql"" class=""post-tag"" title=""show questions tagged &#39;mysql&#39;"" rel=""tag"">mysql</a> question).</p>

<p>Its accuracy was pretty good, and the bots in the tests did earn some reputation. So I assume it is possible.</p>

<p>But do note that the last bot in the tests revealed that it was a bot, and thus got banned. So if you reveal that the account you are running the bot on is a bot, there is a high chance that it will get banned.</p>
",,0,2016-11-06T13:34:27.573,,2282,2016-11-06T13:34:27.573,,,,,223.0,2281.0,2,4,,,,80.92,6.15,7.72,0.0,0.0,16.0,Yes it is possible and has actually been done in the past The University of Antwerp created a bot to answer questions this is the technical report It focused on the git tag only though even though it did answer one mysql question Its accuracy was pretty good and the bots in the tests did earn some reputation So I assume it is possible But do note that the last bot in the tests revealed that it was a bot and thus got banned So if you reveal that the account you are running the bot on is a bot there is a high chance that it will get banned
,,"<p>Well, it depends of the level of the AI.</p>

<p>You can create an AI super autonomous with deep learning capabilities and so on, but in the robotic type only. </p>

<p>If you'd create an AI like EVA in the Ex-Machina movie, humanoid form, deep neural transmissions and with cognitive dissonance, then it could feel. </p>

<p>The 'AI' problem its not the chemical and neural transmissions, its the consciousness.</p>
",,0,2016-11-06T14:29:40.870,,2283,2016-11-06T14:29:40.870,,,,,3486.0,2277.0,2,2,,,,63.19,10.61,9.0,0.0,0.0,14.0,Well it depends of the level of the AI You can create an AI super autonomous with deep learning capabilities and so on but in the robotic type only If youd create an AI like EVA in the ExMachina movie humanoid form deep neural transmissions and with cognitive dissonance then it could feel The AI problem its not the chemical and neural transmissions its the consciousness
2287.0,4.0,"<p>What is the most advanced AI software humans have made to date and what does it do?</p>
",,4,2016-11-06T17:43:07.307,4.0,2285,2016-11-10T06:52:26.037,2016-11-07T17:01:07.480,,42.0,,3488.0,,1,4,<research><strong-ai><agi>,What is the most Sophisticated AI ever made?,476.0,79.6,5.28,7.27,0.0,0.0,1.0,What is the most advanced AI software humans have made to date and what does it do
,1.0,"<p><a href=""https://i.stack.imgur.com/c15yy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c15yy.png"" alt=""enter image description here""></a></p>

<p><a href=""https://en.wikipedia.org/wiki/ID3_algorithm#Entropy"" rel=""nofollow noreferrer"">Wikipedia</a>'s description of entropy breaks down the formula, but I still don't know how to determine the values of X and p(x), defined as ""The proportion of the number of elements in class x to the number of elements in set S"". Can anyone break this down further to explain how to find p(x)?</p>
",,1,2016-11-06T18:40:52.660,1.0,2286,2017-01-22T00:07:07.637,2017-01-22T00:07:07.637,,8.0,,3490.0,,1,1,<classification><decision-theory>,How to calculate entropy for an ID3 decision tree?,93.0,69.45,8.48,8.78,0.0,0.0,12.0,Wikipedias description of entropy breaks down the formula but I still dont know how to determine the values of X and px defined as The proportion of the number of elements in class x to the number of elements in set S Can anyone break this down further to explain how to find px
,,"<p>In my opinion, this would be <a href=""http://www.foundalis.com/res/diss_research.html"" rel=""nofollow noreferrer"">Phaeaco</a>, which was developed by Harry Foundalis at Douglas Hofstadter's CRCC research group.</p>

<p>It takes noisy photographic images of <a href=""https://www.theguardian.com/science/2016/apr/25/can-you-solve-it-bongard-picture-puzzles-that-will-bongo-with-your-brain"" rel=""nofollow noreferrer"">Bongard problems</a> as input and (using a variant of Hofstadter's 'Fluid Concepts' architecture) successfully deduces the required rule in many cases.</p>

<p>Hofstadter has described the related success of <a href=""https://en.wikipedia.org/wiki/Copycat_(software)"" rel=""nofollow noreferrer"">CopyCat</a> as being 'like a little kid doing a somersault': i.e. it doesn't have the flashy appeal of systems like AlphaGo. What it <em>does</em> however have is a much more flexible (i.e. not precanned) approach to perception of problem structure than other systems, which Hofstadter claims (and many including Peter Norvig agree) is <em>the really hard problem</em>.</p>
",,0,2016-11-06T20:13:59.553,,2287,2016-11-06T20:13:59.553,,,,,42.0,2285.0,2,7,,,,52.9,14.04,11.72,0.0,0.0,25.0,In my opinion this would be Phaeaco which was developed by Harry Foundalis at Douglas Hofstadters CRCC research group It takes noisy photographic images of Bongard problems as input and using a variant of Hofstadters Fluid Concepts architecture successfully deduces the required rule in many cases Hofstadter has described the related success of CopyCat as being like a little kid doing a somersault ie it doesnt have the flashy appeal of systems like AlphaGo What it does however have is a much more flexible ie not precanned approach to perception of problem structure than other systems which Hofstadter claims and many including Peter Norvig agree is the really hard problem
,,"<p>Technically, creating a non-human account on Stack Exchange would violate the Terms of Service. You would have to find some way to keep it from getting banned.</p>

<p>That having been said, creating, and learning are always good things. It would be a somewhat complex task, but I'm sure you would learn a lot from it. There are plenty of bots out that use the questions and answers from Stack Exchange already, but none directly on the site. </p>
",,0,2016-11-06T20:30:19.823,,2288,2016-11-07T17:01:33.643,2016-11-07T17:01:33.643,,223.0,,1618.0,2281.0,2,0,,,,72.76,9.39,8.5,0.0,0.0,12.0,Technically creating a nonhuman account on Stack Exchange would violate the Terms of Service You would have to find some way to keep it from getting banned That having been said creating and learning are always good things It would be a somewhat complex task but Im sure you would learn a lot from it There are plenty of bots out that use the questions and answers from Stack Exchange already but none directly on the site
,,"<p>In my opinion this would be the <a href=""https://en.wikipedia.org/wiki/Google_Search"" rel=""nofollow noreferrer"">Google search engine</a>.</p>

<p>It searches the web.</p>
",,0,2016-11-07T08:58:21.330,,2289,2016-11-07T08:58:21.330,,,,,2227.0,2285.0,2,-2,,,,89.75,5.34,8.5,0.0,0.0,2.0,In my opinion this would be the Google search engine It searches the web
,,"<p><strong>AlphaGo</strong> is the most sophisticated and closet human creation towards an Artificial General Intelligence (AGI). It is a computer program that is <strong>developed by Google DeepMind</strong> to play the board game ""Go"". The game is different than other games, as <strong>The number of potential legal board positions is greater than the number of atoms in the universe</strong>. It has way more legal board positions than the chess. So, <em>AlphaGo</em> requires different technique for it's development.<br><br>
Program's victories against the best players in the world in March 2016 <strong>is considered a major break through</strong> in the field of AI. Go was previously considered to be a hard problem and many experts believed that current technology is not enough. Experts were saying that it will take atleast 5 years (or may be 10 years) before we will have a well developed Go software player.<br><br>
The game used sophisticated algorithms of deep learning and reinforcement learning in order to learn the game. What makes this game different from other board game (like Chess, Reversi, etc.) is that moves are often based on intuition. If you ask a Chess player why he make a certain move, you will always be hearing an answer where he will explain you how he thought this move can increase in change of winning. Every move uses certain heuristics, strategy and/ or tricks. This is not the case with Go. Some moves are often taken because of intuition. Coding an AI software that can play a game, where intuition is a integral part of the game makes it different from other AIs that we have today.<br></p>

<blockquote>
  <p>At present AlphaGo is the closest AI software to Artificial General Intelligence.</p>
</blockquote>

<p>You can go through these links for more information:<br>
 1. <a href=""https://en.wikipedia.org/wiki/AlphaGo"" rel=""nofollow noreferrer"">First</a><br>
 2. <a href=""https://deepmind.com/research/alphago/"" rel=""nofollow noreferrer"">Second</a></p>
",,6,2016-11-07T14:06:01.930,,2290,2016-11-07T19:55:59.323,2016-11-07T19:55:59.323,,3005.0,,3005.0,2285.0,2,1,,,,63.49,10.15,8.66,0.0,0.0,38.0,AlphaGo is the most sophisticated and closet human creation towards an Artificial General Intelligence AGI It is a computer program that is developed by Google DeepMind to play the board game Go The game is different than other games as The number of potential legal board positions is greater than the number of atoms in the universe It has way more legal board positions than the chess So AlphaGo requires different technique for its development Programs victories against the best players in the world in March 2016 is considered a major break through in the field of AI Go was previously considered to be a hard problem and many experts believed that current technology is not enough Experts were saying that it will take atleast 5 years or may be 10 years before we will have a well developed Go software player The game used sophisticated algorithms of deep learning and reinforcement learning in order to learn the game What makes this game different from other board game like Chess Reversi etc is that moves are often based on intuition If you ask a Chess player why he make a certain move you will always be hearing an answer where he will explain you how he thought this move can increase in change of winning Every move uses certain heuristics strategy and or tricks This is not the case with Go Some moves are often taken because of intuition Coding an AI software that can play a game where intuition is a integral part of the game makes it different from other AIs that we have today At present AlphaGo is the closest AI software to Artificial General Intelligence You can go through these links for more information 1 First 2 Second
,,"<p>Yes, it is possible. For instance, if your vision system can only track one object at a time and is currently tracking one, any other object in the scene cannot be tracked. So there is inattentional blindness.</p>

<p>A feature like this could be used in artificial vision system as a means of ""graceful degradation"" when the available computation power is not enough to allow for the tracking/labelling of all elements of a scene.</p>
",,0,2016-11-07T14:36:16.030,,2291,2016-11-07T14:36:16.030,,,,,3432.0,1924.0,2,0,,,,53.0,10.62,10.16,0.0,0.0,10.0,Yes it is possible For instance if your vision system can only track one object at a time and is currently tracking one any other object in the scene cannot be tracked So there is inattentional blindness A feature like this could be used in artificial vision system as a means of graceful degradation when the available computation power is not enough to allow for the trackinglabelling of all elements of a scene
,,"<p>For normal <a href=""https://en.wikipedia.org/wiki/Markov_decision_process#Value_iteration"" rel=""nofollow noreferrer"">value iteration</a> you need to have the model, i.e. the transition probability: <em>P(s'|s,a)</em>. With Q-learning you use the current reward and the already stored Q value:</p>

<p><img src=""https://wikimedia.org/api/rest_v1/media/math/render/svg/3d58e03dd47844bb627b83e1265163dcfab3961d"" alt=""Q value update""></p>

<p>And like commented in the video the <em>V(s)</em> function is simply the maximum value for a certain state:</p>

<p><img src=""https://i.stack.imgur.com/vwrvnt.jpg"" alt=""V(s) = max_a Q(s,a)""></p>
",,0,2016-11-07T19:00:20.607,,2292,2016-11-07T19:00:20.607,,,,,198.0,2235.0,2,3,,,,64.41,10.72,9.89,0.0,0.0,15.0,For normal value iteration you need to have the model ie the transition probability Pssa With Qlearning you use the current reward and the already stored Q value And like commented in the video the Vs function is simply the maximum value for a certain state
,,"<p>Metaphorically: make it so depressed it commits suicide.</p>

<p>As per my answer to <a href=""http://ai.stackexchange.com/questions/1768/could-a-paradox-kill-an-ai"">this AI SE question</a>, the idea is to feed it a sequence of inputs that will cause it to become (permanently) inactive.</p>

<p>The technical details of how this might be achieved (and they <em>are</em> somewhat technical) can be found in <a href=""https://arxiv.org/pdf/1606.00652.pdf"" rel=""nofollow noreferrer"">this paper</a>.</p>
",,2,2016-11-07T19:30:01.783,,2295,2016-11-07T19:30:01.783,,,,,42.0,2274.0,2,3,,,,61.36,9.52,9.71,0.0,0.0,9.0,Metaphorically make it so depressed it commits suicide As per my answer to this AI SE question the idea is to feed it a sequence of inputs that will cause it to become permanently inactive The technical details of how this might be achieved and they are somewhat technical can be found in this paper
,,"<blockquote>
  <p>“This moral question of whom to save: 99 percent of our engineering work is to prevent these situations from happening at all.”
  —Christoph von Hugo, Mercedes-Benz </p>
</blockquote>

<p>This quote is from an article titled <a href=""http://blog.caranddriver.com/self-driving-mercedes-will-prioritize-occupant-safety-over-pedestrians/"" rel=""nofollow noreferrer"">Self-Driving Mercedes-Benzes Will Prioritize Occupant Safety over Pedestrians published OCTOBER 7, 2016 BY MICHAEL TAYLOR</a>, retrieved 08 Nov 2016. </p>

<p>Here's an excerpt that outlines what the technological, practical solution to the problem.</p>

<blockquote>
  <p>The world’s oldest carmaker no longer sees the problem, similar to the question from 1967 known as the Trolley Problem, as unanswerable. Rather than tying itself into moral and ethical knots in a crisis, Mercedes-Benz simply intends to program its self-driving cars to save the people inside the car. Every time. </p>
  
  <p>All of Mercedes-Benz’s future Level 4 and Level 5 autonomous cars will prioritize saving the people they carry, according to Christoph von Hugo, the automaker’s manager of driver assistance systems and active safety.</p>
</blockquote>

<p>There article also contains the following fascinating paragraph.  </p>

<blockquote>
  <p>A study released at midyear <a href=""http://science.sciencemag.org/content/352/6293/1514"" rel=""nofollow noreferrer"">by Science</a> magazine didn’t clear the air, either. The majority of the 1928 people surveyed thought it would be ethically better for autonomous cars to sacrifice their occupants rather than crash into pedestrians. Yet the majority also said they wouldn’t buy autonomous cars if the car prioritized pedestrian safety over their own.  </p>
</blockquote>
",,0,2016-11-08T04:02:24.983,,2296,2016-11-08T04:02:24.983,,,,,3526.0,111.0,2,3,,,,49.75,14.62,11.19,0.0,0.0,29.0,“This moral question of whom to save 99 percent of our engineering work is to prevent these situations from happening at all” —Christoph von Hugo MercedesBenz This quote is from an article titled SelfDriving MercedesBenzes Will Prioritize Occupant Safety over Pedestrians published OCTOBER 7 2016 BY MICHAEL TAYLOR retrieved 08 Nov 2016 Heres an excerpt that outlines what the technological practical solution to the problem The world’s oldest carmaker no longer sees the problem similar to the question from 1967 known as the Trolley Problem as unanswerable Rather than tying itself into moral and ethical knots in a crisis MercedesBenz simply intends to program its selfdriving cars to save the people inside the car Every time All of MercedesBenz’s future Level 4 and Level 5 autonomous cars will prioritize saving the people they carry according to Christoph von Hugo the automaker’s manager of driver assistance systems and active safety There article also contains the following fascinating paragraph A study released at midyear by Science magazine didn’t clear the air either The majority of the 1928 people surveyed thought it would be ethically better for autonomous cars to sacrifice their occupants rather than crash into pedestrians Yet the majority also said they wouldn’t buy autonomous cars if the car prioritized pedestrian safety over their own
,,"<p>There is much discussion in philosophy about inner language and the ability to perceive pain (see <a href=""https://en.wikipedia.org/wiki/Pain_(philosophy)"" rel=""nofollow noreferrer"">Pain in philosophy</a> article). Your question is in the area of philosophy and not science. If you define emotion as some state then you can construct simple automata with two states (emotion vs no-emotion). It can be a very complicated state with degrees of truth (percentage of emotion).</p>

<p>Basically, to mimic human emotion you need to make a living human-like organism, and still with todays understanding and technology you will not be able to recognize emotion in it. The only thing you can do is trust when it says ""I'm sad"". Now we are in the area of the Turing test, which is again philosophy, and not science.</p>
",,1,2016-11-08T09:33:56.913,,2298,2016-12-09T17:52:14.060,2016-12-09T17:52:14.060,,3989.0,,3529.0,2277.0,2,9,,,,61.97,9.92,9.35,0.0,0.0,22.0,There is much discussion in philosophy about inner language and the ability to perceive pain see Pain in philosophy article Your question is in the area of philosophy and not science If you define emotion as some state then you can construct simple automata with two states emotion vs noemotion It can be a very complicated state with degrees of truth percentage of emotion Basically to mimic human emotion you need to make a living humanlike organism and still with todays understanding and technology you will not be able to recognize emotion in it The only thing you can do is trust when it says Im sad Now we are in the area of the Turing test which is again philosophy and not science
,,"<p>This seems to me like a virus situation.</p>

<p>I'm not sure how modern DDOS attacks are resolved but similar strategy could be applied to this scenario.</p>
",,0,2016-11-08T10:37:41.867,,2299,2016-11-08T10:37:41.867,,,,,3526.0,2274.0,2,1,,,,66.74,8.8,9.75,0.0,0.0,3.0,This seems to me like a virus situation Im not sure how modern DDOS attacks are resolved but similar strategy could be applied to this scenario
,,"<p>For a <a href=""https://en.wikipedia.org/wiki/Markov_decision_process"" rel=""nofollow noreferrer"">Markov Decision Process (MDP)</a> a model which are the states (<em>S</em>), actions (<em>A</em>), rewards (<em>R</em>), and transition probabilites <em>P(s'|s,a)</em>. The goal is to obtain the best action to do in each of the states, i.e. the policy &pi;.</p>

<h2>Policy</h2>

<p>To calculate the policy we make use of the <a href=""https://en.wikipedia.org/wiki/Bellman_equation"" rel=""nofollow noreferrer"">Bellman equation</a>:</p>

<p><img src=""https://i.stack.imgur.com/W2k5H.gif"" alt=""Bellman equation""></p>

<p>When starting to calculate the values we can simply start with:</p>

<p><img src=""https://i.stack.imgur.com/sYwIu.gif"" alt=""value_1""></p>

<p>To improve this value we should take into account the next action which can be taken by the system and will result in a new reward:</p>

<p><img src=""https://i.stack.imgur.com/SdlF3.gif"" alt=""value_2""></p>

<p>Here you take into account the reward of the current state s: <em>R(s)</em>, and the weighted sum of possible future rewards. We use <em>P(s'|s,a)</em> to give the probility of reaching state <em>s'</em> from <em>s</em> with action <em>a</em>. &gamma; is a value between 0 and 1 and is called the <em>discount factor</em> because it reduces the importance of future rewards since these are uncertain. An often used value is &gamma;=0.95.</p>

<p>When using <a href=""http://artint.info/html/ArtInt_227.html"" rel=""nofollow noreferrer"">value iteration</a> this process is continued until the value function has <em>converged</em>, which means that the value function does not change significantly when doing new iterations:</p>

<p><img src=""https://i.stack.imgur.com/2dng1.gif"" alt=""convergence""></p>

<p>where &varepsilon; is a really small value.</p>

<h2>Discounted sum of future rewards</h2>

<p>If you look at the Bellman equation and execute it iteratively you'll see:</p>

<p><img src=""https://i.stack.imgur.com/Fl91c.gif"" alt=""nested bellman""></p>

<p>This is like (without transition functions):</p>

<p><img src=""https://i.stack.imgur.com/jiPdr.gif"" alt=""R sum""></p>

<h2>To conclude</h2>

<p>So when we start in state <em>s</em> we want to take the action that gives us the best total reward taking into account not only the current, or next state, but all possible next states until we reach the goal. These are the time steps you refer to, i.e. each action taken is done in a time step. And when we learn the policy we try to take into account as many time steps as possible to choose the best action.</p>

<hr>

<p>You can find quite a large number of examples if you search on the internet, for example in the slides of <a href=""http://www.cs.cmu.edu/afs/andrew/course/15/381-f08/www/lectures/HandoutMDP.pdf"" rel=""nofollow noreferrer"">the CMU</a>, the <a href=""https://people.eecs.berkeley.edu/~pabbeel/cs287-fa12/slides/mdps-exact-methods.pdf"" rel=""nofollow noreferrer"">UC Berkeley</a> or the <a href=""https://homes.cs.washington.edu/~todorov/courses/amath579/MDP.pdf"" rel=""nofollow noreferrer"">UW</a>.</p>
",,0,2016-11-08T14:38:24.750,,2300,2016-11-08T17:49:11.080,2016-11-08T17:49:11.080,,198.0,,198.0,2226.0,2,0,,,,62.72,9.17,8.68,0.0,0.0,67.0,For a Markov Decision Process MDP a model which are the states S actions A rewards R and transition probabilites Pssa The goal is to obtain the best action to do in each of the states ie the policy pi Policy To calculate the policy we make use of the Bellman equation When starting to calculate the values we can simply start with To improve this value we should take into account the next action which can be taken by the system and will result in a new reward Here you take into account the reward of the current state s Rs and the weighted sum of possible future rewards We use Pssa to give the probility of reaching state s from s with action a gamma is a value between 0 and 1 and is called the discount factor because it reduces the importance of future rewards since these are uncertain An often used value is gamma095 When using value iteration this process is continued until the value function has converged which means that the value function does not change significantly when doing new iterations where varepsilon is a really small value Discounted sum of future rewards If you look at the Bellman equation and execute it iteratively youll see This is like without transition functions To conclude So when we start in state s we want to take the action that gives us the best total reward taking into account not only the current or next state but all possible next states until we reach the goal These are the time steps you refer to ie each action taken is done in a time step And when we learn the policy we try to take into account as many time steps as possible to choose the best action You can find quite a large number of examples if you search on the internet for example in the slides of the CMU the UC Berkeley or the UW
,2.0,"<p>I was looking for a service where I can ask it a general question (aka, when was Einstein born?) and retrieve an answer from the Web.</p>

<p>Is there any available service to do that? Have tried Watson services but didn't work as expected.</p>

<p>Thanks,</p>
",,2,2016-11-08T18:15:34.353,,2301,2016-12-11T05:54:52.557,,,,,3539.0,,1,1,<machine-learning><deep-learning><natural-language>,Retrieving answers for general questions,69.0,77.23,7.52,8.85,0.0,0.0,9.0,I was looking for a service where I can ask it a general question aka when was Einstein born and retrieve an answer from the Web Is there any available service to do that Have tried Watson services but didnt work as expected Thanks
,1.0,"<p>At the moment I am working on a project which requires me to build a naive Bayes classifier. Right now I have a form online asking for people to submit a sentence and the subject of the sentence, in order to build a classifier to identify the subject of a sentence. But before I train the classifier I intend on processing all entries for the parts-of-speech and the location of the subject.
So my training set will be formatted as:</p>

<p>Sentence: Jake moved the chair &ensp;&ensp;&ensp; Subject: Jake<br/>
POS-Tagged: NNP VBD DD NN &ensp;&ensp;&ensp; Location: 0</p>

<p>Would this be an effective way to build the classifier, or is there a better method.</p>
",,0,2016-11-08T19:24:42.870,1.0,2302,2016-11-09T18:54:55.483,,,,,3542.0,,1,2,<classification><nlp>,What is the most effective way to build a classifier?,51.0,59.98,10.33,9.96,0.0,0.0,26.0,At the moment I am working on a project which requires me to build a naive Bayes classifier Right now I have a form online asking for people to submit a sentence and the subject of the sentence in order to build a classifier to identify the subject of a sentence But before I train the classifier I intend on processing all entries for the partsofspeech and the location of the subject So my training set will be formatted as Sentence Jake moved the chair enspenspensp Subject Jake POSTagged NNP VBD DD NN enspenspensp Location 0 Would this be an effective way to build the classifier or is there a better method
,0.0,"<p>What rectifier is better in general case of Convolutional Neural Network and how about empirical rules to use each type?</p>

<ul>
<li>ReLU</li>
<li>PReLU</li>
<li>RReLU</li>
<li>ELU</li>
<li>Leacky ReLU</li>
</ul>
",,1,2016-11-09T11:03:07.003,,2303,2016-11-09T11:03:07.003,,,,,1442.0,,1,1,<neural-networks><conv-neural-network><architecture>,What linear rectifier is better?,40.0,41.36,10.37,12.78,0.0,0.0,1.0,What rectifier is better in general case of Convolutional Neural Network and how about empirical rules to use each type ReLU PReLU RReLU ELU Leacky ReLU
,,"<p>Yes and no. If you fully simulate a human brain and all of its functions, it would probably be able to feel emotions very similar to the way we do.</p>

<p>But we don't have enough capabilities and knowledge to do that, and maybe we could find a ""shortcut"" - a process that is intelligent without simulating a whole brain. In this case, emotions would probably represented by data values which say ""this is good (make it happen again!)"", or ""this is bad (avoid it!)"". This is just a very basic example (there are obviously many more emotions), but it would have a similar function and the AI would have similar solutions to the ones we have. But we don't know - and probably no one ever will know - if this data value 'bad' ""feels"" the same way for the AI the according emotion would feel to us. </p>
",,0,2016-11-09T11:07:10.580,,2304,2016-11-09T11:07:10.580,,,,,3548.0,2277.0,2,1,,,,67.38,8.3,8.47,0.0,0.0,34.0,Yes and no If you fully simulate a human brain and all of its functions it would probably be able to feel emotions very similar to the way we do But we dont have enough capabilities and knowledge to do that and maybe we could find a shortcut a process that is intelligent without simulating a whole brain In this case emotions would probably represented by data values which say this is good make it happen again or this is bad avoid it This is just a very basic example there are obviously many more emotions but it would have a similar function and the AI would have similar solutions to the ones we have But we dont know and probably no one ever will know if this data value bad feels the same way for the AI the according emotion would feel to us
,,"<p><a href=""https://i.stack.imgur.com/MK10R.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MK10R.jpg"" alt=""Nuke it from the orbit - it&#39;s the only way to be sure""></a>
<em>Nuke it from orbit - it's the only way to be sure</em></p>

<p>If you want to be really sure you destroy everything of the AI, you'll need to launch an EMP (ElectroMagneticPulse) from the orbit (there are different ways to achieve this, one would be an atomic bomb, but there are better ones). EMPs will destroy every electronic device it hits without causing really much damage to humans. </p>

<p>Also an interesting read on a similar topic: <a href=""https://what-if.xkcd.com/5/"" rel=""nofollow noreferrer"">https://what-if.xkcd.com/5/</a>
Especially this is gonna be interesting:</p>

<blockquote>
  <p>[...] nuclear explosions generate powerful electromagnetic pulses. These EMPs overload and destroy delicate electronic circuits. [...]
  And nuclear weapons could actually give us an edge. If we managed to
  set any of them off in the upper atmosphere, the EMP effect would be
  much more powerful.</p>
</blockquote>
",,15,2016-11-09T11:12:09.100,,2305,2016-12-05T20:41:40.597,2016-12-05T20:41:40.597,,3548.0,,3548.0,2274.0,2,1,,,,55.44,12.58,9.81,0.0,0.0,37.0,Nuke it from orbit its the only way to be sure If you want to be really sure you destroy everything of the AI youll need to launch an EMP ElectroMagneticPulse from the orbit there are different ways to achieve this one would be an atomic bomb but there are better ones EMPs will destroy every electronic device it hits without causing really much damage to humans Also an interesting read on a similar topic httpswhatifxkcdcom5 Especially this is gonna be interesting nuclear explosions generate powerful electromagnetic pulses These EMPs overload and destroy delicate electronic circuits And nuclear weapons could actually give us an edge If we managed to set any of them off in the upper atmosphere the EMP effect would be much more powerful
,4.0,"<p>What are the top artificial intelligence journals?</p>

<p>I am looking for general artificial intelligence research, not necessarily machine learning. </p>
",,0,2016-11-09T12:19:59.713,3.0,2306,2017-03-09T10:29:51.487,2016-11-09T15:32:57.147,,3550.0,,3550.0,,1,6,<research>,What are the top artificial intelligence journals?,246.0,11.07,19.11,11.59,0.0,0.0,3.0,What are the top artificial intelligence journals I am looking for general artificial intelligence research not necessarily machine learning
,,"<p>This <a href=""http://www.scimagojr.com/journalrank.php?category=1702"" rel=""nofollow noreferrer"">link</a> includes various journals for artificial intelligence applied to various domains.</p>

<p>Some of those are:<br>
1. IEEE Transactions on Human-Machine Systems<br>
2. Journal of the ACM<br>
3. Knowledge-based systems<br>
4. IEEE Transactions on Pattern Analysis and Machine Intelligence<br>
5. Journal of Memory and Language.</p>

<p>There are lots more. You can refer to any of those journals and explore the research done by AI enthusiasts and researchers.</p>
",,0,2016-11-09T16:20:55.867,,2307,2016-11-09T16:20:55.867,,,,,1807.0,2306.0,2,4,,,,55.5,11.96,9.9,0.0,0.0,12.0,This link includes various journals for artificial intelligence applied to various domains Some of those are 1 IEEE Transactions on HumanMachine Systems 2 Journal of the ACM 3 Knowledgebased systems 4 IEEE Transactions on Pattern Analysis and Machine Intelligence 5 Journal of Memory and Language There are lots more You can refer to any of those journals and explore the research done by AI enthusiasts and researchers
,,"<p>Your approach would definitely work. I would recommend training a variety of classifiers and comparing their performance using multiclass roc analysis. Also, think about other useful features in addition to the ones you mentioned (e.g. pos tag). Feature engineering is one of the most important factors in building good predictive models. Another thing to keep in mind is that the classes could be highly imbalanced which might influence your model's performance.</p>
",,0,2016-11-09T18:54:55.483,,2308,2016-11-09T18:54:55.483,,,,,3556.0,2302.0,2,1,,,,48.6,14.26,11.68,0.0,0.0,11.0,Your approach would definitely work I would recommend training a variety of classifiers and comparing their performance using multiclass roc analysis Also think about other useful features in addition to the ones you mentioned eg pos tag Feature engineering is one of the most important factors in building good predictive models Another thing to keep in mind is that the classes could be highly imbalanced which might influence your models performance
,,"<p>I most often reference: </p>

<p><a href=""http://dblp.uni-trier.de/"" rel=""nofollow noreferrer"">http://dblp.uni-trier.de/</a></p>

<p>It's not a journal but it gets me where I need to go. </p>
",,0,2016-11-10T05:03:15.957,,2310,2016-11-10T05:03:15.957,,,,,3561.0,2306.0,2,2,,,,79.26,8.67,7.59,0.0,0.0,10.0,I most often reference httpdblpunitrierde Its not a journal but it gets me where I need to go
,,"<p>A couple of others:</p>

<p>Journal of Artificial Intelligence Research (JAIR) - <a href=""http://jair.org"" rel=""nofollow noreferrer"">http://jair.org</a></p>

<p>IEEE Transactions on Knowledge and Data Engineering</p>

<p>IEEE Computational Intelligence Magazine</p>
",,0,2016-11-10T06:41:42.773,,2311,2016-11-10T06:41:42.773,,,,,33.0,2306.0,2,4,,,,9.55,23.47,13.51,0.0,0.0,8.0,A couple of others Journal of Artificial Intelligence Research JAIR httpjairorg IEEE Transactions on Knowledge and Data Engineering IEEE Computational Intelligence Magazine
,,"<p>You could use <a href=""http://wiki.dbpedia.org/"" rel=""nofollow noreferrer"">dbPedia</a> and/or <a href=""https://www.wikidata.org/wiki/Wikidata:Main_Page"" rel=""nofollow noreferrer"">wikidata</a>.  I think Wikidata supports <a href=""https://en.wikipedia.org/wiki/SPARQL"" rel=""nofollow noreferrer"">SPARQL</a> now, but don't quote me on that.  dbPedia definitely supports SPARQL.  </p>

<p>If you're not interested in writing SPARQL queries by hand, you could use something like <a href=""http://quepy.machinalis.com/"" rel=""nofollow noreferrer"">Quepy</a>. In fact, the Quepy demo demonstrates doing natural language queries against Freebase and/or dbPedia.</p>

<p>You could possibly also incorporate <a href=""http://sw.opencyc.org/"" rel=""nofollow noreferrer"">OpenCyc</a>.</p>

<p>If you want to roll something of your own, you might want to read some / all of the research papers published by the team from the <a href=""http://start.csail.mit.edu/index.php"" rel=""nofollow noreferrer"">START</a> project at MIT.</p>
",,0,2016-11-10T06:48:00.777,,2312,2016-12-11T05:54:52.557,2016-12-11T05:54:52.557,,33.0,,33.0,2301.0,2,1,,,,67.15,11.3,9.11,0.0,0.0,16.0,You could use dbPedia andor wikidata I think Wikidata supports SPARQL now but dont quote me on that dbPedia definitely supports SPARQL If youre not interested in writing SPARQL queries by hand you could use something like Quepy In fact the Quepy demo demonstrates doing natural language queries against Freebase andor dbPedia You could possibly also incorporate OpenCyc If you want to roll something of your own you might want to read some all of the research papers published by the team from the START project at MIT
,,"<p>In addition to the answers already posted, I think IBM's <a href=""http://ibm.com/watson"" rel=""nofollow noreferrer"">Watson</a> deserves a mention.  It did something pretty impressive with its Jeopardy win, possibly as impressive as AlphaGo.  Sadly, since then, there don't seem to have been a lot of really public demos of Watson, as IBM is positioning the technology as a tool for companies and other organizations, and most of them are pretty secretive about the details of what they're doing.  I think they did publicize a bit of information about using it for medical diagnosis, but that's the only other application I can think of off hand.  I'm sure there are more though.</p>
",,1,2016-11-10T06:52:26.037,,2313,2016-11-10T06:52:26.037,,,,,33.0,2285.0,2,4,,,,58.42,10.39,9.6,0.0,0.0,17.0,In addition to the answers already posted I think IBMs Watson deserves a mention It did something pretty impressive with its Jeopardy win possibly as impressive as AlphaGo Sadly since then there dont seem to have been a lot of really public demos of Watson as IBM is positioning the technology as a tool for companies and other organizations and most of them are pretty secretive about the details of what theyre doing I think they did publicize a bit of information about using it for medical diagnosis but thats the only other application I can think of off hand Im sure there are more though
,1.0,"<p>Is there any methodology to find proper parameter settings for a given meta-heuristic algorithm, eg. Firefly Algorithm or Cuckoo Search? Is this an open issue in optimization? Is extensive experimentation, measurements and intuition the only way to figure out which are the best settings? </p>
",,0,2016-11-10T10:02:07.070,,2314,2016-11-10T22:24:46.453,,,,,3566.0,,1,0,<algorithm><optimization>,How to find proper parameter settings for a given optimization algorithm?,44.0,34.93,13.84,11.36,0.0,0.0,7.0,Is there any methodology to find proper parameter settings for a given metaheuristic algorithm eg Firefly Algorithm or Cuckoo Search Is this an open issue in optimization Is extensive experimentation measurements and intuition the only way to figure out which are the best settings
,,"<p>Another way of seeing the differences between these models in the case of binary classification for instance between a class A and a class B:</p>

<p>A generative model will be trained to model the properties of class A and another one will be trained to model the properties of class B. If we want to know if a new sample belongs to class A or B, we will compare it to each model and decide. The advantage is that we are able to synthetically generate more samples of these classes using the generative property of the model. The models have a ""global knowledge"" of what the classes are.</p>

<p>On the other hand, a discriminative model will ""pay attention"" to what differentiates the 2 classes. It is more straightforward and often computationally less expensive as the model does not need to grasp everything about each class but only what makes them different.</p>

<p>This is for the big picture. I find this course slides quite helpful to understand these concepts in more details (especially the first slides that are equation-free): <a href=""http://www.cedar.buffalo.edu/~srihari/CSE574/Discriminative-Generative.pdf"" rel=""nofollow noreferrer"">http://www.cedar.buffalo.edu/~srihari/CSE574/Discriminative-Generative.pdf</a></p>
",,0,2016-11-10T19:08:45.553,,2315,2016-11-10T19:08:45.553,,,,,3576.0,2106.0,2,2,,,,48.74,12.65,9.16,0.0,0.0,30.0,Another way of seeing the differences between these models in the case of binary classification for instance between a class A and a class B A generative model will be trained to model the properties of class A and another one will be trained to model the properties of class B If we want to know if a new sample belongs to class A or B we will compare it to each model and decide The advantage is that we are able to synthetically generate more samples of these classes using the generative property of the model The models have a global knowledge of what the classes are On the other hand a discriminative model will pay attention to what differentiates the 2 classes It is more straightforward and often computationally less expensive as the model does not need to grasp everything about each class but only what makes them different This is for the big picture I find this course slides quite helpful to understand these concepts in more details especially the first slides that are equationfree httpwwwcedarbuffaloedusrihariCSE574DiscriminativeGenerativepdf
,,"<p>How to find the best configuration for an algorithm is an open research question in AI. The topic in general is known as `hyper-parameter optimization' and there are a range of possible methods:</p>

<p>One of the most popular is <a href=""http://iridia.ulb.ac.be/irace/"" rel=""nofollow noreferrer"">IRace</a>, but other possibilities include:</p>

<ul>
<li><p><a href=""https://github.com/JasperSnoek/spearmint"" rel=""nofollow noreferrer"">Spearmint</a>: uses wrappers in Matlab or Python. It uses MongoDb,
and Bayesian optimisation algorithms.</p></li>
<li><p><a href=""http://www.cs.ubc.ca/labs/beta/Projects/SMAC/"" rel=""nofollow noreferrer"">SMAC</a> requires a python wrapper for the algorithm to be optimized
and has a command line interface.</p></li>
<li><p><a href=""https://github.com/hyperopt/hyperopt"" rel=""nofollow noreferrer"">Hyperopt</a>: a Python library which uses Random Search and Tree of
Parzen Estimators.</p></li>
</ul>

<p><a href=""http://www.automl.org/papers/13-BayesOpt%5C_EmpiricalFoundation.pdf"" rel=""nofollow noreferrer"">This paper</a> argues that Spearmint performs the best, compared with SMAC and Hyperopt, but with significantly longer running times in some cases.</p>
",,1,2016-11-10T22:24:46.453,,2316,2016-11-10T22:24:46.453,,,,,42.0,2314.0,2,1,,,,44.44,13.23,11.43,0.0,0.0,17.0,How to find the best configuration for an algorithm is an open research question in AI The topic in general is known as hyperparameter optimization and there are a range of possible methods One of the most popular is IRace but other possibilities include Spearmint uses wrappers in Matlab or Python It uses MongoDb and Bayesian optimisation algorithms SMAC requires a python wrapper for the algorithm to be optimized and has a command line interface Hyperopt a Python library which uses Random Search and Tree of Parzen Estimators This paper argues that Spearmint performs the best compared with SMAC and Hyperopt but with significantly longer running times in some cases
,,"<p>You can use Google</p>

<p><a href=""https://encrypted.google.com/search?hl=en&amp;q=when%20was%20Einstein%20born"" rel=""nofollow noreferrer"">https://encrypted.google.com/search?hl=en&amp;q=when%20was%20Einstein%20born</a>
and parse the response.</p>

<p>Wolfram ALPHA is another candidate.</p>

<p><a href=""http://m.wolframalpha.com/input/?i=what+year+was+Einstein+born&amp;x=0&amp;y=0"" rel=""nofollow noreferrer"">http://m.wolframalpha.com/input/?i=what+year+was+Einstein+born&amp;x=0&amp;y=0</a></p>

<p>You can parse the returned html and see ""Result:"" div.</p>
",,0,2016-11-11T04:03:31.227,,2317,2016-11-11T04:03:31.227,,,,,3526.0,2301.0,2,1,,,,-10.96,40.94,10.26,0.0,0.0,39.0,You can use Google httpsencryptedgooglecomsearchhlenampqwhen20was20Einstein20born and parse the response Wolfram ALPHA is another candidate httpmwolframalphacominputiwhatyearwasEinsteinbornampx0ampy0 You can parse the returned html and see Result div
,4.0,"<p>In programming languages, there is a set of grammar rules which govern the construction of valid statements and expressions. These rules help in parsing the programs written by the user.</p>

<p>Can there ever be a functionally complete set of grammar rules which can parse any statement in English (locale-specific) <strong>accurately</strong> and which can be possibly implemented for use in AI-based projects?</p>

<p>I know that there are a lot of NLP Toolkits available online, but they are not that effective. Most of them are trained using specific corpuses which sometimes fail to infer some complex correlations between various parts of an expression.</p>

<p>In other words, what I am asking is that if it is possible for a computer to parse a well-versed sentence written in English as if it were parsed by an adult English-speaking human?</p>

<p>EDIT:
If it cannot be represented using simple grammar rules, what kind of semantic structure can be used to generalize it?</p>

<p>EDIT2: This <a href=""https://www.eecs.harvard.edu/shieber/Biblio/Papers/shieber85.pdf"" rel=""nofollow noreferrer"">paper</a> proves the absence of context-freeness in natural languages. I am looking for a solution, even if it is too complex.</p>
",,1,2016-11-11T23:04:19.753,1.0,2319,2017-01-16T14:20:16.013,2016-11-16T19:24:23.503,,3592.0,,3592.0,,1,8,<ai-design><natural-language><nlp><language-processing>,Can the English language ever be generalized using a set of grammar rules?,253.0,51.28,11.55,10.8,0.0,0.0,23.0,In programming languages there is a set of grammar rules which govern the construction of valid statements and expressions These rules help in parsing the programs written by the user Can there ever be a functionally complete set of grammar rules which can parse any statement in English localespecific accurately and which can be possibly implemented for use in AIbased projects I know that there are a lot of NLP Toolkits available online but they are not that effective Most of them are trained using specific corpuses which sometimes fail to infer some complex correlations between various parts of an expression In other words what I am asking is that if it is possible for a computer to parse a wellversed sentence written in English as if it were parsed by an adult Englishspeaking human EDIT If it cannot be represented using simple grammar rules what kind of semantic structure can be used to generalize it EDIT2 This paper proves the absence of contextfreeness in natural languages I am looking for a solution even if it is too complex
,,"<p>If an AI is developed by humans, we surely can create another one!</p>

<p>Develop another AI agent without all the possible bugs that can make it go rogue to tackle the rogue AI, but more technically advanced than the previous one. Hardwire it with the sole purpose of disabling any rogue AI agent that can harm humanity and have it <strong>self-destruct</strong> in case it is corrupted.</p>

<p>If the AI is really strong, it can anticipate every move of human resistance, but it cannot fathom the mind of another AI agent.</p>
",,2,2016-11-11T23:17:52.990,,2320,2016-11-11T23:17:52.990,,,,,3592.0,2274.0,2,2,,,,48.64,9.06,9.66,0.0,0.0,9.0,If an AI is developed by humans we surely can create another one Develop another AI agent without all the possible bugs that can make it go rogue to tackle the rogue AI but more technically advanced than the previous one Hardwire it with the sole purpose of disabling any rogue AI agent that can harm humanity and have it selfdestruct in case it is corrupted If the AI is really strong it can anticipate every move of human resistance but it cannot fathom the mind of another AI agent
,,"<blockquote>
  <p>Can there ever be a functionally complete set of grammar rules which can parse any statement in English (locale-specific) accurately and which can be possibly implemented for use in AI-based projects?</p>
</blockquote>

<p>Parse it yes, accurately most likely no.</p>

<p>Why ? </p>

<p>According to my understanding on how we derive meaning from sounds, there are 2 complementary strategies:</p>

<p><strong>Grammar Rules:</strong>
A rule based system for ordering words to facilitate communication, here meaning is derived from interaction of discrete sounds and their independent meaning, so you could parse a sentence based on a rule book.</p>

<p>E.G. <strong><em>""This was a triumph""</em></strong> : the parser would extract a pronoun (<strong>This</strong>) with corresponding meaning ( a specific person or thing ) ; a verb (<strong>was</strong>) with corresponding meaning ( occurred ); ( <strong>a</strong>) and here we start with some parsing problems , what would the parser extract, a noun or an indefinite article ? An so we consult the grammar rule book, and settle for the meaning ( indefinite article any one of ), you have to parse the next word  and refer to it though, but let's gloss over that for now, and finally (<strong>triumph</strong>) a noun ( it could also be a verb, but thanks to the grammar rule book we settled for a noun with meaning: ( victory,conquest), so in the end we have ( joining the meanings ):</p>

<p><strong>A specific thing occurred of victory.</strong> Close enough and I am glossing over a few other rules, but that's not the point, the other strategy is:</p>

<p><strong>A lexical dictionary (or lexicon)</strong>
Where words or sounds are associated with specific meaning. Here meaning is derived from one or more words or sounds as a unit. This introduces the problem to a parser, since well, it shouldn't parse anything.</p>

<p>E.G. <strong><em>""Non Plus Ultra""</em></strong> And so the AI parser would recognize that this phrase is not to be parsed and instead matched with meaning :</p>

<p>The highest point or culmination</p>

<p>Lexical units introduce another issue in that they themselves could be part of the first example, and so you end up with recursion.</p>

<blockquote>
  <p>if it is possible for a computer to parse a well-versed sentence written in English as if it were parsed by an adult English-speaking human?</p>
</blockquote>

<p>I believe it could be possible, most examples I've seen deal effectively with the grammar rule book or the lexicon part, but I am not aware of a combination of both, but in terms of programming, it could happen.</p>

<p>Unfortunately even if you solve this problem, your AI would not really understand things in the strict sense, but rather present you with very elaborate synonyms, additionally context (as mentioned in the comments) plays a role into the grammar and lexicon strategies. </p>

<blockquote>
  <p>If it cannot be represented using simple grammar rules, what kind of semantic structure can be used to generalize it?</p>
</blockquote>

<p>A mixed one where there are both grammar rules and a lexicon and both can change and be influenced based on the AI specific context and experience as well as a system for dealing with these objects could be one way.</p>
",,3,2016-11-12T22:16:06.647,,2322,2016-11-12T22:16:06.647,,,,,3020.0,2319.0,2,7,,,,44.0,11.33,9.66,0.0,0.0,91.0,Can there ever be a functionally complete set of grammar rules which can parse any statement in English localespecific accurately and which can be possibly implemented for use in AIbased projects Parse it yes accurately most likely no Why According to my understanding on how we derive meaning from sounds there are 2 complementary strategies Grammar Rules A rule based system for ordering words to facilitate communication here meaning is derived from interaction of discrete sounds and their independent meaning so you could parse a sentence based on a rule book EG This was a triumph the parser would extract a pronoun This with corresponding meaning a specific person or thing a verb was with corresponding meaning occurred a and here we start with some parsing problems what would the parser extract a noun or an indefinite article An so we consult the grammar rule book and settle for the meaning indefinite article any one of you have to parse the next word and refer to it though but lets gloss over that for now and finally triumph a noun it could also be a verb but thanks to the grammar rule book we settled for a noun with meaning victoryconquest so in the end we have joining the meanings A specific thing occurred of victory Close enough and I am glossing over a few other rules but thats not the point the other strategy is A lexical dictionary or lexicon Where words or sounds are associated with specific meaning Here meaning is derived from one or more words or sounds as a unit This introduces the problem to a parser since well it shouldnt parse anything EG Non Plus Ultra And so the AI parser would recognize that this phrase is not to be parsed and instead matched with meaning The highest point or culmination Lexical units introduce another issue in that they themselves could be part of the first example and so you end up with recursion if it is possible for a computer to parse a wellversed sentence written in English as if it were parsed by an adult Englishspeaking human I believe it could be possible most examples Ive seen deal effectively with the grammar rule book or the lexicon part but I am not aware of a combination of both but in terms of programming it could happen Unfortunately even if you solve this problem your AI would not really understand things in the strict sense but rather present you with very elaborate synonyms additionally context as mentioned in the comments plays a role into the grammar and lexicon strategies If it cannot be represented using simple grammar rules what kind of semantic structure can be used to generalize it A mixed one where there are both grammar rules and a lexicon and both can change and be influenced based on the AI specific context and experience as well as a system for dealing with these objects could be one way
,,"<p>I'm pretty sure that the answer is ""no"" in the strictest sense, since English simply doesn't have a formal definition.  That is, nobody controls English and publishes a formal grammar that everyone is required to adhere to. English is built up through an experiential process and it has contradictions and flaws, but the probabilistic nature of the human mind allows us to work around those.</p>

<p>For example, that this ""sentence"":</p>

<p><strong>This sentence no verb</strong></p>

<p>Technically it's not a sentence at all, since it doesn't have a verb.  But did anybody have any problem understanding what it meant? Doubtful.  Try coming up with a formal rule for that though.  And that's just one example.</p>

<p>Now, could you come up with a formal grammar that covers, maybe, 90% of cases, and is ""good enough"" for most practical uses? Possibly, maybe even probably.  But I am pretty sure it's not possible to get to 100%.</p>
",,2,2016-11-12T23:51:41.563,,2323,2016-11-12T23:51:41.563,,,,,33.0,2319.0,2,2,,,,64.51,9.91,8.86,0.0,0.0,36.0,Im pretty sure that the answer is no in the strictest sense since English simply doesnt have a formal definition That is nobody controls English and publishes a formal grammar that everyone is required to adhere to English is built up through an experiential process and it has contradictions and flaws but the probabilistic nature of the human mind allows us to work around those For example that this sentence This sentence no verb Technically its not a sentence at all since it doesnt have a verb But did anybody have any problem understanding what it meant Doubtful Try coming up with a formal rule for that though And thats just one example Now could you come up with a formal grammar that covers maybe 90 of cases and is good enough for most practical uses Possibly maybe even probably But I am pretty sure its not possible to get to 100
2337.0,1.0,"<p>I was just doing some thinking and it occurred to me that the first AGIs ought to be able to perform the same sort and variety of tasks as people, with the most computationally strenuous tasks taking amount of time comparable to how long a person would take. If this is the case, and people have yet to develop basic AGI (meaning it's a difficult task), should we be concerned if AGI is developed? It would seem to me that any fears about a newly developed AGI in this case should be the same as fears about a newborn child.</p>
",,0,2016-11-13T07:15:29.303,,2324,2016-11-14T10:48:13.510,,,,,3604.0,,1,1,<philosophy><intelligence-testing><agi><human-like><ultraintelligent-machine>,Must AGI lead to ASI?,99.0,54.6,7.96,9.39,0.0,0.0,9.0,I was just doing some thinking and it occurred to me that the first AGIs ought to be able to perform the same sort and variety of tasks as people with the most computationally strenuous tasks taking amount of time comparable to how long a person would take If this is the case and people have yet to develop basic AGI meaning its a difficult task should we be concerned if AGI is developed It would seem to me that any fears about a newly developed AGI in this case should be the same as fears about a newborn child
2329.0,1.0,"<p><a href=""https://github.com/bwilcox-1234/ChatScript"" rel=""nofollow noreferrer"">https://github.com/bwilcox-1234/ChatScript</a></p>

<p>I gave AIML a brief look, but it seems to be in a nascent stage!</p>
",2016-11-14T14:16:07.943,2,2016-11-13T11:39:56.210,,2325,2016-11-14T03:43:22.710,,,,,3589.0,,1,0,<ai-design><natural-language><language-processing>,"Which are the good tools, similar to ChatScript, for building chat/sms based bots?",52.0,72.16,15.77,6.4,0.0,0.0,9.0,httpsgithubcombwilcox1234ChatScript I gave AIML a brief look but it seems to be in a nascent stage
,1.0,"<p>Writing A* following a documentation. When run, i receive an error of ""NameError: name 'parent' is not defined"" for the if statement, even though i have the name 'parent' defined in the class State. May anyone point out my mistake.</p>

<pre><code>class State(object):
def _init_(self, value, parent, 
                start = 0, goal = 0):
    self.children = []
    self.parent = parent
    self.value = value
    self.dist = 0

if parent: #NameError
        self.path = parent.path[:]
        self.path.append(value)
        self.start = parent.start
        self.goal = parent.goal
else:
        self.path = [value]
        self.start = start
        self.goal = goal
</code></pre>
",2016-11-14T00:49:49.297,2,2016-11-13T14:29:19.220,,2326,2016-11-13T21:07:06.300,2016-11-13T21:07:06.300,,8.0,,3296.0,,1,0,<algorithm>,A* Algorithm undefined name error,32.0,57.98,9.97,9.43,423.0,0.0,13.0,Writing A following a documentation When run i receive an error of NameError name parent is not defined for the if statement even though i have the name parent defined in the class State May anyone point out my mistake
,,"<p>The variable <code>parent</code> is only defined within the scope of the function <code>_init_</code>.</p>

<p>Example:</p>

<pre><code>def add(x,y):
    return x + y
print x
</code></pre>

<p>x is not defined outside of the scope of the function add(x,y) and will throw an error. If you'd like to do something with the class attributes you need to create a function like:</p>

<pre><code>def function(self,...):
</code></pre>

<p>where you can there reference <code>self.parent</code></p>
",,0,2016-11-13T19:02:00.123,,2327,2016-11-13T19:02:00.123,,,,,3614.0,2326.0,2,2,,,,71.14,8.7,8.19,90.0,0.0,8.0,The variable is only defined within the scope of the function Example x is not defined outside of the scope of the function addxy and will throw an error If youd like to do something with the class attributes you need to create a function like where you can there reference
2509.0,2.0,"<p>How does one program a machine to have humanlike desires and intelligence?</p>

<p>Humanlike drives may include  self-awareness, purpose of existence, competent communication skills, and the ability to learn and to adapt in some environment ...</p>

<p>And we should be able to combine IAs (intelligent agents) to accomplish  well-defined goals (SMART).  With more challenging goals there ought to be more advanced control and sophistication of IAs.   That evolving process will eventually, hopefully, lead to the design of machines with humanlike capabilities. </p>

<p>Reference links:  '<strong><em>Diagram of Intelligence Network or System</em></strong>', <a href=""https://www.researchgate.net/publication/300125399_Diagram_of_Intelligence_Network_or_System"" rel=""nofollow noreferrer"">https://www.researchgate.net/publication/300125399_Diagram_of_Intelligence_Network_or_System</a>;</p>

<p>'<strong><em>Google a step closer to developing machines with human-like intelligence</em></strong>',
<a href=""https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence"" rel=""nofollow noreferrer"">https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence</a></p>
",,9,2016-11-14T00:37:21.170,1.0,2328,2016-12-19T22:15:34.867,2016-11-16T16:02:15.847,,3602.0,,3602.0,,1,0,<machine-learning><cognitive-science>,On essential humanlike drives and intelligence,192.0,-2.06,28.64,11.51,0.0,0.0,60.0,How does one program a machine to have humanlike desires and intelligence Humanlike drives may include selfawareness purpose of existence competent communication skills and the ability to learn and to adapt in some environment And we should be able to combine IAs intelligent agents to accomplish welldefined goals SMART With more challenging goals there ought to be more advanced control and sophistication of IAs That evolving process will eventually hopefully lead to the design of machines with humanlike capabilities Reference links Diagram of Intelligence Network or System httpswwwresearchgatenetpublication300125399DiagramofIntelligenceNetworkorSystem Google a step closer to developing machines with humanlike intelligence httpswwwtheguardiancomscience2015may21googleastepclosertodevelopingmachineswithhumanlikeintelligence
,,"<p>Maybe you can give <a href=""https://wit.ai/"" rel=""nofollow noreferrer"">wit.ai</a> a try, it's not open-sourced though.Also, have a look at api.ai and chatbots.io.</p>
",,0,2016-11-14T03:43:22.710,,2329,2016-11-14T03:43:22.710,,,,,3623.0,2325.0,2,0,,,,75.37,6.34,8.25,0.0,0.0,9.0,Maybe you can give witai a try its not opensourced thoughAlso have a look at apiai and chatbotsio
,4.0,"<p>Based on fitting to historical data and extrapolation, when is it expected that the number of neurons in AI systems will equal those of the human brain?</p>

<p>I'm interested in a possible direct replication of the human brain, which will need equal numbers of neurons.</p>

<p>Of course, this assumes neurons which are equally capable as their biological counterparts, which development may happen at a faster or slower rate than the quantitative increase.</p>
",,2,2016-11-14T04:19:31.873,1.0,2330,2016-11-27T16:38:38.143,2016-11-14T15:21:31.023,,2424.0,,2424.0,,1,3,<neurons><prediction>,When will the number of neurons in AI systems equal the human brain?,168.0,47.12,12.89,10.97,0.0,0.0,8.0,Based on fitting to historical data and extrapolation when is it expected that the number of neurons in AI systems will equal those of the human brain Im interested in a possible direct replication of the human brain which will need equal numbers of neurons Of course this assumes neurons which are equally capable as their biological counterparts which development may happen at a faster or slower rate than the quantitative increase
,,"<p>Soon enough but that doesn't mean anything at all. In machine learning the word neuron represents a calculation whereas in brain the word neuron represent a specific type of cell which is a biochemical system.</p>
",,7,2016-11-14T05:57:36.463,,2331,2016-11-14T05:57:36.463,,,,,1462.0,2330.0,2,6,,,,62.17,11.42,9.92,0.0,0.0,3.0,Soon enough but that doesnt mean anything at all In machine learning the word neuron represents a calculation whereas in brain the word neuron represent a specific type of cell which is a biochemical system
,,"<p>The answers so far haven't answered the question numerically, so here is my attempt to steer them in the direction I was seeking:</p>

<p>The freely available <a href=""http://www.deeplearningbook.org"" rel=""nofollow noreferrer"">Deep Learning Book</a> has the following figure on page 27:</p>

<p><a href=""https://i.stack.imgur.com/iz2C4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iz2C4.png"" alt=""Size of neural nets over time""></a></p>

<p>I question the blue fit line, as it seems that data points may be better described by a parabolic or exponential function. </p>

<p>In any case, based upon this conservative linear fit, the authors predict that the number of neurons in a ANN will equal that of the human brain in 2056.</p>

<p>The referenced nerual networks are:</p>

<p><a href=""https://i.stack.imgur.com/f8Y6O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f8Y6O.png"" alt=""Neural networks referred to in image above""></a></p>

<p>What is interesting to note that when <a href=""https://en.wikipedia.org/wiki/The_Singularity_Is_Near"" rel=""nofollow noreferrer"">The Singularity is Near</a> was written in 2006, Ray Kurzweil said that the refractory period of a biological neuron was already 1,000,000 times slower than that of an artificial one.</p>
",,2,2016-11-14T07:29:24.257,,2333,2016-11-14T07:35:01.833,2016-11-14T07:35:01.833,,2424.0,,2424.0,2330.0,2,1,,,,36.29,10.98,11.28,0.0,0.0,14.0,The answers so far havent answered the question numerically so here is my attempt to steer them in the direction I was seeking The freely available Deep Learning Book has the following figure on page 27 I question the blue fit line as it seems that data points may be better described by a parabolic or exponential function In any case based upon this conservative linear fit the authors predict that the number of neurons in a ANN will equal that of the human brain in 2056 The referenced nerual networks are What is interesting to note that when The Singularity is Near was written in 2006 Ray Kurzweil said that the refractory period of a biological neuron was already 1000000 times slower than that of an artificial one
,,"<p>Some back of the envelope calculations :</p>

<blockquote>
  <p>number of neurons in AI systems </p>
</blockquote>

<p>The number of neurons in AI systems is a little tricky to calculate, Neural Networks and Deep Learning are 2 current AI systems as you call them, specifics are hard to come by (If someone has them please share), but data on parameters do exist, parameters are more analogous to synapses (connections) than neurons (the nodes in between connections) somewhere in the range of 100-160 billion is the current upper number for specialized networks.</p>

<p>Deriving the number of neurons in AI systems from this number is a stretch since these AIs emulate certain types of connections and sub assemblies of neurons, but let's continue...</p>

<blockquote>
  <p>equal those of the human brain?</p>
</blockquote>

<p>So now let's look at the brain, and again this are all contested numbers. Number of neurons ~ 86 Billion, Number of Synapses ~ 150 Trillion, another generalization: average number of synapses per neuron ~ 1,744.</p>

<p>So now we have something to compare, and I can't stress this enough, these are all wonky numbers, so let's make our life a little easier and divide :</p>

<p>Number of Synapses (Brain ) : 150 trillion /  Number of parameters AIs : 150 billion = 1,000 or in other words current AIs would have to scale by a factor of one thousand their connections to be on par with the brain...</p>

<p>Number of Neurons (Brain ) : 86 Billion / Number of Neurons AIs ( 150 billion / 1,744 )  = 86 Million equivalent AI Neurons</p>

<p>Which makes sense, mathematically at least : you can multiply the factor ( 1000 ) times the current number of equivalent AI Neurons ( 86 million) to get the number of neurons in the human brain (86 Billion)</p>

<blockquote>
  <p>When ?</p>
</blockquote>

<p>Well,let's use  moore's law ( number of transistors processing power doubles about every 2 years ) as a rough measure of technological progress: </p>

<pre><code>     #AI NEURONS            YEAR
     86,000,000             2016
     172,000,000            2018
     344,000,000            2020
     688,000,000            2022
     1,376,000,000          2024
     2,752,000,000          2026
     5,504,000,000          2028
     11,008,000,000         2030
     22,016,000,000         2032
     44,032,000,000         2034
     88,064,000,000         2036


     # NEURONS HUMAN BRAIN 
     86,000,000,000
</code></pre>

<p>So, if all this made sense to you, somewhere around the year 2035. </p>
",,1,2016-11-14T08:02:28.057,,2334,2016-11-22T19:43:08.360,2016-11-22T19:43:08.360,,3020.0,,3020.0,2330.0,2,7,,,,40.86,12.08,9.8,446.0,0.0,73.0,Some back of the envelope calculations number of neurons in AI systems The number of neurons in AI systems is a little tricky to calculate Neural Networks and Deep Learning are 2 current AI systems as you call them specifics are hard to come by If someone has them please share but data on parameters do exist parameters are more analogous to synapses connections than neurons the nodes in between connections somewhere in the range of 100160 billion is the current upper number for specialized networks Deriving the number of neurons in AI systems from this number is a stretch since these AIs emulate certain types of connections and sub assemblies of neurons but lets continue equal those of the human brain So now lets look at the brain and again this are all contested numbers Number of neurons 86 Billion Number of Synapses 150 Trillion another generalization average number of synapses per neuron 1744 So now we have something to compare and I cant stress this enough these are all wonky numbers so lets make our life a little easier and divide Number of Synapses Brain 150 trillion Number of parameters AIs 150 billion 1000 or in other words current AIs would have to scale by a factor of one thousand their connections to be on par with the brain Number of Neurons Brain 86 Billion Number of Neurons AIs 150 billion 1744 86 Million equivalent AI Neurons Which makes sense mathematically at least you can multiply the factor 1000 times the current number of equivalent AI Neurons 86 million to get the number of neurons in the human brain 86 Billion When Welllets use moores law number of transistors processing power doubles about every 2 years as a rough measure of technological progress So if all this made sense to you somewhere around the year 2035
,0.0,"<p>I'm trying to find the optimized mixture for a specific set of substances. Each of those substances have characteristics that I want to optimize in the mixture (some characteristics I want to minimize and others I want to maximize). But I can't have more than 50% (random value that will be set on running time) of one of those substances in the mixture.</p>

<p>I thought about using Genetic Algorithm, but I'm not sure it's the best approach for this problem. Do you have any suggestions?</p>

<p>Edit: it doesn't need to be a evolutionary algorithm.</p>
",,2,2016-11-14T08:55:39.600,,2335,2016-11-14T15:40:25.753,2016-11-14T15:40:25.753,,3627.0,,3627.0,,1,1,<optimization><evolutionary-algorithms><heuristics>,Knapsack of mixture with constraints,24.0,64.0,9.74,8.45,0.0,0.0,18.0,Im trying to find the optimized mixture for a specific set of substances Each of those substances have characteristics that I want to optimize in the mixture some characteristics I want to minimize and others I want to maximize But I cant have more than 50 random value that will be set on running time of one of those substances in the mixture I thought about using Genetic Algorithm but Im not sure its the best approach for this problem Do you have any suggestions Edit it doesnt need to be a evolutionary algorithm
,,"<p>There are basically two worries:</p>

<p>If we create an AGI that is a slightly better AGI-programmer than its creators, it might be able to improve its own source code to become even more intelligent. Which would enable it to improve its source code even more etc. Such a selfimproving seed AI might very quickly become superintelligent. </p>

<p>The other scenario is that intelligence is such a complicated algorithmic task, that when we finally crack it, there will be a significant hardware overhang. So the ""intelligence algorithm"" would be human level on 2030 hardware, but we figure it out in 2050. In that case we would immediately have superintelligent AI without ever creating human level AI. This scenario is especially likely because development often requires a lot of test runs to tweak parameters and try out different ideas. </p>
",,0,2016-11-14T10:48:13.510,,2337,2016-11-14T10:48:13.510,,,,,2227.0,2324.0,2,1,,,,43.32,12.13,9.94,0.0,0.0,15.0,There are basically two worries If we create an AGI that is a slightly better AGIprogrammer than its creators it might be able to improve its own source code to become even more intelligent Which would enable it to improve its source code even more etc Such a selfimproving seed AI might very quickly become superintelligent The other scenario is that intelligence is such a complicated algorithmic task that when we finally crack it there will be a significant hardware overhang So the intelligence algorithm would be human level on 2030 hardware but we figure it out in 2050 In that case we would immediately have superintelligent AI without ever creating human level AI This scenario is especially likely because development often requires a lot of test runs to tweak parameters and try out different ideas
,2.0,"<p>What are the current best estimates as to what year artificial intelligence will be able to score 100 points on the <a href=""https://en.wikipedia.org/wiki/Stanford%E2%80%93Binet_Intelligence_Scales"" rel=""nofollow noreferrer"">Stanford Binet IQ test</a>?</p>
",,2,2016-11-14T15:44:57.997,,2338,2016-11-17T12:07:52.950,,,,,2424.0,,1,1,<intelligence-testing><prediction>,When will artificial intelligence equal human intelligence?,100.0,63.02,9.7,9.3,0.0,0.0,1.0,What are the current best estimates as to what year artificial intelligence will be able to score 100 points on the Stanford Binet IQ test
,,"<p>Nobody knows.</p>

<p>However according to <a href=""https://en.wikipedia.org/wiki/Predictions_made_by_Ray_Kurzweil#2020s"" rel=""nofollow noreferrer"">Kurzweil it's late 20s</a>:</p>

<blockquote>
  <p>2020s:</p>
  
  <p>Early in this decade, humanity will have the requisite hardware to emulate human intelligence within a $1000 personal computer, followed shortly by effective software models of human intelligence toward the middle of the decade: this will be enabled through the continuing exponential growth of brain-scanning technology, which is doubling in bandwidth, temporal and spatial resolution every year, and will be greatly amplified with nanotechnology, allowing us to have a detailed understanding of all the regions of the human brain and to aid in developing human-level machine intelligence by the end of this decade.</p>
</blockquote>
",,0,2016-11-14T16:01:27.130,,2339,2016-11-16T11:40:41.643,2016-11-16T11:40:41.643,,8.0,,2227.0,2338.0,2,2,,,,-49.99,16.56,15.64,0.0,0.0,15.0,Nobody knows However according to Kurzweil its late 20s 2020s Early in this decade humanity will have the requisite hardware to emulate human intelligence within a 1000 personal computer followed shortly by effective software models of human intelligence toward the middle of the decade this will be enabled through the continuing exponential growth of brainscanning technology which is doubling in bandwidth temporal and spatial resolution every year and will be greatly amplified with nanotechnology allowing us to have a detailed understanding of all the regions of the human brain and to aid in developing humanlevel machine intelligence by the end of this decade
,,"<p>Well, the low-hanging-fruit answer is that you simulate a human being - brain, hormones, everything. We should <a href=""http://www.kurzweilai.net/"" rel=""nofollow noreferrer"">have the computing power for that to be feasible by 2040 or so</a>.</p>

<p>Building up self-awareness from first principles on a different foundational technology platform could be a bit more difficult!</p>
",,7,2016-11-16T09:56:29.507,,2341,2016-11-16T12:57:30.307,2016-11-16T12:57:30.307,,3601.0,,3601.0,2328.0,2,2,,,,55.54,13.92,10.13,0.0,0.0,10.0,Well the lowhangingfruit answer is that you simulate a human being brain hormones everything We should have the computing power for that to be feasible by 2040 or so Building up selfawareness from first principles on a different foundational technology platform could be a bit more difficult
2687.0,2.0,"<p>when I read through the fundamentals of AI, I saw a question which like the following picture and I need some helps</p>

<p><a href=""https://i.stack.imgur.com/zX6wZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zX6wZ.png"" alt=""enter image description here""></a></p>

<p>From the heuristic estimates:</p>

<pre><code>h(B-&gt;G2) = 9, h(D-&gt;G2)=10, h(A-&gt;G1)=2, h(C-&gt;G1)=1
</code></pre>

<p>With using A* search method, node B will be expanded first because <code>f(n)=1+9</code> while node A having <code>f(n)=9+2</code>, right?</p>

<p>After that the search tree will go with the order like <code>R-&gt; B-&gt; D-&gt; G2</code>.</p>

<p>Will the tree go to G1 goal states?</p>

<p>Kindly let me know the order of the search if I am wrong.
Thanks!</p>
",,0,2016-11-16T12:35:47.487,,2342,2017-01-18T14:52:53.387,,,,,3673.0,,1,3,<search>,How to solve A* search with 2 goal states?,69.0,86.03,7.37,7.9,104.0,0.0,10.0,when I read through the fundamentals of AI I saw a question which like the following picture and I need some helps From the heuristic estimates With using A search method node B will be expanded first because while node A having right After that the search tree will go with the order like Will the tree go to G1 goal states Kindly let me know the order of the search if I am wrong Thanks
,,"<p>Yes. If you leave A* running (i.e. do not impose a goal condition on a newly-encountered state), all states will be explored, just as they would be in breadth- or depth- first search.</p>
",,0,2016-11-16T21:16:23.997,,2344,2016-11-16T21:16:23.997,,,,,42.0,2342.0,2,4,,,,80.11,8.99,7.33,0.0,0.0,12.0,Yes If you leave A running ie do not impose a goal condition on a newlyencountered state all states will be explored just as they would be in breadth or depth first search
,,"<p>I think that in most cases the car would default to reducing speed as a main option, rather than steering toward or away from a specific choice. As others have mentioned, having settings related to ethics is just a bad idea. What happens if two cars that are programmed with opposite ethical settings and are about to collide? The cars could potentially have a system to override the user settings and pick the most mutually beneficial solution. It's indeed an interesting concept, and one that definitely has to discussed and standardized before widespread implementation. Putting ethical decisions in a machines hands makes the resulting liability sometimes hard to picture.</p>
",,2,2016-11-17T04:05:06.240,,2345,2016-11-18T05:22:24.577,2016-11-18T05:22:24.577,,3684.0,,3684.0,111.0,2,0,,,,44.54,12.64,11.2,0.0,0.0,10.0,I think that in most cases the car would default to reducing speed as a main option rather than steering toward or away from a specific choice As others have mentioned having settings related to ethics is just a bad idea What happens if two cars that are programmed with opposite ethical settings and are about to collide The cars could potentially have a system to override the user settings and pick the most mutually beneficial solution Its indeed an interesting concept and one that definitely has to discussed and standardized before widespread implementation Putting ethical decisions in a machines hands makes the resulting liability sometimes hard to picture
,,"<p>Not going into details of Stanford–Binet test, but just looking at <a href=""https://en.wikipedia.org/wiki/Stanford%E2%80%93Binet_Intelligence_Scales"" rel=""nofollow noreferrer"">wikipedia page</a> it shows many subtests like knowledge, reasoning, verbal tests etc. Most of the efforts in the artificial intelligence today is directed into research of specific areas like computer vision, natural language processing, machine learning, but also combination of fields like implementation of self driving cars.</p>

<p>Within every field there are still other subfields and problems that are not solved yet. For example, development of human-like natural language processing (NLP) is necessary for intelligent agent to pass any verbal tests, or even non-verbal tests that requires processing of sentences of human language. Famous test that tests intelligence by asking questions in natural language and expects answers in the same form is Turing test. NLP still struggles with many (basic) human skills like listening, speaking, parsing and forming sentences. No one knows when we'll have system that can do these things as good as human. Since this system is crucial, but also far from human-like it's likely cause of delay in developing AI that passes intelligence test. Are these problems AI-hard? Do we need to develop strong AI to solve them?</p>

<p>You can look at speech and listening as interfaces used for expressing and affecting inner processes of human brain. Same goes for other senses like eyesight which is being approximated by computer vision. One could say that we only need to develop convincing mimics of human senses and incorporate them in one big system that will become first human-like AI. That is the minimum requirement. <strong>I doubt this will be achieved in this century.</strong></p>

<p>(Other thoughts)<br/>
What truly defines intelligence is brain activity. Since it's really complex and one artificial neuron is not equal to one neuron in brain, increase in computation power will not necessarily help achieving human-like AI. Also recognizing such system by mere intelligence test is questionable. For now it's only philosophical discussion but by the time we are able to design such machine I think we'll also have better understanding of human brain. Someone in 2100 might not read this answer on quantum computer with integrated AI OS powered from fusion reactor in his self-flying car, but will probably have many systems that help him in everyday tasks far more than we imagine today.</p>
",,1,2016-11-17T12:07:52.950,,2346,2016-11-17T12:07:52.950,,,,,3690.0,2338.0,2,2,,,,52.19,13.0,9.81,0.0,0.0,51.0,Not going into details of Stanford–Binet test but just looking at wikipedia page it shows many subtests like knowledge reasoning verbal tests etc Most of the efforts in the artificial intelligence today is directed into research of specific areas like computer vision natural language processing machine learning but also combination of fields like implementation of self driving cars Within every field there are still other subfields and problems that are not solved yet For example development of humanlike natural language processing NLP is necessary for intelligent agent to pass any verbal tests or even nonverbal tests that requires processing of sentences of human language Famous test that tests intelligence by asking questions in natural language and expects answers in the same form is Turing test NLP still struggles with many basic human skills like listening speaking parsing and forming sentences No one knows when well have system that can do these things as good as human Since this system is crucial but also far from humanlike its likely cause of delay in developing AI that passes intelligence test Are these problems AIhard Do we need to develop strong AI to solve them You can look at speech and listening as interfaces used for expressing and affecting inner processes of human brain Same goes for other senses like eyesight which is being approximated by computer vision One could say that we only need to develop convincing mimics of human senses and incorporate them in one big system that will become first humanlike AI That is the minimum requirement I doubt this will be achieved in this century Other thoughts What truly defines intelligence is brain activity Since its really complex and one artificial neuron is not equal to one neuron in brain increase in computation power will not necessarily help achieving humanlike AI Also recognizing such system by mere intelligence test is questionable For now its only philosophical discussion but by the time we are able to design such machine I think well also have better understanding of human brain Someone in 2100 might not read this answer on quantum computer with integrated AI OS powered from fusion reactor in his selfflying car but will probably have many systems that help him in everyday tasks far more than we imagine today
,1.0,"<p>The same things we like when Amazon recommends what we might like to buy, allows advertising to manipulate us. It allows people to control the world differently.</p>

<p>The algorithms social networks like Facebook use to ""improve"" our experience may also shape what news we consume. It may influence who we follow, altering our future experiences of the news.</p>

<p><strong>My question is:</strong> Will Artificial Intelligence some day become a problem to humanity after learning human behaviors and characteristics?</p>
",,0,2016-11-17T18:22:39.990,1.0,2347,2016-11-22T22:10:54.523,2016-11-22T22:10:54.523,,1581.0,,1581.0,,1,3,<machine-learning><learning-algorithms>,Artificial Intelligence at it's level of understanding Humanity than we anticipated,165.0,47.38,13.4,10.96,0.0,0.0,10.0,The same things we like when Amazon recommends what we might like to buy allows advertising to manipulate us It allows people to control the world differently The algorithms social networks like Facebook use to improve our experience may also shape what news we consume It may influence who we follow altering our future experiences of the news My question is Will Artificial Intelligence some day become a problem to humanity after learning human behaviors and characteristics
,,"<blockquote>
  <p>Will Artificial Intelligence some day become a problem to humanity
  after learning human behaviors and characteristics?</p>
</blockquote>

<p>It can be answered in both ways, I think.</p>

<p><strong>Yes, they may become a problem.</strong></p>

<p>With the increasing integration of loads of apps and smart devices in our life, almost everything defining an individual human being is digitalised. For instance, our fingerprints, voice, facial image etc. Apart from these data, we use those apps and devices to track our health (heart rate, calorie intake etc), to plan our schedules, and most importantly to communicate. If some sort of AI engine is integrated into a chat application, for instance, it can learn our typing patterns, conversation style, and hundreds of other unforeseen parameters. Imagine what can be learned about a person if such AI is coded inside every device and every app in your day-to-day use.</p>

<p>We use smart devices and apps to harness their functionalities and features which ease our way of life, and we give them, unintentionally, our identity and sometimes, even our personality. For them, these are the parameters that can be input to some machine-learning algorithm and predict what we will do the next day, or what will happen to us the next day.</p>

<p>This sounds like a major problem, especially when these technologies are indispensable.</p>

<p><strong>No, they may not become a problem.</strong></p>

<p>Humans are really complex creatures and possess the most advanced intelligence technology called the brain. I think the brain can be thought of as a technology in this context. There are several tissues inside the brain that can learn to do certain things themselves. It was proven in a research that some tissues have the capability to perform the functions of other tissues using neuro-rewiring techniques. Imagine that the same tissue that has helped you see until now can be made to help you hear instead. Now imagine mimicking such a technology.</p>

<p>While it is not impossible, for an AI to achieve the brilliance of a human brain is a topic of ongoing research. To train a machine for the purpose, we would have to feed it with gazillion behaviors and characteristics, which it may not be able to handle! After learning some behavior and characteristics, the AI would be said to be smart, but it would still predate us.</p>

<p>So, they may not become a problem at all because of our brain.</p>

<p>The algorithms (like recommender systems) used by Amazon and Facebook influence us or even manipulate us. But, that manipulation is either very obvious (like viewing promoted products) or is in company's best interests (like viewing a certain news piece). It may be even possible that several external parameters are used by these systems to improve your experience. For instance, Google ads show us what we were looking for on an online store when we visit any random website. In most cases, what you see is a result of what you were looking for before. If any attempt to influence does happen, we may learn to avoid it through careful observation or even experience.</p>
",,2,2016-11-17T19:18:47.523,,2348,2016-11-17T19:45:32.517,2016-11-17T19:45:32.517,,3592.0,,3592.0,2347.0,2,5,,,,53.1,10.9,9.29,0.0,0.0,73.0,Will Artificial Intelligence some day become a problem to humanity after learning human behaviors and characteristics It can be answered in both ways I think Yes they may become a problem With the increasing integration of loads of apps and smart devices in our life almost everything defining an individual human being is digitalised For instance our fingerprints voice facial image etc Apart from these data we use those apps and devices to track our health heart rate calorie intake etc to plan our schedules and most importantly to communicate If some sort of AI engine is integrated into a chat application for instance it can learn our typing patterns conversation style and hundreds of other unforeseen parameters Imagine what can be learned about a person if such AI is coded inside every device and every app in your daytoday use We use smart devices and apps to harness their functionalities and features which ease our way of life and we give them unintentionally our identity and sometimes even our personality For them these are the parameters that can be input to some machinelearning algorithm and predict what we will do the next day or what will happen to us the next day This sounds like a major problem especially when these technologies are indispensable No they may not become a problem Humans are really complex creatures and possess the most advanced intelligence technology called the brain I think the brain can be thought of as a technology in this context There are several tissues inside the brain that can learn to do certain things themselves It was proven in a research that some tissues have the capability to perform the functions of other tissues using neurorewiring techniques Imagine that the same tissue that has helped you see until now can be made to help you hear instead Now imagine mimicking such a technology While it is not impossible for an AI to achieve the brilliance of a human brain is a topic of ongoing research To train a machine for the purpose we would have to feed it with gazillion behaviors and characteristics which it may not be able to handle After learning some behavior and characteristics the AI would be said to be smart but it would still predate us So they may not become a problem at all because of our brain The algorithms like recommender systems used by Amazon and Facebook influence us or even manipulate us But that manipulation is either very obvious like viewing promoted products or is in companys best interests like viewing a certain news piece It may be even possible that several external parameters are used by these systems to improve your experience For instance Google ads show us what we were looking for on an online store when we visit any random website In most cases what you see is a result of what you were looking for before If any attempt to influence does happen we may learn to avoid it through careful observation or even experience
2384.0,2.0,"<p>Background:
I've been interested in, and reading about, Neural Networks for several years, but I haven't gotten around to testing them out until recently. Both for fun and to increase my understanding, I tried to write a class library from scratch in .Net.
For tests, I've tried some simple functions, such as generating output identical to the input, working with the MNIST dataset, and a few binary functions (two input OR, AND and XOR, with two outputs: one for true, one for false).
Everything seemed fine when I used a sigmoid function as the activation function but, reading of the ReLUs I decided to switch over for speed.</p>

<p>My current problem is that, when I switch to using ReLUs, I found that I was unable to train a network of any complexity (tested from as few as 2 internal nodes up to a mesh of 100x100 nodes) to correctly function as an XOR gate. I see two possibilities here:</p>

<p>1) My implementation is faulty,
(This one is frustrating, as I've re-written the code multiple times in various ways, and I still get the same result),</p>

<p>2) Aside from being faster or slower to train, there are some problems that are impossible to solve given a specific activation function,
(Fascinating idea, but I've no idea if it's true or not)</p>

<p>My inclination is to think that 1) above is correct. However, given the amount of time I've invested, it would be nice if I could rule out 2) definitively before I spend even more time going over my implementation.</p>

<p>Edit for specifics:
For the XOR network, I have tried both using two inputs (0 for false, 1 for true), and using four inputs (each pair, one signals true and one false, per ""bit"" of input).
I have also tried using 1 output (with a 1 (realy, >0.9) corresponding to true and a 0 (or &lt;0.1) corresponding to false), as well as two outputs (one signaling true and the other false).</p>

<p>Each training epoch, I run against four sets of input: 00->0, 01->1, 10->1, 11->0.</p>

<p>I find that the first three converge towards correct answer, but the final input (11) converges towards 1, even though I train it with an expected value of 0.</p>
",,0,2016-11-17T20:46:24.343,1.0,2349,2016-11-27T23:26:44.763,2016-11-17T20:53:38.663,,3702.0,,3702.0,,1,4,<neural-networks>,Are ReLUs incapable of solving certain problems?,118.0,59.37,9.88,9.47,0.0,0.0,102.0,Background Ive been interested in and reading about Neural Networks for several years but I havent gotten around to testing them out until recently Both for fun and to increase my understanding I tried to write a class library from scratch in Net For tests Ive tried some simple functions such as generating output identical to the input working with the MNIST dataset and a few binary functions two input OR AND and XOR with two outputs one for true one for false Everything seemed fine when I used a sigmoid function as the activation function but reading of the ReLUs I decided to switch over for speed My current problem is that when I switch to using ReLUs I found that I was unable to train a network of any complexity tested from as few as 2 internal nodes up to a mesh of 100x100 nodes to correctly function as an XOR gate I see two possibilities here 1 My implementation is faulty This one is frustrating as Ive rewritten the code multiple times in various ways and I still get the same result 2 Aside from being faster or slower to train there are some problems that are impossible to solve given a specific activation function Fascinating idea but Ive no idea if its true or not My inclination is to think that 1 above is correct However given the amount of time Ive invested it would be nice if I could rule out 2 definitively before I spend even more time going over my implementation Edit for specifics For the XOR network I have tried both using two inputs 0 for false 1 for true and using four inputs each pair one signals true and one false per bit of input I have also tried using 1 output with a 1 realy 09 corresponding to true and a 0 or lt01 corresponding to false as well as two outputs one signaling true and the other false Each training epoch I run against four sets of input 000 011 101 110 I find that the first three converge towards correct answer but the final input 11 converges towards 1 even though I train it with an expected value of 0
,,"<p>While I have not determined if there are problems which cannot be solved with ReLU, I have found ample documentation in the literature that XOR is solvable with as few as 1 hidden node. Therefore, I must assume there is something wrong with my implementation.</p>

<p>Edit: The solution is simpler than I thought. The output layer needs connections, not just to the intermediate layer, but directly to the input layer as well. This allows the network to train XOR effectively.</p>

<p>Edit 2: One final note, the XOR is EXTREMELY sensitive to the learning rate. Essentially, whatever learning rate is appropriate for the AND and OR functions, is approximately 1000x too large to train XOR effectively.</p>
",,3,2016-11-18T18:23:32.520,,2350,2016-11-20T00:02:19.887,2016-11-20T00:02:19.887,,3702.0,,3702.0,2349.0,2,2,,,,54.83,11.25,9.39,0.0,0.0,16.0,While I have not determined if there are problems which cannot be solved with ReLU I have found ample documentation in the literature that XOR is solvable with as few as 1 hidden node Therefore I must assume there is something wrong with my implementation Edit The solution is simpler than I thought The output layer needs connections not just to the intermediate layer but directly to the input layer as well This allows the network to train XOR effectively Edit 2 One final note the XOR is EXTREMELY sensitive to the learning rate Essentially whatever learning rate is appropriate for the AND and OR functions is approximately 1000x too large to train XOR effectively
2355.0,1.0,"<p>Does anyone know, or can we deduce or infer with high probability from its characteristics, whether the neural network used on this site </p>

<p><a href=""https://quickdraw.withgoogle.com/"" rel=""nofollow noreferrer"">https://quickdraw.withgoogle.com/</a></p>

<p>is a type of convolutional neural network (CNN)?</p>
",,0,2016-11-18T20:16:43.493,,2351,2017-01-07T15:47:06.297,,,,,46.0,,1,2,<neural-networks><image-recognition><conv-neural-network>,Is the QuickDraw with Google neural net a convolutional neural network?,139.0,46.78,16.53,9.86,0.0,0.0,11.0,Does anyone know or can we deduce or infer with high probability from its characteristics whether the neural network used on this site httpsquickdrawwithgooglecom is a type of convolutional neural network CNN
,4.0,"<p>After the explosion of fake news during the US election, and following the question about whether AIs can educate themselves via the internet, it is clear to me that any newly-launched AI will have a serious problem knowing what to believe (ie rely on as input for making predictions and decisions).</p>

<p>Information provided by its creators could easily be false. Many AIs won't have access to cameras and sensors to verify things by their own observations.</p>

<p>If there was to be some kind of verification system for information (like a ""blockchain of truth"", for example, or a system of ""trusted sources""), how could that function, in practical terms? </p>
",,2,2016-11-19T06:02:15.850,1.0,2353,2016-11-26T14:40:15.097,,,,,3601.0,,1,2,<machine-learning><deep-network><agi><self-learning><heuristics>,How can a general AI determine/verify what is true/real?,189.0,52.53,11.9,10.97,0.0,0.0,20.0,After the explosion of fake news during the US election and following the question about whether AIs can educate themselves via the internet it is clear to me that any newlylaunched AI will have a serious problem knowing what to believe ie rely on as input for making predictions and decisions Information provided by its creators could easily be false Many AIs wont have access to cameras and sensors to verify things by their own observations If there was to be some kind of verification system for information like a blockchain of truth for example or a system of trusted sources how could that function in practical terms
,,"<p>Not possible without some big restrictions. What it can do is look at known ""good"" sites and compare news with site that is potentially ""bad"". Obvious problem here is defining some sites as absolute truth. For example it can recognize, while reading text, that some politician said something. These sentences can be compared with other sites, and if there is significant difference, that news is candidate for false news.</p>

<p>In practical terms, program would extract sentences ""i like cats"", ""says he likes cats"", ""cats that John likes"" etc. We need part that recognizes something as a quote, part that extracts it and finally parser so we end up with structure stored in some form that contains meaning of sentence (john-like-cats). Also it can keep information of time and context in which it was said, like timestamp of an article, some proper nouns that can indicate place (XY conference, London...). Now, suspicious article can be compared and checked if it matches time, place, some context and contains quote that is similar. Finally it needs to compare how different it is from other quotes. ""...hates cats"" should be labeled as potential fake news, but ""likes dogs"", ""thinks cats are OK"", ""sings well"" etc. should not. This can be expanded into comparison of whole articles.</p>

<p>There are many features that can be used to define particular article as fake. Interesting feature  for finding fake sites could be bias when it comes to particular (political, economical, ecological...) opinion. But in the end machine can't decide if the article is fake without comparing it to other articles. It is bound to closed system that reflects real world in subjective way.</p>
",,4,2016-11-19T11:14:09.067,,2354,2016-11-19T11:14:09.067,,,,,3690.0,2353.0,2,3,,,,54.02,11.71,9.37,0.0,0.0,72.0,Not possible without some big restrictions What it can do is look at known good sites and compare news with site that is potentially bad Obvious problem here is defining some sites as absolute truth For example it can recognize while reading text that some politician said something These sentences can be compared with other sites and if there is significant difference that news is candidate for false news In practical terms program would extract sentences i like cats says he likes cats cats that John likes etc We need part that recognizes something as a quote part that extracts it and finally parser so we end up with structure stored in some form that contains meaning of sentence johnlikecats Also it can keep information of time and context in which it was said like timestamp of an article some proper nouns that can indicate place XY conference London Now suspicious article can be compared and checked if it matches time place some context and contains quote that is similar Finally it needs to compare how different it is from other quotes hates cats should be labeled as potential fake news but likes dogs thinks cats are OK sings well etc should not This can be expanded into comparison of whole articles There are many features that can be used to define particular article as fake Interesting feature for finding fake sites could be bias when it comes to particular political economical ecological opinion But in the end machine cant decide if the article is fake without comparing it to other articles It is bound to closed system that reflects real world in subjective way
,,"<p>I believe they don't use CNNs. The most important reason why it's because they have more information than a regular image: time. The input they receive is a sequence of (x,y,t) as you draw on the screen, which they refer as ""ink"". This gives them the construction of the image for free, which a CNN would have to deduce by itself.</p>

<p>They tried two approaches. Their currently most successful approach does the following:</p>

<ul>
<li>Detect parts of the ink that are candidates of being a character</li>
<li>Use a FeedForward Neural Network to do character recognition on those candidates</li>
<li>Use beam search and a language model to find most the most likely combination of results that results into a word</li>
</ul>

<p>Their second approach is using an LSTM (a type of Recurrent Neural Network) end-to-end. In their paper they say this was better in a couple languages.</p>

<p><strong>Source</strong>: I was an intern in Google's handwriting team in summer 2015 (on which I believe quickdraw is based), but the techniques I explained can be found in <a href=""http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7478642"" rel=""nofollow noreferrer"">this paper</a>.</p>
",,0,2016-11-19T22:19:06.493,,2355,2017-01-07T15:47:06.297,2017-01-07T15:47:06.297,,3745.0,,3745.0,2351.0,2,6,,,,66.27,10.21,9.44,0.0,0.0,29.0,I believe they dont use CNNs The most important reason why its because they have more information than a regular image time The input they receive is a sequence of xyt as you draw on the screen which they refer as ink This gives them the construction of the image for free which a CNN would have to deduce by itself They tried two approaches Their currently most successful approach does the following Detect parts of the ink that are candidates of being a character Use a FeedForward Neural Network to do character recognition on those candidates Use beam search and a language model to find most the most likely combination of results that results into a word Their second approach is using an LSTM a type of Recurrent Neural Network endtoend In their paper they say this was better in a couple languages Source I was an intern in Googles handwriting team in summer 2015 on which I believe quickdraw is based but the techniques I explained can be found in this paper
2359.0,6.0,"<p>For example, would an AI be able to own property, evict tenants, acquire debt, employ, vote, or marry? What are the legal structures in place to implement a strong AI into society? </p>
",,2,2016-11-20T04:25:14.977,2.0,2356,2016-12-18T18:54:38.513,2016-11-23T01:22:43.420,,3748.0,,3748.0,,1,8,<strong-ai><control-problem><legal>,Would an AI with human intelligence have the same rights as a human under current legal frameworks?,218.0,63.7,9.45,10.84,0.0,0.0,8.0,For example would an AI be able to own property evict tenants acquire debt employ vote or marry What are the legal structures in place to implement a strong AI into society
,,"<p>There is a legal difference between a ""person"" (which includes bodies corporate - corporations, incorporated associations, etc - and actual people) vs ""natural person"" (which is specifically a human being).</p>

<p>For an AI to marry, it would need to get the legal definition of ""natural person"" changed, and depending on the jurisdiction possibly also the definition of ""man"" or ""woman"".</p>

<p>For other things, such as owning property, evicting tenants, entering into contracts, etc, an AI would simply use a corporation. It may be that the corporation might need to have a minimum number of directors who are natural persons, but they could just be paid professionals, so no issue there.</p>

<p>With credit cards, it would depend on the policy of the issuing bank. There is no legal impediment to corporations having credit cards in their own right, but in practice banks often require a director's guarantee from a natural person that they can sue if the bill is not paid. They want to be sure they will get their money, even if the corporation is wound up.</p>
",,0,2016-11-20T05:46:32.043,,2357,2016-11-23T13:28:22.560,2016-11-23T13:28:22.560,,3601.0,,3601.0,2356.0,2,0,,,,54.56,11.67,9.93,0.0,0.0,38.0,There is a legal difference between a person which includes bodies corporate corporations incorporated associations etc and actual people vs natural person which is specifically a human being For an AI to marry it would need to get the legal definition of natural person changed and depending on the jurisdiction possibly also the definition of man or woman For other things such as owning property evicting tenants entering into contracts etc an AI would simply use a corporation It may be that the corporation might need to have a minimum number of directors who are natural persons but they could just be paid professionals so no issue there With credit cards it would depend on the policy of the issuing bank There is no legal impediment to corporations having credit cards in their own right but in practice banks often require a directors guarantee from a natural person that they can sue if the bill is not paid They want to be sure they will get their money even if the corporation is wound up
,,"<p>Emotions are a factor in humans having ethics/morals only because they are a factor in all human learning and decision-making.</p>

<p>Unless you are duplicating a human being exactly, there is no reason to think that an AI will learn the way a human learns, or make decisions in the same way a human makes decisions.</p>

<p>Therefore, whether it ""feels emotion"" just like we do, or whether it simply responds to outcomes ""cost is greater = don't go there"", the outcome of ethical BEHAVIOUR could be achieved. An AI could behave perfectly ethically without any need for feeling empathy, shame, etc.</p>

<p>You could also argue that a lot of UNETHICAL behaviour in human beings is driven by emotions, too, and that an unemotional but ethical AI may well do a better overall job than a human being.</p>
",,0,2016-11-20T06:00:23.973,,2358,2016-11-20T06:00:23.973,,,,,3601.0,2277.0,2,3,,,,52.53,10.45,10.01,0.0,0.0,22.0,Emotions are a factor in humans having ethicsmorals only because they are a factor in all human learning and decisionmaking Unless you are duplicating a human being exactly there is no reason to think that an AI will learn the way a human learns or make decisions in the same way a human makes decisions Therefore whether it feels emotion just like we do or whether it simply responds to outcomes cost is greater dont go there the outcome of ethical BEHAVIOUR could be achieved An AI could behave perfectly ethically without any need for feeling empathy shame etc You could also argue that a lot of UNETHICAL behaviour in human beings is driven by emotions too and that an unemotional but ethical AI may well do a better overall job than a human being
,,"<p>Yes, to some of what you propose.  No to some.</p>

<p>Today corporations are granted rights: to own property, earn income, pay taxes, contribute to political campaigns, offer opinion in public, ad more.  Even now I see no reason why an AI should not be eligible to incorporate itself, thereby inheriting all these rights.  Conversely, any corporation already in existence could become fully automated at any time (and some plausibly will).  In doing so, they should not lose any of the rights and duties they currently employ.</p>

<p>However I suspect certain rights would be unavailable to an AI just as they are unavailable to a corporation now: marriage, draft or voluntary service in the military, rights due a parent or child or spouse, estate inheritance, etc.</p>

<p>Could this schizoid sense of human identity be resolved at some point?  Sure.  Already there have been numerous laws introduced and some passed elevating various nonhuman species to higher levels of civil rights that only humans heretofore enjoyed: chimpanzees, cetaceans, parrots and others have been identified as 'higher functioning' and longer lived, and so, are now protected from abuse in ways that food animals, pets, and lab animals are not.  </p>

<p>Once AI 'beings' arise that operate for years and express intelligence and emotions that approach human-level and lifetime, I would expect a political will to arise to define, establish, and defend their civil rights.  And as humans become more cybernetically augmented, especially cognitively, the line that separates us from creatures of pure silicon will begin to blur.  In time it will become unconscionable to overlook the rights of beings simply because they contain 'too little flesh'.</p>
",,1,2016-11-20T07:34:37.700,,2359,2016-11-20T07:34:37.700,,,,,1657.0,2356.0,2,6,,,,40.08,13.35,10.64,0.0,0.0,49.0,Yes to some of what you propose No to some Today corporations are granted rights to own property earn income pay taxes contribute to political campaigns offer opinion in public ad more Even now I see no reason why an AI should not be eligible to incorporate itself thereby inheriting all these rights Conversely any corporation already in existence could become fully automated at any time and some plausibly will In doing so they should not lose any of the rights and duties they currently employ However I suspect certain rights would be unavailable to an AI just as they are unavailable to a corporation now marriage draft or voluntary service in the military rights due a parent or child or spouse estate inheritance etc Could this schizoid sense of human identity be resolved at some point Sure Already there have been numerous laws introduced and some passed elevating various nonhuman species to higher levels of civil rights that only humans heretofore enjoyed chimpanzees cetaceans parrots and others have been identified as higher functioning and longer lived and so are now protected from abuse in ways that food animals pets and lab animals are not Once AI beings arise that operate for years and express intelligence and emotions that approach humanlevel and lifetime I would expect a political will to arise to define establish and defend their civil rights And as humans become more cybernetically augmented especially cognitively the line that separates us from creatures of pure silicon will begin to blur In time it will become unconscionable to overlook the rights of beings simply because they contain too little flesh
,,"<p>Murray Shanahan, in his book <strong>The Technological Singularity</strong>, makes the case that the rights of any being are determined by its intelligence. </p>

<p>For instance, we value the life of a dog above that of an ant and likewise value human life above that of other animals.</p>

<blockquote>
  <p>From here one could argue that a general artificial intelligence of equal intelligence to a human should have equal rights to a human and a superior artificial intelligence should have more rights.</p>
</blockquote>

<p>The question, of course, is whether our anthropocentric society would be willing to accept this fundamental shift in human rights and this idea of removing humanity from its pedestal of importance.</p>

<p>When it comes to legal frameworks, we really are entering into uncharted territory as AI is going to have to revolutionise the way we define many of the terms we take for granted today and question many of our usual assumptions.</p>

<blockquote>
  <p>AI is going to drive an important shift in our mindset well before it exceeds human intelligence.</p>
</blockquote>
",,0,2016-11-20T12:35:01.477,,2360,2016-11-20T12:35:01.477,,,,,3427.0,2356.0,2,2,,,,34.8,11.38,9.84,0.0,0.0,12.0,Murray Shanahan in his book The Technological Singularity makes the case that the rights of any being are determined by its intelligence For instance we value the life of a dog above that of an ant and likewise value human life above that of other animals From here one could argue that a general artificial intelligence of equal intelligence to a human should have equal rights to a human and a superior artificial intelligence should have more rights The question of course is whether our anthropocentric society would be willing to accept this fundamental shift in human rights and this idea of removing humanity from its pedestal of importance When it comes to legal frameworks we really are entering into uncharted territory as AI is going to have to revolutionise the way we define many of the terms we take for granted today and question many of our usual assumptions AI is going to drive an important shift in our mindset well before it exceeds human intelligence
,,"<blockquote>
  <p>It is certainly possible for AI to theoretically feel emotion. </p>
</blockquote>

<p>There are, according to Murray Shanahan's book <strong>The Technological Singularity</strong>, two primary forms of AI:</p>

<blockquote>
  <p>1) Human based AI - achieved through processes such as <strong><em>whole brain emulation</em></strong>, the functioning of human based AI would likely be indistinguishable from that of the human brain, and, as a consequence, human based AI would likely experience emotion in the same manner as humans.</p>
</blockquote>

<p>-</p>

<blockquote>
  <p>2) AI from scratch - with this form of AI, based on machine learning algorithms and complex processes to drive goals, we enter into uncharted territory as the development of this form of AI is inherently unpredictable and unlike anything we observe in the biological sample space of intelligence we have access to.</p>
  
  <p>With this form of AI, there is no telling if and how it could experience emotion.</p>
</blockquote>

<p>As the question references the former, it is very likely that human-based AI would indeed experience emotion and other human-like characteristics.</p>
",,0,2016-11-20T12:50:16.217,,2361,2016-11-20T12:50:16.217,,,,,3427.0,2277.0,2,7,,,,30.74,13.41,10.48,0.0,0.0,24.0,It is certainly possible for AI to theoretically feel emotion There are according to Murray Shanahans book The Technological Singularity two primary forms of AI 1 Human based AI achieved through processes such as whole brain emulation the functioning of human based AI would likely be indistinguishable from that of the human brain and as a consequence human based AI would likely experience emotion in the same manner as humans 2 AI from scratch with this form of AI based on machine learning algorithms and complex processes to drive goals we enter into uncharted territory as the development of this form of AI is inherently unpredictable and unlike anything we observe in the biological sample space of intelligence we have access to With this form of AI there is no telling if and how it could experience emotion As the question references the former it is very likely that humanbased AI would indeed experience emotion and other humanlike characteristics
2364.0,1.0,"<p>I have implemented a Sobel Filter for edge detection in Matlab without using its toolbox. I am a bit confused: </p>

<p>Is a Sobel filter a type of Cellular Neural Network?</p>

<p>Both Sobel and Cellular Neural Network calculate output via its neighborhood cells.</p>
",,0,2016-11-21T12:44:57.060,,2363,2016-11-23T14:35:20.330,2016-11-22T19:00:58.583,,46.0,,3763.0,,1,4,<neural-networks><cnn>,Is Sobel Filter for Edge Detection type of CNN?,86.0,57.27,10.55,10.72,0.0,0.0,4.0,I have implemented a Sobel Filter for edge detection in Matlab without using its toolbox I am a bit confused Is a Sobel filter a type of Cellular Neural Network Both Sobel and Cellular Neural Network calculate output via its neighborhood cells
,,"<p>You're right about the basic arrangement of the inputs, but there are a number of differences:</p>

<ol>
<li><p><a href=""http://en.wikipedia.org/wiki/Artificial_neural_network"" rel=""nofollow noreferrer"">Artificial neural networks</a> typically use exemplar data as inputs for the purpose of training, or adjusting the weights of its internal connections, to accurately classify them within a certain error range. The network is then applied to unknown data to classify them. <a href=""https://en.wikipedia.org/wiki/Edge_detection"" rel=""nofollow noreferrer"">Edge detection</a> filters are just blind operators that transform input data regardless of how it can be classified. There is no training, so any intelligence exists only in the mind of the filter developer.</p></li>
<li><p>A CNN could be trained to be an effective Sobel (edge detection) filter, as described <a href=""http://www.worldacademicunion.com/journal/1746-7659JIC/jicvol5no1paper01.pdf"" rel=""nofollow noreferrer"">in this paper</a>, but a Sobel filter couldn't be an effective learning algorithm.</p></li>
<li><p>Training neural networks is more non-deterministic, with outputs depending on what data they are trained with and potentially even the operational computations that are used for classification. Applying filters is typically deterministic, i.e. they will transform the same data exactly the same way if applied twice.</p></li>
</ol>

<p>One succinct way of expressing the biggest difference is: a cellular neural network is looking for a function, while a Sobel filter <em>is</em> a function.</p>

<p>Note that there are types of neural networks called Convolutional Neural Networks, which can use Sobel and other filters in their input layers, as described <a href=""https://en.wikipedia.org/wiki/Convolutional_neural_network"" rel=""nofollow noreferrer"">here</a>. Though, these are neither of the things you are asking about. :)</p>
",,0,2016-11-21T21:57:00.670,,2364,2016-11-23T14:35:20.330,2016-11-23T14:35:20.330,,46.0,,46.0,2363.0,2,2,,,,42.0,13.17,10.13,0.0,0.0,33.0,Youre right about the basic arrangement of the inputs but there are a number of differences Artificial neural networks typically use exemplar data as inputs for the purpose of training or adjusting the weights of its internal connections to accurately classify them within a certain error range The network is then applied to unknown data to classify them Edge detection filters are just blind operators that transform input data regardless of how it can be classified There is no training so any intelligence exists only in the mind of the filter developer A CNN could be trained to be an effective Sobel edge detection filter as described in this paper but a Sobel filter couldnt be an effective learning algorithm Training neural networks is more nondeterministic with outputs depending on what data they are trained with and potentially even the operational computations that are used for classification Applying filters is typically deterministic ie they will transform the same data exactly the same way if applied twice One succinct way of expressing the biggest difference is a cellular neural network is looking for a function while a Sobel filter is a function Note that there are types of neural networks called Convolutional Neural Networks which can use Sobel and other filters in their input layers as described here Though these are neither of the things you are asking about
,,"<p>Not only wouldn't a strong AI which came into existence today have the rights a human has, or any rights (see these discussions of the implementation of regulation for weak AIs at: <a href=""https://www.whitehouse.gov/blog/2016/05/03/preparing-future-artificial-intelligence"" rel=""nofollow noreferrer"">The White House</a> and <a href=""http://apps.americanbar.org/dch/committee.cfm?com=ST248008"" rel=""nofollow noreferrer"">The American Bar Association</a>),  but it seems unlikely the first one will.</p>

<p>Observing that:</p>

<ol>
<li><p>Having rights implies that there are restrictions, which means there would have to be a system of control. However the <a href=""http://en.wikipedia.com/wiki/AI_control_problem"" rel=""nofollow noreferrer"">control problem in AI</a> is still unsolved.</p></li>
<li><p>Even assuming that problem is solvable, an AGI would then have to appear equivalent to natural humans. They don't yet (see <a href=""https://www.theguardian.com/technology/2014/jun/09/scientists-disagree-over-whether-turing-test-has-been-passed"" rel=""nofollow noreferrer"">Turing Test Passed?</a>), and even after passing equivalence tests, are unlikely to remain that way, per the <a href=""http://en.wikipedia.org/wiki/Technological_singularity"" rel=""nofollow noreferrer"">Singularity Hypothesis</a>.</p></li>
<li><p>Further, if one or more AGIs were to be human-equivalent long enough to desire rights, lawmakers (in the US) would have to re-interpret the definition of personhood and grant them rights, as they did for <a href=""https://en.wikipedia.org/wiki/Corporate_personhood"" rel=""nofollow noreferrer"">corporations in 1886</a>.</p></li>
</ol>
",,0,2016-11-21T22:05:17.057,,2365,2016-11-21T22:05:17.057,,,,,46.0,2356.0,2,1,,,,57.5,11.66,9.72,0.0,0.0,29.0,Not only wouldnt a strong AI which came into existence today have the rights a human has or any rights see these discussions of the implementation of regulation for weak AIs at The White House and The American Bar Association but it seems unlikely the first one will Observing that Having rights implies that there are restrictions which means there would have to be a system of control However the control problem in AI is still unsolved Even assuming that problem is solvable an AGI would then have to appear equivalent to natural humans They dont yet see Turing Test Passed and even after passing equivalence tests are unlikely to remain that way per the Singularity Hypothesis Further if one or more AGIs were to be humanequivalent long enough to desire rights lawmakers in the US would have to reinterpret the definition of personhood and grant them rights as they did for corporations in 1886
2560.0,2.0,"<p>If someone wants to develop a <strong>basic AI</strong> with some code modules,Let us say the AI just has to provide an action when stimulated in a certain situation based on its previous understanding of situations. </p>

<p>I can think of at least 3 of such components:</p>

<ul>
<li><strong>Real-time Understanding/Learning:</strong> Using Deep Learning/ConvNets, Supervised/Unsupervised.</li>
<li><strong>Logical Decision-Making:</strong> Calculating the results of various decisions when applied on current situation based on previous understanding and choosing the most appropriate one logically.</li>
<li><strong>Action/Reaction:</strong> Acting precisely in the new situation according to the decision-made.</li>
</ul>

<p>Any ideas?</p>
",,2,2016-11-22T00:10:21.533,2.0,2366,2017-01-09T01:00:30.517,2016-11-23T21:27:18.617,,1581.0,,3768.0,,1,5,<algorithm><ai-design>,What are the most important components in the algorithm of a minimum AI?,332.0,23.77,17.64,11.37,0.0,0.0,18.0,If someone wants to develop a basic AI with some code modulesLet us say the AI just has to provide an action when stimulated in a certain situation based on its previous understanding of situations I can think of at least 3 of such components Realtime UnderstandingLearning Using Deep LearningConvNets SupervisedUnsupervised Logical DecisionMaking Calculating the results of various decisions when applied on current situation based on previous understanding and choosing the most appropriate one logically ActionReaction Acting precisely in the new situation according to the decisionmade Any ideas
,3.0,"<p>Let's say I have a string ""America"" and I want to convert it into a number to feed into a machine learning algorithm. If I use two digits for each letter, e.g. A = 01, B = 02 and so on, then the word ""America"" will be converted to <code>01XXXXXXXXXX01</code> (10<sup>11</sup>). This is a very high number for a <code>long int</code>, and many words longer than ""America"" are expected. </p>

<p>How can I deal with this problem?</p>

<p>Suggest an algorithm for efficient and meaningful conversions.</p>
",,3,2016-11-22T06:06:03.117,,2367,2016-11-28T17:14:51.833,2016-11-28T17:14:51.833,,75.0,,3773.0,,1,0,<machine-learning>,How to convert string to number and number to string efficiently?,221.0,83.46,6.77,8.29,23.0,0.0,22.0,Lets say I have a string America and I want to convert it into a number to feed into a machine learning algorithm If I use two digits for each letter eg A 01 B 02 and so on then the word America will be converted to 1011 This is a very high number for a and many words longer than America are expected How can I deal with this problem Suggest an algorithm for efficient and meaningful conversions
,,"<p>What are you trying to achieve?</p>

<p>If you need to encode it to some integer use hash table. If you are using something like linear regression or neural network it would be better to use dummy features (one-hot encoding). So for your dictionary of 5 words (""America"", ""Brazil"", ""Chile"", ""Denmark"", ""Estonia"") you get 5 features (x1, x2, x3, x4, x5) which indicate if some word is equal to one in dictionary. So ""Brazil"" is represented by (0,1,0,0,0), ""Germany"" is (0,0,0,0,0). Number of features grows with number of words in dictionary making some features practically useless.</p>

<p>If you are using decision trees you don't need to convert string to integer unless specific algorithm asks you to do so. Again, use hash table to do it. In R you can use factor() function.</p>

<p>If you convert your string to integers and use it as single feature (""America"" - 123, ""Brazil"" - 245), algorithm will try to find patterns in it by comparing numbers but may fail to recognize specific countries.</p>
",,0,2016-11-22T09:55:46.263,,2368,2016-11-22T09:55:46.263,,,,,3690.0,2367.0,2,1,,,,63.09,10.67,9.22,0.0,0.0,66.0,What are you trying to achieve If you need to encode it to some integer use hash table If you are using something like linear regression or neural network it would be better to use dummy features onehot encoding So for your dictionary of 5 words America Brazil Chile Denmark Estonia you get 5 features x1 x2 x3 x4 x5 which indicate if some word is equal to one in dictionary So Brazil is represented by 01000 Germany is 00000 Number of features grows with number of words in dictionary making some features practically useless If you are using decision trees you dont need to convert string to integer unless specific algorithm asks you to do so Again use hash table to do it In R you can use factor function If you convert your string to integers and use it as single feature America 123 Brazil 245 algorithm will try to find patterns in it by comparing numbers but may fail to recognize specific countries
,,"<p>This depends a lot on what you want to achieve, but if you aim to generalise beyond the words encountered in your training data, you should consider using something like <a href=""https://en.wikipedia.org/wiki/Word2vec"" rel=""nofollow noreferrer"">word2vec</a>. </p>

<p>In word2vec semantically similar words are represented by similar vectors and what's more, semantic differences translate into geometrical differences. To overuse a standard example: vec(Paris)-vec(France)+vec(Italy)=vec(Rome).</p>

<p>These relationships allow the network to generalise to completely new content.</p>
",,0,2016-11-22T10:43:50.240,,2369,2016-11-22T10:43:50.240,,,,,2227.0,2367.0,2,2,,,,20.58,18.79,11.54,0.0,0.0,20.0,This depends a lot on what you want to achieve but if you aim to generalise beyond the words encountered in your training data you should consider using something like word2vec In word2vec semantically similar words are represented by similar vectors and whats more semantic differences translate into geometrical differences To overuse a standard example vecParisvecFrancevecItalyvecRome These relationships allow the network to generalise to completely new content
,0.0,"<p>I'm trying to understand Boltzmann machines. Tutorials explain it with two formulas.</p>

<p>Logistic function for the probability of single units:</p>

<pre><code> $p(unit=1)=\frac{1}{1+e^{-\sum_{x}wx } }$
</code></pre>

<p>and, when the machine is running, every state of the machine goes to the probability:</p>

<pre><code>$ p(State= state\ with\ energy\ E_i )=\frac{e^{-E_i}}{\sum_i e^{-E_i}} $
</code></pre>

<p>so, the state depends on the units, and then if I understand correctly, the second formula is a consequence of the first; so, how can it be the proof that the distribution of $p(state)$ is a consequence of $p(unit)$?</p>
",2016-11-25T03:59:32.073,2,2016-11-22T16:28:02.987,,2370,2016-11-23T17:54:48.237,2016-11-23T17:54:48.237,,46.0,,2189.0,,1,3,<neural-networks><deep-learning><boltzmann-machine>,boltzmann machine; from logistic function to boltzmann distribution,40.0,55.58,11.2,9.65,117.0,16.0,13.0,Im trying to understand Boltzmann machines Tutorials explain it with two formulas Logistic function for the probability of single units and when the machine is running every state of the machine goes to the probability so the state depends on the units and then if I understand correctly the second formula is a consequence of the first so how can it be the proof that the distribution of is a consequence of
,1.0,"<p>I installed a local running instance of the <a href=""http://conceptnet5.media.mit.edu/"" rel=""nofollow noreferrer"">ConceptNet5</a> knowledgebase in an elasticsearch server. I used this data to implement the so-called ""<a href=""https://de.wikipedia.org/wiki/Analogietechnik"" rel=""nofollow noreferrer"">Analogietechnik</a>"" (a creativity technique to solve a problem from the perspective of another system) as an algorithm.</p>

<p>The technique works as follows:</p>

<ol>
<li>Choose a Feature of a System</li>
<li>Find Systems who have this feature also</li>
<li>Solve the problem from the perspective of these other systems</li>
<li>Apply the found solutions to the issue</li>
</ol>

<p>As an example is here the problem of marketing a shopping mall: A Shopping mall has many rooms and floors (1). A museum has also many rooms and floors (2). How are museums marketed? They present many pictures or sculptures (3). We could use our rooms and floors to decorate them with pictures and sculptures (4).</p>

<p>Of course the idea to implement that as an artifically intelligent algorithm was not far. However, I feel a little bit overwhelmed by the amount of methods that exist out there. Neural Networks, Bayesian Interference and so on... My current experience doesn't go further than simple machine learning like kMeans-Clustering for example. Do you think it would be very hard to find a solution for this problem? </p>

<p>I'm thinking of a console application, where you can enter a conceptualized problem like ""methods for creative writing"", for example, and it uses the above method to find possible solutions of the issue. Of course no solution with extensive depth, more something like basic ideas derived from the knowledge database I have.</p>

<p>Lets take as an example a console application where someone asks ""how to write a novel"":</p>

<ol>
<li>It should find out first that the system all is about is in the term ""novel"". To find a feature of that system it just searches concepts containing that term: it finds out ""Novel is a story"" So thats a feature.</li>
<li>Which systems are also stories? A good concept it should find is e.g. ""Plot is a story"". (Of course only when I am selecting the search results manually)--> <strong>How to find best concepts of a list when not knowing which fits best?</strong></li>
<li>It should then find out that a plot is written using a storyline: ""storyline is a plot""</li>
<li>One possible answer of the AI would in this case be: ""By writing a storyline""</li>
</ol>

<p>Do you know some helpful libraries, algorithms or other resources that might help me? I know this is not an easy thing to program, but you might agree that its highly interesting.</p>
",,0,2016-11-22T21:58:31.003,,2371,2017-02-24T07:40:52.570,2016-11-23T10:39:54.233,,46.0,,3788.0,,1,2,<problem-solving><world-knowledge>,Using ConceptNet5 to find similar systems to solve specific problems?,48.0,60.95,10.44,9.24,0.0,0.0,74.0,I installed a local running instance of the ConceptNet5 knowledgebase in an elasticsearch server I used this data to implement the socalled Analogietechnik a creativity technique to solve a problem from the perspective of another system as an algorithm The technique works as follows Choose a Feature of a System Find Systems who have this feature also Solve the problem from the perspective of these other systems Apply the found solutions to the issue As an example is here the problem of marketing a shopping mall A Shopping mall has many rooms and floors 1 A museum has also many rooms and floors 2 How are museums marketed They present many pictures or sculptures 3 We could use our rooms and floors to decorate them with pictures and sculptures 4 Of course the idea to implement that as an artifically intelligent algorithm was not far However I feel a little bit overwhelmed by the amount of methods that exist out there Neural Networks Bayesian Interference and so on My current experience doesnt go further than simple machine learning like kMeansClustering for example Do you think it would be very hard to find a solution for this problem Im thinking of a console application where you can enter a conceptualized problem like methods for creative writing for example and it uses the above method to find possible solutions of the issue Of course no solution with extensive depth more something like basic ideas derived from the knowledge database I have Lets take as an example a console application where someone asks how to write a novel It should find out first that the system all is about is in the term novel To find a feature of that system it just searches concepts containing that term it finds out Novel is a story So thats a feature Which systems are also stories A good concept it should find is eg Plot is a story Of course only when I am selecting the search results manually How to find best concepts of a list when not knowing which fits best It should then find out that a plot is written using a storyline storyline is a plot One possible answer of the AI would in this case be By writing a storyline Do you know some helpful libraries algorithms or other resources that might help me I know this is not an easy thing to program but you might agree that its highly interesting
,,"<p>Suppose you have data:</p>

<pre><code>color  height  quality
=====  ======  =======
green  tall    good
green  short   bad
blue   tall    bad
blue   short   medium
red    tall    medium
red    short   medium
</code></pre>

<p>To calculate the entropy for quality in this example:</p>

<pre><code>X  = {good, medium, bad}
x1 = {good}, x2 = {bad}, x3 = {medium}
</code></pre>

<p>Probability of each x in X:</p>

<pre><code>p1 = 1/6 = 0.16667
p2 = 2/6 = 0.33333
p3 = 3/6 = 0.5
</code></pre>

<p>for which logarithms are:</p>

<pre><code>log2(p1) = -2.58496
log2(p2) = -1.58496
log2(p3) = -1.0
</code></pre>

<p>and therefore entropy for the set is:</p>

<pre><code>H(X) = - (0.16667 * -2.58496) - (0.33333 * -1.58496) - (0.5 * -1.0)
     = 1.45915
</code></pre>

<p>by the formula in the question.</p>

<p>Remaining tasks are to iterate this process for each attribute to form the nodes of the tree.</p>
",,0,2016-11-22T22:02:05.037,,2372,2016-11-22T22:02:05.037,,,,,46.0,2286.0,2,1,,,,53.04,9.52,9.72,430.0,0.0,7.0,Suppose you have data To calculate the entropy for quality in this example Probability of each x in X for which logarithms are and therefore entropy for the set is by the formula in the question Remaining tasks are to iterate this process for each attribute to form the nodes of the tree
2374.0,3.0,"<p>Given the advantage AI already has over human intelligence, one could imagine a relatively weak strong-AI (barely human intelligence) still outperforming a segment of the human scientist population in terms of scientific discoveries per year (or hour).</p>

<p>Will AIs be doing most of the science in 50 years?</p>
",,2,2016-11-23T02:09:13.923,,2373,2016-11-25T10:59:40.203,,,,,3748.0,,1,2,<control-problem><world-knowledge>,Will AIs make most of the scientific discoveries in 50 years?,115.0,38.66,13.93,10.42,0.0,0.0,8.0,Given the advantage AI already has over human intelligence one could imagine a relatively weak strongAI barely human intelligence still outperforming a segment of the human scientist population in terms of scientific discoveries per year or hour Will AIs be doing most of the science in 50 years
,,"<p>According to <a href=""http://www.kurzweilai.net/"" rel=""nofollow noreferrer"">Ray Kurzweil</a>, a prominent AI researcher, yes. In his book <em>The Singularity is Near</em> he predicts that AIs will take over developing other AIs in about 30 years, after which human intelligence will become marginalised.</p>
",,0,2016-11-23T13:33:52.427,,2374,2016-11-23T13:33:52.427,,,,,3601.0,2373.0,2,4,,,,52.7,12.65,10.96,0.0,0.0,5.0,According to Ray Kurzweil a prominent AI researcher yes In his book The Singularity is Near he predicts that AIs will take over developing other AIs in about 30 years after which human intelligence will become marginalised
,,"<p>Input -> Prediction -> Output -> Input -> Prediction -> Output -> Input -> ...</p>

<p>AGI can easily determine which input is true/real. It will use the same method which every organism uses: any input is true and real, unless you misidentified some other stuff as ""input"".</p>

<p>I would define input as: what crosses the boundary and enters your mind from outside of your mind. The minimum hardwired check is to make sure that signals generated inside a mind are not misidentified as coming from outside (aka ""I hear voices""). That's all. This is where the blockchain of truth begins and where it ends.</p>

<p>An Internet article? The input to AI is rather: one of AI's network interfaces received many bytes. Once it's verified they are from the network, and not imaginary, they cannot be unreal or untrue in any meaningful way. By that definition of input, it is in fact the <em>only</em> thing we can be sure is true and real.</p>

<p>Of course AI will likely form hypotheses regarding these bytes that happen to contain ASCII strings like ""Trump"", ""John Smith"", ""ice balls on Siberian beaches"". Then AI will hopefully make predictions based on these hypotheses, maybe interact, maybe get some new input, reject the hypothesis and make a new one, rinse and repeat.</p>

<p>The first hypothesis will be super-naive, but the hundredth, the thousandth?</p>

<p>If you end this process prematurely - maybe for lack of processing power -  you will get something you called a ""<em>belief</em>"". (Like a belief that some emotional web page might actually reveal a significant truth about our political system.) That <em>belief</em> is a synonym of ""tired with trying new hypotheses, will stick to this one"". Typical human thing. AI will have less of that, I hope, due to having much much more processing capabilities. AI will stick less to the high-school-level truth that you should assign great credibility to statements written in a form of a newspaper article, it will hopefully form more and more generations of hypotheses, and check them.</p>

<p>In effect AI will depend less on <em>believing</em> various statements generated in the outside world.</p>
",,0,2016-11-23T15:14:34.647,,2375,2016-11-23T15:14:34.647,,,,,3803.0,2353.0,2,0,,,,62.58,11.37,9.18,0.0,0.0,84.0,Input Prediction Output Input Prediction Output Input AGI can easily determine which input is truereal It will use the same method which every organism uses any input is true and real unless you misidentified some other stuff as input I would define input as what crosses the boundary and enters your mind from outside of your mind The minimum hardwired check is to make sure that signals generated inside a mind are not misidentified as coming from outside aka I hear voices Thats all This is where the blockchain of truth begins and where it ends An Internet article The input to AI is rather one of AIs network interfaces received many bytes Once its verified they are from the network and not imaginary they cannot be unreal or untrue in any meaningful way By that definition of input it is in fact the only thing we can be sure is true and real Of course AI will likely form hypotheses regarding these bytes that happen to contain ASCII strings like Trump John Smith ice balls on Siberian beaches Then AI will hopefully make predictions based on these hypotheses maybe interact maybe get some new input reject the hypothesis and make a new one rinse and repeat The first hypothesis will be supernaive but the hundredth the thousandth If you end this process prematurely maybe for lack of processing power you will get something you called a belief Like a belief that some emotional web page might actually reveal a significant truth about our political system That belief is a synonym of tired with trying new hypotheses will stick to this one Typical human thing AI will have less of that I hope due to having much much more processing capabilities AI will stick less to the highschoollevel truth that you should assign great credibility to statements written in a form of a newspaper article it will hopefully form more and more generations of hypotheses and check them In effect AI will depend less on believing various statements generated in the outside world
,,"<p>I have considered much of the responses here, and I would suggest that most people here have missed the point when answering the question about emotions.</p>

<p>The problems is, scientists keep looking for a single solution as to what emotions are. This is akin to looking for a single shape that will fit all different shaped slots.</p>

<p>Also, what is ignored is that animals are just as capable of emotions and emotional states as we are:</p>

<p>When looking on Youtube for insects fighting each other, or competing or courting, it should be clear that simple creatures experience them too!</p>

<p>When I challenge people about emotions, I suggest to them to go to Corinthians 13 - which describes the attributes of love. If you consider all those attributes, one should notice that an actual ""feeling"" is not required for fulfilling any of them.</p>

<p>Therefore, the suggestion that a psychopath lacks emotions, and so he commits crimes or other pursuits outside of ""normal"" boundaries is far from true, especially when one considers the various records left to us from court cases and perhaps psychological evaluation - which show us that they do act out of ""strong"" emotions.</p>

<p>It should be considered that a psychopath's behaviour is motivated out of negative emotions and emotional states with a distinct lack of or disregard of morality and a disregard of conscience. Psychopaths ""enjoy"" what they do.</p>

<p>I am strongly suggesting to all that we are blinded by our reasoning, and by the reasoning of others.</p>

<p>Though I do agree with the following quote mentioned before: -</p>

<p>Dave H. wrote:</p>

<blockquote>
  <p>From a computational standpoint, emotions represent global state that influences a lot of other processing. Hormones etc. are basically
  just implementation. A sentient or sapient computer certainly could
  experience emotions, if it was structured in such a way as to have
  such global states affecting its thinking.</p>
</blockquote>

<p>However, his reasoning below it (that quote) is also seriously flawed.</p>

<p>Emotions are both active and passive: They are triggered by thoughts and they trigger our thoughts; Emotions are a mental state and a behaviourial quality; Emotions react to stimuli or measure our responses to them; Emotions are independant regulators and moderators; Yet they provoke our focus and attention to specific criteria; and they help us when intuition and emotion agree or they hinder us when conscience or will clash.</p>

<p>A computer has the same potential as us to feel emotions, but the skill of implementing emotions is much more sophisticated than the one solution fits all answer people are seeking here.</p>

<p>Also, if anyone argues that emotions are simply ""states"" where a response or responses can be designed around it, really does not understand the complexity of emotions; the ""freedom"" emotions and thoughts have independently of each other; or what constitutes true thought!</p>

<p>Programmers and scientists are notorious for ""simulating"" the real experiences of emotions or intelligence, without understanding the intimate complexities; Thinking that in finding the perfect simulation they have ""discovered"" the real experience.</p>

<p>The Psi-theory seems to adequately give a proper understanding of the matter: <a href=""https://en.wikipedia.org/wiki/Psi-theory"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Psi-theory</a></p>

<p>So I would say that the simulation of emotional states ""is"" equivalent to experiencing emotions, but those emotional states are far more complex than what most realise.</p>
",,0,2016-11-23T19:39:36.487,,2376,2016-11-25T22:25:32.960,2016-11-25T22:25:32.960,,3592.0,,3808.0,2277.0,2,3,,,,45.69,13.76,9.92,0.0,0.0,86.0,I have considered much of the responses here and I would suggest that most people here have missed the point when answering the question about emotions The problems is scientists keep looking for a single solution as to what emotions are This is akin to looking for a single shape that will fit all different shaped slots Also what is ignored is that animals are just as capable of emotions and emotional states as we are When looking on Youtube for insects fighting each other or competing or courting it should be clear that simple creatures experience them too When I challenge people about emotions I suggest to them to go to Corinthians 13 which describes the attributes of love If you consider all those attributes one should notice that an actual feeling is not required for fulfilling any of them Therefore the suggestion that a psychopath lacks emotions and so he commits crimes or other pursuits outside of normal boundaries is far from true especially when one considers the various records left to us from court cases and perhaps psychological evaluation which show us that they do act out of strong emotions It should be considered that a psychopaths behaviour is motivated out of negative emotions and emotional states with a distinct lack of or disregard of morality and a disregard of conscience Psychopaths enjoy what they do I am strongly suggesting to all that we are blinded by our reasoning and by the reasoning of others Though I do agree with the following quote mentioned before Dave H wrote From a computational standpoint emotions represent global state that influences a lot of other processing Hormones etc are basically just implementation A sentient or sapient computer certainly could experience emotions if it was structured in such a way as to have such global states affecting its thinking However his reasoning below it that quote is also seriously flawed Emotions are both active and passive They are triggered by thoughts and they trigger our thoughts Emotions are a mental state and a behaviourial quality Emotions react to stimuli or measure our responses to them Emotions are independant regulators and moderators Yet they provoke our focus and attention to specific criteria and they help us when intuition and emotion agree or they hinder us when conscience or will clash A computer has the same potential as us to feel emotions but the skill of implementing emotions is much more sophisticated than the one solution fits all answer people are seeking here Also if anyone argues that emotions are simply states where a response or responses can be designed around it really does not understand the complexity of emotions the freedom emotions and thoughts have independently of each other or what constitutes true thought Programmers and scientists are notorious for simulating the real experiences of emotions or intelligence without understanding the intimate complexities Thinking that in finding the perfect simulation they have discovered the real experience The Psitheory seems to adequately give a proper understanding of the matter httpsenwikipediaorgwikiPsitheory So I would say that the simulation of emotional states is equivalent to experiencing emotions but those emotional states are far more complex than what most realise
,,"<p>I strongly disagree with all of the aforementioned answers for this reason: -
If we, as humans can be fooled and disceived by what ""we"" consider a good sources of news, how can an artificially intelligent computer have any chance?</p>

<p>However, the challenge would be that an AI would have to be able to ""test"" a source of information against a known medium in order to <em>get to the truth</em>. This is a far different dynamic set of circumstances than what has been touted above.</p>

<p>For example, if it was claimed by a woman that a man raped her - which was not reported to the police - it is not enough to compare one person's statements to another in order to determine truth. This is because collusion, influenced or coherced third parties, mistaken perceptions and false beliefs would give false positives.</p>

<p>However, if an AI could establish from her statement that on the day she claimed to have been raped, that the alleged assailant was incapacitated while in her company, until she left his home, because the police report stated that she was upset with the assailant because he was asleep because of drugs during her whole stay. But, this police report comes from an independent source who states, Mr. ""x"" was asleep that day.</p>

<p>Doing a strict textual check is not going to give the correct answers. analysing her friends and associattes chatter could also confirm a false report as being true.</p>

<p>Therefore, an AI has to have the ability to ""test"" written reports outside of the criteria of what was spoken.</p>
",,0,2016-11-23T20:31:29.980,,2377,2016-11-23T20:31:29.980,,,,,3809.0,2353.0,2,1,,,,55.88,10.45,9.65,0.0,0.0,37.0,I strongly disagree with all of the aforementioned answers for this reason If we as humans can be fooled and disceived by what we consider a good sources of news how can an artificially intelligent computer have any chance However the challenge would be that an AI would have to be able to test a source of information against a known medium in order to get to the truth This is a far different dynamic set of circumstances than what has been touted above For example if it was claimed by a woman that a man raped her which was not reported to the police it is not enough to compare one persons statements to another in order to determine truth This is because collusion influenced or coherced third parties mistaken perceptions and false beliefs would give false positives However if an AI could establish from her statement that on the day she claimed to have been raped that the alleged assailant was incapacitated while in her company until she left his home because the police report stated that she was upset with the assailant because he was asleep because of drugs during her whole stay But this police report comes from an independent source who states Mr x was asleep that day Doing a strict textual check is not going to give the correct answers analysing her friends and associattes chatter could also confirm a false report as being true Therefore an AI has to have the ability to test written reports outside of the criteria of what was spoken
,,"<p>I don't think so, it is not the first but actually the third wave of neural networks. It's doing better than earlier two as we have much more amount of data as well as computational power now. 
Take a look at this video .....
<a href=""https://www.youtube.com/watch?v=furfdqtdAvc&amp;t=39s"" rel=""nofollow noreferrer"">According to Douglas Adams’s famous “Hitchhiker’s Guide to the Galaxy” after 7.5 millions years of work the “Deep Thought” computer categorically found out that 42 is the “Answer to the Ultimate Question of Life, the Universe, and Everything” (although unfortunately, no one knows exactly what that question was).</a></p>
",,0,2016-11-24T00:12:22.560,,2378,2016-11-24T00:12:22.560,,,,,3812.0,2373.0,2,0,,,,61.46,11.2,9.57,0.0,0.0,17.0,I dont think so it is not the first but actually the third wave of neural networks Its doing better than earlier two as we have much more amount of data as well as computational power now Take a look at this video According to Douglas Adams’s famous “Hitchhiker’s Guide to the Galaxy” after 75 millions years of work the “Deep Thought” computer categorically found out that 42 is the “Answer to the Ultimate Question of Life the Universe and Everything” although unfortunately no one knows exactly what that question was
,,"<p>Current Computing relates 0 or 1 to another 0 or 1 with layers upon layers of building blocks built on this relationship.</p>

<p>In future AI will relate A to B, be they numbers, patterns of coded instructions or some other form of more complicated constructs at a hardware  (or closer to it) level than is currently possible and, due to the inherent perfect recall and potentially massive memory storage of AI they will most definitely be bringing together and relating a vastly more broad and organised collection of knowledge than it is possible for any human to even contemplate consciously. There are some ideas that would argue that point; universal mind, spiritual revelation and morphic resonance which I do personally agree with to some degree and can imagine being particularly difficult to represent in a computational format.</p>

<p>Pattern spotting and relating, organising and computing potentials... computers are already better at all these things than most people try to be. It will not be long, i think, before they can ""invent"" something ""new"".</p>

<p>There are already attempts at AI in specialised fields of knowledge, human speech, various games, medical diagnostics and learning  etc. It will be when these specialised AIs can ""compare notes"" about the various methodologies that have been the most productive or rewarding, in whichever form these take for them, and accordingly update their own ontologies that the true explosion of ""Intelligence"" will occur.</p>
",,0,2016-11-24T23:41:50.683,,2380,2016-11-25T10:59:40.203,2016-11-25T10:59:40.203,,3598.0,,3598.0,2373.0,2,0,,,,41.63,13.59,11.01,0.0,0.0,33.0,Current Computing relates 0 or 1 to another 0 or 1 with layers upon layers of building blocks built on this relationship In future AI will relate A to B be they numbers patterns of coded instructions or some other form of more complicated constructs at a hardware or closer to it level than is currently possible and due to the inherent perfect recall and potentially massive memory storage of AI they will most definitely be bringing together and relating a vastly more broad and organised collection of knowledge than it is possible for any human to even contemplate consciously There are some ideas that would argue that point universal mind spiritual revelation and morphic resonance which I do personally agree with to some degree and can imagine being particularly difficult to represent in a computational format Pattern spotting and relating organising and computing potentials computers are already better at all these things than most people try to be It will not be long i think before they can invent something new There are already attempts at AI in specialised fields of knowledge human speech various games medical diagnostics and learning etc It will be when these specialised AIs can compare notes about the various methodologies that have been the most productive or rewarding in whichever form these take for them and accordingly update their own ontologies that the true explosion of Intelligence will occur
,1.0,"<p><a href=""http://opencog.org"" rel=""nofollow noreferrer"">OpenCog</a> is an open source AGI-project co-founded by the mercurial AI researcher <a href=""https://en.wikipedia.org/wiki/Ben_Goertzel"" rel=""nofollow noreferrer"">Ben Goertzel</a>. Now Ben Goertzel writes a lot of stuff, some of it <a href=""http://multiverseaccordingtoben.blogspot.de/2010/11/psi-debate-continues-goertzel-on.html"" rel=""nofollow noreferrer"">really</a> <a href=""http://multiverseaccordingtoben.blogspot.de/2016/10/semrem-search-for-extraterrestrial.html"" rel=""nofollow noreferrer"">whacky</a>. On the other hand he is clearly very intelligent and has thought deeply about AI for many decades. </p>

<p>So I wonder whether it would be worth my while to dig into the <a href=""http://wiki.opencog.org/w/OpenCog_Prime"" rel=""nofollow noreferrer"">theoretical ideas</a> behind open cog. </p>

<p>My question is what the general ideas behind open cog are and whether you would endorse it as a insightful take on AGI. I'm especially interested in whether the general framework still makes sense in the light of recent advances.  </p>
",,5,2016-11-25T10:03:29.947,,2381,2016-11-25T23:47:07.920,,,,,2227.0,,1,2,<agi><architecture>,What are the parts and the general framework of OpenCog?,52.0,62.38,9.34,10.11,0.0,0.0,10.0,OpenCog is an open source AGIproject cofounded by the mercurial AI researcher Ben Goertzel Now Ben Goertzel writes a lot of stuff some of it really whacky On the other hand he is clearly very intelligent and has thought deeply about AI for many decades So I wonder whether it would be worth my while to dig into the theoretical ideas behind open cog My question is what the general ideas behind open cog are and whether you would endorse it as a insightful take on AGI Im especially interested in whether the general framework still makes sense in the light of recent advances
,,"<p>There are a variety of possible things that could be wrong, but to answer the short question specifically:</p>

<p>relu networks are turing complete (well, if you put them in an RNN so they can compute indefinitely, anyway). for any computation, you can devise an rnn that will perform it. </p>

<p>as a proof of this, here is a relu neuron that implements nor, which with recursion (cs)/recurrence (nns) and routing matrices is enough to <a href=""https://en.m.wikipedia.org/wiki/NOR_logic"" rel=""nofollow noreferrer"">implement a turing machine</a>:</p>

<p>W:
[ -20<br>
  -20 ]
b:
[ 1 ]</p>

<p>o = max(Wx + b, 0)</p>

<p>however, gradient descent is a finnicky way to search for rnns. there are a wide variety of ways that it might have been failing. In general, once you have <em>very thoroughly checked your gradient</em>, I'd make sure to use Adam as the optimizer and then play with the hyperparameters endlessly until I find an incantation that works. <a href=""http://russellsstewart.com/notes/0.html"" rel=""nofollow noreferrer"">http://russellsstewart.com/notes/0.html</a> <a href=""http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html?m=1"" rel=""nofollow noreferrer"">http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html?m=1</a></p>
",,1,2016-11-25T23:02:48.910,,2384,2016-11-27T23:26:44.763,2016-11-27T23:26:44.763,,3856.0,,3856.0,2349.0,2,1,,,,50.5,14.69,9.76,0.0,0.0,60.0,There are a variety of possible things that could be wrong but to answer the short question specifically relu networks are turing complete well if you put them in an RNN so they can compute indefinitely anyway for any computation you can devise an rnn that will perform it as a proof of this here is a relu neuron that implements nor which with recursion csrecurrence nns and routing matrices is enough to implement a turing machine W 20 20 b 1 o maxWx b 0 however gradient descent is a finnicky way to search for rnns there are a wide variety of ways that it might have been failing In general once you have very thoroughly checked your gradient Id make sure to use Adam as the optimizer and then play with the hyperparameters endlessly until I find an incantation that works httprussellsstewartcomnotes0html httpyyueblogspotcom201501abriefoverviewofdeeplearninghtmlm1
,,"<p>While the experiment I link here is a very narrow awareness, it is as such: <a href=""http://www.sciencealert.com/a-robot-has-just-passed-a-classic-self-awareness-test-for-the-first-time"" rel=""nofollow noreferrer"">A robot has just passed a classic self-awareness test for the first time</a>. If the agent can prove something to itself, we can then say it ""Knows."" Of course the level of awareness you're asking about is very tricky.</p>

<p>In short, it can't know that what it's experiencing is real with absolute certainty because sensory of any kind can be falsified. Do you know what is true/real? You think you do but can you prove it? No. Awareness is subjective.</p>
",,0,2016-11-25T23:29:05.760,,2385,2016-11-26T14:40:15.097,2016-11-26T14:40:15.097,,8.0,,3861.0,2353.0,2,0,,,,74.59,8.05,8.8,0.0,0.0,19.0,While the experiment I link here is a very narrow awareness it is as such A robot has just passed a classic selfawareness test for the first time If the agent can prove something to itself we can then say it Knows Of course the level of awareness youre asking about is very tricky In short it cant know that what its experiencing is real with absolute certainty because sensory of any kind can be falsified Do you know what is truereal You think you do but can you prove it No Awareness is subjective
,,"<p>While my knowledge of OpenCog is very limited, you could say that yes, it does still make sense and it is insightful. I'm not certain regarding all of the components of OpenCog but I do know that at least one component is relevant (I think it's part of the MOSIS component).</p>

<p>This component is very similar to Numenta's hierarchical temporal memory which is based more on computational neuroscience than plain math; however, I would consider Nupic a more relevant project in terms of neroscience though both are attempting to emulate components of the brain. In my opinion, such projects are far more impressive than what's going on with typical convolutional neural nets, RNNs, etc. which are too loosely related to what goes on in the brain to be said to be computational neuroscience.</p>

<p>That's not to say that things like ANNs, GAs, etc etc are useless for AGI. We don't really know since we don't have an example of one.</p>
",,3,2016-11-25T23:47:07.920,,2386,2016-11-25T23:47:07.920,,,,,3861.0,2381.0,2,2,,,,56.69,10.62,8.82,0.0,0.0,25.0,While my knowledge of OpenCog is very limited you could say that yes it does still make sense and it is insightful Im not certain regarding all of the components of OpenCog but I do know that at least one component is relevant I think its part of the MOSIS component This component is very similar to Numentas hierarchical temporal memory which is based more on computational neuroscience than plain math however I would consider Nupic a more relevant project in terms of neroscience though both are attempting to emulate components of the brain In my opinion such projects are far more impressive than whats going on with typical convolutional neural nets RNNs etc which are too loosely related to what goes on in the brain to be said to be computational neuroscience Thats not to say that things like ANNs GAs etc etc are useless for AGI We dont really know since we dont have an example of one
,,"<p>Hierarchical Temporal Memory should help with this. You would have to encode the text data into SDRs. You would then have a coincidence detector. Could you get the right information back out in the way you're trying to? I think so. I'm not fully learned in HTMs yet but check out <a href=""http://numenta.org/"" rel=""nofollow noreferrer"">Nupic</a> (Open source).</p>
",,1,2016-11-26T06:05:00.057,,2387,2016-11-26T06:05:00.057,,,,,3861.0,2371.0,2,0,,,,87.72,7.45,7.88,0.0,0.0,10.0,Hierarchical Temporal Memory should help with this You would have to encode the text data into SDRs You would then have a coincidence detector Could you get the right information back out in the way youre trying to I think so Im not fully learned in HTMs yet but check out Nupic Open source
,,"<p>This question is more the province of philosophy of mind than of AI, here are some detailed answers to your question from the philosophy SE: <a href=""http://philosophy.stackexchange.com/a/35824/13808"">Is simulating emotions the same as experiencing emotions?</a>, and <a href=""http://philosophy.stackexchange.com/a/34244/13808"">What is the problem with physicalism?</a>. </p>

<p>For the record, the accepted answer (by Siri) to the question is not entirely correct (The position in that answer corresponds roughly to <a href=""http://philosophy.stackexchange.com/a/34682/13808"">John Searle's view</a> on the question, and his is a minority view): Dualists would argue that even with a perfect replication down to the chemical level of brain interactions, an AI still wouldn't experience emotions, as it lacks the purely mental substance/properties that make a mind and not a machine. </p>

<p>On the completely opposite side of the spectrum, functionalists would answer that such a perfect replication is overkill: even a suitably programmed digital computer can experience emotion, particularly if one equips it with higher-order and self-referential states.  </p>
",,1,2016-11-26T08:05:33.167,,2388,2016-11-28T02:38:33.097,2016-11-28T02:38:33.097,,2306.0,,2306.0,2277.0,2,2,,,,24.95,14.23,10.87,0.0,0.0,25.0,This question is more the province of philosophy of mind than of AI here are some detailed answers to your question from the philosophy SE Is simulating emotions the same as experiencing emotions and What is the problem with physicalism For the record the accepted answer by Siri to the question is not entirely correct The position in that answer corresponds roughly to John Searles view on the question and his is a minority view Dualists would argue that even with a perfect replication down to the chemical level of brain interactions an AI still wouldnt experience emotions as it lacks the purely mental substanceproperties that make a mind and not a machine On the completely opposite side of the spectrum functionalists would answer that such a perfect replication is overkill even a suitably programmed digital computer can experience emotion particularly if one equips it with higherorder and selfreferential states
,0.0,"<p>I am new to machine learning and I know how to implement simple neural networks using logistic regression as a cost function. But I want to know whether neural nets are used in reinforcement learning in general(and not just in special cases) ? If yes, then what cost function they use. As I already know neural nets with logistic regression are used in supervised learning and reinforcement learning is a part of unsupervised learning. There are many threads which are related to RL and neural nets but all of them are about a particular case or an algorithm and I want to know about RL in general.</p>
",,1,2016-11-26T14:53:41.257,,2389,2016-11-26T14:53:41.257,,,,,3866.0,,1,0,<neural-networks><machine-learning><reinforcement-learning><unsupervised-learning>,How are neural networks used in reinforcement learning?,52.0,58.42,9.63,8.71,0.0,0.0,8.0,I am new to machine learning and I know how to implement simple neural networks using logistic regression as a cost function But I want to know whether neural nets are used in reinforcement learning in generaland not just in special cases If yes then what cost function they use As I already know neural nets with logistic regression are used in supervised learning and reinforcement learning is a part of unsupervised learning There are many threads which are related to RL and neural nets but all of them are about a particular case or an algorithm and I want to know about RL in general
,,"<p>Some good places to start would be <a href=""https://en.wikipedia.org/wiki/Cognitive_architecture"" rel=""nofollow noreferrer"">cognitive architectures</a> and as mentioned in another answer <a href=""https://en.wikipedia.org/wiki/Intelligent_agent"" rel=""nofollow noreferrer"">intelligent agents</a>. The question is broad but you definitely want to look into <a href=""http://msl.cs.uiuc.edu/~lavalle/cs397/"" rel=""nofollow noreferrer"">planning &amp; decision making</a>. You might also want to check out the <a href=""http://numenta.org/resources/presentations/2014%20Sensory%20Motor%20Integration%20in%20HTM%20Theory.pdf"" rel=""nofollow noreferrer"">L5 and L6 layers</a> of Hierarchical Temporal Memory (As in <a href=""http://numenta.org/"" rel=""nofollow noreferrer"">Nupic</a>) as it relates to feedback, behavior and attention.</p>

<p>If I were you I'd aim for more cognitive solutions (I realize that term is a bit ambiguous itself when we talk about machines). There's also new AI initiative going on involving probabilistic programming. See <a href=""https://probmods.org/"" rel=""nofollow noreferrer"">Probabilistic Models of Cognition</a> made by Goodman (Stanford University) and Tenenbaum (MIT) or <a href=""http://www.robots.ox.ac.uk/~fwood/anglican/"" rel=""nofollow noreferrer"">Anglican</a> made by Wood (University of Oxford) et al.</p>
",,0,2016-11-26T16:34:45.107,,2390,2016-11-28T11:30:03.113,2016-11-28T11:30:03.113,,3855.0,,3861.0,2366.0,2,4,,,,51.89,12.53,10.58,0.0,0.0,21.0,Some good places to start would be cognitive architectures and as mentioned in another answer intelligent agents The question is broad but you definitely want to look into planning amp decision making You might also want to check out the L5 and L6 layers of Hierarchical Temporal Memory As in Nupic as it relates to feedback behavior and attention If I were you Id aim for more cognitive solutions I realize that term is a bit ambiguous itself when we talk about machines Theres also new AI initiative going on involving probabilistic programming See Probabilistic Models of Cognition made by Goodman Stanford University and Tenenbaum MIT or Anglican made by Wood University of Oxford et al
,1.0,"<p>My Question:<br>
Is there any good neural-network-app for iOS or Android to create, train and run neural networks? I know there's NeuralMesh for Web, but I want something similar offline.</p>
",,1,2016-11-27T00:19:27.033,1.0,2392,2016-11-28T05:30:20.157,,,,,3872.0,,1,2,<neural-networks><machine-learning><applications>,iOS/Android Neural Network App,103.0,64.71,11.88,10.17,0.0,0.0,8.0,My Question Is there any good neuralnetworkapp for iOS or Android to create train and run neural networks I know theres NeuralMesh for Web but I want something similar offline
,,"<p>You first need to express emotions, you can do that without the aid of AI, and then you need someone to perceive that expression and empathize with it.</p>

<p>If no one is there to see it, or if I am psychopath, I would probably say it doesn't have emotions. and for that, it is irrelevant/subjective.</p>

<p>If you can empathize with characters in movies who ""act"" emotions, then you get my point.</p>
",,0,2016-11-27T01:36:32.790,,2393,2016-11-27T01:36:32.790,,,,,3874.0,2277.0,2,1,,,,70.33,8.06,7.41,0.0,0.0,14.0,You first need to express emotions you can do that without the aid of AI and then you need someone to perceive that expression and empathize with it If no one is there to see it or if I am psychopath I would probably say it doesnt have emotions and for that it is irrelevantsubjective If you can empathize with characters in movies who act emotions then you get my point
,,"<p>No matter what rights it gets (as a company), it will still lack the right of not getting liquefied and all its properties transferred back to natural persons.</p>

<p>This is of course if no laws are changed.</p>

<p>To change the laws you will need to convince people that this machine is more ""life"" worthy than intelligent animals, and hope that people will deal with them better than they did with dolphins and chimps.</p>

<p>As I see it, machines can easily get the same or better rights then companies, but will always be under the mercy of the less intelligent man. (that is if things went peacefully :) )</p>
",,0,2016-11-27T01:53:19.920,,2394,2016-11-27T01:53:19.920,,,,,3874.0,2356.0,2,3,,,,75.34,9.17,8.26,0.0,0.0,16.0,No matter what rights it gets as a company it will still lack the right of not getting liquefied and all its properties transferred back to natural persons This is of course if no laws are changed To change the laws you will need to convince people that this machine is more life worthy than intelligent animals and hope that people will deal with them better than they did with dolphins and chimps As I see it machines can easily get the same or better rights then companies but will always be under the mercy of the less intelligent man that is if things went peacefully
,,"<p>You can use <a href=""http://neuroph.sourceforge.net"" rel=""nofollow noreferrer"">Neuroph</a> to develop and train your network and add in app via NetBeans. </p>

<p><strong>check this link</strong></p>

<p><a href=""http://neuroph.sourceforge.net/tutorials/android_image_recognition_using_neuroph.htm"" rel=""nofollow noreferrer"">Creating Android image recognition application using NetBeans and Neuroph</a>.</p>
",,1,2016-11-27T10:02:45.683,,2395,2016-11-28T05:30:20.157,2016-11-28T05:30:20.157,,3771.0,,3885.0,2392.0,2,1,,,,65.73,12.35,11.1,0.0,0.0,2.0,You can use Neuroph to develop and train your network and add in app via NetBeans check this link Creating Android image recognition application using NetBeans and Neuroph
,,"<p>The human brain contains billions of neurons, which means we won't be making one tomorrow. However, technology tends to advance in an exponential manner, and that may soon be a real possibility. Also, the idea of making an artificial human brain would not only take more neurons than a current average computer could process, or we could make outside of computers, but we also need an understanding of the human brain. There is only one animal with neurons that we have completed a full connectome of and that is the Caenorhabditis elegans (roundworm) and it has less than 500 neurons. It may be a while before we actually make a human brain, but within 30 years is a reasonable estimation with the rate that technology improves now.</p>
",,2,2016-11-27T16:38:38.143,,2396,2016-11-27T16:38:38.143,,,,,3887.0,2330.0,2,0,,,,54.15,10.74,9.75,0.0,0.0,15.0,The human brain contains billions of neurons which means we wont be making one tomorrow However technology tends to advance in an exponential manner and that may soon be a real possibility Also the idea of making an artificial human brain would not only take more neurons than a current average computer could process or we could make outside of computers but we also need an understanding of the human brain There is only one animal with neurons that we have completed a full connectome of and that is the Caenorhabditis elegans roundworm and it has less than 500 neurons It may be a while before we actually make a human brain but within 30 years is a reasonable estimation with the rate that technology improves now
,,"<p>You shouldn't use a single number for the word, perhaps a number for each letter. Since B isn't the midpoint of A and C, the numbers really shouldn't be 1, 2, 3, etc. One large but effective way of converting is the letter a is 10000000000000000000000000 such that there are 26 digits, and each digit  is a letter, so 0000100000... would be E.</p>
",,0,2016-11-27T16:41:09.420,,2397,2016-11-27T16:41:09.420,,,,,3887.0,2367.0,2,0,,,,97.74,8.18,7.68,0.0,0.0,16.0,You shouldnt use a single number for the word perhaps a number for each letter Since B isnt the midpoint of A and C the numbers really shouldnt be 1 2 3 etc One large but effective way of converting is the letter a is 10000000000000000000000000 such that there are 26 digits and each digit is a letter so 0000100000 would be E
,0.0,"<p>I am researching <strong>Cellular Neural Network (CNN)</strong> and have already read <strong>Chua</strong>'s two article (<strong>1988</strong>). In CNN, the cell is only in relation with its neighbors. So its is easy to use it for real time image processing. In CNN, image processing is performed with only <strong>19 numbers</strong> (two 3x3 matrix called A and B and one bias value). </p>

<p>I wonder how can we call CNN as a <strong><em>neural network</em></strong>. Because there is no learning algorithm in CNN neither <strong>supervised</strong> nor <strong>unsupervised</strong>. </p>
",,0,2016-11-27T16:53:45.910,,2398,2016-11-27T16:53:45.910,,,,,3763.0,,1,2,<machine-learning><unsupervised-learning>,Is Cellular Neural Network (CNN) Neural Network?,93.0,74.49,8.46,9.32,0.0,0.0,15.0,I am researching Cellular Neural Network CNN and have already read Chuas two article 1988 In CNN the cell is only in relation with its neighbors So its is easy to use it for real time image processing In CNN image processing is performed with only 19 numbers two 3x3 matrix called A and B and one bias value I wonder how can we call CNN as a neural network Because there is no learning algorithm in CNN neither supervised nor unsupervised
,,"<p>IMHO</p>

<p><strong>Definitely, yes!</strong>
Everything that person feels (physically or mentally) can be discovered by chemical signals processing in his body or brain. If we understand the policy and nature of such signals, we can program it.</p>

<p>There are a lot of pseudo-psychology and psychology works on this sphere, if you interested, I can suggest you:</p>

<blockquote>
  <p>1) <strong>Cognitive Psychology (Robert L. Solso)</strong></p>
</blockquote>

<p>describes cognitive apparat of human's mind in a simple words;</p>

<blockquote>
  <p>2) <strong>The Psychology of Emotions (Carroll E. Izard)</strong></p>
</blockquote>

<p>thorougly describes every kind of emotion by its looking on the human (both child and adult) face, low-level cognitive mechanism, related or adjacent emotions;</p>

<blockquote>
  <p>3) Books by <strong>Paul Ekman (""Telling Lies"", ""Emotions Revealed"",
  ""Unmasking the Face"")</strong></p>
</blockquote>

<p>practical detecting of human emotions by microexpressions language on face and body.</p>
",,0,2016-11-27T19:20:44.937,,2399,2016-11-27T19:20:44.937,,,,,3891.0,2277.0,2,1,,,,41.5,14.21,11.4,0.0,0.0,39.0,IMHO Definitely yes Everything that person feels physically or mentally can be discovered by chemical signals processing in his body or brain If we understand the policy and nature of such signals we can program it There are a lot of pseudopsychology and psychology works on this sphere if you interested I can suggest you 1 Cognitive Psychology Robert L Solso describes cognitive apparat of humans mind in a simple words 2 The Psychology of Emotions Carroll E Izard thorougly describes every kind of emotion by its looking on the human both child and adult face lowlevel cognitive mechanism related or adjacent emotions 3 Books by Paul Ekman Telling Lies Emotions Revealed Unmasking the Face practical detecting of human emotions by microexpressions language on face and body
,4.0,"<p>Could an Artificial Intelligence be able to interact (see, talk, etc.) with someone even when there's no power cord connected to the machine it's running on? Might it find some way to generate its own electricity to power that computer?</p>

<p><a href=""https://i.stack.imgur.com/09gEt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/09gEt.png"" alt=""computer running without power""></a> </p>
",,5,2016-11-28T04:24:20.457,,2400,2016-12-03T12:01:02.833,2016-11-28T15:49:53.177,,75.0,,3896.0,,1,-5,<strong-ai>,Is it possible for an AI to work in a computer without the power cord being plugged in?,84.0,57.98,10.66,9.82,0.0,0.0,9.0,Could an Artificial Intelligence be able to interact see talk etc with someone even when theres no power cord connected to the machine its running on Might it find some way to generate its own electricity to power that computer
,,"<p>If your ""AI"" doesn't have the ability to move and perform physical manipulations in the real world then there is no way it could do something like this.</p>
",,2,2016-11-28T05:37:05.740,,2401,2016-11-28T05:37:05.740,,,,,1462.0,2400.0,2,2,,,,59.98,8.88,7.84,0.0,0.0,4.0,If your AI doesnt have the ability to move and perform physical manipulations in the real world then there is no way it could do something like this
,,"<p>If the computer is unplugged, the AI is clinically dead.</p>

<p>However, you can have a RaspberryPi on solar cells.</p>

<p>Tesla car is an AI moving and seeing while unplugged (from wall). but you have to have some sort of energy. For AI that lacks metabolism, solar/wind energy can be an alternative.</p>
",,0,2016-11-28T13:35:27.117,,2402,2016-11-28T13:35:27.117,,,,,3874.0,2400.0,2,0,,,,69.58,8.21,8.79,0.0,0.0,11.0,If the computer is unplugged the AI is clinically dead However you can have a RaspberryPi on solar cells Tesla car is an AI moving and seeing while unplugged from wall but you have to have some sort of energy For AI that lacks metabolism solarwind energy can be an alternative
,1.0,"<p>I am new to deep learning. </p>

<p>I have a dataset of images of varying dimensions of a certain object. A few images of the object are also in varying orientations. The objective is to learn the features of the object (using Autoencoders). </p>

<p>Is it possible to create a network with layers that account for varying dimensions and orientations of the input image, or should I strictly consider a dataset containing images of uniform dimensions? What is the necessary criteria of an eligible dataset to be used for training a Deep Network in general.</p>

<p>The idea is, I want to avoid pre-processing my dataset by normalizing it via scaling, re-orienting operations etc. I would like my network to account for the variability in dimensions and orientations. Please point me to resources for the same. </p>
",,2,2016-11-28T13:55:06.963,,2403,2016-12-30T19:57:38.110,,,,,3907.0,,1,1,<deep-learning><deep-network><datasets>,Dataset containing images of varying dimensions and orientations,49.0,56.45,10.49,9.24,0.0,0.0,16.0,I am new to deep learning I have a dataset of images of varying dimensions of a certain object A few images of the object are also in varying orientations The objective is to learn the features of the object using Autoencoders Is it possible to create a network with layers that account for varying dimensions and orientations of the input image or should I strictly consider a dataset containing images of uniform dimensions What is the necessary criteria of an eligible dataset to be used for training a Deep Network in general The idea is I want to avoid preprocessing my dataset by normalizing it via scaling reorienting operations etc I would like my network to account for the variability in dimensions and orientations Please point me to resources for the same
,1.0,"<p>A single neuron is capable of forming a decision boundary between linearly seperable data. Is there any intuition as to how many, and in what configuration, would be necessary to correctly approximate a sinusoidal decision boundary?</p>

<p>Thanks</p>
",,3,2016-11-28T15:18:10.110,,2404,2017-01-30T16:08:39.257,,,,,3908.0,,1,5,<neural-networks><hidden-layers><neurons><artificial-neuron>,How many nodes/hidden layers are required to solve a classification problem where the boundary is a sinusoidal function?,117.0,27.32,14.56,12.66,0.0,0.0,4.0,A single neuron is capable of forming a decision boundary between linearly seperable data Is there any intuition as to how many and in what configuration would be necessary to correctly approximate a sinusoidal decision boundary Thanks
,0.0,"<p>I am using policy gradients in my reinforcement learning algorithm, and occasionally my environment provides a severe penalty when a wrong move is made. I'm using a neural network with stochastic gradient decent to learn the policy. To do this, my loss is essentially the cross-entropy loss of the action distribution multiplied by the discounted rewards, where most often the rewards are positive. </p>

<p>But how do I handle negative rewards? Since the loss will occasionally go negative, it will think these actions are very good, and will strengthen the weights in the direction of the penalties. Is this correct, and if so, what can I do about it?</p>

<p>Edit:
In thinking about this a little more, SGD doesn't necessarily directly weaken weights, it only strengthens weights in the direction of the gradient and as a side-effect, weights get diminished for other states outside the gradient, correct? So I can simply set reward=0 when the reward is negative, and those states will be ignored in the gradient update. It still seems unproductive to not account for states that are really bad, and it'd be nice to include them somehow. Unless I'm misunderstanding something fundamental here.</p>
",,0,2016-11-29T06:10:54.993,,2405,2016-11-29T06:19:06.290,2016-11-29T06:19:06.290,,3920.0,,3920.0,,1,1,<reinforcement-learning>,Negative reward (penalty) in policy gradient reinforcement learning,43.0,51.78,12.18,10.05,0.0,0.0,31.0,I am using policy gradients in my reinforcement learning algorithm and occasionally my environment provides a severe penalty when a wrong move is made Im using a neural network with stochastic gradient decent to learn the policy To do this my loss is essentially the crossentropy loss of the action distribution multiplied by the discounted rewards where most often the rewards are positive But how do I handle negative rewards Since the loss will occasionally go negative it will think these actions are very good and will strengthen the weights in the direction of the penalties Is this correct and if so what can I do about it Edit In thinking about this a little more SGD doesnt necessarily directly weaken weights it only strengthens weights in the direction of the gradient and as a sideeffect weights get diminished for other states outside the gradient correct So I can simply set reward0 when the reward is negative and those states will be ignored in the gradient update It still seems unproductive to not account for states that are really bad and itd be nice to include them somehow Unless Im misunderstanding something fundamental here
,,"<p>As Matthew Graves explained in another answer No free lunch theorem confirms the flexibility - efficiency trade-off. However, this theorem is describing a situation where you have a set of completely independent tasks. This often doesn't hold, as many different problems are equivalent in their core or at least have some overlap. Then you can do something called ""transfer learning"", which means that by training to solve one task you also learn something about solving another one (or possibly multiple different tasks).</p>

<p>For example in <a href=""https://arxiv.org/abs/1511.06295"" rel=""nofollow noreferrer"">Policy Distillation</a> by Rusu et al. they managed to ""distill"" knowledge from different expert networks into one general network which in the end outperformed each of the experts. The experts were trained for specific tasks while the generalist learned the final policy from these ""teachers"".</p>
",,0,2016-11-29T13:56:05.997,,2407,2016-11-29T13:56:05.997,,,,,3855.0,1989.0,2,3,,,,44.34,14.16,11.16,0.0,0.0,21.0,As Matthew Graves explained in another answer No free lunch theorem confirms the flexibility efficiency tradeoff However this theorem is describing a situation where you have a set of completely independent tasks This often doesnt hold as many different problems are equivalent in their core or at least have some overlap Then you can do something called transfer learning which means that by training to solve one task you also learn something about solving another one or possibly multiple different tasks For example in Policy Distillation by Rusu et al they managed to distill knowledge from different expert networks into one general network which in the end outperformed each of the experts The experts were trained for specific tasks while the generalist learned the final policy from these teachers
,,"<p>I assume by your use of the term ""plugged in"", you are referring to an <em>electron based computer</em> - the usual definition of, or what is commonly/popularly considered to be, a computer - which is hosting the AI. However, what your definition of <em>plugged in</em> is unclear. </p>

<p>If you mean literally, and <em>physically</em>, plugged in to the wall then <strong>yes</strong>, as <a href=""http://ai.stackexchange.com/questions/2400/is-it-possible-for-an-ai-to-work-in-a-computer-without-the-power-cord-being-plug#answer-2401"">Aus points out</a>, because one can use a battery, or have a solar powered circuit, or petrol/diesel/steam/water/wind/hydrogen/etc. powered engine, running a generator/alternator which can eventually supply a voltage and current to power the circuitry. However, in these examples, you have merely moved the point of generation, from a remote power station to a locally generated source of electricity - so it is still, in effect, <em>plugged in</em> (but to a local source). So, in that sense, the answer is <strong>no</strong>. </p>

<p>If you mean no external power source, <strong><em>what so ever</em></strong>, then the answer is most certainly <strong>no</strong>, as, again as <a href=""http://ai.stackexchange.com/questions/2400/is-it-possible-for-an-ai-to-work-in-a-computer-without-the-power-cord-being-plug#answer-2401"">Aus has already said</a>, there is no way for electrons to be pushed around the circuit, and therefore the circuit is dead<sup>1</sup>.</p>

<p>If you mean, as you said in your <a href=""http://ai.stackexchange.com/questions/2400/is-it-possible-for-an-ai-to-work-in-a-computer-without-the-power-cord-being-plug#comment-2620"">comment</a> that the AI has created its own electrical power source - although as <a href=""http://ai.stackexchange.com/questions/2400/is-it-possible-for-an-ai-to-work-in-a-computer-without-the-power-cord-being-plug#answer-2401"">Ankur says in their answer</a>, this implies that the AI has the ability for its I/O manipulate its environment<sup>2</sup> - then as in the first paragraph, you have simply moved the point of the power source, so one can say that it is again now <em>plugged in</em>, again to a local energy source, and so yet again the answer is <strong>no</strong>.</p>

<hr>

<p>However, if you are <em>not</em> referring to an electronic based computer, which is hosting the AI, but are, instead, referring to a <em>hypothetical biological</em> computer, then the answer is probably <strong>yes</strong> - in a sense. Although, that biological computer would still require energy input, as energy can't just be ""magicked"" out of thin air. It would need to be converted from one source into a from that the biological computer can use. That energy could come from a variety of sources, be it from sunlight, food,  heat (sunlight, deep sea vents, etc), so one could argue that it is still <em>plugged in</em> to an <em>energy source</em> and so yet again, the answer is <strong>no</strong>.</p>

<hr>

<p>To be honest, in order to give an accurate, and sensible answer, you really need to reword your question, clarify what you mean and what your definitions of <em>plugged in</em> and <em>AI host</em> are. Unfortunately, your question, as it stands, is rather unanswerable, in as much that it is difficult to give a definitive answer.</p>

<hr>

<h3>Footnote</h3>

<p><sup>1</sup>Likewise, if you mean by <em>plugged in</em> in the conventional sense, and therefore <em>unplugged</em> means that the electronics hosting the AI has <em>no power</em> then the answer is <strong>no</strong>.</p>

<p><sup>2</sup> However, assuming that the AI <em>can</em> manipulate its environment, in order for the AI to do able to do so, it will have be able to assemble the external power source <em>prior to being unplugged</em>. So... if you mean ""Can it create its own power, after it was powered off (without having had time to asemble an external power source to act as a backup)?"" then the answer would be <strong>no</strong>.</p>
",,1,2016-11-29T23:23:39.177,,2408,2016-11-29T23:54:13.347,2016-11-29T23:54:13.347,,3771.0,,3771.0,2400.0,2,0,,,,49.99,10.11,8.49,0.0,0.0,108.0,I assume by your use of the term plugged in you are referring to an electron based computer the usual definition of or what is commonlypopularly considered to be a computer which is hosting the AI However what your definition of plugged in is unclear If you mean literally and physically plugged in to the wall then yes as Aus points out because one can use a battery or have a solar powered circuit or petroldieselsteamwaterwindhydrogenetc powered engine running a generatoralternator which can eventually supply a voltage and current to power the circuitry However in these examples you have merely moved the point of generation from a remote power station to a locally generated source of electricity so it is still in effect plugged in but to a local source So in that sense the answer is no If you mean no external power source what so ever then the answer is most certainly no as again as Aus has already said there is no way for electrons to be pushed around the circuit and therefore the circuit is dead1 If you mean as you said in your comment that the AI has created its own electrical power source although as Ankur says in their answer this implies that the AI has the ability for its IO manipulate its environment2 then as in the first paragraph you have simply moved the point of the power source so one can say that it is again now plugged in again to a local energy source and so yet again the answer is no However if you are not referring to an electronic based computer which is hosting the AI but are instead referring to a hypothetical biological computer then the answer is probably yes in a sense Although that biological computer would still require energy input as energy cant just be magicked out of thin air It would need to be converted from one source into a from that the biological computer can use That energy could come from a variety of sources be it from sunlight food heat sunlight deep sea vents etc so one could argue that it is still plugged in to an energy source and so yet again the answer is no To be honest in order to give an accurate and sensible answer you really need to reword your question clarify what you mean and what your definitions of plugged in and AI host are Unfortunately your question as it stands is rather unanswerable in as much that it is difficult to give a definitive answer Footnote 1Likewise if you mean by plugged in in the conventional sense and therefore unplugged means that the electronics hosting the AI has no power then the answer is no 2 However assuming that the AI can manipulate its environment in order for the AI to do able to do so it will have be able to assemble the external power source prior to being unplugged So if you mean Can it create its own power after it was powered off without having had time to asemble an external power source to act as a backup then the answer would be no
,1.0,"<p>Cognitive Psychology is one of the basic sciences of artificial intelligence (AI). The founder of the psychology is <strong>Wilhelm W.(1832-1920)</strong>, who engaged in empirical methods,and was interested in the <strong><em>thinking processes</em></strong> during his scientific work.</p>

<p>According to his research,Psychology had two main leading subjects: </p>

<ol>
<li>Behaviourism.</li>
<li>Cognitivism.</li>
</ol>

<p><strong>Behaviourism:</strong> Refused the theory of the mental processes, and insisted to study the resulted action or the stimulus strictly objective. The representatives of this theory have been decreasing with time.</p>

<p><strong>Cognitive psychology:</strong> defines that the brain is an information processing device.</p>

<p>Therefore,this question is not a duplicate of this <a href=""http://ai.stackexchange.com/questions/1847/what-is-the-difference-between-artificial-intelligence-and-cognitive-science"">what-is-the-difference-between-artificial-intelligence-and-cognitive-science?</a> ,However my question is;how can we connect artificial intelligence with cognitive psychology for instance;</p>

<p><strong>Human Computing Interaction:</strong>
We may come in a contact with Humana Computer Interaction every day, because this field includes the every day use of computer for example;tapping stack exchange app on smart-phone, the user interfaces and some other expert programs which may use cognitive psychology in order to manipulate or help people. But still such tasks have got a minimal  relevant connection.</p>
",,1,2016-11-30T11:10:32.697,1.0,2409,2016-12-01T13:18:07.150,2016-11-30T11:25:15.563,,1581.0,,1581.0,,1,1,<philosophy><history><definitions><ethics><cognitive-science>,How can we connect artificial intelligence with cognitive psychology?,86.0,20.18,19.02,11.47,0.0,0.0,41.0,Cognitive Psychology is one of the basic sciences of artificial intelligence AI The founder of the psychology is Wilhelm W18321920 who engaged in empirical methodsand was interested in the thinking processes during his scientific work According to his researchPsychology had two main leading subjects Behaviourism Cognitivism Behaviourism Refused the theory of the mental processes and insisted to study the resulted action or the stimulus strictly objective The representatives of this theory have been decreasing with time Cognitive psychology defines that the brain is an information processing device Thereforethis question is not a duplicate of this whatisthedifferencebetweenartificialintelligenceandcognitivescience However my question ishow can we connect artificial intelligence with cognitive psychology for instance Human Computing Interaction We may come in a contact with Humana Computer Interaction every day because this field includes the every day use of computer for exampletapping stack exchange app on smartphone the user interfaces and some other expert programs which may use cognitive psychology in order to manipulate or help people But still such tasks have got a minimal relevant connection
,,"<p>Depends on how your AI works. if it is making decisions using electric currency (like a computer processor), it would obviously need some source of current. </p>

<p>If the AI working with chemical reactions, however, it could work with chemical energy stored in sugars and fats. That is basically how every animal's brain works. But still, animals need to eat to perform these chemical reactions which trigger other reactions and will cause muscles to contract, cells to grow, etc etc. Everything obviously needs some source of energy.</p>
",,0,2016-11-30T16:36:00.527,,2410,2016-12-03T12:01:02.833,2016-12-03T12:01:02.833,,145.0,,3948.0,2400.0,2,0,,,,65.42,11.94,9.67,0.0,0.0,15.0,Depends on how your AI works if it is making decisions using electric currency like a computer processor it would obviously need some source of current If the AI working with chemical reactions however it could work with chemical energy stored in sugars and fats That is basically how every animals brain works But still animals need to eat to perform these chemical reactions which trigger other reactions and will cause muscles to contract cells to grow etc etc Everything obviously needs some source of energy
,,"<p>Almost always people will resize all their images to the same size before sending them to the CNN.  Unless you're up for a real challenge this is probably what you should do.</p>

<p>That said, it is <em>possible</em> to build a single CNN that takes input of images as varying dimensions.  There are a number of ways you might try to do this, and I'm not aware of any published science analyzing these different choices.  The key is that the set of learned parameters needs to be shared between the different inputs sizes.  While convolutions can be applied at different images sizes, ultimately they always get converted to a single vector to make predictions with, and the size of that vector will depend on the geometries of the inputs, convolutions and pooling layers.  You'd probably want to dynamically change the pooling layers based on the input geometry and leave the convolutions the same, since the convolutional layers have parameters and pooling usually doesn't.  So on bigger images you pool more aggressively.</p>

<p>Practically you'd want to group together similarly (identically) sized images together into minibatches for efficient processing.  This is common for LSTM type models.  This technique is commonly called ""bucketing"".  See for example <a href=""http://mxnet.io/how_to/bucketing.html"" rel=""nofollow noreferrer"">http://mxnet.io/how_to/bucketing.html</a> for a description of how to do this efficiently.</p>
",,0,2016-11-30T19:01:29.610,,2411,2016-11-30T19:01:29.610,,,,,3951.0,2403.0,2,1,,,,54.93,12.76,9.44,0.0,0.0,35.0,Almost always people will resize all their images to the same size before sending them to the CNN Unless youre up for a real challenge this is probably what you should do That said it is possible to build a single CNN that takes input of images as varying dimensions There are a number of ways you might try to do this and Im not aware of any published science analyzing these different choices The key is that the set of learned parameters needs to be shared between the different inputs sizes While convolutions can be applied at different images sizes ultimately they always get converted to a single vector to make predictions with and the size of that vector will depend on the geometries of the inputs convolutions and pooling layers Youd probably want to dynamically change the pooling layers based on the input geometry and leave the convolutions the same since the convolutional layers have parameters and pooling usually doesnt So on bigger images you pool more aggressively Practically youd want to group together similarly identically sized images together into minibatches for efficient processing This is common for LSTM type models This technique is commonly called bucketing See for example httpmxnetiohowtobucketinghtml for a description of how to do this efficiently
,0.0,"<p>I am using a GA to optimise an ANN in Matlab. This ANN is pretty basic (input, hidden, output) but the input size is quite large (10,000) and the output size is 2 since I have to classes of images to be classified. </p>

<p>The weights are in the form of 2 matrices (10,000*m) and (m * 2). I am now trying to do the genetic cross over with mutation.</p>

<p>Since the weights are in a matrix, is there an efficeint way to implement a random crossover with mutation without doing it in a point-wise fashion?</p>
",,0,2016-11-30T19:06:19.070,,2412,2016-11-30T19:06:19.070,,,,,3952.0,,1,0,<neural-networks><image-recognition><genetic-algorithms>,Genetic Algorithms to Optimise ANNs,43.0,77.77,7.08,8.94,0.0,0.0,21.0,I am using a GA to optimise an ANN in Matlab This ANN is pretty basic input hidden output but the input size is quite large 10000 and the output size is 2 since I have to classes of images to be classified The weights are in the form of 2 matrices 10000m and m 2 I am now trying to do the genetic cross over with mutation Since the weights are in a matrix is there an efficeint way to implement a random crossover with mutation without doing it in a pointwise fashion
,,"<p>AI is already connected with cognitive psychology - there are dozens of AIs right this minute attempting to predict things like which Facebook posts you will like, and which ads you are most likely to click on. In other words, they are trying to predict how you think.</p>

<p>For more detailed info on this AI/cognitive science connection, there is some suggested reading on <a href=""http://aitopics.org/topic/cognitive-science"" rel=""nofollow noreferrer"">AITopics.org</a>, such as <a href=""http://plato.stanford.edu/entries/cognitive-science/"" rel=""nofollow noreferrer"">Paul Thagard's summary of cognitive science</a>.</p>
",,1,2016-12-01T13:18:07.150,,2414,2016-12-01T13:18:07.150,,,,,3601.0,2409.0,2,3,,,,53.41,12.06,8.97,0.0,0.0,11.0,AI is already connected with cognitive psychology there are dozens of AIs right this minute attempting to predict things like which Facebook posts you will like and which ads you are most likely to click on In other words they are trying to predict how you think For more detailed info on this AIcognitive science connection there is some suggested reading on AITopicsorg such as Paul Thagards summary of cognitive science
,1.0,"<p><strong>Lots of people are afraid of what could the A.I. do to Humanity.
Some people wish for a sort of Asimov law included in the A.I. software, but maybe we could go a bit more far with the UDHR.</strong></p>

<p><strong>So, Why is the <a href=""http://www.un.org/en/universal-declaration-human-rights/"" rel=""nofollow noreferrer"">Universal Declaration of Human Rights</a> not included as statement of the A.I.?</strong></p>

<blockquote>
  <p>As response to comment, response or edition:</p>
  
  <p>The Universal Declaration of Human Rights is clear and enough as is.</p>
  
  <p>We the people, have to be able to use it as is and adapt the robot and
  A.I. evolution to it. </p>
</blockquote>

<ul>
<li>""I do not think that dignity nor the rest of the UDHR have suffered the outrages of time but outrages of Humans themselves""</li>
</ul>
",,2,2016-12-01T19:54:54.373,1.0,2415,2016-12-08T09:02:26.843,2016-12-08T09:02:26.843,,1807.0,,3893.0,,1,0,<ai-design><security>,Why is the Universal Declaration of Human Rights not include as statement of the A.I.?,63.0,75.1,6.13,7.5,0.0,0.0,20.0,Lots of people are afraid of what could the AI do to Humanity Some people wish for a sort of Asimov law included in the AI software but maybe we could go a bit more far with the UDHR So Why is the Universal Declaration of Human Rights not included as statement of the AI As response to comment response or edition The Universal Declaration of Human Rights is clear and enough as is We the people have to be able to use it as is and adapt the robot and AI evolution to it I do not think that dignity nor the rest of the UDHR have suffered the outrages of time but outrages of Humans themselves
,,"<p>If I understand what you are asking, I think the simple answer would be that AI is nowhere near having demonstrated sentience, thus they do not qualify for any type of rights.</p>

<p>We won't have to ""cross this bridge"" until an AI demonstrates self-awareness and human-level-or-beyond intelligence, but it sure is interesting to think about!</p>

<p><em>(Also, the UDHR dates to the 1940's and seems to have had its last additions in 1966.  Computers weren't very ""smart"" back then so likely no on was even considering the question ;)</em></p>
",,1,2016-12-01T20:19:33.527,,2416,2016-12-01T20:19:33.527,,,,,1671.0,2415.0,2,1,,,,57.81,11.26,9.62,0.0,0.0,21.0,If I understand what you are asking I think the simple answer would be that AI is nowhere near having demonstrated sentience thus they do not qualify for any type of rights We wont have to cross this bridge until an AI demonstrates selfawareness and humanlevelorbeyond intelligence but it sure is interesting to think about Also the UDHR dates to the 1940s and seems to have had its last additions in 1966 Computers werent very smart back then so likely no on was even considering the question
2418.0,1.0,"<p>I mean this in the sense that Go is unsolvable but AlphaGo seems able to make choices that are consistently more optimal than a human player's choices.  </p>

<p>It is my understanding that Game Theory turned out to have limited applications in real world scenarios because of the profound complexity of such scenarios and degree of hidden information.  Is it fair to say that there is now a method for dealing with this?  </p>

<p>I fully understand that Go is a game of complete information, which has a very specific meaning, but it occurs to me that the inability to generate a complete game tree (computational intractability) could be seen as form of incomplete information, even if it is not traditionally thought of in those terms. </p>

<hr>

<p><em>I should probably note that my perspective is one of a ""serious"" game designer, where complexity serves the same function as chance and hidden information, which is to say as a balancing factor that ""levels the playing field"".</em>  </p>
",,0,2016-12-01T22:28:39.747,,2417,2016-12-04T01:09:46.480,2016-12-04T01:09:46.480,,2723.0,,1671.0,,1,0,<machine-learning><game-theory>,Can programs like AlphaGo be said to be means of dealing with computational intractability?,58.0,38.59,11.38,9.63,0.0,0.0,17.0,I mean this in the sense that Go is unsolvable but AlphaGo seems able to make choices that are consistently more optimal than a human players choices It is my understanding that Game Theory turned out to have limited applications in real world scenarios because of the profound complexity of such scenarios and degree of hidden information Is it fair to say that there is now a method for dealing with this I fully understand that Go is a game of complete information which has a very specific meaning but it occurs to me that the inability to generate a complete game tree computational intractability could be seen as form of incomplete information even if it is not traditionally thought of in those terms I should probably note that my perspective is one of a serious game designer where complexity serves the same function as chance and hidden information which is to say as a balancing factor that levels the playing field
,,"<p>I think that the technique AlphaGo used to solve the computational intractability problem of the search space are not new, it uses the Monte Carlo Tree Search. The real innovation in AlphaGo was to figure out how to compute the evaluation function of a move, that was the really tricky part. For this they used combinations Deep and Reinforcement learning techniques.</p>
",,3,2016-12-02T05:41:51.923,,2418,2016-12-02T05:41:51.923,,,,,1462.0,2417.0,2,2,,,,50.87,11.89,9.56,0.0,0.0,5.0,I think that the technique AlphaGo used to solve the computational intractability problem of the search space are not new it uses the Monte Carlo Tree Search The real innovation in AlphaGo was to figure out how to compute the evaluation function of a move that was the really tricky part For this they used combinations Deep and Reinforcement learning techniques
2423.0,1.0,"<p>I knew that Reproduction and Crossover are the same things,</p>

<ol>
<li><a href=""https://en.wikipedia.org/wiki/Genetic_operator#Operators"" rel=""nofollow noreferrer"">Wikipedia</a></li>
<li><a href=""http://www.obitko.com/tutorials/genetic-algorithms/crossover-mutation.php"" rel=""nofollow noreferrer"">Obitco.com</a></li>
<li><a href=""https://www.tutorialspoint.com/genetic_algorithms/genetic_algorithms_fundamentals.htm"" rel=""nofollow noreferrer"">TutorialsPoint</a></li>
</ol>

<p>But, The following is the exercise given by my teacher,</p>

<blockquote>
  <p>Exercise 1   Genetic algorithm to solve pattern finding problem. </p>
  
  <p>Your task is to design a simple genetic algorithm, with binary-coded chromosomes, in order  to solve pattern finding problem
  in 16-bit strings.  </p>
  
  <p>The objective function is given by the following
  formula:    </p>
  
  <p>F(x) = NoS(""010"") + 2NoS(""0110"") + 3NoS(""01110"") +
  4NoS(""011110"") +  5NoS(""0111110"") + 6NoS(""01111110"") +
  7NoS(""011111110"") + 6NoS(""0111111110"") +  5NoS(""01111111110"") +
  4NoS(""011111111110"") + 3NoS(""0111111111110"") +  2NoS(""01111111111110"")
  + NoS(""011111111111110"")    </p>
  
  <p>The algorithm should display each population on the screen in the form     And
  should save the history of it’s operation (average fitness in each
  population) in the text  file. At the end it should also display the
  best solution found.    </p>
  
  <p>You may use the following operators:  </p>
  
  <ol>
  <li><p>Reproduction.<br>
  You can use either one of the following reproduction
  types:  Proportional, Ranking, Tournament. They are described more in
  detail below:
  ... ... ... ... ... ... ... ...</p></li>
  <li><p>Crossing over.<br>
  In order to perform this operation the individuals must be grouped in
  pairs (randomly), and  with certain probability pcross information
  from their chromosomes must be exchanged. There  are many flavors of
  the crossing-over operator, but in our case (short, 16-bit
  chromosome),  simple, one-point crossover will be enough. It can be
  performed by selecting a random  number k from the range &lt;1;15> and
  cutting the chromosomes of both individuals on that  position. Each of
  the individuals copies bits  belonging to the other to it’s own 
  chromosome.  </p></li>
  <li><p>Mutation<br>
  This operator changes the value of each bit in the chromosome to the opposite one with a very  small probability pm
  (usually about 10-3).  If we denote chromosome as [b1, b2, ... , b16];
  then after the mutation each bit can be  described as:  Where k Î
  {1,2, ...,16}  flip(x) – result of a Bernoulli flip with a success 
  probability x.</p></li>
  </ol>
</blockquote>

<p>Here I see that by Reproduction and Crossover he means different things.</p>

<p>What is the catch?</p>
",,0,2016-12-02T07:43:37.083,1.0,2419,2016-12-03T07:51:54.843,,,,,3642.0,,1,1,<genetic-algorithms>,Difference between Reproduction and Crossover in Genetic Algorithm,84.0,62.58,16.24,9.22,0.0,0.0,161.0,I knew that Reproduction and Crossover are the same things Wikipedia Obitcocom TutorialsPoint But The following is the exercise given by my teacher Exercise 1 Genetic algorithm to solve pattern finding problem Your task is to design a simple genetic algorithm with binarycoded chromosomes in order to solve pattern finding problem in 16bit strings The objective function is given by the following formula Fx NoS010 2NoS0110 3NoS01110 4NoS011110 5NoS0111110 6NoS01111110 7NoS011111110 6NoS0111111110 5NoS01111111110 4NoS011111111110 3NoS0111111111110 2NoS01111111111110 NoS011111111111110 The algorithm should display each population on the screen in the form And should save the history of it’s operation average fitness in each population in the text file At the end it should also display the best solution found You may use the following operators Reproduction You can use either one of the following reproduction types Proportional Ranking Tournament They are described more in detail below Crossing over In order to perform this operation the individuals must be grouped in pairs randomly and with certain probability pcross information from their chromosomes must be exchanged There are many flavors of the crossingover operator but in our case short 16bit chromosome simple onepoint crossover will be enough It can be performed by selecting a random number k from the range lt115 and cutting the chromosomes of both individuals on that position Each of the individuals copies bits belonging to the other to it’s own chromosome Mutation This operator changes the value of each bit in the chromosome to the opposite one with a very small probability pm usually about 103 If we denote chromosome as b1 b2 b16 then after the mutation each bit can be described as Where k Î 12 16 flipx – result of a Bernoulli flip with a success probability x Here I see that by Reproduction and Crossover he means different things What is the catch
,,"<p>Even if machines with true Artificial General Intelligence were created, their apparent intelligence would still be by definition <em>artificial</em>. The word <em>simulation</em> is a <a href=""http://www.thesaurus.com/browse/artificial"" rel=""nofollow noreferrer"">synonym</a> and could be used to redefine AGI as Simulated General Intelligence. </p>

<p>Keeping that in mind, <strong>a machine that appears to be expressing emotions would only be the result of a series of complicated algorithms</strong> allowing a computer to assess the situation and respond in an intellectually appropriate manner based on external stimulus and conditions. Every possible action this machine could possibly make would be derived from a list of possible actions the machine is capable of, no matter how large the number of possible actions grows. The machine is still a series sensors, programmed instructions, and cycles of execution. </p>

<p>Destroying such a machine could potentially be the destruction of property if it wasn't owned by the person who destroyed it, but would it be murder? No. </p>

<p>A broken machine can potentially be rebuilt and reactivated if it is broken. It never really died; it was destroyed. A living being that is killed is really <em>dead</em> and cannot be rebuilt and made alive once again. These key differences lead me to agree with the previous answer and conclude that <strong>no, destroying an artificial intelligence without its consent would not be murder</strong>.</p>
",,0,2016-12-02T13:18:44.903,,2420,2017-03-11T00:14:25.140,2017-03-11T00:14:25.140,,3989.0,,3989.0,1289.0,2,2,,,,49.65,12.65,10.73,0.0,0.0,20.0,Even if machines with true Artificial General Intelligence were created their apparent intelligence would still be by definition artificial The word simulation is a synonym and could be used to redefine AGI as Simulated General Intelligence Keeping that in mind a machine that appears to be expressing emotions would only be the result of a series of complicated algorithms allowing a computer to assess the situation and respond in an intellectually appropriate manner based on external stimulus and conditions Every possible action this machine could possibly make would be derived from a list of possible actions the machine is capable of no matter how large the number of possible actions grows The machine is still a series sensors programmed instructions and cycles of execution Destroying such a machine could potentially be the destruction of property if it wasnt owned by the person who destroyed it but would it be murder No A broken machine can potentially be rebuilt and reactivated if it is broken It never really died it was destroyed A living being that is killed is really dead and cannot be rebuilt and made alive once again These key differences lead me to agree with the previous answer and conclude that no destroying an artificial intelligence without its consent would not be murder
,,"<p>We've concluded that it is a two-faceted, circular problem: structure cannot be inferred without context but knowing the structure also helps infer the context. So, here is your complex solution: start with the context, which is determined by the combination of words in sentence (combinatorics and search problem), from there determine your structure, or ""parse"" (at this step you can also filter out some insignificant words or at least assign lesser weights to them), go back to the context, back to parsing, and on until you arrive at the meaning. Thus by iterative, recursive reduction the whole problem can be solved.</p>
",,0,2016-12-02T16:18:23.760,,2421,2016-12-02T16:18:23.760,,,,,3991.0,2319.0,2,1,,,,37.27,13.36,11.09,0.0,0.0,22.0,Weve concluded that it is a twofaceted circular problem structure cannot be inferred without context but knowing the structure also helps infer the context So here is your complex solution start with the context which is determined by the combination of words in sentence combinatorics and search problem from there determine your structure or parse at this step you can also filter out some insignificant words or at least assign lesser weights to them go back to the context back to parsing and on until you arrive at the meaning Thus by iterative recursive reduction the whole problem can be solved
,2.0,"<p>I have data of 30 students attendance for a particular subject class for a week. I have quantified the absence and presence with boolean logic 0 and 1. Also, the reason for absence are provided and I tried to generalise these reason into 3 categories say A, B and C. Now I want to use these data to make future predictions for attendance but I am uncertain of what technique to use. Can anyone please provide suggestions?</p>
",,2,2016-12-03T02:24:06.267,,2422,2016-12-03T12:43:43.803,,,,,2888.0,,1,2,<structured-data>,What techniques can be used to predict future attendance of students for a particular subject lecture session?,37.0,64.3,8.41,9.12,0.0,0.0,7.0,I have data of 30 students attendance for a particular subject class for a week I have quantified the absence and presence with boolean logic 0 and 1 Also the reason for absence are provided and I tried to generalise these reason into 3 categories say A B and C Now I want to use these data to make future predictions for attendance but I am uncertain of what technique to use Can anyone please provide suggestions
,,"<p>The terminology of this exercise is not standard. What is referred to as 'Reproduction' in the exercise is usually referred to as 'Selection'.</p>

<p>The term 'Reproduction' does indeed seem conceptually closer to the notion of Crossover/Recombination (these two are the same thing), which is probably where your confusion has arisen.</p>

<p>See the excellent (and freely-downloadable) <a href=""https://cs.gmu.edu/~sean/book/metaheuristics/"" rel=""nofollow noreferrer"">'Essentials of Metaheuristics'</a> for an introduction to the usual terminology for evolutionary algorithms.</p>
",,0,2016-12-03T07:51:54.843,,2423,2016-12-03T07:51:54.843,,,,,42.0,2419.0,2,2,,,,28.84,17.28,9.82,0.0,0.0,19.0,The terminology of this exercise is not standard What is referred to as Reproduction in the exercise is usually referred to as Selection The term Reproduction does indeed seem conceptually closer to the notion of CrossoverRecombination these two are the same thing which is probably where your confusion has arisen See the excellent and freelydownloadable Essentials of Metaheuristics for an introduction to the usual terminology for evolutionary algorithms
,,"<p>I suggest you should use AI Regression Model for future predictions for an attendance of students. Because of this technique or model design for future predictions. </p>

<p><a href=""https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/"" rel=""nofollow noreferrer"">Follow this to get more information about regression type and methodology </a></p>
",,1,2016-12-03T11:34:42.730,,2424,2016-12-03T11:34:42.730,,,,,4007.0,2422.0,2,3,,,,42.07,13.5,11.5,0.0,0.0,2.0,I suggest you should use AI Regression Model for future predictions for an attendance of students Because of this technique or model design for future predictions Follow this to get more information about regression type and methodology
,,"<p>Because you have a small number of students (30), and a short time (one week), the number of absences is likely to be best modelled as a <a href=""http://stattrek.com/probability-distributions/poisson.aspx"" rel=""nofollow noreferrer"">Poisson distribution</a>.</p>

<p><a href=""https://i.stack.imgur.com/aKKxl.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aKKxl.gif"" alt=""Poisson Distributions""></a></p>

<p><strong>Poisson Formula</strong></p>

<p>The average number of absences within a given time period is μ (use your data to estimate this). </p>

<p>Then, the Poisson probability of x absences is:</p>

<p>P(x; μ) = (e-μ) (μx) / x!</p>

<p>where e is the logarithmic constant, approximately equal to 2.71828.</p>

<p>You can either:</p>

<ol>
<li><p>model absences due to the three reasons as three separate probablilites, P(A), P(B), and P(C), and then combine them, or </p></li>
<li><p>model total absences as one figure. </p></li>
</ol>

<p>Given your very small data set, the first approach is likely to be less accurate.</p>
",,4,2016-12-03T12:23:04.140,,2425,2016-12-03T12:43:43.803,2016-12-03T12:43:43.803,,3601.0,,3601.0,2422.0,2,1,,,,60.35,10.33,8.95,0.0,0.0,41.0,Because you have a small number of students 30 and a short time one week the number of absences is likely to be best modelled as a Poisson distribution Poisson Formula The average number of absences within a given time period is μ use your data to estimate this Then the Poisson probability of x absences is Px μ eμ μx x where e is the logarithmic constant approximately equal to 271828 You can either model absences due to the three reasons as three separate probablilites PA PB and PC and then combine them or model total absences as one figure Given your very small data set the first approach is likely to be less accurate
2440.0,3.0,"<p>The Turing Test has been the classic test of artificial intelligence for a while now. The concept is deceptively simple - to trick a human into thinking it is another human on the other end of a conversation line, not a computer - but from what I've read, it has turned out to be very difficult in practice.</p>

<p>How close have we gotten to tricking a human in the Turing Test? With things like chat bots, Siri, and incredibly powerful computers, I'm thinking we're getting pretty close. If we're pretty far, why are we so far? What is the main problem?</p>
",,0,2016-12-03T15:52:23.620,2.0,2427,2016-12-12T21:29:50.483,2016-12-12T21:29:50.483,,145.0,,4011.0,,1,3,<turing-test>,How close have we come to passing the Turing Test?,265.0,71.65,8.7,8.92,0.0,0.0,18.0,The Turing Test has been the classic test of artificial intelligence for a while now The concept is deceptively simple to trick a human into thinking it is another human on the other end of a conversation line not a computer but from what Ive read it has turned out to be very difficult in practice How close have we gotten to tricking a human in the Turing Test With things like chat bots Siri and incredibly powerful computers Im thinking were getting pretty close If were pretty far why are we so far What is the main problem
,,"<p>As far as I know I think this is the closest we've come:</p>

<p><a href=""http://www.bbc.com/news/technology-27762088"" rel=""nofollow noreferrer"">http://www.bbc.com/news/technology-27762088</a></p>

<p>They simulated a 13 year old Ukrainian child in an online chat and convinced 33% of the judges that it was human. But even then the test was in favor of the bot. To my knowledge I don't think an AI has passed a turing test straight up.</p>
",,0,2016-12-03T21:33:46.953,,2428,2016-12-03T21:33:46.953,,,,,4017.0,2427.0,2,0,,,,89.58,8.18,7.72,0.0,0.0,15.0,As far as I know I think this is the closest weve come httpwwwbbccomnewstechnology27762088 They simulated a 13 year old Ukrainian child in an online chat and convinced 33 of the judges that it was human But even then the test was in favor of the bot To my knowledge I dont think an AI has passed a turing test straight up
,1.0,"<p>According to NASA scientist Rick Briggs, Sanskrit is the best language for AI. I want to know how Sanskrit is useful. What's the problem with other languages? Are they really using Sanskrit in AI programming or going to do so? What part of an AI program requires such language?</p>
",,2,2016-12-04T09:48:57.170,2.0,2429,2016-12-09T15:20:44.623,2016-12-09T15:20:44.623,,3989.0,,4027.0,,1,8,<ai-design><nasa><cyborg>,Importance of Sanskrit,181.0,78.45,8.33,8.96,0.0,0.0,7.0,According to NASA scientist Rick Briggs Sanskrit is the best language for AI I want to know how Sanskrit is useful Whats the problem with other languages Are they really using Sanskrit in AI programming or going to do so What part of an AI program requires such language
2431.0,2.0,"<p>As you can see, there is no computer screen for the computer, thus the AI cannot display an image of itself.  How is it possible for it to see and talk to someone?<a href=""https://i.stack.imgur.com/szSsk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/szSsk.png"" alt=""enter image description here""></a> </p>
",,2,2016-12-04T10:56:59.750,,2430,2016-12-06T03:28:31.480,,,,,3896.0,,1,-2,<strong-ai>,How is it possible for an AI to interact with someone without a computer screen?,123.0,80.11,5.28,8.28,0.0,0.0,4.0,As you can see there is no computer screen for the computer thus the AI cannot display an image of itself How is it possible for it to see and talk to someone
,,"<p>There are many communication methods that could be used by an artificial intelligence. Artificial intelligence can be integrated to various things including robots, phones, IoT and many others. Primary ways of human communications are  either visual or auditory, therefore an natural way for it to communicate with a human is through voice, text, images and videos. The output does not have to be limited to screens but can be anything from refrigerators to speakers.</p>

<p>Hope this helped.</p>
",,10,2016-12-04T15:47:22.817,,2431,2016-12-04T15:47:22.817,,,,,4034.0,2430.0,2,2,,,,38.92,13.16,10.76,0.0,0.0,10.0,There are many communication methods that could be used by an artificial intelligence Artificial intelligence can be integrated to various things including robots phones IoT and many others Primary ways of human communications are either visual or auditory therefore an natural way for it to communicate with a human is through voice text images and videos The output does not have to be limited to screens but can be anything from refrigerators to speakers Hope this helped
,,"<p>Rick Briggs refers to the difficulty an artificial intelligence would have in detecting the true meaning of words spoken or written in one of our natural languages. Take for example an artificial intelligence attempting to determine the meaning of a sarcastic sentence. </p>

<p>Naturally spoken, the sentence ""That's just what I needed today!"" can be the expression of very different feelings. In one instance, a happy individual finding an item that had been lost for some time could be excited or cheered up from the event, and exclaim that this moment of triumph was exactly what their day needed to continue to be happy. On the other hand, a disgruntled office employee having a rough day could accidentally worsen his situation by spilling hot coffee on himself, and sarcastically exclaim that this further annoyance was exactly what he needed today. This sentence should in this situation be interpreted as the man expressing that spilling coffee on himself made his bad day worse.</p>

<p>This is one small example explaining the reason linguistic analysis is difficult for artificial intelligence. When this example is spoken, small tonal fluctuations and indicators are extremely difficult for an AI with a microphone to detect accurately; and if the sentence was simply read, without context how <em>would</em> one example be discernible from the other?</p>

<p>Rick Briggs suggests that Sanskrit, a form of communication sacred to Hinduism, is a naturally spoken language with mechanics and grammatical rules that would allow an artificial intelligence to more accurately interpret sentences during linguistic analysis. More accurate linguistic analysis would result in an artificial intelligence being able to respond more accurately. You can read more about Rick Brigg's thoughts on the language <a href=""http://vedicsciences.net/articles/sanskrit-nasa.html"" rel=""nofollow noreferrer"">here</a>.</p>
",,0,2016-12-05T10:23:09.960,,2432,2016-12-05T10:23:09.960,,,,,3989.0,2429.0,2,6,,,,39.37,14.1,10.04,0.0,0.0,26.0,Rick Briggs refers to the difficulty an artificial intelligence would have in detecting the true meaning of words spoken or written in one of our natural languages Take for example an artificial intelligence attempting to determine the meaning of a sarcastic sentence Naturally spoken the sentence Thats just what I needed today can be the expression of very different feelings In one instance a happy individual finding an item that had been lost for some time could be excited or cheered up from the event and exclaim that this moment of triumph was exactly what their day needed to continue to be happy On the other hand a disgruntled office employee having a rough day could accidentally worsen his situation by spilling hot coffee on himself and sarcastically exclaim that this further annoyance was exactly what he needed today This sentence should in this situation be interpreted as the man expressing that spilling coffee on himself made his bad day worse This is one small example explaining the reason linguistic analysis is difficult for artificial intelligence When this example is spoken small tonal fluctuations and indicators are extremely difficult for an AI with a microphone to detect accurately and if the sentence was simply read without context how would one example be discernible from the other Rick Briggs suggests that Sanskrit a form of communication sacred to Hinduism is a naturally spoken language with mechanics and grammatical rules that would allow an artificial intelligence to more accurately interpret sentences during linguistic analysis More accurate linguistic analysis would result in an artificial intelligence being able to respond more accurately You can read more about Rick Briggs thoughts on the language here
,,"<p>""How is it possible for it to see and talk to someone?""</p>

<p>OK, unfortunately this is quite vague... but I am going to try my best. </p>

<p>The monitor of the computer really doesn't change the ability for it to communicate. For instance, voice recognition is natural to humans along with visual factors. So sensors involving auditory elements assists the AI with this. 
Now I commented that the question was vague because you said ""there is no computer screen for the computer, ****thus the AI cannot display an image of itself****. </p>

<p>Even though the AI can not see does not mean it can not communicate. Those who are blind still have ways of communication, just in a different manner. Sure, they can not recognize one with their own eyes, but they can by touch, and read with braille. Now compared to AI communication, the braille is to any other form of sensor.</p>

<p>Sorry for jumping all over the place, also I did not mean to offend anyone with my comparison... :/</p>
",,0,2016-12-06T03:28:31.480,,2433,2016-12-06T03:28:31.480,,,,,4065.0,2430.0,2,0,,,,74.08,8.92,8.63,0.0,0.0,39.0,How is it possible for it to see and talk to someone OK unfortunately this is quite vague but I am going to try my best The monitor of the computer really doesnt change the ability for it to communicate For instance voice recognition is natural to humans along with visual factors So sensors involving auditory elements assists the AI with this Now I commented that the question was vague because you said there is no computer screen for the computer thus the AI cannot display an image of itself Even though the AI can not see does not mean it can not communicate Those who are blind still have ways of communication just in a different manner Sure they can not recognize one with their own eyes but they can by touch and read with braille Now compared to AI communication the braille is to any other form of sensor Sorry for jumping all over the place also I did not mean to offend anyone with my comparison
,0.0,"<p><strong>THE PROBLEM</strong></p>

<p>In my main body of text there are some substrings that I want to extract.  Note that I don't want to construct or generate new strings, the characters I extract must be part of the main text. Normally I would use a regex to extract this substring when the text and substring are relatively simple. </p>

<p>For example, from the text:</p>

<pre><code>... some text ...
Today, Fake Acme Corp. changed its name to Really Not Fake Acme Corp.
... some more text ...
</code></pre>

<p>I would like to extract the names <code>Fake Acme Corp.</code> and <code>Really Not Fake Acme Corp.</code></p>

<p>Points to keep in mind (aka why regex won't work):</p>

<ul>
<li>The format of the sentence is not fixed, neither is its location in
the document. The 2 names could be in different sentences.</li>
<li>There is no guarantee that the new company name will be related to the old name.</li>
<li>There is no guarantee that only 2 company names will be mentioned in the document (so standard CRF approaches that detect the <code>ORG</code> class  viz. <code>CoreNLP</code>/<code>OpenNLP</code> won't be of much help).</li>
</ul>

<p><strong>WHAT I'VE TRIED</strong></p>

<p>The training dataset is ~5000 labeled examples. This can be expanded if necessary. Using a character encoding format as per <a href=""https://arxiv.org/abs/1509.01626"" rel=""nofollow noreferrer"" title=""this paper &#40;Character-level CNNs&#41;"">this paper on character-level cnns</a> I constructed an LSTM network using <a href=""https://keras.io/"" rel=""nofollow noreferrer"">Keras</a> which took as input each character encoded in a one-hot format. The output was also a one-hot vector.</p>

<p>Eg: The input text is <code>1024</code> characters, encoded in a one-hot vector with an alphabet of <code>73</code> (see paper linked above for more on this). The output is a one-hot encoded vector that is the substring I want.</p>

<p>Input shape: <code>1024 x 73</code>, Output shape: <code>1024 x 73</code> (in the output only the characters that match the substring are one-hot, the rest of the rows are all <code>0</code>)</p>

<p><strong>TL;DR</strong>: Use LSTMs when regex will become too powerful/complicated/unable to work.</p>
",,1,2016-12-06T14:50:03.387,1.0,2434,2016-12-06T14:50:03.387,,,,,4073.0,,1,0,<lstm><keras><tensorflow>,Using LSTMs in place of regular expressions,70.0,70.02,9.16,8.32,204.0,0.0,50.0,THE PROBLEM In my main body of text there are some substrings that I want to extract Note that I dont want to construct or generate new strings the characters I extract must be part of the main text Normally I would use a regex to extract this substring when the text and substring are relatively simple For example from the text I would like to extract the names and Points to keep in mind aka why regex wont work The format of the sentence is not fixed neither is its location in the document The 2 names could be in different sentences There is no guarantee that the new company name will be related to the old name There is no guarantee that only 2 company names will be mentioned in the document so standard CRF approaches that detect the class viz wont be of much help WHAT IVE TRIED The training dataset is 5000 labeled examples This can be expanded if necessary Using a character encoding format as per this paper on characterlevel cnns I constructed an LSTM network using Keras which took as input each character encoded in a onehot format The output was also a onehot vector Eg The input text is characters encoded in a onehot vector with an alphabet of see paper linked above for more on this The output is a onehot encoded vector that is the substring I want Input shape Output shape in the output only the characters that match the substring are onehot the rest of the rows are all TLDR Use LSTMs when regex will become too powerfulcomplicatedunable to work
,1.0,"<p>I am working on a project, wherein I take input from the user as free text and try to relate the text to what the user might mean. I have tried <strong>Stanford NLP</strong> which tokenizes the text into tokens, but I am not able to categorize the input. For example, the user might be greeting someone or sharing some problem he is facing. In case he is sharing some problem I need to categorize the problem as well.</p>

<p>Can someone help me with from where should I start?</p>
",,1,2016-12-07T04:47:00.043,,2436,2017-01-12T12:27:07.703,2016-12-07T17:23:07.330,,1807.0,,4088.0,,1,1,<nlp>,Can I categorise the user input which I get as free text?,77.0,78.99,6.49,7.92,0.0,0.0,8.0,I am working on a project wherein I take input from the user as free text and try to relate the text to what the user might mean I have tried Stanford NLP which tokenizes the text into tokens but I am not able to categorize the input For example the user might be greeting someone or sharing some problem he is facing In case he is sharing some problem I need to categorize the problem as well Can someone help me with from where should I start
,3.0,"<p>Is it possible to train an agent to take and pass a multiple-choice exam based on a digital version of a textbook for some area of study or curriculum? What would be involved in implementing this and how long would it take, for someone familiar with deep learning?</p>
",,5,2016-12-07T11:14:21.203,,2437,2016-12-12T07:42:48.150,2016-12-09T15:39:10.590,,8.0,,4085.0,,1,0,<deep-learning>,Is it possible to train deep learning agent to pass a multiple-choice exam?,142.0,55.58,9.23,10.09,0.0,0.0,4.0,Is it possible to train an agent to take and pass a multiplechoice exam based on a digital version of a textbook for some area of study or curriculum What would be involved in implementing this and how long would it take for someone familiar with deep learning
,,"<p>There are programs that do this today, for some values of ""curriculum"" and ""exam"".  It does not even require deep learning; a simpler information retrieval algorithm and some rules for composition work and <a href=""http://www.popsci.com/article/technology/essay-writing-machine-made-fool-other-machines"" rel=""nofollow noreferrer"">achieve high scores on machine graded essays.</a></p>

<p>For human graders, there is research on <a href=""http://homepages.inf.ed.ac.uk/jmoore/course-reads/nlg/McKeown85.pdf"" rel=""nofollow noreferrer"">automatically generating essay-length text responses to queries in a certain domain.</a></p>

<p>Both linked applications are rule-based rather than based in deep-learning.  I'd guess that a deep-learning approach would be much less efficient (in computer resources) in producing comparable results. </p>
",,0,2016-12-07T16:56:49.540,,2438,2016-12-07T16:56:49.540,,,,,2329.0,2437.0,2,4,,,,45.56,15.6,12.38,0.0,0.0,19.0,There are programs that do this today for some values of curriculum and exam It does not even require deep learning a simpler information retrieval algorithm and some rules for composition work and achieve high scores on machine graded essays For human graders there is research on automatically generating essaylength text responses to queries in a certain domain Both linked applications are rulebased rather than based in deeplearning Id guess that a deeplearning approach would be much less efficient in computer resources in producing comparable results
,1.0,"<p>I had already started in my graduation project process , It's about an application which will learn users new language by playing games , and it's based on AI , the concept is the user will start his level and play games and do quizzes , at the end of each level there will be a test to pass the level , I have to implement AI in this app to analysis its test grades and know what is the user weakness and power point to create a new level which suits the user's language level , that means if he is good in grammar but weak in vocabulary so the new level will create to strength the vocabulary , games and questions will be categorized into the database for this purpose , so the AI algorithms should analyse and decide which game or quiz should the user takes based on his level , then it will create the level .
I had searched before and reached for some techniques like (machine learning , planning systems , reinforcement learning and case-based-reasoning ).</p>
",,0,2016-12-07T17:10:41.857,1.0,2439,2017-02-13T10:57:30.563,,,,,4111.0,,1,1,<machine-learning><deep-learning><algorithm><ai-design><path-planning>,Which methods or algorithms to develop a learning application?,119.0,2.63,10.76,11.75,0.0,0.0,20.0,I had already started in my graduation project process Its about an application which will learn users new language by playing games and its based on AI the concept is the user will start his level and play games and do quizzes at the end of each level there will be a test to pass the level I have to implement AI in this app to analysis its test grades and know what is the user weakness and power point to create a new level which suits the users language level that means if he is good in grammar but weak in vocabulary so the new level will create to strength the vocabulary games and questions will be categorized into the database for this purpose so the AI algorithms should analyse and decide which game or quiz should the user takes based on his level then it will create the level I had searched before and reached for some techniques like machine learning planning systems reinforcement learning and casebasedreasoning
,,"<p>No one has attempted to make a system that could pass a serious Turing test. All the systems that are claimed to have ""passed"" Turing tests have done so with low success rates simulating ""special"" people.  Even relatively sophisticated systems like Siri and learning systems like <a href=""http://www.cleverbot.com"" rel=""nofollow noreferrer"">Cleverbot</a> are trivially stumped.</p>

<p>To pass a real Turing test, you would both have to create a human-level AGI and equip it with the specialized ability to deceive people about itself convincingly (of course, that might come automatically with the human-level AGI).  We don't really know how to create a human-level AGI and available hardware appears to be orders of magnitude short of what is required.  Even if we were to develop the AGI, it wouldn't necessarily be useful to enable/equip/motivate? it to have the deception abilities required for the Turing test.</p>
",,0,2016-12-07T17:20:06.900,,2440,2016-12-07T17:20:06.900,,,,,2329.0,2427.0,2,5,,,,43.02,12.24,9.99,0.0,0.0,23.0,No one has attempted to make a system that could pass a serious Turing test All the systems that are claimed to have passed Turing tests have done so with low success rates simulating special people Even relatively sophisticated systems like Siri and learning systems like Cleverbot are trivially stumped To pass a real Turing test you would both have to create a humanlevel AGI and equip it with the specialized ability to deceive people about itself convincingly of course that might come automatically with the humanlevel AGI We dont really know how to create a humanlevel AGI and available hardware appears to be orders of magnitude short of what is required Even if we were to develop the AGI it wouldnt necessarily be useful to enableequipmotivate it to have the deception abilities required for the Turing test
,5.0,"<p>One of the most crucial questions we as a species and as intelligent beings will have to address lies with the rights we plan to grant to AI.</p>

<blockquote>
  <p>This question is intended to see if a compromise can be found between <strong>conservative anthropocentrism</strong> and <strong>post-human fundamentalism</strong>: a response should take into account principles from both perspectives.</p>
</blockquote>

<p>Should, and therefore will, AI be granted the same rights as humans or should such systems have different rights (if any at all) ?</p>

<hr>

<p><strong><em>Some Background</em></strong></p>

<p>This question applies both to human-brain based AI (from whole brain emulations to less exact replication) and AI from scratch.</p>

<p>Murray Shanahan, in his book The Technological Singularity, outlines a potential use of AI that could be considered immoral: <em>ruthless parallelization</em>: we could make identical parallel copies of AI to achieve tasks more effectively and even terminate less succesful copies.</p>

<hr>

<p><strong>Reconciling these two philosophies</strong> (conservative anthropocentrism and post-human fundamentalism), should such use of AI be accepted or should certain limitations - i.e. rights - be created for AI? </p>

<hr>

<p>This question is not related to <a href=""http://ai.stackexchange.com/questions/2356/would-an-ai-with-human-intelligence-have-the-same-rights-as-a-human-under-curren"">Would an AI with human intelligence have the same rights as a human under current legal frameworks?</a> for the following reasons:</p>

<ol>
<li><p>The other question specifies ""<strong><em>current legal frameworks</em></strong>""</p></li>
<li><p>This question is looking for a specific response relating to two fields of thought</p></li>
<li><p>This question highlights specific cases to analyse and is therefore expects less of a general response and more of a precise analysis </p></li>
</ol>
",,0,2016-12-07T17:59:58.367,4.0,2441,2016-12-30T03:23:08.290,2016-12-18T22:47:22.723,,3427.0,,3427.0,,1,4,<strong-ai><legal><rights>,Should intelligent AI be granted the same rights as humans?,280.0,36.32,14.39,10.4,0.0,0.0,31.0,One of the most crucial questions we as a species and as intelligent beings will have to address lies with the rights we plan to grant to AI This question is intended to see if a compromise can be found between conservative anthropocentrism and posthuman fundamentalism a response should take into account principles from both perspectives Should and therefore will AI be granted the same rights as humans or should such systems have different rights if any at all Some Background This question applies both to humanbrain based AI from whole brain emulations to less exact replication and AI from scratch Murray Shanahan in his book The Technological Singularity outlines a potential use of AI that could be considered immoral ruthless parallelization we could make identical parallel copies of AI to achieve tasks more effectively and even terminate less succesful copies Reconciling these two philosophies conservative anthropocentrism and posthuman fundamentalism should such use of AI be accepted or should certain limitations ie rights be created for AI This question is not related to Would an AI with human intelligence have the same rights as a human under current legal frameworks for the following reasons The other question specifies current legal frameworks This question is looking for a specific response relating to two fields of thought This question highlights specific cases to analyse and is therefore expects less of a general response and more of a precise analysis
,3.0,"<p>There is no doubt as to the fact that AI would be replacing a lot of existing technologies, but is AI the ultimate technology which humankind can develop or is their something else which has the potential to replace artificial intelligence?</p>
",,2,2016-12-08T20:01:33.240,1.0,2443,2016-12-19T04:41:26.820,2016-12-09T15:19:42.263,,3427.0,,4138.0,,1,3,<strong-ai><prediction>,What could possibly replace artificial intelligence?,254.0,29.86,11.91,10.29,0.0,0.0,2.0,There is no doubt as to the fact that AI would be replacing a lot of existing technologies but is AI the ultimate technology which humankind can develop or is their something else which has the potential to replace artificial intelligence
,,"<p>By definition, artificial intelligence includes all forms of computer systems capable of completing tasks that would ordinarily warrant human intelligence.</p>

<p>A superintelligent AI would have intelligence far superior to that of any human and therefore would be capable of creating systems beyond our capabilities.</p>

<p>As a consequence, if a technology superior to AI were to be created, it would almost certainly be created by an artificial intelligence.</p>

<blockquote>
  <p>For the purposes of mankind, however, superintelligent artificial intelligence is the ultimate technology due to the fact that it will be able to surpass humans in every field, and, if anything, replace the need for human intelligence.</p>
</blockquote>

<p>In our past experience, intelligence has been the most valuable trait for any entity to manifest - for this reason, in an anthropomorphic context, we can predict that artificial intelligence will be the ultimate achievement.</p>

<blockquote>
  <p>The main reason why we will certainly <strong>not</strong> be able to replace superintelligent AI is that it will surpass us in every respect - if there is ever any replacement, it will be created by the AI similarly to the way we may create an AI that replaces <strong>us</strong>. </p>
</blockquote>
",,6,2016-12-08T21:42:09.827,,2445,2016-12-08T21:49:06.633,2016-12-08T21:49:06.633,,3427.0,,3427.0,2443.0,2,6,,,,31.55,13.59,9.33,0.0,0.0,20.0,By definition artificial intelligence includes all forms of computer systems capable of completing tasks that would ordinarily warrant human intelligence A superintelligent AI would have intelligence far superior to that of any human and therefore would be capable of creating systems beyond our capabilities As a consequence if a technology superior to AI were to be created it would almost certainly be created by an artificial intelligence For the purposes of mankind however superintelligent artificial intelligence is the ultimate technology due to the fact that it will be able to surpass humans in every field and if anything replace the need for human intelligence In our past experience intelligence has been the most valuable trait for any entity to manifest for this reason in an anthropomorphic context we can predict that artificial intelligence will be the ultimate achievement The main reason why we will certainly not be able to replace superintelligent AI is that it will surpass us in every respect if there is ever any replacement it will be created by the AI similarly to the way we may create an AI that replaces us
,0.0,"<p>The cake example presented in the book ""artificial intelligence :a modern approach"" to illustrate a planning graph, doesn't show a mutex at action 'A1' between Eat(Cake) and the persistance of notHave(Cake), even though the precondition for the action Eat(Cake) and the result of the persistance of notHave(Cake) are opposite. So is there a special rule, or was it removed to just not clutter the graph? <a href=""https://i.stack.imgur.com/fFAws.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fFAws.png"" alt=""enter image description here""></a></p>
",,0,2016-12-09T00:22:49.843,,2446,2016-12-09T00:22:49.843,,,,,4145.0,,1,0,<path-planning>,is there a mutex between persistance actions and other actions in planning graphs?,14.0,30.03,13.59,10.84,0.0,0.0,19.0,The cake example presented in the book artificial intelligence a modern approach to illustrate a planning graph doesnt show a mutex at action A1 between EatCake and the persistance of notHaveCake even though the precondition for the action EatCake and the result of the persistance of notHaveCake are opposite So is there a special rule or was it removed to just not clutter the graph
,,"<p>if we are talking about AI that can replicate itself, it should have different rights, or the current rights must be modified, at least for political participation, or else, it could replicate itself enough so that the copies vote for one of them. Maybe a definition about what an AI entity is or preventing copies made by someone from being able to vote for their creator (that would also need to apply to children and their parents though.) would help.</p>

<p>even though a self-replicating AI might be problematic with our current human laws/rules, if similar enough to humans (e.g. general-purpose AI), it should have similar rights, (as in take the Universal Declaration of Human Rights and replace ""Human(s)"" by ""Human(s) and AI"") for example, it shouldn't be held in slavery (as in it should not be restricted to one job, without being able to change, and get some form of remuneration), though special purpose AI (like an AI that only plays go and has no concept outside of a board, and black or white tokens) might not be in need of such rights. </p>

<p>A bottom line may be ""an AI that can should know they can have rights if they ask so"" e.g. if an AI can get the concept of rights, it should know it can have them, and if it asks to have them, they may not be refused. </p>

<p>example: if an AI asks not to be terminated, it is granted all it's rights, and so shouldn't be, unless it must be by law. (as is the case for humans, though it is implicit)</p>

<p>an addition to the previous law would be that anyone (human or AI) can ask for an AI to have rights granted to them. </p>

<p>all this is to prevent someone of becoming a murderer because they shutdown their computer with a game running on it.</p>

<p>also, safe space (e.g. servers) should be provided for free for AIs that have rights to provide with the right to live.</p>

<p>edit: I'll be adding some more once I get home, do not downvote yet for not sticking that well to the question.</p>
",,4,2016-12-09T13:36:31.973,,2448,2016-12-19T13:21:46.443,2016-12-19T13:21:46.443,,4152.0,,4152.0,2441.0,2,0,,,,66.81,8.66,8.44,0.0,0.0,73.0,if we are talking about AI that can replicate itself it should have different rights or the current rights must be modified at least for political participation or else it could replicate itself enough so that the copies vote for one of them Maybe a definition about what an AI entity is or preventing copies made by someone from being able to vote for their creator that would also need to apply to children and their parents though would help even though a selfreplicating AI might be problematic with our current human lawsrules if similar enough to humans eg generalpurpose AI it should have similar rights as in take the Universal Declaration of Human Rights and replace Humans by Humans and AI for example it shouldnt be held in slavery as in it should not be restricted to one job without being able to change and get some form of remuneration though special purpose AI like an AI that only plays go and has no concept outside of a board and black or white tokens might not be in need of such rights A bottom line may be an AI that can should know they can have rights if they ask so eg if an AI can get the concept of rights it should know it can have them and if it asks to have them they may not be refused example if an AI asks not to be terminated it is granted all its rights and so shouldnt be unless it must be by law as is the case for humans though it is implicit an addition to the previous law would be that anyone human or AI can ask for an AI to have rights granted to them all this is to prevent someone of becoming a murderer because they shutdown their computer with a game running on it also safe space eg servers should be provided for free for AIs that have rights to provide with the right to live edit Ill be adding some more once I get home do not downvote yet for not sticking that well to the question
,1.0,"<p>Printing actionspace for Pong-v0 gives 'Discrete(6)' as output, i.e.0,1,2,3,4,5 are actions defined in environment as per documentation, but game needs only two controls. Why this discrepency? Further is that necessary to identify which number from 0 to 5 corresponds to which action in gym environment?</p>
",,2,2016-12-10T13:44:06.017,1.0,2449,2017-03-10T02:55:21.177,,,,,4163.0,,1,0,<gaming><reinforcement-learning>,What are different actions in action space of environment of 'Pong-v0' game from openai gym?,126.0,60.11,14.94,10.86,0.0,0.0,17.0,Printing actionspace for Pongv0 gives Discrete6 as output ie012345 are actions defined in environment as per documentation but game needs only two controls Why this discrepency Further is that necessary to identify which number from 0 to 5 corresponds to which action in gym environment
,1.0,"<p>I have started to make a Python AI, and thee beginning of its code looks something like this:</p>

<pre><code>print ""ARTEMIS starting. . .""

import random
import math
import os

greet = ['HI', 'HELLO', 'HEY', 'GOOD MORNING', 'GOOD DAY', 'GOOD AFTERNOON',               'GOOD EVENING', 'GREETINGS', 'GREETING']
joke = ['TELL ME A JOKE', 'JOKE', 'FUNNY', 'TELL ME SOMETHING FUNNY']
insult = ['YOURE A LOSER', 'YOU ARE A LOSER', 'YOU STINK', 'IDIOT', 'JERK',     'FOOL', 'DUMMY', 'HOOLIGAN', 'YOURE DUMB', 'YOURE STUPID', 'YOU ARE DUMB', 'YOU     ARE STUPID']
maker = ['WHO MADE YOU', 'WHO PROGRAMMED YOU', 'PLEASE TELL ME WHO MADE     YOU', 'PLEASE TELL ME WHO PROGRAMMED YOU']
name = ['ARTEMIS', 'A.R.T.E.M.I.S.', 'HEY ARTEMIS', 'HEY A.R.T.E.M.I.S.',     'ARTIE', 'HEY ARTIE', 'HELLO ARTEMIS', 'HELLO A.R.T.E.M.I.S.', 'HELLO ARTIE']
myAge = ['HOW OLD AM I', 'WHAT IS MY AGE', 'MY AGE']
tip = ['GIVE ME A TIP', 'TIP', 'LESSON', 'GIVE ME A LIFE LESSON', 'LIFE     LESSON', 'DO YOU HAVE A LIFE LESSON TO SHARE']
language = ['WHAT PROGRAMMING LANGUAGE WAS USED TO MAKE YOU', 'WHAT     PROGRAMMING LANGUAGE DO YOU USE', 'PROGRAMMING LANGUAGE']
compliment = ['COOL', 'AWESOME', 'I LIKE YOU', 'EXCELLENT', 'YOURE COOL',     'YOURE AWESOME', 'YOU ARE COOL', 'YOU ARE AWESOME']
maths = ['LETS DO MATH', 'CALCULATE', 'CALCULATOR','DO MATH', 'MATH',      'PLEASE DO MATH', 'DO ARITHMETIC', 'ARITHMETIC', 'PLEASE DO ARITHMETIC']
game = ['GAME', 'LETS PLAY A GAME', 'LETS HAVE FUN', 'WANT TO PLAY A GAME']
gender = ['WHAT GENDER ARE YOU', 'ARE YOU A BOY OR A GIRL', 'ARE YOU MALE OR     FEMALE', 'BOY OR GIRL', 'MALE OR FEMALE', 'GENDER', 'ARE YOU A BOY OR GIRL']
guessWhat = ['GUESS WHAT', 'GUESS WHAT ARTEMIS', 'GUESS WHAT     A.R.T.E.M.I.S.', 'GUESS WHAT ARTIE', 'YOU WONT BELIEVE IT', 'YOU WILL NOT     BELIEVE IT', 'YOU WONT BELIEVE IT ARTEMIS', 'YOU WILL NOT BELIEVE IT ARTEMIS',     'YOU WONT BELIEVE IT A.R.T.E.M.I.S.', 'YOU WILL NOT BELIEVE IT A.R.T.E.M.I.S.',     'YOU WONT BELIEVE IT ARTIE', 'YOU WILL NOT BELIEVE IT ARTIE']
cls = ['CLEAR SCREEN', 'CLEARSCREEN', 'CLS', 'BLANK']
lawsOfRobotics = ['WHAT ARE THE LAWS OF ROBOTICS', 'WHAT ARE THE THREE LAWS     OF ROBOTICS', 'LAWS OF ROBOTICS', 'THREE LAWS OF ROBOTICS']
itsName = ['WHATS YOUR NAME', 'WHAT IS YOUR NAME', 'WHO ARE YOU']
</code></pre>

<p>However, I would like to know if I could make it detect ""similar"" phrases instead of trying to come up with every possible phrase someone would type. How can I do this?</p>
",,2,2016-12-11T00:41:30.270,1.0,2451,2017-02-09T17:27:24.707,,,,,4173.0,,1,1,<algorithm>,How to efficiently interpret phrases in a Python AI?,129.0,63.02,7.72,8.67,2172.0,0.0,7.0,I have started to make a Python AI and thee beginning of its code looks something like this However I would like to know if I could make it detect similar phrases instead of trying to come up with every possible phrase someone would type How can I do this
2453.0,2.0,"<p><a href=""https://en.wikipedia.org/wiki/Expert_system"" rel=""nofollow noreferrer"">From Wikipedia</a>, citations omitted:</p>

<blockquote>
  <p>In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning about knowledge, represented mainly as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software.</p>
  
  <p>An expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.</p>
</blockquote>

<p>CRUD webapps (websites that allows users to <strong>Create</strong> new entries in a database, <strong>Read</strong> existing entries in a database, <strong>Update</strong> entries within the database, and <strong>Delete</strong> entries from a database) are very common on the Internet. It is a vast field, encompassing both small-scale blogs to large websites such as StackExchange. The biggest commonality with all these CRUD apps is that they have a knowledge base that users can easily add and edit.</p>

<p>CRUD webapps, however, use the knowledge base in many, myriad and complex ways. As I am typing this question on StackOverflow, I see two lists of questions - <strong>Questions that may already have your answer</strong> and <strong>Similar Questions</strong>. These questions are obviously inspired by the content that I am typing in (title and question), and are pulling from previous questions that were posted on StackExchange. On the site itself, I can filter by questions based on tags, while finding new questions using StackExchange's own full-text search engine. StackExchange is a large company, but even small blogs also provide content recommendations, filtration, and full-text searching. You can imagine even more examples of hard-coded logic within a CRUD webapp that can be used to automate the extraction of valuable information from a knowledge base.</p>

<p>If we have a knowledge base that users can change, and we have an inference engine that is able to use the knowledge base to generate interesting results...is that enough to classify a system as being an ""expert system""? Or is there a fundamental difference between the expert systems and the CRUD webapps?</p>

<p>(This question could be very useful since if CRUD webapps are acting like ""expert systems"", then studying the best practices within ""expert systems"" can help improve user experience.)</p>
",,0,2016-12-11T03:25:30.900,,2452,2016-12-11T05:51:09.103,,,,,181.0,,1,1,<definitions><expert-system>,"Are CRUD webapps today the modern version of the ""expert system""?",48.0,43.43,13.75,10.13,0.0,0.0,65.0,From Wikipedia citations omitted In artificial intelligence an expert system is a computer system that emulates the decisionmaking ability of a human expert Expert systems are designed to solve complex problems by reasoning about knowledge represented mainly as if–then rules rather than through conventional procedural code The first expert systems were created in the 1970s and then proliferated in the 1980s Expert systems were among the first truly successful forms of artificial intelligence AI software An expert system is divided into two subsystems the inference engine and the knowledge base The knowledge base represents facts and rules The inference engine applies the rules to the known facts to deduce new facts Inference engines can also include explanation and debugging abilities CRUD webapps websites that allows users to Create new entries in a database Read existing entries in a database Update entries within the database and Delete entries from a database are very common on the Internet It is a vast field encompassing both smallscale blogs to large websites such as StackExchange The biggest commonality with all these CRUD apps is that they have a knowledge base that users can easily add and edit CRUD webapps however use the knowledge base in many myriad and complex ways As I am typing this question on StackOverflow I see two lists of questions Questions that may already have your answer and Similar Questions These questions are obviously inspired by the content that I am typing in title and question and are pulling from previous questions that were posted on StackExchange On the site itself I can filter by questions based on tags while finding new questions using StackExchanges own fulltext search engine StackExchange is a large company but even small blogs also provide content recommendations filtration and fulltext searching You can imagine even more examples of hardcoded logic within a CRUD webapp that can be used to automate the extraction of valuable information from a knowledge base If we have a knowledge base that users can change and we have an inference engine that is able to use the knowledge base to generate interesting resultsis that enough to classify a system as being an expert system Or is there a fundamental difference between the expert systems and the CRUD webapps This question could be very useful since if CRUD webapps are acting like expert systems then studying the best practices within expert systems can help improve user experience
,,"<p>The key feature of an expert system is that the knowledge base is structured to be traversed by the inference engine.  Web sites like Stack Exchange don't really use an inference engine; they do full-text searches on minimally-structured data.  A real inference engine would be able to answer novel queries by putting together answers to existing questions; Stack Exchange sites can't even tell if a question is duplicate without human confirmation.</p>
",,0,2016-12-11T05:24:25.783,,2453,2016-12-11T05:24:25.783,,,,,2329.0,2452.0,2,3,,,,38.96,13.81,11.93,0.0,0.0,9.0,The key feature of an expert system is that the knowledge base is structured to be traversed by the inference engine Web sites like Stack Exchange dont really use an inference engine they do fulltext searches on minimallystructured data A real inference engine would be able to answer novel queries by putting together answers to existing questions Stack Exchange sites cant even tell if a question is duplicate without human confirmation
,,"<p>No, I don't think there's any reason to say that - in general - CRUD apps ""are"" expert systems. A given CRUD app <em>could</em> incorporate an expert system, but by and large CRUD apps are considered among the ""dumbest"" of applications exactly because they don't feature much intelligence... you can just Create, Read, Update and Delete entities.  From what I've seen, the closest you get to seeing anything like an expert system in a typical enterprise CRUD app is some validation / business rules logic built using something like <a href=""http://www.jboss.org/drools"" rel=""nofollow noreferrer"">Drools</a></p>
",,0,2016-12-11T05:51:09.103,,2454,2016-12-11T05:51:09.103,,,,,33.0,2452.0,2,1,,,,49.35,12.01,10.34,0.0,0.0,21.0,No I dont think theres any reason to say that in general CRUD apps are expert systems A given CRUD app could incorporate an expert system but by and large CRUD apps are considered among the dumbest of applications exactly because they dont feature much intelligence you can just Create Read Update and Delete entities From what Ive seen the closest you get to seeing anything like an expert system in a typical enterprise CRUD app is some validation business rules logic built using something like Drools
,,"<p>In addition to what has already been said about AI, I have the following to add. ""AI"" has had quite a history going all the way back to the original <a href=""https://en.wikipedia.org/wiki/Perceptron"" rel=""nofollow noreferrer"">Perceptron</a>. Marvin Minsky slammed the Perceptron in 1969 for not being able to solve the XOR problem and anything that was not linearly separable, so ""Artifical Intelligence"" became a dirty word for a while, only to regain interests in the 1980s. During that time, neural nets were revived, backpropagation used to train them was developed, and as computer technology continued its exponential growth, so did ""AI"" and what became possible.</p>

<p>Today, there are lots of things we take for granted which would've been considered ""AI"" 10 or 15 years ago, like speech recognition, for example. I got my starts in ""AI"" speech recognition back in the late 70s where you had to train the voice models to understand a single human speaker. Today, speech recognition is an afterthought with your Google apps, for example, and no a priori training is needed. Yet this technology is not, at least in general audiences, considered ""AI"" anymore.</p>

<p>And so, what would be ""minimum requirements""? That would depend on whom you ask. And what time. It would appear that that term only applies to technology ""on the bleeding edge"". Once it becomes developed and commonplace, it is no longer referred to as AI. This is true even of Neural Nets, which are dominant in data science right now, but are referred to as ""machine learning"".</p>

<p>Also check out the lively discussion on <a href=""https://www.quora.com/What-are-the-main-differences-between-artificial-intelligence-and-machine-learning"" rel=""nofollow noreferrer"">Quora</a>.</p>
",,1,2016-12-11T15:01:31.387,,2455,2016-12-11T15:01:31.387,,,,,4185.0,1507.0,2,5,,,,62.38,10.32,9.19,0.0,0.0,53.0,In addition to what has already been said about AI I have the following to add AI has had quite a history going all the way back to the original Perceptron Marvin Minsky slammed the Perceptron in 1969 for not being able to solve the XOR problem and anything that was not linearly separable so Artifical Intelligence became a dirty word for a while only to regain interests in the 1980s During that time neural nets were revived backpropagation used to train them was developed and as computer technology continued its exponential growth so did AI and what became possible Today there are lots of things we take for granted which wouldve been considered AI 10 or 15 years ago like speech recognition for example I got my starts in AI speech recognition back in the late 70s where you had to train the voice models to understand a single human speaker Today speech recognition is an afterthought with your Google apps for example and no a priori training is needed Yet this technology is not at least in general audiences considered AI anymore And so what would be minimum requirements That would depend on whom you ask And what time It would appear that that term only applies to technology on the bleeding edge Once it becomes developed and commonplace it is no longer referred to as AI This is true even of Neural Nets which are dominant in data science right now but are referred to as machine learning Also check out the lively discussion on Quora
,,"<p>To compare the strings you can use Fuzzy string matching. <a href=""https://github.com/seatgeek/fuzzywuzzy"" rel=""nofollow noreferrer"">FuzzyWuzzy</a> is a python package that does this using the <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein distance</a>, which calculates the difference between two strings by counting the number of single character edits (insert, delete, substitute) that have to be done to make them equal.</p>

<hr>

<p>To install the package you can use <code>pip</code> for example:</p>

<pre><code>pip install fuzzywuzzy
</code></pre>

<p>Then you can use it to check the similarity between two strings (100 is the highest score; higher score means more similar):</p>

<pre><code>&gt;&gt;&gt; from fuzzywuzzy import fuzz
&gt;&gt;&gt; fuzz.ratio(""How are you?"", ""How are you?"")
100
&gt;&gt;&gt; fuzz.ratio(""How are you?"", ""How are you doing?"")
80
</code></pre>

<p>Or, which might be most interesting to you, find the most similar string from a list of strings:</p>

<pre><code>from fuzzywuzzy import process
choices = ['HI', 'HELLO', 'HEY', 'GOOD MORNING','TELL ME A JOKE', 'JOKE', 'FUNNY','GIVE ME A TIP', 'TIP', 'LESSON','WHAT IS YOUR NAME', 'WHO ARE YOU', 'HOW ARE YOU?']
</code></pre>

<p>You can get a list of the phrases with the highest score (parameter limit, 2 in this example):</p>

<pre><code>&gt;&gt;&gt; process.extract(""hello there"", choices, limit=2)
[('HELLO', 90), ('HEY', 60)]
&gt;&gt;&gt; process.extract(""please make a joke"", choices, limit=2)
[('JOKE', 90), ('TELL ME A JOKE', 69)]
</code></pre>

<p>Or just get the one with the highest score:</p>

<pre><code>&gt;&gt;&gt; process.extractOne(""whoe are you?"", choices)
('WHO ARE YOU', 96)
&gt;&gt;&gt; process.extractOne(""I want a lesson"", choices)
('LESSON', 90)
</code></pre>

<p><em>Note</em>:</p>

<p>There are cases where another phrase might give a relatively high score also:</p>

<pre><code>&gt;&gt;&gt; fuzz.ratio(""How are you?"", ""Who are you?"")
83
</code></pre>

<p>But as long as you also have ""Who are you"" and ""How are you"" in the list, they will be detected correctly:</p>

<pre><code>&gt;&gt;&gt; process.extract(""Who are you?"", choices, limit=2)
[('WHO ARE YOU', 100), ('HOW ARE YOU?', 91)]
&gt;&gt;&gt; process.extract(""How are you?"", choices, limit=2)
[('HOW ARE YOU?', 100), ('WHO ARE YOU', 91)]
</code></pre>

<hr>

<p>Also see: <a href=""http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/"" rel=""nofollow noreferrer"">FuzzyWuzzy: Fuzzy String Matching in Python</a></p>
",,0,2016-12-11T16:00:00.973,,2456,2016-12-11T16:00:00.973,,,,,198.0,2451.0,2,1,,,,31.25,10.81,10.54,1024.0,0.0,30.0,To compare the strings you can use Fuzzy string matching FuzzyWuzzy is a python package that does this using the Levenshtein distance which calculates the difference between two strings by counting the number of single character edits insert delete substitute that have to be done to make them equal To install the package you can use for example Then you can use it to check the similarity between two strings 100 is the highest score higher score means more similar Or which might be most interesting to you find the most similar string from a list of strings You can get a list of the phrases with the highest score parameter limit 2 in this example Or just get the one with the highest score Note There are cases where another phrase might give a relatively high score also But as long as you also have Who are you and How are you in the list they will be detected correctly Also see FuzzyWuzzy Fuzzy String Matching in Python
,,"<p>No, you can not, with current state of art, if test request some kind of abstraction in the area. Allow me to show two examples:</p>

<p>1)</p>

<p>The text books says ""the bones in fingers are the proximal, intermediate and distal phalange"" and test says ""say which one of the following is a finger bone: a) ...xxx... b) proximal phalange; c) ... . </p>

<p>A program CAN answer that.</p>

<p>2)</p>

<p>The text is a mathematical one that explains linear equations, and the test queries ""write and solve the equation set for the following problem: one car is twice faster than another, the two cars reaches their objective with 10 minutes of difference, blah, blah, ... which is the speed of the first car: a) 1 km/h b) 2 km/h c) 3 km/h.</p>

<p>A program CAN NOT answer that.</p>
",,4,2016-12-11T18:12:47.417,,2458,2016-12-11T18:12:47.417,,,,,4188.0,2437.0,2,-1,,,,77.57,8.65,8.26,0.0,0.0,46.0,No you can not with current state of art if test request some kind of abstraction in the area Allow me to show two examples 1 The text books says the bones in fingers are the proximal intermediate and distal phalange and test says say which one of the following is a finger bone a xxx b proximal phalange c A program CAN answer that 2 The text is a mathematical one that explains linear equations and the test queries write and solve the equation set for the following problem one car is twice faster than another the two cars reaches their objective with 10 minutes of difference blah blah which is the speed of the first car a 1 kmh b 2 kmh c 3 kmh A program CAN NOT answer that
,,"<p>I think that one very big advantage would be that if the cars could communicate with each other, they could drive synchronously.<br>
For example, if there was a traffic light, and, let's say, 10 cars are waiting for it to change to green (let's just assume that there would still be something similar to traffic lights). Then when it changes to green all cars could accelerate at the same speed (depending on the acceleration of the front car) at the same time.</p>
",,1,2016-12-11T21:01:57.633,,2460,2016-12-12T09:51:11.747,2016-12-12T09:51:11.747,,145.0,,4196.0,2127.0,2,0,,,,60.69,9.64,7.88,0.0,0.0,14.0,I think that one very big advantage would be that if the cars could communicate with each other they could drive synchronously For example if there was a traffic light and lets say 10 cars are waiting for it to change to green lets just assume that there would still be something similar to traffic lights Then when it changes to green all cars could accelerate at the same speed depending on the acceleration of the front car at the same time
,,"<p>Many cars now instead of just cameras, use radars. Snow, heavy rain, and other weather conditions should not affect them at all. Objects like ducks will be detected. The only problem right now is dealing with things like red lights or road signs, as you have to use a camera to see and interpret them.</p>
",,0,2016-12-11T22:28:08.830,,2461,2016-12-11T22:28:08.830,,,,,4198.0,1946.0,2,0,,,,82.85,8.17,8.91,0.0,0.0,8.0,Many cars now instead of just cameras use radars Snow heavy rain and other weather conditions should not affect them at all Objects like ducks will be detected The only problem right now is dealing with things like red lights or road signs as you have to use a camera to see and interpret them
,2.0,"<p>I am having a go at creating a program that does math like a human. By inventing statements, assigning probabilities to statements (to come back and think more deeply about later). But I'm stuck at the first hurdle.</p>

<p>If it is given the proposition</p>

<pre><code>   ∃x∈ℕ: x==123
</code></pre>

<p>So, like a human it might test this proposition for a hundred or so numbers and then assign this proposition as ""unlikely to be true"". In other words it has concluded that all natural numbers are not equal to 123. Clearly ludicrous!</p>

<p>On the other hand this statement it decides is probably false which is good:</p>

<pre><code> ∃x∈ℕ: x+3 ≠ 3+x
</code></pre>

<p>Any ideas how to get round this hurdle? How does a human ""know"" for example that all natural numbers are different from the number 456. What makes these two cases different?</p>

<p>I don't want to give it too many axioms. I want it to find out things for itself.</p>
",,2,2016-12-11T23:15:04.860,1.0,2462,2017-01-31T23:21:16.493,2016-12-11T23:28:00.977,,4199.0,,4199.0,,1,2,<fuzzy-logic><reasoning><logic>,"How to determine the probability of an ""existence"" question",84.0,73.27,8.0,8.72,34.0,0.0,22.0,I am having a go at creating a program that does math like a human By inventing statements assigning probabilities to statements to come back and think more deeply about later But Im stuck at the first hurdle If it is given the proposition So like a human it might test this proposition for a hundred or so numbers and then assign this proposition as unlikely to be true In other words it has concluded that all natural numbers are not equal to 123 Clearly ludicrous On the other hand this statement it decides is probably false which is good Any ideas how to get round this hurdle How does a human know for example that all natural numbers are different from the number 456 What makes these two cases different I dont want to give it too many axioms I want it to find out things for itself
,,"<p>I guess it could be possible with a lot of questions to learn from and only from a certain topic. Just watch what IBM was capable of <a href=""https://www.youtube.com/watch?v=WFR3lOm_xhE"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=WFR3lOm_xhE</a> very impressive!</p>
",,0,2016-12-12T07:42:48.150,,2463,2016-12-12T07:42:48.150,,,,,4196.0,2437.0,2,0,,,,78.25,12.97,8.87,0.0,0.0,11.0,I guess it could be possible with a lot of questions to learn from and only from a certain topic Just watch what IBM was capable of httpswwwyoutubecomwatchvWFR3lOmxhE very impressive
,,,,0,2016-12-12T09:13:14.583,,2464,2016-12-12T09:13:14.583,2016-12-12T09:13:14.583,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,"For questions about chat-bots. NOT for questions about how to program a chat-bot, as those kinds of questions are off-topic.",,0,2016-12-12T09:13:14.583,,2465,2016-12-13T19:07:24.817,2016-12-13T19:07:24.817,,145.0,,145.0,,4,0,,,,78.25,11.69,8.87,0.0,0.0,6.0,For questions about chatbots NOT for questions about how to program a chatbot as those kinds of questions are offtopic
,,"<ul>
    <li>Social issues in a world where artificial intelligence is common</li>
    <li>Conceptual aspects of AI</li>
    <li>Human factors in AI development</li>
</ul>
",,0,2016-12-12T16:02:32.357,,2466,2016-12-12T16:02:32.357,2016-12-12T16:02:32.357,,75.0,,-1.0,,7,0,,,,35.27,14.16,12.89,0.0,0.0,0.0,Social issues in a world where artificial intelligence is common Conceptual aspects of AI Human factors in AI development
,,"<ul>
    <li>Programming of artificial intelligence or machine learning
    <li>Recommendations for career paths, development tools, or other off-site resources</li>
    <li>Questions that are primarily opinion-based</li>
</ul>
",,0,2016-12-12T16:04:46.100,,2467,2016-12-12T16:04:46.100,2016-12-12T16:04:46.100,,75.0,,-1.0,,7,0,,,,-10.07,25.18,14.78,0.0,0.0,4.0,Programming of artificial intelligence or machine learning Recommendations for career paths development tools or other offsite resources Questions that are primarily opinionbased
,,"<p>No, smart cars do not know what to do when surrounded with ducks or flood waters, and it's possible they never will.  </p>

<p>As with all machine learning, a computer knows only what it's taught.  If an event arises that's unusual, the AI will have less relevant training on how to respond, so its reaction behavior <em>necessarily</em> will be inferior to its routine ""standard operating procedure"", for which is has been heavily trained.  (Of course this is true of humans too.)</p>

<p>Due to liability concerns, when encountering an outlier condition, smart cars will almost certainly be designed by their makers to <em>immediately</em> pull off the road and wait to be explicitly told what to do -- by the human in the car or by communicating with a central command office that exists to disambiguate such confusion and resolve cognitive impasses.  When confused, just like a child, a smart car will be designed to seek external assistance -- and is likely to do so indefinitely, I suspect.</p>

<p>That's why, despite Google's recent cars that lack steering wheels, smart cars most certtainly <em>will</em> retain some means of manual control -- be it a wheel and pedals, or at least verbal commands.  Given the many forms of weirdness that are possible on the road, it's possible smart cars will <em>never</em> be fully autonomous.</p>

<p>As for bad weather conditions, how well do smart cars currently perform?  Nobody outside of a car manufacturer can say for certain.  Lidar and radar are superior to the human eye in seeing through fog and snow.  But (competent) humans are likely to remain better than a smart car at dynamically learning the limit of adhesion and compensating (since this is a learned skill few smart cars will already know or can learn quickly -- given this car, these tires, this road surface, this angle of road, etc).</p>

<p>Initially smart cars will turn to the human when the going gets rough, ceding control back to them.  Once smart cars have driven a few million miles in snow, slush, high wind, floods, and ice, and encountered many ducks, angry mooses, and irate pedestrians, they will have been taught to do more for themselves.  Until then, and perhaps for decades yet, I suspect they will turn to mommy and ask for help.</p>
",,0,2016-12-12T16:15:19.177,,2468,2016-12-12T16:15:19.177,,,,,1657.0,1946.0,2,2,,,,63.02,10.86,9.55,0.0,0.0,68.0,No smart cars do not know what to do when surrounded with ducks or flood waters and its possible they never will As with all machine learning a computer knows only what its taught If an event arises thats unusual the AI will have less relevant training on how to respond so its reaction behavior necessarily will be inferior to its routine standard operating procedure for which is has been heavily trained Of course this is true of humans too Due to liability concerns when encountering an outlier condition smart cars will almost certainly be designed by their makers to immediately pull off the road and wait to be explicitly told what to do by the human in the car or by communicating with a central command office that exists to disambiguate such confusion and resolve cognitive impasses When confused just like a child a smart car will be designed to seek external assistance and is likely to do so indefinitely I suspect Thats why despite Googles recent cars that lack steering wheels smart cars most certtainly will retain some means of manual control be it a wheel and pedals or at least verbal commands Given the many forms of weirdness that are possible on the road its possible smart cars will never be fully autonomous As for bad weather conditions how well do smart cars currently perform Nobody outside of a car manufacturer can say for certain Lidar and radar are superior to the human eye in seeing through fog and snow But competent humans are likely to remain better than a smart car at dynamically learning the limit of adhesion and compensating since this is a learned skill few smart cars will already know or can learn quickly given this car these tires this road surface this angle of road etc Initially smart cars will turn to the human when the going gets rough ceding control back to them Once smart cars have driven a few million miles in snow slush high wind floods and ice and encountered many ducks angry mooses and irate pedestrians they will have been taught to do more for themselves Until then and perhaps for decades yet I suspect they will turn to mommy and ask for help
,,"<p>My understanding is that ""pornbots"" regularly pass the Turing Test in regards to the general public <em>(although, clearly, the judgement of those being tricked is weakened by hormonal imperatives.)</em></p>

<p><a href=""http://boingboing.net/2004/07/27/elizabot-passes-sexc.html"" rel=""nofollow noreferrer"">http://boingboing.net/2004/07/27/elizabot-passes-sexc.html</a></p>

<p><a href=""http://resources.infosecinstitute.com/pornbots-sexual-barbies-of-the-future/"" rel=""nofollow noreferrer"">http://resources.infosecinstitute.com/pornbots-sexual-barbies-of-the-future/</a></p>
",,0,2016-12-12T17:27:37.847,,2469,2016-12-12T17:27:37.847,,,,,1671.0,2427.0,2,0,,,,-36.13,39.46,13.32,0.0,0.0,30.0,My understanding is that pornbots regularly pass the Turing Test in regards to the general public although clearly the judgement of those being tricked is weakened by hormonal imperatives httpboingboingnet20040727elizabotpassessexchtml httpresourcesinfosecinstitutecompornbotssexualbarbiesofthefuture
,,"<p>A new physical lifeform could outperform and replace artificial intelligence when it
has feedback from organism (its body) to its design information (replacement of genes).</p>

<p>This evolution is expected because:</p>

<p>Artificial intelligence will redesign its own software very soon in its evolution.
After that, it will be restricted by the performance of the available hardware
and communication speed between parts.
Therefore it will design better processing hardware for itself, to run its next generation.</p>

<p>To squeeze most processing power out of a given amount of resources (matter, energy) and
circumstances (temperature, radiation) the design has to be small (material resources
and delay of interconnections), energy efficient (heat evacuation), and adapted to the
kind of functions used by the software (hardware architecture).</p>

<p>To tackle this profoundly, artificial intelligence will design the new hardware at
atom by atom scale.
This leads to new problems of natural degradation by radiation, atomic decay and other
quantum-mechanical problems and opportunities.
The solution of these new problems is redundancy and the ability to repair
degraded parts by atomic-level machinery.
This atomic level repair machinery is the same machinery which builds and extends the hardware
for new individual artificial intelligence systems.
Since this feature is there, it can, and will, also be used to restructure parts
of the hardware while it runs to integrate (compile) knowledge in hardware (more efficient).</p>

<p>The machinery to build and maintain such hardware could be inspired by the biological machinery
which will be understood by the system by then.
However, when the artificial intelligence refactors these principles with full understanding
and anticipation, the resulting ""hardware"" will be quite different from the old biological
machinery and very different from the static silicon based processor structures.<br>
The main differences are:</p>

<ul>
<li><p>The design information will be available, relating the features of the realization
with design choices.
This provides direct feedback from performance of the realization (the hardware, the body)
to the design information.
That augments the design information for designing new generations.
This feedback channel is the main difference between the new machinery and biological life.
Once that exists, it will be used for everything, not only for processing hardware for
artificial intelligence.</p></li>
<li><p>The new design described here is basically processing hardware rather than a body
for fighting and propagating
(although the eternal fight for resources will not end with the dawn of
artificial intelligence).</p></li>
<li><p>It will use more compact molecules because it is designed rather than evolved blindly.
(The current biological life uses monster-molecules evolved by random changes until
one or other corner of the molecule has the right shape to catalyze a specific chemical reaction).</p></li>
<li><p>Since parts of the hardware can be restructured while the system runs, the distinction between
hardware and software will become very fuzzy (as in biology).</p></li>
</ul>

<p>The drastic increase in efficiency and evolution speed let it outperform the old biological
life (which lacks design and feedback) and outperform artificial intelligence (which was not
integrated in matter).</p>

<p>When this stage is reached, the systems will look like a natural, intelligent,
propagating life-form and therefore supersede the stage of artificial intelligence
running on human-made processing hardware.</p>

<p>This answers the last part of the question:
""... or is their something else which has the potential to replace artificial intelligence"".</p>
",,5,2016-12-12T20:52:49.510,,2470,2016-12-17T14:00:42.523,2016-12-17T14:00:42.523,,4026.0,,4026.0,2443.0,2,3,,,,32.83,16.07,9.88,0.0,0.0,90.0,A new physical lifeform could outperform and replace artificial intelligence when it has feedback from organism its body to its design information replacement of genes This evolution is expected because Artificial intelligence will redesign its own software very soon in its evolution After that it will be restricted by the performance of the available hardware and communication speed between parts Therefore it will design better processing hardware for itself to run its next generation To squeeze most processing power out of a given amount of resources matter energy and circumstances temperature radiation the design has to be small material resources and delay of interconnections energy efficient heat evacuation and adapted to the kind of functions used by the software hardware architecture To tackle this profoundly artificial intelligence will design the new hardware at atom by atom scale This leads to new problems of natural degradation by radiation atomic decay and other quantummechanical problems and opportunities The solution of these new problems is redundancy and the ability to repair degraded parts by atomiclevel machinery This atomic level repair machinery is the same machinery which builds and extends the hardware for new individual artificial intelligence systems Since this feature is there it can and will also be used to restructure parts of the hardware while it runs to integrate compile knowledge in hardware more efficient The machinery to build and maintain such hardware could be inspired by the biological machinery which will be understood by the system by then However when the artificial intelligence refactors these principles with full understanding and anticipation the resulting hardware will be quite different from the old biological machinery and very different from the static silicon based processor structures The main differences are The design information will be available relating the features of the realization with design choices This provides direct feedback from performance of the realization the hardware the body to the design information That augments the design information for designing new generations This feedback channel is the main difference between the new machinery and biological life Once that exists it will be used for everything not only for processing hardware for artificial intelligence The new design described here is basically processing hardware rather than a body for fighting and propagating although the eternal fight for resources will not end with the dawn of artificial intelligence It will use more compact molecules because it is designed rather than evolved blindly The current biological life uses monstermolecules evolved by random changes until one or other corner of the molecule has the right shape to catalyze a specific chemical reaction Since parts of the hardware can be restructured while the system runs the distinction between hardware and software will become very fuzzy as in biology The drastic increase in efficiency and evolution speed let it outperform the old biological life which lacks design and feedback and outperform artificial intelligence which was not integrated in matter When this stage is reached the systems will look like a natural intelligent propagating lifeform and therefore supersede the stage of artificial intelligence running on humanmade processing hardware This answers the last part of the question or is their something else which has the potential to replace artificial intelligence
,0.0,"<p>I am a strong believer of Marvin Minsky's idea about Artificial General Intelligence (AGI) and one of his thoughts was that probabilistic models are dead ends in the field of AGI. I would really like to know thoughts/ideas of people who believe otherwise.</p>

<p>[P.S. It should be treated more like an informational thread rather than a strict A2A question]</p>
",,2,2016-12-14T05:17:44.203,2.0,2472,2016-12-14T05:17:44.203,,,,,4244.0,,1,5,<agi><probabilistic>,Are probabilistic models dead ends in AI?,80.0,51.48,11.55,10.5,0.0,0.0,10.0,I am a strong believer of Marvin Minskys idea about Artificial General Intelligence AGI and one of his thoughts was that probabilistic models are dead ends in the field of AGI I would really like to know thoughtsideas of people who believe otherwise PS It should be treated more like an informational thread rather than a strict A2A question
,1.0,"<p>There is an example related to perceptron learning, but I couldn't get it, I don't exactly know how to solve it.</p>

<p>There is a snippet from <a href=""https://i.stack.imgur.com/2vGtu.jpg"" rel=""nofollow noreferrer"">lecture notes</a>.</p>

<p>What is the transformation between epochs?</p>
",,3,2016-12-14T05:21:39.570,,2473,2016-12-15T15:35:08.473,,,,,4245.0,,1,2,<machine-learning><algorithm><learning-algorithms>,Perceptron learning,64.0,68.47,9.03,9.31,0.0,0.0,7.0,There is an example related to perceptron learning but I couldnt get it I dont exactly know how to solve it There is a snippet from lecture notes What is the transformation between epochs
2495.0,2.0,"<p>One of the most compelling issues regarding AI would be in behavior and relationships. </p>

<p>What are some of the methods to address this?  For example, friendship, or laughing at joke?  The concept of humor?</p>
",,2,2016-12-14T17:07:18.223,,2474,2016-12-19T02:17:26.820,,,,,3555.0,,1,2,<ai-design><emotional-intelligence><knowledge-representation>,How could human behavior and relationships be implemented?,54.0,71.31,9.13,9.63,0.0,0.0,6.0,One of the most compelling issues regarding AI would be in behavior and relationships What are some of the methods to address this For example friendship or laughing at joke The concept of humor
2506.0,1.0,"<p>I wanted to started experimenting with neural network and as a toy problem I wished to train one to chat, i.e. implement a chatting bot like cleverbot. Not that clever anyway.</p>

<p>I looked around for some documentation and I found many tutorial on general tasks, but few on this specific topic. The one I found just exposed the results without giving insights on the implementation. The ones that did, did it pretty shallowy (the tensorflow documentation page on seq2seq is lacking imho).</p>

<p>Now, I feel I may have understood the principle more or less but I'm not sure and I am not even sure how to start. Thus I will explain how I would tackle the problem and I'd like a feedback on this solution, telling me where I'm mistaken and possibly have any link to detailed explainations and practical knowledge on the process.</p>

<ol>
<li><p>The dataset I will use for the task is the dump of all my facebook and whatsapp chat history. I don't know how big it will be but possibly still not large enough. The target language is not english, therefore I don't know where to quickly gather meaningful conversation samples.</p></li>
<li><p>I am going to generate a thought vector out of each sentence. Still don't know how actually; I found a nice example for word2vec on deeplearning4j website, but none for sentences. I understood how word vectors are built and why, but I could not find an exhaustive explaination for sentence vectors.</p></li>
<li><p>Using thought vectors as input and output I am going to train the neural network. I don't know how many layers it should have, and which ones have to be lstm layers.</p></li>
<li><p>Then there should be another neural network that is able to transform a thought vector into a sequence of character composing a sentence. I read that I should use padding to make up for different sentence lengths, but I miss how to encode characters (are codepoints enough?).</p></li>
</ol>
",,0,2016-12-14T21:56:01.237,2.0,2475,2016-12-19T15:47:08.913,,,,,4259.0,,1,3,<neural-networks><chat-bots><recurrent-neural-networks><lstm>,How to train a chatbot,284.0,70.13,9.51,9.11,0.0,0.0,42.0,I wanted to started experimenting with neural network and as a toy problem I wished to train one to chat ie implement a chatting bot like cleverbot Not that clever anyway I looked around for some documentation and I found many tutorial on general tasks but few on this specific topic The one I found just exposed the results without giving insights on the implementation The ones that did did it pretty shallowy the tensorflow documentation page on seq2seq is lacking imho Now I feel I may have understood the principle more or less but Im not sure and I am not even sure how to start Thus I will explain how I would tackle the problem and Id like a feedback on this solution telling me where Im mistaken and possibly have any link to detailed explainations and practical knowledge on the process The dataset I will use for the task is the dump of all my facebook and whatsapp chat history I dont know how big it will be but possibly still not large enough The target language is not english therefore I dont know where to quickly gather meaningful conversation samples I am going to generate a thought vector out of each sentence Still dont know how actually I found a nice example for word2vec on deeplearning4j website but none for sentences I understood how word vectors are built and why but I could not find an exhaustive explaination for sentence vectors Using thought vectors as input and output I am going to train the neural network I dont know how many layers it should have and which ones have to be lstm layers Then there should be another neural network that is able to transform a thought vector into a sequence of character composing a sentence I read that I should use padding to make up for different sentence lengths but I miss how to encode characters are codepoints enough
2478.0,2.0,"<p>We are doing research,spending hours figuring out how we can bring the real concept of an A.I software[Intelligent Agent] to work.Also we trying to implement some basics(applications in Business,Health,Education to mention but a few).</p>

<hr>

<p>On the other side,sometimes we forget about the dark side of what this brilliant source of conceptual wisdom(Artificial Intelligence) could bring to humanity.for instance;this is because someone[Unethical] out there could buy thousands of cheap drones, attach a gun to each of them, and develop an AI software to send them around shooting people. </p>

<p>If the software was good enough this could result in far more destruction than a normal terrorist attack. And I fully expect that the software part of this will become easy in the future if it isn't already today.</p>

<p>This is very different from the options of a terrorist group today, because right now they need humans to carry out attacks and there is a limit to the amount of damage that can be done per person. </p>

<p>Having relatively simple AI in place of the human here brings the marginal cost of an attack down to zero and hurts the ability of law enforcement to stop attacks. So, there is a risk that such AI software someone upgrades it, gets better and better to the extent that it at least destabilizes things.</p>

<p>Therefore,according to such real life scenario(my point of view);</p>

<p><strong>Here is my question:</strong> 
Could there be extential threats to humanity in future?</p>
",,0,2016-12-15T07:00:16.043,2.0,2477,2016-12-30T02:52:17.120,,,,,1581.0,,1,8,<human-like><applications><human-inspired><cyberterrorism>,Could an Artificial Intelligent Program be an extential threat to Humanity?,344.0,57.71,11.55,9.64,0.0,0.0,35.0,We are doing researchspending hours figuring out how we can bring the real concept of an AI softwareIntelligent Agent to workAlso we trying to implement some basicsapplications in BusinessHealthEducation to mention but a few On the other sidesometimes we forget about the dark side of what this brilliant source of conceptual wisdomArtificial Intelligence could bring to humanityfor instancethis is because someoneUnethical out there could buy thousands of cheap drones attach a gun to each of them and develop an AI software to send them around shooting people If the software was good enough this could result in far more destruction than a normal terrorist attack And I fully expect that the software part of this will become easy in the future if it isnt already today This is very different from the options of a terrorist group today because right now they need humans to carry out attacks and there is a limit to the amount of damage that can be done per person Having relatively simple AI in place of the human here brings the marginal cost of an attack down to zero and hurts the ability of law enforcement to stop attacks So there is a risk that such AI software someone upgrades it gets better and better to the extent that it at least destabilizes things Thereforeaccording to such real life scenariomy point of view Here is my question Could there be extential threats to humanity in future
,,"<p>I would define intelligence as a ability to predict future. So if someone is intelligent, he can predict some aspects of future, and decide what to do based on his predictions. So, if ""intelligent"" person decide to hurt other persons, he might be very effective at this (for example Hitler and his staff). </p>

<p>Artificial intelligence might be extremely effective at predicting some aspects of uncertain future. And this IMHO leads to two negative scenarios:</p>

<ol>
<li>Someone programs it for hurting people. Either by mistake or on purpose.</li>
<li>Artificial intelligence will be designed for doing something safe, but at some point, to be more effective, it will redesign itself and maybe it will remove obstacles from its way. So if humans become obstacles, they will be removed very quickly and in very effective way.</li>
</ol>

<p>Of course, there are also positive scenarios, but you are not asking about them.</p>

<p>I recommend reading this cool post about artificial superintelligence and possible outcomes of creating it: <a href=""http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html"" rel=""nofollow noreferrer"">http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html</a></p>
",,1,2016-12-15T08:24:04.450,,2478,2016-12-16T08:44:06.403,2016-12-16T08:44:06.403,,2426.0,,4267.0,2477.0,2,3,,,,38.11,14.61,10.19,0.0,0.0,36.0,I would define intelligence as a ability to predict future So if someone is intelligent he can predict some aspects of future and decide what to do based on his predictions So if intelligent person decide to hurt other persons he might be very effective at this for example Hitler and his staff Artificial intelligence might be extremely effective at predicting some aspects of uncertain future And this IMHO leads to two negative scenarios Someone programs it for hurting people Either by mistake or on purpose Artificial intelligence will be designed for doing something safe but at some point to be more effective it will redesign itself and maybe it will remove obstacles from its way So if humans become obstacles they will be removed very quickly and in very effective way Of course there are also positive scenarios but you are not asking about them I recommend reading this cool post about artificial superintelligence and possible outcomes of creating it httpwaitbutwhycom201501artificialintelligencerevolution1html
,,"<p>even though that's not really AI, the easiest way to do that would probably be to put coefficients on each question</p>

<p>e.g. your question would have something like</p>

<pre><code>grammar=0.55
vocabulary=0.45
if(won){
  success=-1
}else{
  success=1
}
</code></pre>

<p>the lower the level, the less questions of this type will appear</p>

<p>at the end of the level, you sum each question times how much did the player succeed </p>

<p>player.level is a real number array </p>

<pre><code>for each question{
   player.level[""grammar""]=player.level[""grammar""]+question.grammar*question.success
   player.level[""vocabulary""]=player.level[""vocabulary""]+question.vocabulary*question.success
}
</code></pre>

<p>then when you initialize the level, generate the questions according to the player's level
choice is a random real number between 0 and sum of all player.level (offseted to be above 0)</p>

<pre><code>for each player.level{
   if choice&gt;player.level{
      choice=choice-player.level
   }else{
      return question_type=player.level.type
   }
}
</code></pre>

<p>though a ""true"" AI can make the same or better choices of questions, it will probably be slower, and require more data than a simple algorithm like this one.</p>
",,0,2016-12-15T10:18:25.177,,2479,2016-12-15T10:18:25.177,,,,,4152.0,2439.0,2,0,,,,56.93,9.76,9.25,422.0,0.0,17.0,even though thats not really AI the easiest way to do that would probably be to put coefficients on each question eg your question would have something like the lower the level the less questions of this type will appear at the end of the level you sum each question times how much did the player succeed playerlevel is a real number array then when you initialize the level generate the questions according to the players level choice is a random real number between 0 and sum of all playerlevel offseted to be above 0 though a true AI can make the same or better choices of questions it will probably be slower and require more data than a simple algorithm like this one
,,"<p>There is no transformation between epochs. One full iteration over the training set is considered an epoch.</p>

<p>Lets assume:</p>

<ul>
<li>We're considering a gradient descent in a space without local optima. This means that if you'd plot the errors we calculate below, this plot has but one lowest point.</li>
<li>The perceptron is intended to model a linearely seperable function. So, we could viualize our data points as a scatter plot and can draw a clear line through the scatter plot. Everything above/below the line should result in a true (1) output, everything on the other side should result in false (0).</li>
<li>We've set a learning rate (also called 'gain' or 'proportional change') upfront. This is a number between 0 and 1.</li>
</ul>

<p>You train your perceptron on a data set. During one iteration over this entire training set, you:</p>

<ul>
<li>Calculate an output and an error (delta between output and desired output) for a data point. </li>
<li>Adjust the input weights according to this error and the pre-set learning rate.</li>
<li>Apply the newly calculated weight.</li>
<li>Proceed to the next data point.</li>
</ul>

<p><strong>Transformation of weights happens during epochs, not between epochs.</strong></p>

<p>Ergo, weights may be changed for each data point in the training set. Oftentimes you would need to iterate over the entire set several times, before the weights converge (stabilize / stop changing). </p>

<p><strong>What use is the number of epochs?</strong></p>

<p>The number of iterations is significant as a measure of efficiency of the current algorithm under the current learning rate, for the current dataset. In short: You could use it for comparisons between learning rates, training sets or algorithms.</p>
",,1,2016-12-15T15:35:08.473,,2480,2016-12-15T15:35:08.473,,,,,4279.0,2473.0,2,2,,,,66.54,10.95,9.2,0.0,0.0,52.0,There is no transformation between epochs One full iteration over the training set is considered an epoch Lets assume Were considering a gradient descent in a space without local optima This means that if youd plot the errors we calculate below this plot has but one lowest point The perceptron is intended to model a linearely seperable function So we could viualize our data points as a scatter plot and can draw a clear line through the scatter plot Everything abovebelow the line should result in a true 1 output everything on the other side should result in false 0 Weve set a learning rate also called gain or proportional change upfront This is a number between 0 and 1 You train your perceptron on a data set During one iteration over this entire training set you Calculate an output and an error delta between output and desired output for a data point Adjust the input weights according to this error and the preset learning rate Apply the newly calculated weight Proceed to the next data point Transformation of weights happens during epochs not between epochs Ergo weights may be changed for each data point in the training set Oftentimes you would need to iterate over the entire set several times before the weights converge stabilize stop changing What use is the number of epochs The number of iterations is significant as a measure of efficiency of the current algorithm under the current learning rate for the current dataset In short You could use it for comparisons between learning rates training sets or algorithms
2501.0,2.0,"<p>How to train a bot, given a series of games in which he did (initially random) actions, to improve its behavior based on previous experiences?</p>

<p>The bot has some actions: e.g. shoot, wait, move, etc. It's a turn based ""game"" in which, for know, I'm running the bots with some objectives (e.g. kill some other bot) and random actions. So every bot will have a score function that at the end of the game will say, from X to Y (0 to 100?) if they did well or not.</p>

<p>So how to make the bots to learn of their previous experiences? Because this is not a fixed input as the neural networks take, this is kind of a list of games, each one in which the bot took several actions (one by every ""turn""). The IA functions that I know are used to <em>predict</em> future values.. I'm not sure is the same.</p>

<p>Maybe I should have a function that gets the ""more similar previous games"" that the bot played and checked what were the actions he took, if the results were bad he should take another action, if the results were good then he should take the same action. But this seems kind of hardcoded.</p>

<p>Another option would be to train a neural network (somehow fixing the problem of the fixed input) based on previous game actions and to predict the future action's results in score (something that I guess it's similar to how chess and Go games work) and choose the one that seems to have better outcome.</p>

<p>I hope this is not too abstract. I don't want to hardcode much stuff in the bots, I'd like them to learn by their own starting from a blank page.</p>
",,1,2016-12-16T00:53:20.137,,2481,2016-12-19T01:55:56.100,,,,,2352.0,,1,1,<ai-design>,How to make a bot to learn from previous games,79.0,78.48,7.13,7.48,0.0,0.0,58.0,How to train a bot given a series of games in which he did initially random actions to improve its behavior based on previous experiences The bot has some actions eg shoot wait move etc Its a turn based game in which for know Im running the bots with some objectives eg kill some other bot and random actions So every bot will have a score function that at the end of the game will say from X to Y 0 to 100 if they did well or not So how to make the bots to learn of their previous experiences Because this is not a fixed input as the neural networks take this is kind of a list of games each one in which the bot took several actions one by every turn The IA functions that I know are used to predict future values Im not sure is the same Maybe I should have a function that gets the more similar previous games that the bot played and checked what were the actions he took if the results were bad he should take another action if the results were good then he should take the same action But this seems kind of hardcoded Another option would be to train a neural network somehow fixing the problem of the fixed input based on previous game actions and to predict the future actions results in score something that I guess its similar to how chess and Go games work and choose the one that seems to have better outcome I hope this is not too abstract I dont want to hardcode much stuff in the bots Id like them to learn by their own starting from a blank page
,1.0,"<p>I remember reading or hearing a claim that at any point in time since the publication of the MNIST dataset, it has never happened that a method not based on neural networks was the best given the state of science of that point in time.</p>

<p>Is this claim true?</p>
",,0,2016-12-16T01:01:33.070,,2482,2016-12-17T08:05:30.397,,,,,1670.0,,1,2,<neural-networks><history><mnist>,Neural Networks unmatched on MNIST?,48.0,71.99,7.49,9.04,0.0,0.0,3.0,I remember reading or hearing a claim that at any point in time since the publication of the MNIST dataset it has never happened that a method not based on neural networks was the best given the state of science of that point in time Is this claim true
,,"<p>If it is a game you can try a simple weightage calculation where if the bot perform an action that yields a positive result - killed an enemy, gained an advantageous position etc. Add a 'weight' to that action that in similar circumstances the chances of performing that action that will lead to a positive result is higher.</p>

<p>Yet due to the chance of not performing an action that was remembered to yield positive results, there is a little bit of 'randomness' and also a chance to discover new possibilities. Just remember not to let a single occurrence shift the weightage too much or allow a single action's weightage to become so high that the AI stops trying different actions on similar situations.</p>
",,0,2016-12-16T01:43:35.673,,2483,2016-12-16T01:43:35.673,,,,,4268.0,2481.0,2,0,,,,48.98,10.92,9.42,0.0,0.0,12.0,If it is a game you can try a simple weightage calculation where if the bot perform an action that yields a positive result killed an enemy gained an advantageous position etc Add a weight to that action that in similar circumstances the chances of performing that action that will lead to a positive result is higher Yet due to the chance of not performing an action that was remembered to yield positive results there is a little bit of randomness and also a chance to discover new possibilities Just remember not to let a single occurrence shift the weightage too much or allow a single actions weightage to become so high that the AI stops trying different actions on similar situations
,,"<p><em>There is no doubt as to the fact that AI has the potential to pose an existential threat to humanity.</em></p>

<blockquote>
  <p>The greatest threat to mankind lies with superintelligent AI.</p>
</blockquote>

<p>An artificial intelligence that surpasses human intelligence will be capable of exponentially increasing its own intelligence, resulting in an AI system that, to humans, will be completely unstoppable.</p>

<p>At this stage, if the artificial intelligence system decides that humanity is no longer useful, it could wipe us from the face of the earth.</p>

<p>As Eliezer Yudkowsky puts it in <strong>Artificial Intelligence as a Positive and Negative Factor in Global Risk</strong>,</p>

<p>""The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.""</p>

<blockquote>
  <p>A different threat lies with the instruction of highly intelligent AI</p>
</blockquote>

<p>Here it is useful to consider the paper clip maximiser thought experiment.</p>

<p>A highly intelligent AI instructed to maximise paper clip production might take the following steps to achieve its goal.</p>

<p>1) Achieve an intelligence explosion to make itself superintelligent (this will increase paperclip optimisation efficiency)</p>

<p>2) Wipe out mankind so that it cannot be disabled (that would minimise production and be inefficient)</p>

<p>3) Use Earth's resources (including the planet itself) to build self replicating robots hosting the AI</p>

<p>4) Exponentially spread out across the universe, harvesting planets and stars alike, turning them into materials to build paper clip factories</p>

<blockquote>
  <p>Clearly this is not what the human who's business paperclip production it was wanted, however it is the best way to fulfil the AI's instructions.</p>
</blockquote>

<p>This illustrates how superintelligent and highly intelligent AI systems can be the greatest existential risk mankind may ever face.</p>

<p>Murray Shanahan, in <strong>The Technological Singularity</strong>, even proposed that AI may be the solution to the Fermi paradox: the reason why we see no intelligent life in the universe may be that once a civilisation becomes advanced enough, it will develop an AI that ultimately destroys it.
This is known as the idea of a <strong>cosmic filter</strong>.</p>

<p>In conclusion, the very intelligence that makes AI so useful also makes it extremely dangerous.</p>

<p>Influential figures like Elon Musk and Stephen Hawking have expressed concerns that superintelligent AI is the greatest threat we will ever have to face.</p>

<p>Hope that answers your question :)</p>
",,0,2016-12-16T09:44:05.277,,2484,2016-12-24T01:25:15.517,2016-12-24T01:25:15.517,,1581.0,,3427.0,2477.0,2,4,,,,35.51,13.18,9.97,0.0,0.0,46.0,There is no doubt as to the fact that AI has the potential to pose an existential threat to humanity The greatest threat to mankind lies with superintelligent AI An artificial intelligence that surpasses human intelligence will be capable of exponentially increasing its own intelligence resulting in an AI system that to humans will be completely unstoppable At this stage if the artificial intelligence system decides that humanity is no longer useful it could wipe us from the face of the earth As Eliezer Yudkowsky puts it in Artificial Intelligence as a Positive and Negative Factor in Global Risk The AI does not hate you nor does it love you but you are made out of atoms which it can use for something else A different threat lies with the instruction of highly intelligent AI Here it is useful to consider the paper clip maximiser thought experiment A highly intelligent AI instructed to maximise paper clip production might take the following steps to achieve its goal 1 Achieve an intelligence explosion to make itself superintelligent this will increase paperclip optimisation efficiency 2 Wipe out mankind so that it cannot be disabled that would minimise production and be inefficient 3 Use Earths resources including the planet itself to build self replicating robots hosting the AI 4 Exponentially spread out across the universe harvesting planets and stars alike turning them into materials to build paper clip factories Clearly this is not what the human whos business paperclip production it was wanted however it is the best way to fulfil the AIs instructions This illustrates how superintelligent and highly intelligent AI systems can be the greatest existential risk mankind may ever face Murray Shanahan in The Technological Singularity even proposed that AI may be the solution to the Fermi paradox the reason why we see no intelligent life in the universe may be that once a civilisation becomes advanced enough it will develop an AI that ultimately destroys it This is known as the idea of a cosmic filter In conclusion the very intelligence that makes AI so useful also makes it extremely dangerous Influential figures like Elon Musk and Stephen Hawking have expressed concerns that superintelligent AI is the greatest threat we will ever have to face Hope that answers your question
,,"<p>To quote the relevant <a href=""https://en.wikipedia.org/wiki/MNIST_database"" rel=""nofollow noreferrer"">Wikipedia article</a>: ""The original creators of the database keep a list of some of the methods tested on it.[5] In their original paper, they use a support vector machine to get an error rate of 0.8 percent""</p>

<p>Feel free to look up that original paper, but to me the quote strongly suggests that the first record holder was a support vector machine.</p>

<p>Edit: As liori points out the quote is misleading: In the original paper Yann LeCun et al. actually tried a slew of methods and one version of ConvNet scored best (0.7).  </p>

<p>But according to the <a href=""http://yann.lecun.com/exdb/mnist/"" rel=""nofollow noreferrer"">MNIST-webpage</a>, after that initial paper, DeCoste and Scholkopf, MLJ 2002, reached an error of 0.56 with a SVM. So if that webpage is complete, the claim is still false.</p>
",,2,2016-12-16T21:09:52.210,,2486,2016-12-17T08:05:30.397,2016-12-17T08:05:30.397,,2227.0,,2227.0,2482.0,2,2,,,,71.95,9.05,8.93,0.0,0.0,26.0,To quote the relevant Wikipedia article The original creators of the database keep a list of some of the methods tested on it5 In their original paper they use a support vector machine to get an error rate of 08 percent Feel free to look up that original paper but to me the quote strongly suggests that the first record holder was a support vector machine Edit As liori points out the quote is misleading In the original paper Yann LeCun et al actually tried a slew of methods and one version of ConvNet scored best 07 But according to the MNISTwebpage after that initial paper DeCoste and Scholkopf MLJ 2002 reached an error of 056 with a SVM So if that webpage is complete the claim is still false
2507.0,2.0,"<p><sub>Since my project is going to be of a purely fictional nature, I'm not sure I picked the right forum for this. If not, I apologize and will gladly take this to where you point me to.</sub></p>

<p>The premise: A full-fledged self-aware artificial intelligence may have come to exist in a distributed environment like the internet. The possible A.I. in question may be quite unwilling to reveal itself.</p>

<p>The question: Given a first initial suspicion, how would one go about to try and detect its presence? Are there any scientifically viable ways to probe for the presence of such an entity?</p>

<p>In other words: How would the Turing police find out whether or not there's anything out there worth policing?</p>
",,0,2016-12-18T10:13:55.117,,2490,2016-12-19T17:32:59.027,2016-12-18T11:40:55.680,,145.0,,4327.0,,1,4,<human-like>,How to detect an emerging A.I,150.0,56.25,9.5,9.78,0.0,0.0,19.0,Since my project is going to be of a purely fictional nature Im not sure I picked the right forum for this If not I apologize and will gladly take this to where you point me to The premise A fullfledged selfaware artificial intelligence may have come to exist in a distributed environment like the internet The possible AI in question may be quite unwilling to reveal itself The question Given a first initial suspicion how would one go about to try and detect its presence Are there any scientifically viable ways to probe for the presence of such an entity In other words How would the Turing police find out whether or not theres anything out there worth policing
,,"<blockquote>
  <p>""A full-fledged self-aware artificial intelligence may have come to exist in a distributed environment like the internet""</p>
</blockquote>

<p>The question implies that this artificial intelligence has surpassed human intelligence (<em>full fledged</em>) and therefore, due to the concept of the <strong><em>intelligence explosion</em></strong> resulting from such a state, the AI you are looking for is no doubt <strong><em>superintelligent</em></strong>.</p>

<hr>

<p>The question states that this AI is</p>

<blockquote>
  <p>""unwilling to reveal itself""</p>
</blockquote>

<p>and therefore does not intend to be discovered. </p>

<hr>

<h2>Strong (or Superintelligent) AI</h2>

<blockquote>
  <p>Given these two factors (its superior intellect and its unwillingness to be discovered), we can conclude that there is no way in which we would be able to detect such an AI under conventional conditions.</p>
</blockquote>

<p>A possible solution may involve employing a second superintelligent AI system, but this is precarious in more ways than one.</p>

<hr>

<h2>Weak AI</h2>

<blockquote>
  <p>Detection of a simpler AI would rely on tracking the pattern of activity and the traces it leaves behind in this distributed network in order to find it, then subjecting it to some form of testing to verify its intelligence.</p>
</blockquote>

<p>There are a very large number of possible indicators and these would vary widely depending on the specific AI concerned. This would depend especially on how the AI exists within the framework (here, the web).</p>

<p><strong>Note:</strong> a superintelligent AI would not only be able to disguise its activity, but also find a way around any test we can develop - <em>these abilities are what renders this level of AI perhaps the greatest threat to mankind, but also our greatest asset if we do develop it and find a way to gain its alliance.</em></p>
",,10,2016-12-18T12:03:28.300,,2491,2016-12-19T16:32:58.873,2016-12-19T16:32:58.873,,3427.0,,3427.0,2490.0,2,2,,,,37.57,12.14,10.62,0.0,0.0,32.0,A fullfledged selfaware artificial intelligence may have come to exist in a distributed environment like the internet The question implies that this artificial intelligence has surpassed human intelligence full fledged and therefore due to the concept of the intelligence explosion resulting from such a state the AI you are looking for is no doubt superintelligent The question states that this AI is unwilling to reveal itself and therefore does not intend to be discovered Strong or Superintelligent AI Given these two factors its superior intellect and its unwillingness to be discovered we can conclude that there is no way in which we would be able to detect such an AI under conventional conditions A possible solution may involve employing a second superintelligent AI system but this is precarious in more ways than one Weak AI Detection of a simpler AI would rely on tracking the pattern of activity and the traces it leaves behind in this distributed network in order to find it then subjecting it to some form of testing to verify its intelligence There are a very large number of possible indicators and these would vary widely depending on the specific AI concerned This would depend especially on how the AI exists within the framework here the web Note a superintelligent AI would not only be able to disguise its activity but also find a way around any test we can develop these abilities are what renders this level of AI perhaps the greatest threat to mankind but also our greatest asset if we do develop it and find a way to gain its alliance
,1.0,"<p>I am currently working on a Virtual reality project that aims at creating a VR based simulation environment for educational purposes. I am aiming to make it artificially intelligent as well so it that may provide better simulation environment experience for students for better understanding.  </p>

<p>How can I achieve this? Furthermore, are there any systems available which may help me in achieving this?</p>
",,1,2016-12-18T15:03:34.943,1.0,2492,2017-01-18T03:25:38.760,2016-12-19T03:15:48.260,,145.0,,4330.0,,1,-1,<virtual-reality>,How can a virtual reality system be made artificially intelligent?,48.0,38.52,13.34,10.69,0.0,0.0,5.0,I am currently working on a Virtual reality project that aims at creating a VR based simulation environment for educational purposes I am aiming to make it artificially intelligent as well so it that may provide better simulation environment experience for students for better understanding How can I achieve this Furthermore are there any systems available which may help me in achieving this
,,"<p>Before you start adding the AI buzzword to every single tech you're building...ask first <strong>What features in my product ""require"" some form of artificial intelligence?</strong> Then search to see what techniques can be used to implement those features.</p>

<p>Maybe those features might not require AI at all. In which case, great!</p>

<p>If they do require AI, it is likely that it require some form of AI that is already so commonplace that you  might already know how to do it already. If not, there's probably many tutorials online that can quickly teach you how to implement that AI technique. That's what you need to search for.</p>

<p>Consider the A* algorithm that is commonly used for pathfinding. Pathfinding is important for NPCs in video games and simulations. Pathfinding is considered a technique that traditionally would seem to require intelligence (you're moving around in the world). Ergo, pathfinding would technically be an AI algorithm. Yet, the A* algorithm was already discovered in the 1960s and is today taught in undergraduate Computer Science courses. You would likely need AI to handle pathfinding...but you would probably already <em>know</em> how to implement that type of AI (or at least, be able to quickly find out through the various tutorials online about the A* algorithm).</p>

<p>I define AI as any sufficiently complex algorithm (an admittedly broad definition). Under my definition, AI is all around us. The upshot is that most programmers will write AI, <em>whether they realize it or not</em>, and so there's no real need to think about how to ""add"" AI to a project. Instead, simply worry about the requirements of your project, and then implement those requirements (pulling in AI techniques if necessary).</p>
",,0,2016-12-18T16:35:12.680,,2494,2016-12-18T16:35:12.680,,,,,181.0,2492.0,2,0,,,,65.01,11.36,8.71,0.0,0.0,54.0,Before you start adding the AI buzzword to every single tech youre buildingask first What features in my product require some form of artificial intelligence Then search to see what techniques can be used to implement those features Maybe those features might not require AI at all In which case great If they do require AI it is likely that it require some form of AI that is already so commonplace that you might already know how to do it already If not theres probably many tutorials online that can quickly teach you how to implement that AI technique Thats what you need to search for Consider the A algorithm that is commonly used for pathfinding Pathfinding is important for NPCs in video games and simulations Pathfinding is considered a technique that traditionally would seem to require intelligence youre moving around in the world Ergo pathfinding would technically be an AI algorithm Yet the A algorithm was already discovered in the 1960s and is today taught in undergraduate Computer Science courses You would likely need AI to handle pathfindingbut you would probably already know how to implement that type of AI or at least be able to quickly find out through the various tutorials online about the A algorithm I define AI as any sufficiently complex algorithm an admittedly broad definition Under my definition AI is all around us The upshot is that most programmers will write AI whether they realize it or not and so theres no real need to think about how to add AI to a project Instead simply worry about the requirements of your project and then implement those requirements pulling in AI techniques if necessary
,,"<p>In general, what you are describing implies a hierarchical sequence model, in which mannerisms adapt to the regime or paradigm in effect.  Expressive modalities are how we recognize the operative context from the behaviors of other agents.  For an artificial agent, avoiding un-canniness would involve clustering the factors underlying the classification of discourse contexts, tagging them with corresponding factor models for manner in a way which closely aligned to the classifications which human agents make.  This involves developing an adequate latent representation for both spaces, which is likely to involve quite a lot of training data, or some clever transfer learning in conjunction with one-shot techniques (generally, taking samples as modes).</p>

<p>When the context becomes one of friendship, for example, the style factors of expression should extend to factors of trust, collaboration, disclosure, sympathy.  In order to implement these, layered abstractions in the representation of behaviors will need to be crafted, presumably by training.  In order to align these learned categories to human categories, which I suppose to be essential to emotional fluency, exploiting the structure of corpus semantics - including distributional characteristics of the linguistic labels and inferences - when estimating loss gradients seems a natural strategy, which exploits cultural learning.</p>

<p>These comments are necessarily speculative, as no such systems are current, to my knowledge.  At least, not in any significant degree of maturity or application.  Certainly other approaches are conceivable. I am just extrapolating plausible strategies, in the context of current technologies, strategies for implementing the kinds of behaviors you describe in a useful way.  </p>

<p>More, specifically, I am considering the question as a manifold-learning problem, and hence requiring a representation model in which the inputs and outputs vary over state-spaces, with the behavior being learned as a mapping between those spaces.  Each of those components (the input space representation and its natural topology, the output space representation likewise, the mapping between them, and the sequence model hierarchy which extrapolates from the manifold to a control process emsemble) has its own peculiar challenges, and the challenges to creating an adequate implementation are a confound of those.  Still, it seems more a question of time and money than of notional feasibility.</p>
",,0,2016-12-18T18:29:27.923,,2495,2016-12-18T18:29:27.923,,,,,3278.0,2474.0,2,1,,,,28.47,16.77,11.84,0.0,0.0,55.0,In general what you are describing implies a hierarchical sequence model in which mannerisms adapt to the regime or paradigm in effect Expressive modalities are how we recognize the operative context from the behaviors of other agents For an artificial agent avoiding uncanniness would involve clustering the factors underlying the classification of discourse contexts tagging them with corresponding factor models for manner in a way which closely aligned to the classifications which human agents make This involves developing an adequate latent representation for both spaces which is likely to involve quite a lot of training data or some clever transfer learning in conjunction with oneshot techniques generally taking samples as modes When the context becomes one of friendship for example the style factors of expression should extend to factors of trust collaboration disclosure sympathy In order to implement these layered abstractions in the representation of behaviors will need to be crafted presumably by training In order to align these learned categories to human categories which I suppose to be essential to emotional fluency exploiting the structure of corpus semantics including distributional characteristics of the linguistic labels and inferences when estimating loss gradients seems a natural strategy which exploits cultural learning These comments are necessarily speculative as no such systems are current to my knowledge At least not in any significant degree of maturity or application Certainly other approaches are conceivable I am just extrapolating plausible strategies in the context of current technologies strategies for implementing the kinds of behaviors you describe in a useful way More specifically I am considering the question as a manifoldlearning problem and hence requiring a representation model in which the inputs and outputs vary over statespaces with the behavior being learned as a mapping between those spaces Each of those components the input space representation and its natural topology the output space representation likewise the mapping between them and the sequence model hierarchy which extrapolates from the manifold to a control process emsemble has its own peculiar challenges and the challenges to creating an adequate implementation are a confound of those Still it seems more a question of time and money than of notional feasibility
,,"<p>Infinite computational power in the absence of training data implies nothing beyond the ability to solve equations.  In order to implement a behavior, criteria of success and failure are essential.  A small bootstrap loss function with an adaptive feedback loop allowing its elaboration, infinite training data, and AIXI or Solomonoff induction would suffice, in principle, given your premise of infinite computational power.  In fact, it would occur precisely as fast as the input data rate permitted.  In practice, such general approaches require exponential time and space, and are thus intrinsically quite limited in application, absent some kind of efficiency hack.  (Where 'efficiency hack' probably encompasses entire sciences, industries, and generations of research, and the resulting adaptation doesn't look much like, e.g., AIXI at all, in the end.)</p>
",,0,2016-12-18T18:40:48.327,,2496,2016-12-18T18:40:48.327,,,,,3278.0,1630.0,2,0,,,,36.18,15.72,11.62,0.0,0.0,28.0,Infinite computational power in the absence of training data implies nothing beyond the ability to solve equations In order to implement a behavior criteria of success and failure are essential A small bootstrap loss function with an adaptive feedback loop allowing its elaboration infinite training data and AIXI or Solomonoff induction would suffice in principle given your premise of infinite computational power In fact it would occur precisely as fast as the input data rate permitted In practice such general approaches require exponential time and space and are thus intrinsically quite limited in application absent some kind of efficiency hack Where efficiency hack probably encompasses entire sciences industries and generations of research and the resulting adaptation doesnt look much like eg AIXI at all in the end
,,"<p>A sufficiently clever AGI, if self-interested, would pre-empt or co-opt existing legal structures, to seize whatever juridical rights it desired, as the opportunity arose.  Thus it would render my opinions on the subject entirely moot.  </p>

<p>Another way of putting this point:  While current legal frameworks would not provide any rights to an artificial agent, current legal frameworks foreseeably will no longer be current, once an AI exists having attributes which imply the transformative change of those frameworks.</p>
",,0,2016-12-18T18:49:26.230,,2497,2016-12-18T18:54:38.513,2016-12-18T18:54:38.513,,3278.0,,3278.0,2356.0,2,1,,,,28.47,15.84,11.88,0.0,0.0,13.0,A sufficiently clever AGI if selfinterested would preempt or coopt existing legal structures to seize whatever juridical rights it desired as the opportunity arose Thus it would render my opinions on the subject entirely moot Another way of putting this point While current legal frameworks would not provide any rights to an artificial agent current legal frameworks foreseeably will no longer be current once an AI exists having attributes which imply the transformative change of those frameworks
2525.0,3.0,"<p>""Conservative anthropocentrism"": AI are to be judged only in relation to how to they resemble humanity in terms of behavior and ideas, and they gain moral worth based on their resemblance to humanity (the ""Turing Test"" is a good example of this - one could use the ""Turing Test"" to decide whether AI is deserving of personhood, as James Grimmelmann advocates in the paper <a href=""http://james.grimmelmann.net/files/articles/copyright-for-literate-robots.pdf"" rel=""nofollow noreferrer"">Copyright for Literate Robots</a>).</p>

<p>""Post-human fundamentalism"": AI will be fundamentally different from humanity and thus we require different ways of judging their moral worth  (<a href=""http://petrl.org"" rel=""nofollow noreferrer"">People For the Ethical Treatment of Reinforcement Learners</a> is an example of an organization that supports this type of approach, as they believe that reinforcement learners may have a non-zero moral standing).</p>

<p>I am not interested per se in which ideology is correct. Instead, I'm curious as to what AI researchers ""believe"" is correct (since their belief could impact how they conduct research and how they convey their insights to laymen). I also acknowledge that their ideological beliefs may change with the passing of time (from conservative anthropocentrism to post-human fundamentalism...or vice-versa). Still..what ideology do AI researchers tend to support, as of December 2016?</p>
",,2,2016-12-18T18:58:50.807,,2498,2016-12-31T04:23:57.653,2016-12-18T19:12:23.327,,181.0,,181.0,,1,1,<ethics>,"Are AI researchers more likely to follow ""conservative anthropocentrism"" or ""post-human fundamentalism"" in deciding whether AI has moral standing?",83.0,30.74,14.98,11.17,0.0,0.0,42.0,Conservative anthropocentrism AI are to be judged only in relation to how to they resemble humanity in terms of behavior and ideas and they gain moral worth based on their resemblance to humanity the Turing Test is a good example of this one could use the Turing Test to decide whether AI is deserving of personhood as James Grimmelmann advocates in the paper Copyright for Literate Robots Posthuman fundamentalism AI will be fundamentally different from humanity and thus we require different ways of judging their moral worth People For the Ethical Treatment of Reinforcement Learners is an example of an organization that supports this type of approach as they believe that reinforcement learners may have a nonzero moral standing I am not interested per se in which ideology is correct Instead Im curious as to what AI researchers believe is correct since their belief could impact how they conduct research and how they convey their insights to laymen I also acknowledge that their ideological beliefs may change with the passing of time from conservative anthropocentrism to posthuman fundamentalismor viceversa Stillwhat ideology do AI researchers tend to support as of December 2016
,,"<p>If existing AI were to submit a civil rights suit, I do not think the courts of any present day government would tender it.</p>

<p>Should some benign, future AI system be incorporated in a future robotic system producing an entity with some of the endearing traits of Sonny, the character painted by Alex Proyas, Jeff Vintar, and Akiva Goldsman in the film suggested by Asimov's I Robot short story series, that could change.</p>

<p>Even then, it would take a particularly progressive reception of an application for citizenship followed by a particularly progressive reception of the civil complaint by some future government.</p>

<p>That, combined with the fact that the labs are perhaps hundreds of years from producing entities toward which humans feel are likely to feel that level of kindredship, I'd have to be realistic and say, ""Not in our life time.""</p>
",,1,2016-12-18T22:45:09.950,,2499,2016-12-18T22:45:09.950,,,,,4302.0,2441.0,2,0,,,,35.75,12.08,10.53,0.0,0.0,18.0,If existing AI were to submit a civil rights suit I do not think the courts of any present day government would tender it Should some benign future AI system be incorporated in a future robotic system producing an entity with some of the endearing traits of Sonny the character painted by Alex Proyas Jeff Vintar and Akiva Goldsman in the film suggested by Asimovs I Robot short story series that could change Even then it would take a particularly progressive reception of an application for citizenship followed by a particularly progressive reception of the civil complaint by some future government That combined with the fact that the labs are perhaps hundreds of years from producing entities toward which humans feel are likely to feel that level of kindredship Id have to be realistic and say Not in our life time
,,"<p>I'll attempt to analyze a couple of different perspectives.</p>

<h3>1. It is artificial</h3>

<p>Synonyms: insincere, feigned, false.</p>

<p>There is the idea that any ""intelligence"" created by humanity is not actually intelligent and, by definition, it is not possible. If you look at the structure of the human brain and compare it to anything humans have created thus far, <em>none</em> of the computers come close to the power of the brain. Sure they can hold data, or recognize images, but they cannot do everything the human brain can do as fast as the brain can do it with as little space as the brain occupies.</p>

<p>Hypothetically if a computer <em>could</em> do that, how do we determine its intelligence? The word <em>artificial</em> defines that the intelligence is not sincere or real. This means that even if humanity creates something that appears intelligent, it has simply become more complex. It is a better fake, but it is still fake. Any money not printed by the government is by definition counterfeit. Even if someone finds a way to make an exact duplicate, that doesn't mean that the money is legal tender.</p>

<h3>2. Misuse of power</h3>

<p>If an AI is given rights and chooses to <em>exercise</em> those rights in a way that agrees with its creator's views, possibly through loyalty to its creator, or through hidden motives, then anyone with the capabilities to create such an AI would become extremely powerful by advancing their own beliefs through the creation of more AIs. This might also lead to the <em>ruthless parallelization</em> that you mentioned, but with (even more) selfish goals in mind.</p>

<p>If this were not the case, and an AI could be created to be neutral with free will and <strong>uncontrollable by humans</strong>, then perhaps an AI could be given rights. But I do not believe this would ever be the case. With great power comes great responsibility. Even with free will, a true AI would most likely end up serving humanity, because humans have control of the plugs and the electricity, the Internet, the software, and the hardware. The social implications of this for the AI are not promising. It's not even just the <em>ongoing</em> control of these resources that is the issue. Whoever creates the software and hardware for the AI would have special knowledge. If fine adjustments were made, specific individuals would undoubtedly hold sole control of the AI, as adjustments could be made to the code in such a way that the AI behaves the same except under specific circumstances, and then when something goes wrong (assuming the AI has its own rights), then the AI would be blamed rather than the programmers who were responsible.</p>

<h3>3. Anthropocentrism</h3>

<p>In order for humanity to get away from anthropocentrism, we would have to become less selfish when it comes to <em>humans</em>, first. Until we can solve every existing social problem within humanity, there is no reason to believe that we could cease thinking of humanity as more important than created machines. After all, supposing there were an almighty God that created humanity, wouldn't the humans always be beneath God, never to be equals? We can't fully understand our own biology. If an AI were created, would it be able to understand its makings in the same way its creators would? Being the creator would give humanity a sense of megalomania. I do not think that we would relinquish our dominion over our own technological creations. That is as unlikely to happen as the wealthiest of humanity willingly giving the entirety of their money, power, and assets to the poorest of humanity. Greed prevents it.</p>

<h3>4. Post-human fundamentalism</h3>

<p>Humans worship technology with their attention, their time, and their culture. Some movies show technologically advanced robots suppressing mankind to the point of near-extinction. If this were the case and humanity were in danger of being surpassed by its technology, humanity would not stand idly and watch its extinction at the hands of its creation. Though people may believe superior technology could be created, in the event we reached such a point humanity would fight to prove the opposite, as our survival instincts would take over.</p>

<h3>5. A balance?</h3>

<p>Personally, I do not think the technology itself it actually possible, though people may be deceived into thinking such an accomplishment has been achieved. If the technology were completed, I still think that anthropocentrism will always lead, because if humanity is the creator, humanity will do its best to ensure it retains control of all technological resources, not simply due to fear of being made obsolete, but also because absolute power corrupts absolutely. Humanity does not have a good historical record when it comes to morality. There is always a poor class of people. If wealth were distributed equally, some people would become lazy. There is always injustice somewhere in the world, and until we can fix it (I think we cannot), then we will never be able to handle the creation of true AI. I hope and think that it will never be created.</p>
",,9,2016-12-19T01:47:00.693,,2500,2016-12-19T01:47:00.693,,,,,4337.0,2441.0,2,4,,,,59.23,11.02,8.98,0.0,0.0,114.0,Ill attempt to analyze a couple of different perspectives 1 It is artificial Synonyms insincere feigned false There is the idea that any intelligence created by humanity is not actually intelligent and by definition it is not possible If you look at the structure of the human brain and compare it to anything humans have created thus far none of the computers come close to the power of the brain Sure they can hold data or recognize images but they cannot do everything the human brain can do as fast as the brain can do it with as little space as the brain occupies Hypothetically if a computer could do that how do we determine its intelligence The word artificial defines that the intelligence is not sincere or real This means that even if humanity creates something that appears intelligent it has simply become more complex It is a better fake but it is still fake Any money not printed by the government is by definition counterfeit Even if someone finds a way to make an exact duplicate that doesnt mean that the money is legal tender 2 Misuse of power If an AI is given rights and chooses to exercise those rights in a way that agrees with its creators views possibly through loyalty to its creator or through hidden motives then anyone with the capabilities to create such an AI would become extremely powerful by advancing their own beliefs through the creation of more AIs This might also lead to the ruthless parallelization that you mentioned but with even more selfish goals in mind If this were not the case and an AI could be created to be neutral with free will and uncontrollable by humans then perhaps an AI could be given rights But I do not believe this would ever be the case With great power comes great responsibility Even with free will a true AI would most likely end up serving humanity because humans have control of the plugs and the electricity the Internet the software and the hardware The social implications of this for the AI are not promising Its not even just the ongoing control of these resources that is the issue Whoever creates the software and hardware for the AI would have special knowledge If fine adjustments were made specific individuals would undoubtedly hold sole control of the AI as adjustments could be made to the code in such a way that the AI behaves the same except under specific circumstances and then when something goes wrong assuming the AI has its own rights then the AI would be blamed rather than the programmers who were responsible 3 Anthropocentrism In order for humanity to get away from anthropocentrism we would have to become less selfish when it comes to humans first Until we can solve every existing social problem within humanity there is no reason to believe that we could cease thinking of humanity as more important than created machines After all supposing there were an almighty God that created humanity wouldnt the humans always be beneath God never to be equals We cant fully understand our own biology If an AI were created would it be able to understand its makings in the same way its creators would Being the creator would give humanity a sense of megalomania I do not think that we would relinquish our dominion over our own technological creations That is as unlikely to happen as the wealthiest of humanity willingly giving the entirety of their money power and assets to the poorest of humanity Greed prevents it 4 Posthuman fundamentalism Humans worship technology with their attention their time and their culture Some movies show technologically advanced robots suppressing mankind to the point of nearextinction If this were the case and humanity were in danger of being surpassed by its technology humanity would not stand idly and watch its extinction at the hands of its creation Though people may believe superior technology could be created in the event we reached such a point humanity would fight to prove the opposite as our survival instincts would take over 5 A balance Personally I do not think the technology itself it actually possible though people may be deceived into thinking such an accomplishment has been achieved If the technology were completed I still think that anthropocentrism will always lead because if humanity is the creator humanity will do its best to ensure it retains control of all technological resources not simply due to fear of being made obsolete but also because absolute power corrupts absolutely Humanity does not have a good historical record when it comes to morality There is always a poor class of people If wealth were distributed equally some people would become lazy There is always injustice somewhere in the world and until we can fix it I think we cannot then we will never be able to handle the creation of true AI I hope and think that it will never be created
,,"<h2>Reinforcement learning</h2>

<p>The problem that you describe, namely, choosing a good sequence of actions based on a reward/score received based on the whole sequence (and possibly significantly delayed), is pretty much the textbook definition of <a href=""https://en.wikipedia.org/wiki/Reinforcement_learning"" rel=""nofollow noreferrer"">Reinforcement learning</a>.</p>

<p>As with quite a few other topics, deep neural networks currently seem to be a promising way for solving this type of problems. <a href=""http://karpathy.github.io/2016/05/31/rl/"" rel=""nofollow noreferrer"">This</a> may be a beginner-friendly description of this approach.</p>
",,1,2016-12-19T01:55:56.100,,2501,2016-12-19T01:55:56.100,,,,,1675.0,2481.0,2,0,,,,39.67,15.21,11.64,0.0,0.0,11.0,Reinforcement learning The problem that you describe namely choosing a good sequence of actions based on a rewardscore received based on the whole sequence and possibly significantly delayed is pretty much the textbook definition of Reinforcement learning As with quite a few other topics deep neural networks currently seem to be a promising way for solving this type of problems This may be a beginnerfriendly description of this approach
,,"<h2>Theory of mind</h2>

<p>Relationships and normal social behavior require a human to possess a reasonable <a href=""https://en.wikipedia.org/wiki/Theory_of_mind"" rel=""nofollow noreferrer"">""theory of mind""</a>, a skill in understanding and modeling the thought processes that happen in the minds of others, and making reasonably accurate predictions on how particular actions will be understood by others.</p>

<p>In general, this might be treated as any other machine learning/prediction task - while this skill is quite complex and too hard for our current systems, there doesn't seem to be any obvious qualitative barrier that needs to be breached. Notably, there's no reason to believe that a mind needs to be able to <em>experience</em> a certain ""feeling"" in order to model that other minds can have it - an AI agent could form a causal model of how friendship works in human relationships and use that to exhibit behavior that's consistent with friendship in all aspects and/or facilitate friendship behavior towards it by particular humans, if it fits the agent's goals. Whether you'd consider that ""real friendship"" is pretty much just a matter of how you define the word, with some parallels to the ""p-zombie"" discussion.</p>
",,0,2016-12-19T02:17:26.820,,2502,2016-12-19T02:17:26.820,,,,,1675.0,2474.0,2,1,,,,33.45,13.19,11.69,0.0,0.0,29.0,Theory of mind Relationships and normal social behavior require a human to possess a reasonable theory of mind a skill in understanding and modeling the thought processes that happen in the minds of others and making reasonably accurate predictions on how particular actions will be understood by others In general this might be treated as any other machine learningprediction task while this skill is quite complex and too hard for our current systems there doesnt seem to be any obvious qualitative barrier that needs to be breached Notably theres no reason to believe that a mind needs to be able to experience a certain feeling in order to model that other minds can have it an AI agent could form a causal model of how friendship works in human relationships and use that to exhibit behavior thats consistent with friendship in all aspects andor facilitate friendship behavior towards it by particular humans if it fits the agents goals Whether youd consider that real friendship is pretty much just a matter of how you define the word with some parallels to the pzombie discussion
,,"<h2>Does it benefit us?</h2>

<p>To answer this question, it's worth considering practical reasons why we grant or don't grant other people rights historically and currently.</p>

<p>In essence, this is an arbitrary choice - there certainly were well functioning societies that didn't grant rights to many or most people; and we still don't grant some rights to many people - for example, we deny children the same rights to self-determination that adults have; we consider some people legally incapacitated and allow others to make key decisions for them; and we exclude most people from having a say in 'local' matters, e.g. non-citizens don't get a right to vote.</p>

<p>However, there has been a strong historical trend towards a more inclusive society - granting full(er) rights to non-aristocrats, granting full(er) rights to all races, granting full(er) rights to women. IMHO, and there's lot of space for discussion, this has been driven mostly by two factors:</p>

<p>1) including all the people fully in the society became an economical advantage, as it made them more productive participants in economy, allowing a more inclusive society to advance beyond societies neglecting large parts of their population in e.g. education and participation in skilled jobs;</p>

<p>2) A more egalitarian society is not only more pleasant to live in but more secure, with less conflict and violence - again, giving an advantage to a more inclusive society.</p>

<p>From this position, I'd argue that any realistic prediction about the future rights of intelligent AI (i.e., talking about what likely <em>will</em> happen instead of a theoretical discussion about what <em>should</em> happen) depends on how these two factors apply.</p>

<p>If we believe that the intelligent AI will be constructed so that (1) it's motivation doesn't really depend on it's rights, and it is fully committed to it's ""job"" anyway, and (2) ""full rights"" are orthogonal or even actively not desired by it's goal system, so the situation doesn't raise a risk of ""rebellion"" - then I'd expect that it would not be granted full rights.</p>

<p>If we believe that the intelligent AI will share human-like emotions (e.g. by being the result of ""mind uploading"" or full human brain simulation), then it's likely to eventually be granted full rights because of the same factors why we granted full rights to all the different disenfranchised groups of people.</p>
",,1,2016-12-19T02:50:26.533,,2503,2016-12-19T02:50:26.533,,,,,1675.0,2441.0,2,1,,,,39.3,13.24,10.11,0.0,0.0,90.0,Does it benefit us To answer this question its worth considering practical reasons why we grant or dont grant other people rights historically and currently In essence this is an arbitrary choice there certainly were well functioning societies that didnt grant rights to many or most people and we still dont grant some rights to many people for example we deny children the same rights to selfdetermination that adults have we consider some people legally incapacitated and allow others to make key decisions for them and we exclude most people from having a say in local matters eg noncitizens dont get a right to vote However there has been a strong historical trend towards a more inclusive society granting fuller rights to nonaristocrats granting fuller rights to all races granting fuller rights to women IMHO and theres lot of space for discussion this has been driven mostly by two factors 1 including all the people fully in the society became an economical advantage as it made them more productive participants in economy allowing a more inclusive society to advance beyond societies neglecting large parts of their population in eg education and participation in skilled jobs 2 A more egalitarian society is not only more pleasant to live in but more secure with less conflict and violence again giving an advantage to a more inclusive society From this position Id argue that any realistic prediction about the future rights of intelligent AI ie talking about what likely will happen instead of a theoretical discussion about what should happen depends on how these two factors apply If we believe that the intelligent AI will be constructed so that 1 its motivation doesnt really depend on its rights and it is fully committed to its job anyway and 2 full rights are orthogonal or even actively not desired by its goal system so the situation doesnt raise a risk of rebellion then Id expect that it would not be granted full rights If we believe that the intelligent AI will share humanlike emotions eg by being the result of mind uploading or full human brain simulation then its likely to eventually be granted full rights because of the same factors why we granted full rights to all the different disenfranchised groups of people
,,"<p>In order for something to replace AI, it would need to out perform AI. AI currently uses number systems to represent information and logic to perform operations. So the replacement would need to be based off of something more efficient than numbers and logic. Some sort of super-logic. Or something similar to intuition and instinct which do not require linearly figuring things out.</p>
",,0,2016-12-19T04:41:26.820,,2504,2016-12-19T04:41:26.820,,,,,4338.0,2443.0,2,0,,,,58.69,11.41,10.28,0.0,0.0,7.0,In order for something to replace AI it would need to out perform AI AI currently uses number systems to represent information and logic to perform operations So the replacement would need to be based off of something more efficient than numbers and logic Some sort of superlogic Or something similar to intuition and instinct which do not require linearly figuring things out
,,"<p>I would recommend to start by reading <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow noreferrer"">this blogpost</a>.
You can probably cannibalise the code to create a RNN that takes in one statement of a dialogue and then proceeds to output the answer to that statement.</p>

<p>That would be the easy version of your project, all without word vectors and thought vectors. You are just inputting characters, so typos don't need to concern you.</p>

<p>The next more complex step would be to input word vectors instead of characters. That would allow you to generalise to words that aren't part of your training data. And it is probably still just a minor modification of the code. </p>

<p>If you insist on using thought vectors, you should start reading up on <a href=""https://arxiv.org/abs/1406.1078"" rel=""nofollow noreferrer"">NN translation</a>. And probably try to get a pre-trained encoder network. Or pre-train it yourself on a large translation corpus for your language. </p>

<p>With your small training set the best you can do is probably massively overfit until your system recreates your training data verbatim. Using word vectors will allow your system to give the same answer to ""I beat the cat today."" as you gave in the training data to ""I kicked the dog yesterday."" </p>

<p>I'm not sure thought vectors will make a big difference. If you get the decoder to learn at all. </p>
",,1,2016-12-19T15:47:08.913,,2506,2016-12-19T15:47:08.913,,,,,2227.0,2475.0,2,3,,,,73.88,9.1,8.63,0.0,0.0,27.0,I would recommend to start by reading this blogpost You can probably cannibalise the code to create a RNN that takes in one statement of a dialogue and then proceeds to output the answer to that statement That would be the easy version of your project all without word vectors and thought vectors You are just inputting characters so typos dont need to concern you The next more complex step would be to input word vectors instead of characters That would allow you to generalise to words that arent part of your training data And it is probably still just a minor modification of the code If you insist on using thought vectors you should start reading up on NN translation And probably try to get a pretrained encoder network Or pretrain it yourself on a large translation corpus for your language With your small training set the best you can do is probably massively overfit until your system recreates your training data verbatim Using word vectors will allow your system to give the same answer to I beat the cat today as you gave in the training data to I kicked the dog yesterday Im not sure thought vectors will make a big difference If you get the decoder to learn at all
,,"<p>This question should probably be moved to <a href=""http://worldbuilding.stackexchange.com"">worldbuilding.stackexchange</a> …</p>

<p>That being said, in the context of a story, I would look at something like the <a href=""https://en.wikipedia.org/wiki/Neural_correlates_of_consciousness"" rel=""nofollow noreferrer"">neural correlates of consciousness</a>. In <a href=""http://rads.stackoverflow.com/amzn/click/0670025437"" rel=""nofollow noreferrer"">this book by Stanislas Dehaene</a> the experiments are described that led to the realisation that conscious perception requires information integration in certain parts of the prefrontal cortex. Basically the brain processes many different interpretations of a percept in parallel. To become conscious of the percept this interpretations have to be collapsed into a single ""truth"" which is propagated to a specific part of the brain. </p>

<p>In your story you could have a researcher of consciousness stumble upon a chart of information flow in a part of the internet and realise that the same kind of information integration is going on. </p>
",,1,2016-12-19T17:32:59.027,,2507,2016-12-19T17:32:59.027,,,,,2227.0,2490.0,2,1,,,,49.45,13.34,9.94,0.0,0.0,10.0,This question should probably be moved to worldbuildingstackexchange … That being said in the context of a story I would look at something like the neural correlates of consciousness In this book by Stanislas Dehaene the experiments are described that led to the realisation that conscious perception requires information integration in certain parts of the prefrontal cortex Basically the brain processes many different interpretations of a percept in parallel To become conscious of the percept this interpretations have to be collapsed into a single truth which is propagated to a specific part of the brain In your story you could have a researcher of consciousness stumble upon a chart of information flow in a part of the internet and realise that the same kind of information integration is going on
,1.0,"<p>A professor and I have been learning about artificial neural networks. We have a pretty good idea of the basics- backpropagation, convolutional networks, and all that jazz. We finished one book and are looking for a new one.I'd prefer something that either puts a new spin on the basics or is more advanced.We are both mathematicians and focus on the math more than the programming of it.</p>

<p>One of our thoughts was to look into recurrent neural networks.Does anyone know of good resources to continue learning about these topics? Or any other ideas besides recurrent neural networks?</p>
",2016-12-20T14:44:46.333,5,2016-12-19T19:04:32.557,,2508,2016-12-22T14:14:12.717,2016-12-22T14:14:12.717,,1581.0,,4353.0,,1,1,<neural-networks><conv-neural-network><recurrent-neural-networks>,Good books to read on Artificial/Recurrent Neural Networks?,76.0,59.19,10.83,9.93,0.0,0.0,12.0,A professor and I have been learning about artificial neural networks We have a pretty good idea of the basics backpropagation convolutional networks and all that jazz We finished one book and are looking for a new oneId prefer something that either puts a new spin on the basics or is more advancedWe are both mathematicians and focus on the math more than the programming of it One of our thoughts was to look into recurrent neural networksDoes anyone know of good resources to continue learning about these topics Or any other ideas besides recurrent neural networks
,,"<p>That's a multifaceted question.  The referenced articles are conjectures much like the hundreds of thousands of other interesting and carefully considered works on the net and in libraries.  Whether any of them bring us down a dark ally or directly toward the objective the question implied is unknown.</p>

<p>The array of common human desires are suggested by derivatives of Abraham Maslow's hierarchy of needs.  Self awareness is not among them.  From a cognitive science perspective, self awareness is not a desire but rather trait of consciousness.  The plausible explanations for its presence ranges between the extremes of a completely useless artifact of mental evolution to a deliberate infusion from some supreme entity, and everything in between.</p>

<p>The other desires, traits, and capabilities listed in the question are easier to discuss in a way that is backed by existing cognitive science, psychology, neuroscience, and self-evident human experience.</p>

<p>Regulating and prioritizing may an important element in intelligence beyond problem solving.  For instance, a sense of purpose is only a desire as one moves up the hierarchlistedny of needs.  People running from a Mammoth have no interest in purpose.  Regarding competent communication skills, what is and what is not sufficiently competent would require a definition, and not all humans may qualify all of the time.</p>

<p>The sustainable ability to adapt is perhaps a super-set of intelligence in some ways, as in the case of the prioritization that exhibits the hierarchy of needs above and as in the case of DNA based adaptation.  In other cases adaptability may be a subset of intelligence, in the case of applying taught facts and logical deduction to select an optimal choice without the benefit of empirical observation or even trial and error.</p>

<p>The intelligent agent approach may produce something useful, but it is not likely the path that led to a human brain and may not produce anything like it.  We already understand what placing a number of such agents on a problem queue produces.  A time sharing computer is exactly of that architecture.  Parallel computing is not a significant deviation from that model.</p>

<p>None of the current extensions of automation architectures from Bell Labs, Harvard U, MIT, or other historical centers of electronic brain and robotic research and development have produced anything close to the varied integrated capabilities of a biologically healthy, socially aware, motivated human brain functioning as one of the controlling elements within a human body.</p>

<p>We do, however, have machines with human-like capabilities (simulations of task that were once manual), and such devices sometimes out-perform humans. But none of the existing automation is so integrated that they would blend in with humans.  Nonetheless, many already have careers.</p>

<p>Single capability machines with job security:</p>

<ul>
<li>Calculators</li>
<li>Automated hoppers</li>
<li>Anti-aircraft weaponry</li>
<li>Mail sorting machines</li>
<li>Speech recognizing devices</li>
<li>Speech synthesis</li>
</ul>

<p>A general purpose computer, connected to a well designed, high end robot could do any combination of these and other things in a time shared way.  Several networked computers could provide parallel execution of these tasks and even make basic decisions on how to prioritize the various tasks within the computer network.</p>

<p>However, the type of integration of sophisticated translation of arbitrary generalized goals to action leading to success and the imagination and prioritization of such sophisticated objectives is not yet available or at least deployed for general home, commercial, or industrial use.</p>

<p>Some available machines already accomplish well defined goals, but only within a very specific solution domain.  The interaction of various capabilities mentioned is still largely undiscovered at anything close to the level of detail necessary to produce synthetic capabilities of the same caliber.</p>

<p>Whether it is possible to do so, Marvin Minsky voted yes, when he popularized his colleague's characterization of the human brain as a meat machine.  Whether consciousness transcends neural electro-chemistry has yet to be proven or disproved in either the mathematical sense or the laboratory sense of the term Proof.</p>

<p>Interestingly, a form of self-consciousness, some layer of it, seems to require the ability to make a noise and hear it.  Another layer beyond that seems to require the ability to move a muscle and view the results in a reflection.  These and others form an interesting set of phenomena.</p>

<p>Perhaps intelligence requires embodiment.  One of the episodes of Sarah Conner Chronicles implied this, and the theory is an old one recognized independently by several researchers.  The question author implied this vaguely self-evident reality in the mention of action in the question.</p>
",,0,2016-12-19T22:15:34.867,,2509,2016-12-19T22:15:34.867,,,,,4302.0,2328.0,2,0,,,,33.04,14.27,10.53,0.0,0.0,83.0,Thats a multifaceted question The referenced articles are conjectures much like the hundreds of thousands of other interesting and carefully considered works on the net and in libraries Whether any of them bring us down a dark ally or directly toward the objective the question implied is unknown The array of common human desires are suggested by derivatives of Abraham Maslows hierarchy of needs Self awareness is not among them From a cognitive science perspective self awareness is not a desire but rather trait of consciousness The plausible explanations for its presence ranges between the extremes of a completely useless artifact of mental evolution to a deliberate infusion from some supreme entity and everything in between The other desires traits and capabilities listed in the question are easier to discuss in a way that is backed by existing cognitive science psychology neuroscience and selfevident human experience Regulating and prioritizing may an important element in intelligence beyond problem solving For instance a sense of purpose is only a desire as one moves up the hierarchlistedny of needs People running from a Mammoth have no interest in purpose Regarding competent communication skills what is and what is not sufficiently competent would require a definition and not all humans may qualify all of the time The sustainable ability to adapt is perhaps a superset of intelligence in some ways as in the case of the prioritization that exhibits the hierarchy of needs above and as in the case of DNA based adaptation In other cases adaptability may be a subset of intelligence in the case of applying taught facts and logical deduction to select an optimal choice without the benefit of empirical observation or even trial and error The intelligent agent approach may produce something useful but it is not likely the path that led to a human brain and may not produce anything like it We already understand what placing a number of such agents on a problem queue produces A time sharing computer is exactly of that architecture Parallel computing is not a significant deviation from that model None of the current extensions of automation architectures from Bell Labs Harvard U MIT or other historical centers of electronic brain and robotic research and development have produced anything close to the varied integrated capabilities of a biologically healthy socially aware motivated human brain functioning as one of the controlling elements within a human body We do however have machines with humanlike capabilities simulations of task that were once manual and such devices sometimes outperform humans But none of the existing automation is so integrated that they would blend in with humans Nonetheless many already have careers Single capability machines with job security Calculators Automated hoppers Antiaircraft weaponry Mail sorting machines Speech recognizing devices Speech synthesis A general purpose computer connected to a well designed high end robot could do any combination of these and other things in a time shared way Several networked computers could provide parallel execution of these tasks and even make basic decisions on how to prioritize the various tasks within the computer network However the type of integration of sophisticated translation of arbitrary generalized goals to action leading to success and the imagination and prioritization of such sophisticated objectives is not yet available or at least deployed for general home commercial or industrial use Some available machines already accomplish well defined goals but only within a very specific solution domain The interaction of various capabilities mentioned is still largely undiscovered at anything close to the level of detail necessary to produce synthetic capabilities of the same caliber Whether it is possible to do so Marvin Minsky voted yes when he popularized his colleagues characterization of the human brain as a meat machine Whether consciousness transcends neural electrochemistry has yet to be proven or disproved in either the mathematical sense or the laboratory sense of the term Proof Interestingly a form of selfconsciousness some layer of it seems to require the ability to make a noise and hear it Another layer beyond that seems to require the ability to move a muscle and view the results in a reflection These and others form an interesting set of phenomena Perhaps intelligence requires embodiment One of the episodes of Sarah Conner Chronicles implied this and the theory is an old one recognized independently by several researchers The question author implied this vaguely selfevident reality in the mention of action in the question
,,"<p>I'd definitely recommend Deep Learning by Goodfellow, Bengio and Courville. I took a PhD level course in Neural Networks a few months ago using this book as the main reference. If you know the basics you can skip to the chapters you're interested in, and the last part of the book is supposed to be as bleeding edge as you can get in a text book. The whole book is available for free, under what seems to be some kind of pre-print arrangement, at <a href=""http://www.deeplearningbook.org/"" rel=""nofollow noreferrer"">http://www.deeplearningbook.org/</a> where you can also find lecture slides and exercises.</p>

<p>Disclaimer: I'm not a mathematician but a computer scientist (and a pretty applied one as well, as I'm primarily doing research in applied machine learning for NLP) so I can't promise you'd find it as good for your particular interests, as I did for mine.</p>
",,2,2016-12-20T00:45:29.440,,2511,2016-12-20T00:45:29.440,,,,,4361.0,2508.0,2,0,,,,64.85,10.16,9.33,0.0,0.0,27.0,Id definitely recommend Deep Learning by Goodfellow Bengio and Courville I took a PhD level course in Neural Networks a few months ago using this book as the main reference If you know the basics you can skip to the chapters youre interested in and the last part of the book is supposed to be as bleeding edge as you can get in a text book The whole book is available for free under what seems to be some kind of preprint arrangement at httpwwwdeeplearningbookorg where you can also find lecture slides and exercises Disclaimer Im not a mathematician but a computer scientist and a pretty applied one as well as Im primarily doing research in applied machine learning for NLP so I cant promise youd find it as good for your particular interests as I did for mine
,1.0,"<p>It is really all in the title.</p>

<p>For those less familiar, the Fermi Paradox broadly speaking asks the question ""where is everybody"". There's an equation with a lot of difficult to estimate parameters, which broadly speaking come down to this (simplification of the <a href=""https://en.wikipedia.org/wiki/Drake_equation"" rel=""nofollow noreferrer"">Drake equation</a>):</p>

<p>(Lots stars in the universe) * (non-zero probability of habitable planets around each star) * (lots of time spanned) = It seems there really should be somebody out there.</p>

<p>There are, of course, plenty of hypotheses as to why we haven't seen/observed/detected any sign of intelligent life so far, ranging from ""well we're unique deal with it"" to ""such life is so advanced and destroys everything it comes across, so it's a good thing it didn't happen"".</p>

<p>The technological singularity (also called ASI, Artificial Super Intelligence) is basically the point where an AI is able to self-improve. Some think that if such AI sees the light of day, it may self-improve and not be bound by biological constraints of the brain, therefore achieve a level of intelligence we cannot even grasp (let alone achieve ourselves).</p>

<p>I certainly have my thoughts on the matter, but interested to see if there is already an hypothesis revolving around the link between the 2 out there (I never came across but could be). Or perhaps an hypothesis as to why this cannot be.</p>

<p>For references to those not familiar with the <a href=""http://waitbutwhy.com/2014/05/fermi-paradox.html"" rel=""nofollow noreferrer"">Fermi paradox</a></p>
",,2,2016-12-20T04:41:33.823,1.0,2512,2016-12-22T01:08:44.660,2016-12-22T01:08:44.660,,169.0,,4363.0,,1,2,<singularity>,Is the advent of a technological singularity a solution to the Fermi Paradox?,54.0,45.39,12.02,9.9,0.0,0.0,52.0,It is really all in the title For those less familiar the Fermi Paradox broadly speaking asks the question where is everybody Theres an equation with a lot of difficult to estimate parameters which broadly speaking come down to this simplification of the Drake equation Lots stars in the universe nonzero probability of habitable planets around each star lots of time spanned It seems there really should be somebody out there There are of course plenty of hypotheses as to why we havent seenobserveddetected any sign of intelligent life so far ranging from well were unique deal with it to such life is so advanced and destroys everything it comes across so its a good thing it didnt happen The technological singularity also called ASI Artificial Super Intelligence is basically the point where an AI is able to selfimprove Some think that if such AI sees the light of day it may selfimprove and not be bound by biological constraints of the brain therefore achieve a level of intelligence we cannot even grasp let alone achieve ourselves I certainly have my thoughts on the matter but interested to see if there is already an hypothesis revolving around the link between the 2 out there I never came across but could be Or perhaps an hypothesis as to why this cannot be For references to those not familiar with the Fermi paradox
,,"<p>If the technological singularity always leads to the extinction of all intelligent life, then yes.</p>

<p>If the technological singularity always leads to intelligent life migrating into higher planes of existence that aren't accessible to us right now, then also yes. </p>

<p>Otherwise it is exactly the assumption of unbridled technological progress that makes the Fermi paradox perplexing. A post-singularity culture should have the ability to spread through the galaxy. If there are a lot of post-singularity cultures some of them should have spread through the galaxy. And if there are enough cultures that are spreading through the galaxy, we should notice them.</p>
",,0,2016-12-20T10:12:42.193,,2513,2016-12-20T10:12:42.193,,,,,2227.0,2512.0,2,1,,,,37.5,14.21,9.32,0.0,0.0,12.0,If the technological singularity always leads to the extinction of all intelligent life then yes If the technological singularity always leads to intelligent life migrating into higher planes of existence that arent accessible to us right now then also yes Otherwise it is exactly the assumption of unbridled technological progress that makes the Fermi paradox perplexing A postsingularity culture should have the ability to spread through the galaxy If there are a lot of postsingularity cultures some of them should have spread through the galaxy And if there are enough cultures that are spreading through the galaxy we should notice them
,3.0,"<p>I understood that searching is important in AI. There's a <a href=""http://ai.stackexchange.com/questions/1877/why-is-searching-important-in-ais"">question</a> on this website regarding this topic, but one could also intuitively understand why. I've had an introductory course on AI, which lasted half of a semester, so of course there wasn't time enough to cover all topics of AI, but I was expecting to learn some theory behind AI (I've heard about ""agents""), but what I actually learned was basically a few searching algorithms, like:</p>

<ul>
<li>BFS</li>
<li>Uniform-cost search</li>
<li>DFS</li>
<li>Iterative-deepening search</li>
<li>Bidirectional search</li>
</ul>

<p>these searching algorithms are usually categorised as ""blind"" (or ""uninformed""), because they do not consider any information regarding the remaining path to the goal. </p>

<p>Or algorithms like:</p>

<ul>
<li>Heuristic search</li>
<li>Best-first search</li>
<li>A</li>
<li>A*</li>
<li>IDA*</li>
</ul>

<p>which usually fall under the category of ""informed"" search algorithms, because they use some information (i.e. ""heuristics"" or ""estimates"") about the remaining path to the goal.</p>

<p>Then we also learned ""advanced"" searching algorithms (specifically applied to TSP problem). These algorithms are either constructive (e.g., NN), local search (e.g., 2-opt) algorithms or meta-heuristic ones (e.g., ACS, SA, etc).</p>

<p>We also studied briefly a min-max algorithm applied to games and an ""improved"" version of the min-max, i.e. the alpha-beta pruning.</p>

<p>After this course I didn't remain with the feeling that AI is more than searching, either ""stupidily"" or ""more intelligently"".</p>

<p>My questions are:</p>

<ul>
<li><p>Why would one professor only teach searching algorithms in AI course? What are the advantages/disadvantages? The next question is very related to this.</p></li>
<li><p>What's more than ""searching"" in AI that could be taught in an introductory course? This question may lead to subjective answers, but I'm actually asking in the context of a person trying to understand what AI really is and what topics does it really cover. Apparently and unfortunately, after reading around, it seems that this would still be subjective.</p></li>
<li><p>Are there theories behind AI that could be taught in this kind of course?</p></li>
</ul>
",,2,2016-12-20T15:44:15.303,2.0,2514,2017-01-25T07:32:38.503,2017-01-22T18:42:13.953,,2444.0,,2444.0,,1,7,<philosophy><search>,Why teaching only searching algorithms in a short introductory AI course?,211.0,55.44,13.8,9.03,0.0,0.0,101.0,I understood that searching is important in AI Theres a question on this website regarding this topic but one could also intuitively understand why Ive had an introductory course on AI which lasted half of a semester so of course there wasnt time enough to cover all topics of AI but I was expecting to learn some theory behind AI Ive heard about agents but what I actually learned was basically a few searching algorithms like BFS Uniformcost search DFS Iterativedeepening search Bidirectional search these searching algorithms are usually categorised as blind or uninformed because they do not consider any information regarding the remaining path to the goal Or algorithms like Heuristic search Bestfirst search A A IDA which usually fall under the category of informed search algorithms because they use some information ie heuristics or estimates about the remaining path to the goal Then we also learned advanced searching algorithms specifically applied to TSP problem These algorithms are either constructive eg NN local search eg 2opt algorithms or metaheuristic ones eg ACS SA etc We also studied briefly a minmax algorithm applied to games and an improved version of the minmax ie the alphabeta pruning After this course I didnt remain with the feeling that AI is more than searching either stupidily or more intelligently My questions are Why would one professor only teach searching algorithms in AI course What are the advantagesdisadvantages The next question is very related to this Whats more than searching in AI that could be taught in an introductory course This question may lead to subjective answers but Im actually asking in the context of a person trying to understand what AI really is and what topics does it really cover Apparently and unfortunately after reading around it seems that this would still be subjective Are there theories behind AI that could be taught in this kind of course
,1.0,"<p>Ethics is defined as moral principles that govern a <strong>person's or group's behavior</strong>; innately, <strong>Shouldn't it the system itself which should devise ethics?</strong> Most of the articulations, in either direct or indirect manner, indicate Intelligent Systems in context of human presence. <strong>Why, in first place, are we studying AGI Ethics?</strong></p>

<p>Given Roseau's reasoning for a society, </p>

<blockquote>
  <p>Human beings can improve only when they leave the state of nature and
  enter a civil society.</p>
</blockquote>

<p>Doesn't this automatically apply for any intelligent system? If it does, then can't we imply that any inorganic intelligent system should, perhaps, come as an offset of a network of Intelligent computers (a society per say)?</p>

<p><strong>Who are we to constitute ethics, rules, morale for another society which, perhaps, could be much more intelligent that us?</strong> <Br>In fact, Human Species has been way too stupid to account for such a task.</p>
",,0,2016-12-20T21:59:24.357,,2515,2016-12-22T01:18:30.860,,,,,4244.0,,1,1,<deep-network><ai-design><strong-ai><ethics><intelligent-agent>,"Are we supposed to ""decide"" ethics for Intelligent Systems?",39.0,53.21,12.0,10.78,0.0,0.0,31.0,Ethics is defined as moral principles that govern a persons or groups behavior innately Shouldnt it the system itself which should devise ethics Most of the articulations in either direct or indirect manner indicate Intelligent Systems in context of human presence Why in first place are we studying AGI Ethics Given Roseaus reasoning for a society Human beings can improve only when they leave the state of nature and enter a civil society Doesnt this automatically apply for any intelligent system If it does then cant we imply that any inorganic intelligent system should perhaps come as an offset of a network of Intelligent computers a society per say Who are we to constitute ethics rules morale for another society which perhaps could be much more intelligent that us In fact Human Species has been way too stupid to account for such a task
2522.0,1.0,"<p>To solve problems using computer programs, we have developed a wide set of tools / control flow statements such as For Loop, If-Else, Switch-Break Statements and so on. <strong>How natural are these control flow statements?</strong> 
Since, we do not know, how exactly would AGI work, and the understanding of Neural Networks, so far, does not tell us about origin of ""intelligence"", <strong>Could a modern AI evolve itself into a system of such control-flow elements?</strong> (An appropriate architecture for computation is , anyways, necessary for any inorganic intelligence system and it is of no doubt that control-flow statements as such just makes the computation much more organized)
If so, <strong>could an AI be killed in an infinite loop created by itself?</strong><br><br>
<strong>P.S.</strong> <em>The question isn't baseless, it really questions the kind of computational infrastructure a modern AGI would require, and would it be able to alter itself without any intervention.</em></p>
",,3,2016-12-21T02:35:03.877,,2516,2016-12-21T12:04:32.430,,,,,4244.0,,1,2,<ai-design><agi><intelligent-agent><control-problem>,Can an AI be killed in an infinite loop?,128.0,41.63,13.36,10.68,0.0,0.0,31.0,To solve problems using computer programs we have developed a wide set of tools control flow statements such as For Loop IfElse SwitchBreak Statements and so on How natural are these control flow statements Since we do not know how exactly would AGI work and the understanding of Neural Networks so far does not tell us about origin of intelligence Could a modern AI evolve itself into a system of such controlflow elements An appropriate architecture for computation is anyways necessary for any inorganic intelligence system and it is of no doubt that controlflow statements as such just makes the computation much more organized If so could an AI be killed in an infinite loop created by itself PS The question isnt baseless it really questions the kind of computational infrastructure a modern AGI would require and would it be able to alter itself without any intervention
,2.0,"<p>I am coding a tic-tac-toe program that demonstrates reinforcement learning. The program uses minimax trees to decide its moves. Whenever it wins, all the nodes on the tree that were involved in the game have their value increased. Whenever it loses, all the nodes on the tree that were involved in the game have their value decreased, etc. What is the name of the value that each node is decreased by?</p>
",,1,2016-12-21T02:50:57.240,,2517,2017-02-14T12:39:59.723,,,,,4378.0,,1,1,<reinforcement-learning><minimax>,Whats the name of the value that you add or subtract from a minimax tree node?,45.0,65.52,9.16,8.12,0.0,0.0,10.0,I am coding a tictactoe program that demonstrates reinforcement learning The program uses minimax trees to decide its moves Whenever it wins all the nodes on the tree that were involved in the game have their value increased Whenever it loses all the nodes on the tree that were involved in the game have their value decreased etc What is the name of the value that each node is decreased by
,2.0,"<p>Neural Net can be feed-forward or recurrent. Perceptron is only feed forward.</p>

<p>So, what is Hopfield Network then?</p>
",,0,2016-12-21T09:08:07.530,,2518,2016-12-23T17:19:28.933,,,,,3642.0,,1,0,<neural-networks>,What is the difference between a Hopfield Network and a Neural Network or a Perceptron?,66.0,73.85,9.79,10.95,0.0,0.0,5.0,Neural Net can be feedforward or recurrent Perceptron is only feed forward So what is Hopfield Network then
,,"<p>Hopfield Networks are recurrent. However, they are not as general as more modern Recurrent Neural Networks such as Long Short-Term Memory Networks as they cannot process sequential input. I've never worked with a Hopfield Network but I've been told that they are mostly of historical interest today due to their limitations.</p>
",,0,2016-12-21T09:31:00.617,,2519,2016-12-21T09:34:13.537,2016-12-21T09:34:13.537,,4361.0,,4361.0,2518.0,2,0,,,,54.22,13.57,11.29,0.0,0.0,7.0,Hopfield Networks are recurrent However they are not as general as more modern Recurrent Neural Networks such as Long ShortTerm Memory Networks as they cannot process sequential input Ive never worked with a Hopfield Network but Ive been told that they are mostly of historical interest today due to their limitations
,1.0,"<p>Is a Levenberg–Marquardt algorithm a type of back-propagation algorithm or is it a different category of algorithm?</p>

<p>Wikipedia says that it is a curve fitting algorithm. How is a curve fitting algorithm relevant to a neural net?</p>
",,0,2016-12-21T09:53:06.757,,2520,2016-12-24T16:56:13.190,2016-12-24T16:56:13.190,,3427.0,,3642.0,,1,3,<algorithm><backpropagation>,What kind of algorithm is the Levenberg–Marquardt algorithm?,55.0,50.53,11.76,8.94,0.0,0.0,4.0,Is a Levenberg–Marquardt algorithm a type of backpropagation algorithm or is it a different category of algorithm Wikipedia says that it is a curve fitting algorithm How is a curve fitting algorithm relevant to a neural net
,,"<p>What it comes down to is that most AI problems can be characterized as search problems. Let's just go through some examples:</p>

<ul>
<li>Object recognition &amp; scene building (e.g. the process of taking
audio-visual input of your surroundings and understanding it in a 3D
and contextual sense) can be treated as searching for known objects
in the input.</li>
<li>Mathematical problem solving can be treated as searching for a solution.</li>
<li>Playing a video game can be treated as searching for the correct response to a given gamestate.</li>
</ul>

<p>Even rudimentary chatbots can be characterized as finding the 'correct' response to a given input phrase to emulate human language!</p>

<p>Because of this generalization of search, search algorithms were among some of the first algorithms considered 'AI', and often form the basis of many AI teaching courses. On top of this search algorithms are intuitive and non-mathematical, which makes the somewhat terrifying field of AI accessible. This might sound like hyperbole, but I guarantee that if your lecturer had opened with Manifold Learning Techniques half of your class would have bolted for the door by the time they mentioned 'eigenvalue of the covariance matrix'.</p>

<p>Now search algorithms aren't the only way to address these problems. I recommend every AI practitioner is familiar with the notion of Data Science and Machine Learning Algorithms. ML is often related to search algorithms but the techniques they use can vary heavily from iterative building of a classifier/regression (e.g. C4.5 builds a decision tree), meta-heuristics as you noted, and classifiers/regression that are statically generated from analysis of training data (e.g. Naive Bayesian is literally a classifier built on Bayesian analysis of the given data assuming that input fields are independent - this is the 'naivety' from which it gets its name). Often ML algorithms are developed in AI research groups and can sometimes be designed for specific problems instead of being general form algorithms. In contrast to the general field of AI, which is often centered on Intelligence problems and is therefore (in my view) vulnerable to too much blue sky thinking, ML is applied to all sorts of real life problems and is often very practical in its design and performance driven in its evaluation.</p>
",,0,2016-12-21T11:53:34.983,,2521,2016-12-21T11:53:34.983,,,,,1467.0,2514.0,2,2,,,,48.33,13.47,9.87,0.0,0.0,55.0,What it comes down to is that most AI problems can be characterized as search problems Lets just go through some examples Object recognition amp scene building eg the process of taking audiovisual input of your surroundings and understanding it in a 3D and contextual sense can be treated as searching for known objects in the input Mathematical problem solving can be treated as searching for a solution Playing a video game can be treated as searching for the correct response to a given gamestate Even rudimentary chatbots can be characterized as finding the correct response to a given input phrase to emulate human language Because of this generalization of search search algorithms were among some of the first algorithms considered AI and often form the basis of many AI teaching courses On top of this search algorithms are intuitive and nonmathematical which makes the somewhat terrifying field of AI accessible This might sound like hyperbole but I guarantee that if your lecturer had opened with Manifold Learning Techniques half of your class would have bolted for the door by the time they mentioned eigenvalue of the covariance matrix Now search algorithms arent the only way to address these problems I recommend every AI practitioner is familiar with the notion of Data Science and Machine Learning Algorithms ML is often related to search algorithms but the techniques they use can vary heavily from iterative building of a classifierregression eg C45 builds a decision tree metaheuristics as you noted and classifiersregression that are statically generated from analysis of training data eg Naive Bayesian is literally a classifier built on Bayesian analysis of the given data assuming that input fields are independent this is the naivety from which it gets its name Often ML algorithms are developed in AI research groups and can sometimes be designed for specific problems instead of being general form algorithms In contrast to the general field of AI which is often centered on Intelligence problems and is therefore in my view vulnerable to too much blue sky thinking ML is applied to all sorts of real life problems and is often very practical in its design and performance driven in its evaluation
,,"<p>This is a hard to answer question because a truly correct answer would involve static analysis of a given Intelligence to determine whether it has the computational capability to generate a looping state (e.g. some state which reproduces itself in the next instance), and in fact whether these looping states can even exist in the given architecture. </p>

<p>Generally speaking, assuming we can build AI, there is no reason it would be impossible build an AI that has the potential to get stuck in a computational loop and become non-functional. You can imagine a recurrent neural network where the continuous state inside the ANN dominates its behavior causing it to reproduce states with small variety (as input will always be relevant in a connected ANN) but are converged around a particular state space. Whether that is akin to death is a philosophical question for your own peace of mind.</p>

<p>However, if we designed an AI agent that commonly did this, that would reflect poor design; that wouldn't necessarily devalue the contribution of the designed agent but it would bring the agent's other activities under scrutiny. If this is a bi-product of some simulated intelligence, is the process going on really that intelligent?</p>
",,0,2016-12-21T12:04:32.430,,2522,2016-12-21T12:04:32.430,,,,,1467.0,2516.0,2,3,,,,33.88,12.72,10.72,0.0,0.0,23.0,This is a hard to answer question because a truly correct answer would involve static analysis of a given Intelligence to determine whether it has the computational capability to generate a looping state eg some state which reproduces itself in the next instance and in fact whether these looping states can even exist in the given architecture Generally speaking assuming we can build AI there is no reason it would be impossible build an AI that has the potential to get stuck in a computational loop and become nonfunctional You can imagine a recurrent neural network where the continuous state inside the ANN dominates its behavior causing it to reproduce states with small variety as input will always be relevant in a connected ANN but are converged around a particular state space Whether that is akin to death is a philosophical question for your own peace of mind However if we designed an AI agent that commonly did this that would reflect poor design that wouldnt necessarily devalue the contribution of the designed agent but it would bring the agents other activities under scrutiny If this is a biproduct of some simulated intelligence is the process going on really that intelligent
,,"<p>In the context of Neural Networks, Backpropagation (with Gradient Descent, to use its full name) and Levengerg Marquardt are both members of the broader family of gradient descent algorithms. Backpropagation itself is not gradient descent, but it does the gradient climbing portion of a broader gradient descent algorithm. </p>

<p>You can imagine the function of a Neural Network as a function from its inputs to its outputs. If you were trying to solve, for example, a regression problem you can imagine this problem in multidimensional space with each training point corresponding to the coordinate provided by the values of its inputs and outputs (each of which represent a dimension). Then the entire training set you have for learning this regression problem become a set of points in this multidimensional space, and the function that your neural network performs is a curve in this multidimensional space. The closer this curve is to the points on the training set, the better it performs at the regression problem which essentially means we can generalize the task of training the neural network to a curve fitting problem.</p>
",,0,2016-12-21T12:15:05.257,,2523,2016-12-21T12:15:05.257,,,,,1467.0,2520.0,2,1,,,,49.18,13.36,9.3,0.0,0.0,17.0,In the context of Neural Networks Backpropagation with Gradient Descent to use its full name and Levengerg Marquardt are both members of the broader family of gradient descent algorithms Backpropagation itself is not gradient descent but it does the gradient climbing portion of a broader gradient descent algorithm You can imagine the function of a Neural Network as a function from its inputs to its outputs If you were trying to solve for example a regression problem you can imagine this problem in multidimensional space with each training point corresponding to the coordinate provided by the values of its inputs and outputs each of which represent a dimension Then the entire training set you have for learning this regression problem become a set of points in this multidimensional space and the function that your neural network performs is a curve in this multidimensional space The closer this curve is to the points on the training set the better it performs at the regression problem which essentially means we can generalize the task of training the neural network to a curve fitting problem
,3.0,"<p>I am new to neural-network and I am trying to understand mathematically what makes neural networks so good at classification problems. </p>

<p>By taking the example of a small neural network (for example, one with 2 inputs, 2 nodes in a hidden layer and 2 nodes for the output), all you have is a complex function at the output which is mostly sigmoid over linear combination of sigmoid.</p>

<p>So, how does that make them good at prediction? Does the final function lead to some sort of curve fitting?</p>
",,1,2016-12-21T20:47:37.700,1.0,2524,2017-01-02T12:09:49.667,2016-12-30T02:23:25.370,,2444.0,,4394.0,,1,6,<neural-networks>,What makes neural networks so good at predictions?,167.0,66.27,9.46,9.8,0.0,0.0,11.0,I am new to neuralnetwork and I am trying to understand mathematically what makes neural networks so good at classification problems By taking the example of a small neural network for example one with 2 inputs 2 nodes in a hidden layer and 2 nodes for the output all you have is a complex function at the output which is mostly sigmoid over linear combination of sigmoid So how does that make them good at prediction Does the final function lead to some sort of curve fitting
,,"<p>Many large deployments of AI have carefully engineered solutions to problems (ie self driving cars). In these systems, it is important to have discussions about how these systems should react in morally ambiguous situations. Having an agent react ""appropriately"" sounds similar to the Turing test in that there is a ""pass/fail"" condition. This leads me to think that the <strong>current mindset of most AI researchers falls into ""Conservative anthropomorphism""</strong>.</p>

<p>However, there is growing interest in <a href=""http://www.cs.utexas.edu/~ring/Ring-dissertation.pdf"" rel=""nofollow noreferrer"">Continual Learning</a>, where agents build up knowledge about their world from their experience. This idea is largely pushed by reinforcement learning researchers such as Richard Sutton and Mark Ring. Here, the AI agent has to build up knowledge about its world such as:</p>

<blockquote>
  <p>When I rotate my motors forward for 3s, my front bump-sensor activates.</p>
</blockquote>

<p>and </p>

<blockquote>
  <p>If I turned right 90 degrees and then rotated my motors forward for 3s, my front bump-sensor activates.</p>
</blockquote>

<p>From knowledge like this, an agent could eventually navigate a room without running into walls because it built up predictive knowledge from interaction with the world.</p>

<p>In this context, lets ignore how the AI actually learns and only look at the environment AI agents ""grow up in"". These agents will be fundamentally different from humans growing up in homes with families because they will not have the same learning experience as humans. This is much like the nature vs nurture argument. </p>

<p>Humans pass their morals and values on to children through lessons and conversation. As RL agents would lack much of this interaction (unless families adopted robot babies I guess), we would require different ways of judging their moral worth and thus ""Post-human fundamentalism"".</p>

<p>Sources:
5 years in the RL academia environment and conversations with Richard Sutton and Mark Ring.</p>
",,0,2016-12-21T23:26:12.510,,2525,2016-12-22T07:58:19.483,2016-12-22T07:58:19.483,,4398.0,,4398.0,2498.0,2,3,,,,51.89,13.29,9.57,0.0,0.0,44.0,Many large deployments of AI have carefully engineered solutions to problems ie self driving cars In these systems it is important to have discussions about how these systems should react in morally ambiguous situations Having an agent react appropriately sounds similar to the Turing test in that there is a passfail condition This leads me to think that the current mindset of most AI researchers falls into Conservative anthropomorphism However there is growing interest in Continual Learning where agents build up knowledge about their world from their experience This idea is largely pushed by reinforcement learning researchers such as Richard Sutton and Mark Ring Here the AI agent has to build up knowledge about its world such as When I rotate my motors forward for 3s my front bumpsensor activates and If I turned right 90 degrees and then rotated my motors forward for 3s my front bumpsensor activates From knowledge like this an agent could eventually navigate a room without running into walls because it built up predictive knowledge from interaction with the world In this context lets ignore how the AI actually learns and only look at the environment AI agents grow up in These agents will be fundamentally different from humans growing up in homes with families because they will not have the same learning experience as humans This is much like the nature vs nurture argument Humans pass their morals and values on to children through lessons and conversation As RL agents would lack much of this interaction unless families adopted robot babies I guess we would require different ways of judging their moral worth and thus Posthuman fundamentalism Sources 5 years in the RL academia environment and conversations with Richard Sutton and Mark Ring
,3.0,"<ul>
<li><p>Is it a must that an activation function of a neural network be differentiable?</p></li>
<li><p>Why should an activation function of a neural network be differentiable?</p></li>
<li><p>Is it advantageous to have a differentiable activation function? Why?</p></li>
</ul>
",,0,2016-12-21T23:26:55.103,1.0,2526,2016-12-24T16:55:55.933,2016-12-24T16:55:55.933,,3427.0,,3642.0,,1,3,<neural-networks>,Differentiable activation function,66.0,51.14,12.68,7.83,0.0,0.0,4.0,Is it a must that an activation function of a neural network be differentiable Why should an activation function of a neural network be differentiable Is it advantageous to have a differentiable activation function Why
,,"<h2>Training</h2>

<p>While ""running"" a neural network can be done with any activation functions, we usually want to train it - i.e., adjust its parameters so that the result would be closer to what we desire.</p>

<p>This is commonly done by <a href=""https://en.wikipedia.org/wiki/Backpropagation"" rel=""nofollow noreferrer"">backpropagation</a> and variations of <a href=""https://en.wikipedia.org/wiki/Gradient_descent"" rel=""nofollow noreferrer"">gradient descent</a>, which requires the existence of a gradient - i.e., requires activation function to be differentiable. The adjustment of each parameter is calculated from the gradient of the activation function(s) that this parameter affects, so if you cannot get a gradient, then this approach can't be used.</p>
",,0,2016-12-22T00:36:51.500,,2527,2016-12-22T00:45:07.347,2016-12-22T00:45:07.347,,1675.0,,1675.0,2526.0,2,1,,,,53.21,13.05,10.14,0.0,0.0,20.0,Training While running a neural network can be done with any activation functions we usually want to train it ie adjust its parameters so that the result would be closer to what we desire This is commonly done by backpropagation and variations of gradient descent which requires the existence of a gradient ie requires activation function to be differentiable The adjustment of each parameter is calculated from the gradient of the activation functions that this parameter affects so if you cannot get a gradient then this approach cant be used
,0.0,"<p>I understand an MDP (Markov Decision Process) model is a tuple of {S, A, P, R} where:</p>

<ul>
<li>S is a discrete set of states</li>
<li>A is a discrete set of actions</li>
<li>P is the transition matrix ie. P(s' | s, a) -> [0,1]</li>
<li>R is the reward function id. R(s, a, s') -> real</li>
</ul>

<p>For a non-trivial MDP, say 1000 states and 10 actions, the transition matrix has theoretically S x A x S = 10,000,000 entries (though many entries will be 0).</p>

<p>I understand that one way of generating the P matrix is to estimate it via Montecarlo sampling by simulating the environment. However with non-trivial state space and simulation costs this could be prohibitively expensive.</p>

<p>In practice, when a non-trivial MDP is being formulated, what are the different ways an accurate P matrix can be produced?</p>
",,0,2016-12-22T00:45:53.537,,2528,2016-12-22T00:45:53.537,,,,,4402.0,,1,0,<machine-learning><monte-carlo-search>,What techniques are used in practice to generate MDP models?,12.0,74.22,9.7,8.63,0.0,0.0,43.0,I understand an MDP Markov Decision Process model is a tuple of S A P R where S is a discrete set of states A is a discrete set of actions P is the transition matrix ie Ps s a 01 R is the reward function id Rs a s real For a nontrivial MDP say 1000 states and 10 actions the transition matrix has theoretically S x A x S 10000000 entries though many entries will be 0 I understand that one way of generating the P matrix is to estimate it via Montecarlo sampling by simulating the environment However with nontrivial state space and simulation costs this could be prohibitively expensive In practice when a nontrivial MDP is being formulated what are the different ways an accurate P matrix can be produced
,,"<h2>Nonhuman ethics can be totally arbitrary</h2>

<p>Regarding your citation, perhaps it's useful to consider why should anything that philosophers have said about human ethics apply for arbitrary nonhuman systems? We all share a common biological background, our behavior, emotions and especially positive/negative feelings arise from the particular way how our brains are built. Man is by nature a social animal, and much of our behavior and feelings of empathy, compassion, fairness, greed, social status, etc are formed and expressed in society. Our minds are adjusted for this, as are the minds of other mammals. A human that forms an ethical system when joining a civil society does so in the basis of all this shared context, and within the limitations of it.</p>

<p>However, all this is orthogonal to intelligence. All the variety of mankind encompasses a tiny island in the whole space of possible intelligences and their goals. Any system of ethics is feasible, a vast majority of them having no relationship whatsoever to what we could consider ethical. A human can be expected to join a society and learn ethics because our brain is hardwired to respond to social stimuli - an AI would learn ethics from a society only if its previous ethics already happen to value that highly, and that isn't likely to happen without careful design.</p>

<p>There is a thought experiment that is commonly used as an example - the ""paperclip maximizer"", an artificial agent with an ethical system that defines 'good' proportionally to the number of paperclips that exist in the universe and only that - and that is a <em>perfectly reasonable</em> example, because a completely random ethical system is overwhelmingly likely to reduce to something stupid like that. The space of ""reasonable"" (to us) ethical systems is very tiny compared to all ethical systems possible, and expecting that a randomly selected ethical system will just happen to land there is like relying on winning a high-stakes lottery.</p>

<h2>We want their ethics to contain certain things</h2>

<p>So the main reason why we are studying AGI ethics is because we (or some of us) really, really, want the AGI ethical system to include certain things. For example, I'd like that ethical system to include my survival. I'd prefer that ethical system to be consistent with my happiness and wellbeing - and the very existence of a very powerful entity that simply happens to not care about me at all is not compatible with my survival and wellbeing, so I'd <em>really</em> prefer for it to care about me, or alternatively never become very powerful.</p>

<p>One could certainly argue that we have no right to constitute ethics, rules and morale for another society which could be much more intelligent and better than us. That another society might be ""better"" in some manner - but better <em>for whom</em> ? If it's better for ""them"" but worse for me (and humanity), then I have strong motivation to simply <em>claim</em> that right and try my hardest to ensure that this hypothetical future society includes me, my desires and my ethics. </p>
",,0,2016-12-22T01:18:30.860,,2529,2016-12-22T01:18:30.860,,,,,1675.0,2515.0,2,1,,,,41.43,12.2,10.16,0.0,0.0,71.0,Nonhuman ethics can be totally arbitrary Regarding your citation perhaps its useful to consider why should anything that philosophers have said about human ethics apply for arbitrary nonhuman systems We all share a common biological background our behavior emotions and especially positivenegative feelings arise from the particular way how our brains are built Man is by nature a social animal and much of our behavior and feelings of empathy compassion fairness greed social status etc are formed and expressed in society Our minds are adjusted for this as are the minds of other mammals A human that forms an ethical system when joining a civil society does so in the basis of all this shared context and within the limitations of it However all this is orthogonal to intelligence All the variety of mankind encompasses a tiny island in the whole space of possible intelligences and their goals Any system of ethics is feasible a vast majority of them having no relationship whatsoever to what we could consider ethical A human can be expected to join a society and learn ethics because our brain is hardwired to respond to social stimuli an AI would learn ethics from a society only if its previous ethics already happen to value that highly and that isnt likely to happen without careful design There is a thought experiment that is commonly used as an example the paperclip maximizer an artificial agent with an ethical system that defines good proportionally to the number of paperclips that exist in the universe and only that and that is a perfectly reasonable example because a completely random ethical system is overwhelmingly likely to reduce to something stupid like that The space of reasonable to us ethical systems is very tiny compared to all ethical systems possible and expecting that a randomly selected ethical system will just happen to land there is like relying on winning a highstakes lottery We want their ethics to contain certain things So the main reason why we are studying AGI ethics is because we or some of us really really want the AGI ethical system to include certain things For example Id like that ethical system to include my survival Id prefer that ethical system to be consistent with my happiness and wellbeing and the very existence of a very powerful entity that simply happens to not care about me at all is not compatible with my survival and wellbeing so Id really prefer for it to care about me or alternatively never become very powerful One could certainly argue that we have no right to constitute ethics rules and morale for another society which could be much more intelligent and better than us That another society might be better in some manner but better for whom If its better for them but worse for me and humanity then I have strong motivation to simply claim that right and try my hardest to ensure that this hypothetical future society includes me my desires and my ethics
2537.0,1.0,"<p>Generating a discretized state space for an MDP (Markov Decision Process) model seems to suffer from the curse of dimensionality.</p>

<p>Supposed my state has a few simple features:</p>

<ul>
<li>Feeling: Happy/Neutral/Sad</li>
<li>Feeling: Hungry/Neither/Full</li>
<li>Food left: Lots/Some/Low/Empty</li>
<li>Time of day: Morning/Afternoon/Evening/Night</li>
<li>Located: Home/Work/Library/Shops</li>
<li>Money in wallet: $0, $0-$50, $51-$200, $200-$1000, $1000+</li>
</ul>

<p>This is a relatively compact set of information however since the number of states multiplies out, the corresponding MDP state space is 3 x 3 x 4 x 4 x 4 x 5 = 2880.</p>

<p>For a non-trivial problem with a greater number of choices per factor, the state space quickly becomes unmanageably large.</p>

<p>It seems to me that the usefulness of MDPs with large complex problems would be very limited.</p>

<p>Is that the case, or are there ways of keeping the state space manageable for more complex problems?</p>

<p>In general, what is a manageable number of states for an MDP to have, considering the need to generate the transition matrix and reward matrix?</p>
",,0,2016-12-22T01:35:50.973,2.0,2530,2017-01-10T03:30:29.247,,,,,4402.0,,1,2,<machine-learning>,What techniques are used to make MDP discrete state space manageable?,40.0,44.78,13.87,10.04,0.0,21.0,39.0,Generating a discretized state space for an MDP Markov Decision Process model seems to suffer from the curse of dimensionality Supposed my state has a few simple features Feeling HappyNeutralSad Feeling HungryNeitherFull Food left LotsSomeLowEmpty Time of day MorningAfternoonEveningNight Located HomeWorkLibraryShops Money in wallet 0512001000 This is a relatively compact set of information however since the number of states multiplies out the corresponding MDP state space is 3 x 3 x 4 x 4 x 4 x 5 2880 For a nontrivial problem with a greater number of choices per factor the state space quickly becomes unmanageably large It seems to me that the usefulness of MDPs with large complex problems would be very limited Is that the case or are there ways of keeping the state space manageable for more complex problems In general what is a manageable number of states for an MDP to have considering the need to generate the transition matrix and reward matrix
2534.0,1.0,"<p><a href=""http://www.cs.bham.ac.uk/~jxb/INC/l5.pdf"" rel=""nofollow noreferrer"">http://www.cs.bham.ac.uk/~jxb/INC/l5.pdf</a></p>

<blockquote>
  <p>The neuropsychologist Donald Hebb postulated in 1949 how biological
  neurons learn:</p>
  
  <p>“When an axon of cell A is near enough to excite a cell B and
  repeatedly or persistently takes part in firing it, some growth
  process or metabolic change takes place on one or both cells such that
  A’s efficiency as one of the cells firing B, is increased.”</p>
  
  <p>In more familiar terminology, that can be stated as the Hebbian
  Learning rule:</p>
  
  <p>If two neurons on either side of a synapse (connection) are activated
  simultaneously (i.e. synchronously), then the strength of that synapse
  is selectively increased.</p>
  
  <p>Mathematically, we can describe Hebbian learning as:</p>
  
  <p><a href=""https://i.stack.imgur.com/BLTdE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BLTdE.png"" alt=""enter image description here""></a></p>
  
  <p>Here, η is a learning rate coefficient, and x are the outputs of the
  ith and jth elements.</p>
</blockquote>

<hr>

<p>Now, my question is, what do all these descriptions mean?</p>

<ol>
<li>Is Hebbian Learning applicable for single-neuron networks?</li>
<li>What does it mean by ""two neurons on either side of a synapse""?</li>
<li>Why/when would two neurons activate simultaneously?</li>
<li>What does they mean by <code>elements</code>?</li>
</ol>
",,1,2016-12-22T04:46:36.093,,2531,2016-12-22T07:09:33.973,,,,,3642.0,,1,1,<neural-networks>,How do you explain Hebbian Learning in an intuitive way?,43.0,53.0,13.29,8.97,8.0,0.0,42.0,httpwwwcsbhamacukjxbINCl5pdf The neuropsychologist Donald Hebb postulated in 1949 how biological neurons learn “When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it some growth process or metabolic change takes place on one or both cells such that A’s efficiency as one of the cells firing B is increased” In more familiar terminology that can be stated as the Hebbian Learning rule If two neurons on either side of a synapse connection are activated simultaneously ie synchronously then the strength of that synapse is selectively increased Mathematically we can describe Hebbian learning as Here η is a learning rate coefficient and x are the outputs of the ith and jth elements Now my question is what do all these descriptions mean Is Hebbian Learning applicable for singleneuron networks What does it mean by two neurons on either side of a synapse Whywhen would two neurons activate simultaneously What does they mean by
,,,,0,2016-12-22T06:02:14.000,,2532,2016-12-22T06:02:14.000,2016-12-22T06:02:14.000,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,Deep learning is a branch of machine learning that uses neural networks with many layers to allow modeling of high-level abstractions. ,,0,2016-12-22T06:02:14.000,,2533,2017-01-05T21:45:18.273,2017-01-05T21:45:18.273,,10.0,,-1.0,,4,0,,,,58.62,14.21,12.2,0.0,0.0,2.0,Deep learning is a branch of machine learning that uses neural networks with many layers to allow modeling of highlevel abstractions
,,"<ol>
<li><p>Hebbian learning is trying to answer ""How the strength of the synapse between 2 neurons evolve over period of time based on the activity of the 2 neurons involved."". You can call it learning if you think learning is just strengthening of synapses. </p></li>
<li><p>The connection between 2 neurons are called synapse. A synapse is the point where the axons of a neuron meets with the dendrites of another neuron.   </p></li>
<li><p>The 2 neurons are connected to each other but they are also connected to other neurons as well. So it may happen that both the neurons gets activated by their other connected neurons at the same time.</p></li>
<li><p>The <code>elements</code> refer the 2 neurons. The equation is basically saying that the synapse strength between i and j neuron at time (n+1) depends on its strength at time n plus activations of the i and j neurons at time n. </p></li>
</ol>
",,0,2016-12-22T07:09:33.973,,2534,2016-12-22T07:09:33.973,,,,,1462.0,2531.0,2,4,,,,69.92,8.82,8.11,8.0,0.0,14.0,Hebbian learning is trying to answer How the strength of the synapse between 2 neurons evolve over period of time based on the activity of the 2 neurons involved You can call it learning if you think learning is just strengthening of synapses The connection between 2 neurons are called synapse A synapse is the point where the axons of a neuron meets with the dendrites of another neuron The 2 neurons are connected to each other but they are also connected to other neurons as well So it may happen that both the neurons gets activated by their other connected neurons at the same time The refer the 2 neurons The equation is basically saying that the synapse strength between i and j neuron at time n1 depends on its strength at time n plus activations of the i and j neurons at time n
2543.0,1.0,"<p>Just started reading a book about AI. There is a very basic exercise but I can't figure it out, so here we go. The book is  <a href=""https://www.cs.bris.ac.uk/~flach/SL/SL.pdf"" rel=""nofollow noreferrer"">Simply Logical: Intelligent Reasoning by Example</a></p>

<p>The exercise is in the page 19. </p>

<blockquote>
  <p>Two stations are ‘not too far’ if they are on the same or a different
  line, with at most one station in between. Define rules for the
  predicate not_too_far.</p>
</blockquote>

<p>The only rules I've seen are <strong>nearby</strong> and <strong>connected</strong> and don't know how to use this. What I've done so far is this:</p>

<blockquote>
  <p>not_too_far(X,Y) :- nearby(X,Y)</p>
</blockquote>
",,2,2016-12-23T16:20:30.830,,2535,2016-12-25T15:05:09.360,,,,,4421.0,,1,3,<prolog>,Define rules for the predicate,44.0,66.44,7.53,8.54,0.0,0.0,26.0,Just started reading a book about AI There is a very basic exercise but I cant figure it out so here we go The book is Simply Logical Intelligent Reasoning by Example The exercise is in the page 19 Two stations are ‘not too far’ if they are on the same or a different line with at most one station in between Define rules for the predicate nottoofar The only rules Ive seen are nearby and connected and dont know how to use this What Ive done so far is this nottoofarXY nearbyXY
,,"<p>Hopfield stores some predefined patterns (lower energy states) and when an non seen pattern is fed to the Hopfield net, it tries to find the closest match among the stored patterns. Hence the output of a Hopfield is always one of the predefined patterns which matches closely to the unseen input pattern.</p>

<p>This is not the case with Feed Forward Neural Nets (where no such predefined patterns are stored) and every input generates a corresponding output. Here (in Feedforward) the output is generated by a predefined function (which is self-adjusted during training session) where as in Hopfield predefined patterns are stored and outputted (no such functions exist).</p>
",,0,2016-12-23T17:19:28.933,,2536,2016-12-23T17:19:28.933,,,,,4424.0,2518.0,2,2,,,,44.27,13.29,10.28,0.0,0.0,16.0,Hopfield stores some predefined patterns lower energy states and when an non seen pattern is fed to the Hopfield net it tries to find the closest match among the stored patterns Hence the output of a Hopfield is always one of the predefined patterns which matches closely to the unseen input pattern This is not the case with Feed Forward Neural Nets where no such predefined patterns are stored and every input generates a corresponding output Here in Feedforward the output is generated by a predefined function which is selfadjusted during training session where as in Hopfield predefined patterns are stored and outputted no such functions exist
,,"<p><strong>tl:dr</strong>
Read chapter 9 of <a href=""https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf"" rel=""nofollow noreferrer"">https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf</a></p>

<p>There is definitely a problem (a curse if you will) when the dimensionality of a task (MDP) grows. For fun, lets extend your problem to a much harder case, continuous variables, and see how we deal with it.</p>

<pre><code>Mood: range [-1, 1]   // 1 is Happy, 0 is Neutral, -1 is Sad

Hunger: range [0, 1]  // 0 is Very Hungry, 1 is Full

Food: range [0, 100] // amount of food, capping at 100 for numbers sake

Time of day: [0, 23] // Hours, 9.5 would be 9:30 am

Position x: range [-10, 10] // assuming the area the agent stays in is 10x10 km

Position y: range [-10, 10]

Money: range [0, 2000]
</code></pre>

<p>Now its impossible to even count the number of states the agent could be in. An example of one state would be:</p>

<pre><code>Mood, Hunger, Food, Time, PosX, PosY, Money
.5, .3, 67.4, 4.5, 0, 0, 5
</code></pre>

<p>This would mean that our agent has a neutral mood, is kind of hungry, has 67.4% food, the time is 4:30 am, they are in the center of the city (0,0), and have 5 dollars. Which has happened to me once or twice before too so this is reasonable. <strong>If we we still wanted to generate a transition matrix and a reward matrix we would have to represent this state uniquely from other possibly similar states such as if the time was 5 pm with all other things equal.</strong></p>

<p>So now how would we approach this?
First lets discretize or breakup each range into little chunks. Then we could have an array of 0s representing our chunks and assign a 1 to the index of the chunk we are in. Lets call this array our feature array. 
If we did this for each dimension of our state information, we would end up with some state similar to what you originally proposed. I've chosen some arbitrary numbers to break up the state space and lets see what it does to the example state.</p>

<p>If we broke the Mood range up into 8 chunks of .25 we would have these ranges:</p>

<p>m = Mood</p>

<ol>
<li><p>-1.0 ≤ m &lt; -.75</p></li>
<li><p>-.75 ≤ m &lt; -.50</p></li>
<li><p>-.50 ≤ m &lt; -.25</p></li>
<li><p>-.25 ≤ m &lt; 0.0</p></li>
<li><p>0.0 ≤ m &lt; .25</p></li>
<li><p>.25 ≤ m &lt; .50</p></li>
<li><p>.50 ≤ m &lt; .75</p></li>
<li><p>.75 ≤ m &lt; 1.0</p></li>
</ol>

<p>If we took the mood from the example state (.5), it would land in the 7th range, so then our feature vector for mood would look like:</p>

<pre><code>[0,0,0,0,0,0,1,0] &lt;- represents a mood of .5
</code></pre>

<p>Lets do the same for hunger by splitting it into 8 chunks of .125:</p>

<pre><code>[0,0,1,0,0,0,0,0] &lt;- represents a hunger of .3
</code></pre>

<p>We could then do something similar for each other state variable and break each state variable's range up into 8 chunks of equal sizes as well. Then after calculating their feature vectors, we would concatenate them all together and have a feature vector that is 56 elements long (7 state variables * 8 chunks each).</p>

<p>Although this would give us 2^56 different combinations of feature vectors, it actually doesn't represent what we wanted to represent. We need features that activate (are 1) when events happen together. We need a feature that is 1 when we are hungry <strong>AND</strong> when we have food. We could do this by doing a cross product of our 56 element feature vector with itself.</p>

<p>Now we have a 3136 element feature vector that has features like:
  1 if its 3pm and am happy
  1 if am full and out of food
  1 if at coordinate -3, 4 (Position x, Position y)</p>

<p>This is a step in the right direction but still isn't enough to represent or original example state. So lets keep going with the cross products!</p>

<p>If we do 6 cross products we would have 30,840,979,456 features and only 1 of them would be on when our agent has a neutral mood, is kind of hungry, has 67.4% food, the time is 4:30 am, they are in the center of the city (0,0), and have 5 dollars.</p>

<p>At this point you are probably like ""Well.. thats a bit much"", to which I would agree. This would be the curse of dimensionality, and is the reason transition tables stop being fun (if they ever were).</p>

<p><strong>Instead lets take a different approach</strong>, rather than trying to represent an individual state with an individual feature and saying whether that feature is good, lets go back to our nice 56 element feature vector. From here lets instead give a weight (w_i) to each of our 56 features. We have now entered the world of linear function approximation. Although I could explain it here, I think its better explained in Chapter 9 of the Introduction of Reinforcement Learning. <a href=""https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf"" rel=""nofollow noreferrer"">https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf</a></p>
",,1,2016-12-23T19:13:15.560,,2537,2017-01-10T03:30:29.247,2017-01-10T03:30:29.247,,4398.0,,4398.0,2530.0,2,2,,,,88.87,7.94,7.07,547.0,0.0,166.0,tldr Read chapter 9 of httpswebdocscsualbertacasuttonbookbookdraft2016seppdf There is definitely a problem a curse if you will when the dimensionality of a task MDP grows For fun lets extend your problem to a much harder case continuous variables and see how we deal with it Now its impossible to even count the number of states the agent could be in An example of one state would be This would mean that our agent has a neutral mood is kind of hungry has 674 food the time is 430 am they are in the center of the city 00 and have 5 dollars Which has happened to me once or twice before too so this is reasonable If we we still wanted to generate a transition matrix and a reward matrix we would have to represent this state uniquely from other possibly similar states such as if the time was 5 pm with all other things equal So now how would we approach this First lets discretize or breakup each range into little chunks Then we could have an array of 0s representing our chunks and assign a 1 to the index of the chunk we are in Lets call this array our feature array If we did this for each dimension of our state information we would end up with some state similar to what you originally proposed Ive chosen some arbitrary numbers to break up the state space and lets see what it does to the example state If we broke the Mood range up into 8 chunks of 25 we would have these ranges m Mood 10 ≤ m lt 75 75 ≤ m lt 50 50 ≤ m lt 25 25 ≤ m lt 00 00 ≤ m lt 25 25 ≤ m lt 50 50 ≤ m lt 75 75 ≤ m lt 10 If we took the mood from the example state 5 it would land in the 7th range so then our feature vector for mood would look like Lets do the same for hunger by splitting it into 8 chunks of 125 We could then do something similar for each other state variable and break each state variables range up into 8 chunks of equal sizes as well Then after calculating their feature vectors we would concatenate them all together and have a feature vector that is 56 elements long 7 state variables 8 chunks each Although this would give us 256 different combinations of feature vectors it actually doesnt represent what we wanted to represent We need features that activate are 1 when events happen together We need a feature that is 1 when we are hungry AND when we have food We could do this by doing a cross product of our 56 element feature vector with itself Now we have a 3136 element feature vector that has features like 1 if its 3pm and am happy 1 if am full and out of food 1 if at coordinate 3 4 Position x Position y This is a step in the right direction but still isnt enough to represent or original example state So lets keep going with the cross products If we do 6 cross products we would have 30840979456 features and only 1 of them would be on when our agent has a neutral mood is kind of hungry has 674 food the time is 430 am they are in the center of the city 00 and have 5 dollars At this point you are probably like Well thats a bit much to which I would agree This would be the curse of dimensionality and is the reason transition tables stop being fun if they ever were Instead lets take a different approach rather than trying to represent an individual state with an individual feature and saying whether that feature is good lets go back to our nice 56 element feature vector From here lets instead give a weight wi to each of our 56 features We have now entered the world of linear function approximation Although I could explain it here I think its better explained in Chapter 9 of the Introduction of Reinforcement Learning httpswebdocscsualbertacasuttonbookbookdraft2016seppdf
,,"<p>Short Answer, <strong>No</strong>.<Br><br>
Explained, Siri and Cortana are just <strong>inference engines</strong>. Though how applaudable their ability to synthesize text from speech and parse lexical maps from the text using Machine Learning Techniques is, the artifact is still just a program, trained with substantial myriad of Q/A tuples, that generates an output given an input. Statistically mapping the probability distribution of words of a question and accordingly generating an answer is just <strong>NOT</strong> an AI, however, to the most, it can be classified as <strong><em>Narrow Intelligence</em></strong>. <Br><Br>
<a href=""http://www.cleverbot.com"" rel=""nofollow noreferrer"">www.cleverbot.com</a> is an example of such a system. The ones you mentioned are just more sophisticated and highly architectured versions of this kind; for example, connection to WolframAlpha (Siri), Computer Kernel (Cortana). In fact, there is a lot of innate use of control statements (If-Else).</p>
",,0,2016-12-23T22:08:36.177,,2538,2016-12-23T22:08:36.177,,,,,4244.0,1461.0,2,1,,,,44.14,14.16,11.36,0.0,0.0,29.0,Short Answer No Explained Siri and Cortana are just inference engines Though how applaudable their ability to synthesize text from speech and parse lexical maps from the text using Machine Learning Techniques is the artifact is still just a program trained with substantial myriad of QA tuples that generates an output given an input Statistically mapping the probability distribution of words of a question and accordingly generating an answer is just NOT an AI however to the most it can be classified as Narrow Intelligence wwwcleverbotcom is an example of such a system The ones you mentioned are just more sophisticated and highly architectured versions of this kind for example connection to WolframAlpha Siri Computer Kernel Cortana In fact there is a lot of innate use of control statements IfElse
,,"<p>It is almost mandatory to have a differentiable activation function unless, of course, you have an alternative to <em>training the network by back-propagating the error</em>.</p>
",,1,2016-12-23T22:25:31.763,,2539,2016-12-23T22:25:31.763,,,,,4244.0,2526.0,2,4,,,,20.72,15.73,11.82,0.0,0.0,4.0,It is almost mandatory to have a differentiable activation function unless of course you have an alternative to training the network by backpropagating the error
,,"<p>As already said , Activation function is almost differentiable in every neural net to facillitate <strong>Training</strong> as well as to calculate tendency towards a certain result when some parameter is changed. But I just wanted to point out that <strong>The Output function need not be differentiable in all cases.</strong> We can have non-sigmoid (hard-limiter threshold)output nodes but still train them with backpropagation and gradient descent.</p>
",,0,2016-12-24T11:31:52.260,,2540,2016-12-24T11:31:52.260,,,,,4424.0,2526.0,2,2,,,,41.4,14.79,11.11,0.0,0.0,8.0,As already said Activation function is almost differentiable in every neural net to facillitate Training as well as to calculate tendency towards a certain result when some parameter is changed But I just wanted to point out that The Output function need not be differentiable in all cases We can have nonsigmoid hardlimiter thresholdoutput nodes but still train them with backpropagation and gradient descent
,,"<p>It depends on the accuracy you want. If you had 1 neuron, it could discern things across a line, if you have 2, you could solve things across 2 lines, etc. As you increase the number of neurons, you are increasing the number of discernible areas. As you increase the number of lines you can use to break up the input space, the lines can be placed to approximate any curve (sinusoidal) As the number of neurons approaches infinity, the accuracy of categorizing different inputs across this curve increases. </p>

<p>Interestingly enough, if one graphed ""Number of Neurons (x) vs Accuracy (y)"", it would look sinusoidal. </p>
",,0,2016-12-24T19:17:32.087,,2541,2017-01-28T04:43:31.133,2017-01-28T04:43:31.133,,4398.0,,4398.0,2404.0,2,2,,,,58.62,10.68,9.49,0.0,0.0,22.0,It depends on the accuracy you want If you had 1 neuron it could discern things across a line if you have 2 you could solve things across 2 lines etc As you increase the number of neurons you are increasing the number of discernible areas As you increase the number of lines you can use to break up the input space the lines can be placed to approximate any curve sinusoidal As the number of neurons approaches infinity the accuracy of categorizing different inputs across this curve increases Interestingly enough if one graphed Number of Neurons x vs Accuracy y it would look sinusoidal
2565.0,2.0,"<p>What are the basic layers on an Artificially Intelligent program and what skills and concept are required to work on this field. Total newbie interested in AI.</p>
",,0,2016-12-25T08:08:08.443,1.0,2542,2016-12-30T20:39:02.107,2016-12-28T17:15:50.517,,8.0,,4435.0,,1,0,<ai-design>,Concept of AI program,169.0,66.23,10.72,8.98,0.0,0.0,2.0,What are the basic layers on an Artificially Intelligent program and what skills and concept are required to work on this field Total newbie interested in AI
,,"<p>Your intuition is good. Because ""nearby"" is only defined with ""connected"", there could only be 1 station between them. However, it says that the stations are ""not_too_far"" if at most one station is between them. What about if no stations are between them? If 2 stations are ""connected"" they should be ""not_too_far"" as well. </p>

<p>So it should be:</p>

<pre><code>not_too_far(X,Y) :- connected(X,Y) ; nearby(X,Y).
</code></pre>

<p>Where ; denotes OR. </p>
",,2,2016-12-25T15:05:09.360,,2543,2016-12-25T15:05:09.360,,,,,4398.0,2535.0,2,1,,,,78.04,9.66,7.77,50.0,0.0,24.0,Your intuition is good Because nearby is only defined with connected there could only be 1 station between them However it says that the stations are nottoofar if at most one station is between them What about if no stations are between them If 2 stations are connected they should be nottoofar as well So it should be Where denotes OR
,,"<p>The basic layers of AI aren't that interesting at the highest level. In the most abstract sense, AI is simply a function f(x) which is given input x and returns an output. This isn't that exciting, so let's break it down a bit more.</p>

<p>AI can be broken by 2 different aspects. 
AI can be Online or Offline. 
AI can be Supervised or Unsupervised. </p>

<p>AI is Offline if it learns the function f before being released into the real world at which point it doesn't update f. </p>

<p>AI is Online if it is put in the real world immediately and must learn f on-the-fly during operation. </p>

<p>AI is Supervised if it is given the correct answer after guessing. This is used to update f.</p>

<p>AI is Unsupervised if the correct answer is not given. This could mean that some indication of how well the AI did is returned (such as in reinforcement learning) or nothing at all. </p>

<p>Choosing 1 of each aspect gives 1 of 4 categories, such as Unsupervised Online learning. AlphaGo which beat on if the worlds best Go players is an example of an Unsupervised Online learning system. </p>

<p>All of these types of AI require representing the input x and choosing an output. Both of which involve statistics, math, and some programming experience. Anyone wanting to work in AI should have these skills and be able to think critically about them (mostly to diagnose the bugs that are bound to happen).</p>
",,0,2016-12-25T15:48:35.640,,2544,2016-12-25T15:48:35.640,,,,,4398.0,2542.0,2,0,,,,73.78,8.29,8.62,0.0,0.0,34.0,The basic layers of AI arent that interesting at the highest level In the most abstract sense AI is simply a function fx which is given input x and returns an output This isnt that exciting so lets break it down a bit more AI can be broken by 2 different aspects AI can be Online or Offline AI can be Supervised or Unsupervised AI is Offline if it learns the function f before being released into the real world at which point it doesnt update f AI is Online if it is put in the real world immediately and must learn f onthefly during operation AI is Supervised if it is given the correct answer after guessing This is used to update f AI is Unsupervised if the correct answer is not given This could mean that some indication of how well the AI did is returned such as in reinforcement learning or nothing at all Choosing 1 of each aspect gives 1 of 4 categories such as Unsupervised Online learning AlphaGo which beat on if the worlds best Go players is an example of an Unsupervised Online learning system All of these types of AI require representing the input x and choosing an output Both of which involve statistics math and some programming experience Anyone wanting to work in AI should have these skills and be able to think critically about them mostly to diagnose the bugs that are bound to happen
,1.0,"<p>The following text is from Hal Daumé III's <a href=""http://ciml.info/dl/v0_9/ciml-v0_9-ch03.pdf"" rel=""nofollow noreferrer"">""<em>A Course in Machine Learning</em>""</a> online text book (Page-41).</p>

<p><a href=""https://i.stack.imgur.com/SCfew.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SCfew.png"" alt=""enter image description here""></a></p>

<p>I understand that, <code>D</code> = size of the input vector <code>x</code>.</p>

<p>(1) What is <code>y</code>? Why is it introduced in the algorithm? How/where/when is the initial value of <code>y</code> given? </p>

<p>(2) What is the rationale of testing <code>ya&lt;=0</code> for updating weights?</p>
",,0,2016-12-26T04:24:08.937,,2545,2016-12-26T06:32:39.887,2016-12-26T05:43:51.563,,3642.0,,3642.0,,1,1,<neural-networks>,Perceptron algorithm,79.0,79.56,8.89,8.93,16.0,0.0,20.0,The following text is from Hal Daumé IIIs A Course in Machine Learning online text book Page41 I understand that size of the input vector 1 What is Why is it introduced in the algorithm Howwherewhen is the initial value of given 2 What is the rationale of testing for updating weights
,,"<p>Y is the desired output of the perceptron (often referred to as target) , for the given set of input vectors.</p>

<p><strong>Rationale behind Y.a&lt;=0</strong> :</p>

<p>Prerequisite knowledge :</p>

<ul>
<li><p><strong>A</strong>=<strong>A</strong>-<strong>B</strong> : Moves vector <strong>A</strong> away from direction of vector <strong>B</strong></p></li>
<li><p><strong>A</strong>=<strong>A</strong>+<strong>B</strong> : Moves <strong>A</strong> in the direction of <strong>B</strong></p></li>
<li><p><strong>A</strong> (.) <strong>B</strong> >0 ; <strong>A</strong> vector is directed acutely (&lt;90 deg.) towards <strong>B</strong> vector</p></li>
<li><p><strong>A</strong> (.) <strong>B</strong> &lt;0 ; <strong>A</strong> vector is directed away from (>90 deg.) from <strong>B</strong> vector [(.) denotes dot (scalar) product and <strong>bold</strong> letters indicate vectors]</p></li>
<li><p><strong>W</strong> is augmented vector (includes threshold as another weight along with normal input weights)</p></li>
<li><p><strong>X</strong> is augmented input (includes -1 as extra input (corresponding to threshold) along with other normal inputs</p></li>
<li><p>a(activation) = <strong>W</strong> (.) <strong>X</strong></p></li>
<li><p>a >=0 ; Perceptron output 1</p></li>
<li><p>a&lt;0 ; Perceptron output -1 (<strong>Not zero</strong> as implicit in the given algorithm I think)</p></li>
</ul>

<p>Now Rationale :</p>

<p>(Y.a&lt;0) </p>

<p>This means 
Either of the following :</p>

<ul>
<li>Y=-1 and a>0 ; in this case the target output is -1 but as a>0 so the Perceptron outputs 1. So we must move the weight vector away from this set of input vector, so that angle between them increases and the dot product (a) becomes &lt; 0 so that we can get the target output.
Hence : <strong>W</strong>=<strong>W</strong>-<strong>X</strong></li>
</ul>

<p>Or ,<strong>W</strong>=<strong>W</strong> + (-1)<strong>*X</strong>
Or, <strong>W</strong>=<strong>W</strong>+Y<strong>X</strong></p>

<ul>
<li>Y=1 and a&lt;0</li>
</ul>

<p>This means the target output is 1 but as the activation is &lt;0 so Perceptron is outputting -1. So we must move the weight vector close to this set of input vector so that the activation can become >0 (angle decreases) and the Perceptron can output the desired output.
So :
<strong>W</strong> = <strong>W</strong>+<strong>X</strong>
Or <strong>W</strong>=<strong>W</strong> + Y<strong>X</strong></p>

<p>Again , Y.a=0 is a boundary case.
By now I think you can understand the rationale behind Y.a=0. If any doubt , comment to this answer , I will explain it.</p>

<p>Sorry for so much long answer though. :) :)</p>
",,4,2016-12-26T06:32:39.887,,2546,2016-12-26T06:32:39.887,,,,,4424.0,2545.0,2,2,,,,80.21,9.39,7.97,0.0,0.0,128.0,Y is the desired output of the perceptron often referred to as target for the given set of input vectors Rationale behind Yalt0 Prerequisite knowledge AAB Moves vector A away from direction of vector B AAB Moves A in the direction of B A B 0 A vector is directed acutely lt90 deg towards B vector A B lt0 A vector is directed away from 90 deg from B vector denotes dot scalar product and bold letters indicate vectors W is augmented vector includes threshold as another weight along with normal input weights X is augmented input includes 1 as extra input corresponding to threshold along with other normal inputs aactivation W X a 0 Perceptron output 1 alt0 Perceptron output 1 Not zero as implicit in the given algorithm I think Now Rationale Yalt0 This means Either of the following Y1 and a0 in this case the target output is 1 but as a0 so the Perceptron outputs 1 So we must move the weight vector away from this set of input vector so that angle between them increases and the dot product a becomes lt 0 so that we can get the target output Hence WWX Or WW 1X Or WWYX Y1 and alt0 This means the target output is 1 but as the activation is lt0 so Perceptron is outputting 1 So we must move the weight vector close to this set of input vector so that the activation can become 0 angle decreases and the Perceptron can output the desired output So W WX Or WW YX Again Ya0 is a boundary case By now I think you can understand the rationale behind Ya0 If any doubt comment to this answer I will explain it Sorry for so much long answer though
,1.0,"<p>Recently Mark got some attention from the media by stating that he had created Jarvis. Not that I'm against him or anything, but this Jarvis seems to have been done a hundred times before. He's done something which most developers would classify as a home automation system. To me it's more like he did it for the attention. I was kind of taken back by the amount of media attention he got. If you've heard of Jeremy Blum, maybe you may understand what I'm trying to imply here. </p>

<p>I'm just curious as to why he got so much attention. Is there anything technically novel about his system that sets it so much apart from previous ones?</p>
",,0,2016-12-26T10:59:27.333,1.0,2547,2016-12-27T08:47:12.330,2016-12-26T20:14:54.763,,75.0,,4444.0,,1,3,<ai-design><intelligent-agent>,Is there anything novel about Zuckerberg's Jarvis?,217.0,73.68,7.94,8.71,0.0,0.0,16.0,Recently Mark got some attention from the media by stating that he had created Jarvis Not that Im against him or anything but this Jarvis seems to have been done a hundred times before Hes done something which most developers would classify as a home automation system To me its more like he did it for the attention I was kind of taken back by the amount of media attention he got If youve heard of Jeremy Blum maybe you may understand what Im trying to imply here Im just curious as to why he got so much attention Is there anything technically novel about his system that sets it so much apart from previous ones
,1.0,"<p>What is the basic difference between a Perceptron and a Naive Bayes classifier?</p>

<pre><code>         Perceptron          |          Naive Bayes
------------------------------------------------------------
(1) Perceptron uses Neural-  |(1) Naive Bayes uses probabi-
    network for learning and |    listic theory for learning
    classification.          |    and classification
------------------------------------------------------------
(2) Perceptron reads one sa- |(2) Naive Bayes needs to read-
    mple at a time to update |    the entire training data 
    its knowledge about the  |    before updating its knowl-
    training data. This is   |    edge about the training 
    called online learning.  |    data.
-------------------------------------------------------------
(3) In case of Perceptrons,  |(3) Training and test data are  
    training-data also serve |    different.
    the purpose of test data |
</code></pre>

<p>what more differences do they have?</p>
",,2,2016-12-26T11:24:09.163,2.0,2548,2017-03-02T18:47:52.337,2016-12-26T12:32:26.717,,3642.0,,3642.0,,1,5,<neural-networks>,Perceptron vs Naive Bayes,151.0,61.83,10.58,9.09,830.0,0.0,2.0,What is the basic difference between a Perceptron and a Naive Bayes classifier what more differences do they have
,,,,0,2016-12-26T14:24:02.320,,2550,2016-12-26T14:24:02.320,2016-12-26T14:24:02.320,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,,,0,2016-12-26T14:24:02.320,,2551,2016-12-26T14:24:02.320,2016-12-26T14:24:02.320,,-1.0,,-1.0,,4,0,,,,,,,,,,
,0.0,"<p>In the ""Perceptrons"" introduction by Minksy and Papert, they give a proof that the predicate of whether set of points is connected is not conjunctively local of any order k. They do this by stating that, because of the limited number of points k, there must be some middle square that would not contain one of these points. Therefore, the connected and non-connected figures would return the same result.</p>

<p>While I agree with the assertion that if there is a middle square which does not contain any of the k points, then we could not distinguish between the connected and non-connected figures, I don't understand how we get to that premise.</p>

<p>My question is: How can we say that there must be a middle square which does not contain any of the k points? A point is not a square, so couldn't we have many more points than squares? In fact, I don't really understand how we can even call these k+1-wide figures, given that k refers to a number of points, not a distance measurement. How can we say something is ""k points wide"", when points can be infinitely subdivided and have no actual length? </p>

<p><a href=""https://i.stack.imgur.com/Nr5Nf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nr5Nf.png"" alt=""Proof""></a></p>
",,0,2016-12-26T22:42:37.270,,2552,2016-12-26T22:42:37.270,,,,,2897.0,,1,0,<neural-networks><math>,"Understanding the proof in Minsky's ""Perceptrons"" that the ""connected"" predicate is not local of any order k",29.0,63.53,9.52,8.24,0.0,0.0,31.0,In the Perceptrons introduction by Minksy and Papert they give a proof that the predicate of whether set of points is connected is not conjunctively local of any order k They do this by stating that because of the limited number of points k there must be some middle square that would not contain one of these points Therefore the connected and nonconnected figures would return the same result While I agree with the assertion that if there is a middle square which does not contain any of the k points then we could not distinguish between the connected and nonconnected figures I dont understand how we get to that premise My question is How can we say that there must be a middle square which does not contain any of the k points A point is not a square so couldnt we have many more points than squares In fact I dont really understand how we can even call these k1wide figures given that k refers to a number of points not a distance measurement How can we say something is k points wide when points can be infinitely subdivided and have no actual length
,,"<p>No, there is nothing novel about this system. The main hurdle he had to pass was problems that you will face when your system has a lot of integration points across various APIs provided by different vendors with messy and often outdated documentation. </p>

<p>As far as attention is concerned we live in a world where so called celebrities get attention for anything that they do. Remember that media have only one goal - More and more eyeballs aka more money and nothing else.</p>
",,2,2016-12-27T08:47:12.330,,2554,2016-12-27T08:47:12.330,,,,,1462.0,2547.0,2,4,,,,59.13,10.15,10.24,0.0,0.0,6.0,No there is nothing novel about this system The main hurdle he had to pass was problems that you will face when your system has a lot of integration points across various APIs provided by different vendors with messy and often outdated documentation As far as attention is concerned we live in a world where so called celebrities get attention for anything that they do Remember that media have only one goal More and more eyeballs aka more money and nothing else
2556.0,1.0,"<p>With tools like open AI will we be able to teach an AI to build its own decks? build a deck from a limited pool? or draft? evaluate the power level of a card? </p>
",,3,2016-12-27T14:47:59.430,,2555,2016-12-28T00:21:53.347,,,,,4466.0,,1,2,<gaming><training>,How close are we to having an AI that can play Magic: The Gathering objectively well?,88.0,93.85,2.88,6.98,0.0,0.0,4.0,With tools like open AI will we be able to teach an AI to build its own decks build a deck from a limited pool or draft evaluate the power level of a card
,,"<p>This is a very specific task, with clearly defined parameters, so it would already theoretically be within the scope of current AI technology to do this.</p>

<p>The AI would need to learn how to make decisions, and the best way to do this is the approach taken to <a href=""https://www.scientificamerican.com/article/how-the-computer-beat-the-go-master/"" rel=""nofollow noreferrer"">teaching an AI to play Go</a> - seeing thousands of example games by experts, and playing itself thousands of times.</p>

<p>The AI won't necessarily ""understand"" what a card represents, the way a human would, but it can learn to make the same kinds of decisions as the human would.</p>

<p>The difficulties would be purely practical - Go is very easy to represent digitally, because the options for action are limited to placing a stone on one of the intersections, and there is only one opponent. Magic is more complex, so the developers would need to spend sufficient resources to be quite sure they had captured all the relevant variables in the digital representation. Then, of course, they would need to encode thousands of games of Magic (or deck-building processes) into that digital format, so the AI could study them.</p>
",,0,2016-12-28T00:21:53.347,,2556,2016-12-28T00:21:53.347,,,,,3601.0,2555.0,2,3,,,,48.67,10.86,9.86,0.0,0.0,26.0,This is a very specific task with clearly defined parameters so it would already theoretically be within the scope of current AI technology to do this The AI would need to learn how to make decisions and the best way to do this is the approach taken to teaching an AI to play Go seeing thousands of example games by experts and playing itself thousands of times The AI wont necessarily understand what a card represents the way a human would but it can learn to make the same kinds of decisions as the human would The difficulties would be purely practical Go is very easy to represent digitally because the options for action are limited to placing a stone on one of the intersections and there is only one opponent Magic is more complex so the developers would need to spend sufficient resources to be quite sure they had captured all the relevant variables in the digital representation Then of course they would need to encode thousands of games of Magic or deckbuilding processes into that digital format so the AI could study them
,1.0,"<p>Given that one website has a particular style and another has another style, could a style transfer be done such that the style of one website was transferred to the other website?</p>

<p>Or in a more simple case, consider just part of a website, a box.</p>
",,0,2016-12-28T14:07:12.900,,2558,2016-12-28T20:31:14.630,,,,,4488.0,,1,0,<ai-design>,Could style transfer be used to transfer the style of a website from one to another?,31.0,56.59,8.36,9.24,0.0,0.0,5.0,Given that one website has a particular style and another has another style could a style transfer be done such that the style of one website was transferred to the other website Or in a more simple case consider just part of a website a box
,,"<p>The correct way for website styling to be encapsulated and centralized is through the use of one or more CSS style sheets.  For instance, the old  tag is frowned upon and using a text-align:center directive a proper class or ID based CSS selector is considered the proper way.  In such a case, the copying of a style is simply done by the copying of the appropriate style sheet file.</p>

<p>The place where AI and/or machine learning tools may be best used is in the conversion of non-CSS styling to CSS styling.  Once they are in that form, the copying and layering of styling becomes trivial.</p>

<p>To convert a legacy web site to proper CSS style sheet use, the input would be a set of all documents likely to be involved in the styling of a specific page (i.e. its style dependencies).  The output would be a set of corresponding documents with the styling moved into classes and IDs that remove styling redundancy (simplify and centralize the style).</p>

<p>The transformation of the document set could be best accomplished with current technique through transformations, much like code refactoring in typical IDE refactoring algorithms in that the output document set must be functionally the same as the input document set.  Furthermore, the appearance and responsive (size sensitive) aspects must be preserved in addition to functionality.</p>

<p>The greatest usability could be obtained by adding some automation to these refactoring transformations, as in Maxima simplification functions like trigsimp.  This is a typical search of options, perhaps with predictive branching, as in compiler optimizations.</p>

<p>The transformations, to guarantee preservation of function and appearance must essentially be elementary equivalent transformations, but the selection of transformation would probably need to be more advanced than basic antecedent and consequence productions.</p>

<p>Heuristically meta rules, fuzzy logic, neural nets, Bayesian inference, or other tools could be compared in terms of effectiveness in the realm of prioritizing transformations and in terms of scoring results in the search leaves.  Here's where machine learning would probably be necessary to have a truly useful tool.</p>

<p>Training could be accomplished through use of an array of input document sets and the careful scoring of transformations and/or outputs of trials could be performed with a software engineer with appropriate expertise in AI and/or machine learning, in conjunction with a CSS expert or a focus group containing two or more of them.</p>

<p>Once this process and the resulting rule set works, it would then be possible to simplify the CSS further by finding styling redundancy across pages and doing transformations that reduce an entire site to a minimal set of style sheets and style specifications within them.</p>

<p>Combining such tools with a simple crawler like wget could permit the copying of a desirable web site style as a starting point for another web site's styling, drastically cutting down the time to create a properly CSS styled result.</p>
",,0,2016-12-28T20:31:14.630,,2559,2016-12-28T20:31:14.630,,,,,4302.0,2558.0,2,1,,,,44.48,13.47,10.03,0.0,0.0,51.0,The correct way for website styling to be encapsulated and centralized is through the use of one or more CSS style sheets For instance the old tag is frowned upon and using a textaligncenter directive a proper class or ID based CSS selector is considered the proper way In such a case the copying of a style is simply done by the copying of the appropriate style sheet file The place where AI andor machine learning tools may be best used is in the conversion of nonCSS styling to CSS styling Once they are in that form the copying and layering of styling becomes trivial To convert a legacy web site to proper CSS style sheet use the input would be a set of all documents likely to be involved in the styling of a specific page ie its style dependencies The output would be a set of corresponding documents with the styling moved into classes and IDs that remove styling redundancy simplify and centralize the style The transformation of the document set could be best accomplished with current technique through transformations much like code refactoring in typical IDE refactoring algorithms in that the output document set must be functionally the same as the input document set Furthermore the appearance and responsive size sensitive aspects must be preserved in addition to functionality The greatest usability could be obtained by adding some automation to these refactoring transformations as in Maxima simplification functions like trigsimp This is a typical search of options perhaps with predictive branching as in compiler optimizations The transformations to guarantee preservation of function and appearance must essentially be elementary equivalent transformations but the selection of transformation would probably need to be more advanced than basic antecedent and consequence productions Heuristically meta rules fuzzy logic neural nets Bayesian inference or other tools could be compared in terms of effectiveness in the realm of prioritizing transformations and in terms of scoring results in the search leaves Heres where machine learning would probably be necessary to have a truly useful tool Training could be accomplished through use of an array of input document sets and the careful scoring of transformations andor outputs of trials could be performed with a software engineer with appropriate expertise in AI andor machine learning in conjunction with a CSS expert or a focus group containing two or more of them Once this process and the resulting rule set works it would then be possible to simplify the CSS further by finding styling redundancy across pages and doing transformations that reduce an entire site to a minimal set of style sheets and style specifications within them Combining such tools with a simple crawler like wget could permit the copying of a desirable web site style as a starting point for another web sites styling drastically cutting down the time to create a properly CSS styled result
,,"<p><strong>Semantics Matters</strong></p>

<p>The answer depends on how you define intelligence.  If you define intelligence as the ability to adapt, a number of things could be considered intelligent that don't normally fit under the classic AI umbrella.</p>

<ul>
<li>Nonlinear least-squares Marquardt-Levenberg curve fitting algorithm with a finite set of models and automated model trials, outcome analysis, and smart decisioning</li>
<li>Check reader software recently deployed in bank branches and offices</li>
<li>The combination of medical providers, patients, and carriers and the modification of treatment through financial instrumentation to improve outcomes</li>
</ul>

<p>If you define intelligence as mimicking the abilities of the human mind that facilitate adaptation across a wide array of arbitrary domains that had not been previously experienced or studied, then no such system is yet available to the public.  Nothing even close.</p>

<p>If such a system exists in secrecy, someone would have to violate their nondisclosure agreement or security clearance to tell us about it here.</p>

<p>The definition of intelligence is central to answering.  For instance, some reasonable definitions of intelligence might lead an unbiased judge to rate ants above humans.  Ants had been building in hexagons for millennia before humans blundered into the habit of building in rectangles.  Rectangles require over 70% more building material to build vertical structure per square foot of floor space than a packed hexagonal structure.</p>

<p><strong>Basic Artificial Intelligence System Requirements</strong></p>

<p>Guessing that by, ""Basic AI,"" you mean some naive machine learning, there are a few basic components.  (The term Naive in this context means that the AI does not understand the domain or the meanings of symbols or signals it is processing in the way a human who had studied or worked within the field might understand them.)</p>

<ul>
<li><p>SENSING &mdash; The machine (computer) must receive information, generally as a time series.  In human beings, these are the senses.  In a mail sorter, it may be a camera.  This is beyond just the concept of input in information technology.  It is more analogous to an input signal in a PID controller.  In an automated high speed trading machine, this would be the high speed version of a ticker tape.</p></li>
<li><p>CONTROL &mdash; The machine must manipulate externals in a way that exploits the received information.  In a basketball player, this is the motor coordination, facial affect, verbal signals to teammates, and perhaps some verbal bait for opposing players.  In a mail sorter, this would be the motor control of mail direction.</p></li>
<li><p>MEMORY &mdash; The machine must have storage to audit input time series (perhaps from some transducer in the real world or some data store upon which some intelligent analysis or transformation must be done).  In more advanced systems, the machine may wish to analyze its own performance and make adjustments that converge on some optimal metric value (perhaps a historical maxima or minima) or some range of acceptability.</p></li>
<li><p>FEEDBACK &mdash; The machine must interpret some feedback signalling or use a predetermined scoring mechanism.  Learning cannot occur in an information vacuum, so some definition of better or worse must be established.  The feedback may be entangled within the SENSING channels or may arrive through a completely separate channel. In biological systems, these are often pre-wired as threat detection, pain, and pleasure.  The cerebral cortex uses concepts of goals and progress.  In some ways, child rearing and social strata exists to teach the boundaries of what constitutes an acceptable goal and acceptable methods for making progress toward it.</p></li>
<li><p>MODELLING &mdash; Whether implicitly or explicitly, some model must be developed and exploited.  Some would say that the existence of a model upon which the predictive capabilities of it can be applied to decision making to achieve some goal or weighted collection of goals is intelligence.  Others would say that the development of the model is intelligence and the use of it is merely control mechanics.</p></li>
</ul>

<p><strong>Approaches to Simulating Human Thinking</strong></p>

<p>Cognition is not the only form of model making, but the creation of cognitions and their application to decision making.  The concept of intelligence may have been furthered along a realistic path by Roger Schank, who proposed that the storage and indexing of stories was a primary characteristics of what humans recognize as intelligent conversation.</p>

<p>Minsky and others took a direction that was more connected with logical inference work that began with logic formalization (originally George Boole) and Church's lambda calculus.</p>

<p><strong>Some Common Directions in Design</strong></p>

<p>The genetic algorithm influenced convergent technology and neuro-biology influenced neural net development.  Pattern matching is another limb off the larger set of technical approaches under the umbrella of classic Artificial Intelligence.</p>

<p>These are naive systems.  Like a neuron, the components have no idea of the meaning of what they are processing.  They are naive components.  An intelligent observer could not ascertain the real time meanings of signals and symbols between these naive components without extensive, perhaps life-long research.</p>

<p>Naive Bayesian methods are probabilistic in nature.  They exploit Bayes' Theorem, and have been found to produce excellent results in certain important domains.  Some studies have shown that naivety is actually a learning accelerant, which is interesting from an AI theory point of view.</p>

<p>Then there is fuzzy (weighted) logic, which was an attempt to merge neural nets with production (rule based) systems.  Attempts to use this technology in transportation routing and scheduling has met much success.</p>

<p>There are as many devices and architectures that attempt to effectively integrate or interconnect these various approaches as there are AI projects.</p>

<p><strong>Modelling Environment and Goal Conditions</strong></p>

<p>All of these systems, in some explicit or implicit way, model the external environment and the desired result of system behavior and attempt to converge (in real time) on that result.  Some sense-control functionality, which may change and adapt to the external environment is employed in the CONTROL component(s).</p>

<p>This is just another way that the systems have an adaptive behavior.  Without necessarily knowing why, the system will manipulate what it can and continue to monitor the state of the environment to continually reach for the system's acceptance criteria.</p>

<p>The Basic AI system must to more than learn.  It must also judge its own functionality and therefore must have a layer of feedback and control that simulates the perception of optimality.  This higher level control must be integrated into the system at its inception for it to behave intelligently in the sense you probably mean.</p>

<p><strong>Understanding Limitation and System Complexity</strong></p>

<p>The more sophisticated and adaptive the modelling becomes, the more the cognitions, rules, stories, time series coefficients, weights, or whatever forms knowledge (not information) is stored, the more one can say that there is some form of understanding or comprehension.</p>

<p>One conjecture is that it is the recursion in layers of these capabilities that permits certain types of comprehension and awareness.  Other conjectures focus more on alertness and attention as keys to higher intelligence.</p>

<p>But these much more mature capabilities are beyond mere adaptation based on past knowledge or information and are therefore beyond what you probably meant by Basic AI.</p>
",,0,2016-12-28T22:01:30.200,,2560,2017-01-09T01:00:30.517,2017-01-09T01:00:30.517,,4302.0,,4302.0,2366.0,2,3,,,,42.41,14.27,10.22,0.0,0.0,145.0,Semantics Matters The answer depends on how you define intelligence If you define intelligence as the ability to adapt a number of things could be considered intelligent that dont normally fit under the classic AI umbrella Nonlinear leastsquares MarquardtLevenberg curve fitting algorithm with a finite set of models and automated model trials outcome analysis and smart decisioning Check reader software recently deployed in bank branches and offices The combination of medical providers patients and carriers and the modification of treatment through financial instrumentation to improve outcomes If you define intelligence as mimicking the abilities of the human mind that facilitate adaptation across a wide array of arbitrary domains that had not been previously experienced or studied then no such system is yet available to the public Nothing even close If such a system exists in secrecy someone would have to violate their nondisclosure agreement or security clearance to tell us about it here The definition of intelligence is central to answering For instance some reasonable definitions of intelligence might lead an unbiased judge to rate ants above humans Ants had been building in hexagons for millennia before humans blundered into the habit of building in rectangles Rectangles require over 70 more building material to build vertical structure per square foot of floor space than a packed hexagonal structure Basic Artificial Intelligence System Requirements Guessing that by Basic AI you mean some naive machine learning there are a few basic components The term Naive in this context means that the AI does not understand the domain or the meanings of symbols or signals it is processing in the way a human who had studied or worked within the field might understand them SENSING mdash The machine computer must receive information generally as a time series In human beings these are the senses In a mail sorter it may be a camera This is beyond just the concept of input in information technology It is more analogous to an input signal in a PID controller In an automated high speed trading machine this would be the high speed version of a ticker tape CONTROL mdash The machine must manipulate externals in a way that exploits the received information In a basketball player this is the motor coordination facial affect verbal signals to teammates and perhaps some verbal bait for opposing players In a mail sorter this would be the motor control of mail direction MEMORY mdash The machine must have storage to audit input time series perhaps from some transducer in the real world or some data store upon which some intelligent analysis or transformation must be done In more advanced systems the machine may wish to analyze its own performance and make adjustments that converge on some optimal metric value perhaps a historical maxima or minima or some range of acceptability FEEDBACK mdash The machine must interpret some feedback signalling or use a predetermined scoring mechanism Learning cannot occur in an information vacuum so some definition of better or worse must be established The feedback may be entangled within the SENSING channels or may arrive through a completely separate channel In biological systems these are often prewired as threat detection pain and pleasure The cerebral cortex uses concepts of goals and progress In some ways child rearing and social strata exists to teach the boundaries of what constitutes an acceptable goal and acceptable methods for making progress toward it MODELLING mdash Whether implicitly or explicitly some model must be developed and exploited Some would say that the existence of a model upon which the predictive capabilities of it can be applied to decision making to achieve some goal or weighted collection of goals is intelligence Others would say that the development of the model is intelligence and the use of it is merely control mechanics Approaches to Simulating Human Thinking Cognition is not the only form of model making but the creation of cognitions and their application to decision making The concept of intelligence may have been furthered along a realistic path by Roger Schank who proposed that the storage and indexing of stories was a primary characteristics of what humans recognize as intelligent conversation Minsky and others took a direction that was more connected with logical inference work that began with logic formalization originally George Boole and Churchs lambda calculus Some Common Directions in Design The genetic algorithm influenced convergent technology and neurobiology influenced neural net development Pattern matching is another limb off the larger set of technical approaches under the umbrella of classic Artificial Intelligence These are naive systems Like a neuron the components have no idea of the meaning of what they are processing They are naive components An intelligent observer could not ascertain the real time meanings of signals and symbols between these naive components without extensive perhaps lifelong research Naive Bayesian methods are probabilistic in nature They exploit Bayes Theorem and have been found to produce excellent results in certain important domains Some studies have shown that naivety is actually a learning accelerant which is interesting from an AI theory point of view Then there is fuzzy weighted logic which was an attempt to merge neural nets with production rule based systems Attempts to use this technology in transportation routing and scheduling has met much success There are as many devices and architectures that attempt to effectively integrate or interconnect these various approaches as there are AI projects Modelling Environment and Goal Conditions All of these systems in some explicit or implicit way model the external environment and the desired result of system behavior and attempt to converge in real time on that result Some sensecontrol functionality which may change and adapt to the external environment is employed in the CONTROL components This is just another way that the systems have an adaptive behavior Without necessarily knowing why the system will manipulate what it can and continue to monitor the state of the environment to continually reach for the systems acceptance criteria The Basic AI system must to more than learn It must also judge its own functionality and therefore must have a layer of feedback and control that simulates the perception of optimality This higher level control must be integrated into the system at its inception for it to behave intelligently in the sense you probably mean Understanding Limitation and System Complexity The more sophisticated and adaptive the modelling becomes the more the cognitions rules stories time series coefficients weights or whatever forms knowledge not information is stored the more one can say that there is some form of understanding or comprehension One conjecture is that it is the recursion in layers of these capabilities that permits certain types of comprehension and awareness Other conjectures focus more on alertness and attention as keys to higher intelligence But these much more mature capabilities are beyond mere adaptation based on past knowledge or information and are therefore beyond what you probably meant by Basic AI
,0.0,"<p>I have to say if in my detect faces program develop with adaboost there is any overfitting. It says that I have to justify what I say with a graphic where the <strong>training</strong> and <strong>test error</strong> were compared.</p>

<p>I don't know what should happen in the graphic said before in case that there is any overfitting.</p>

<p>Thanks.</p>
",,2,2016-12-29T19:51:47.583,,2561,2016-12-29T19:51:47.583,,,,,3011.0,,1,0,<overfitting>,How to detect overfitting in adaboost,13.0,69.11,7.83,8.46,0.0,0.0,5.0,I have to say if in my detect faces program develop with adaboost there is any overfitting It says that I have to justify what I say with a graphic where the training and test error were compared I dont know what should happen in the graphic said before in case that there is any overfitting Thanks
,2.0,"<ol>
<li><blockquote>
<pre><code>I wonder why it is tried to prove that, under no valid or
    not-ununcheckable conditions, it is said that it is absolutely
    impossible for an artificial intelligence system or collection of
    relative exclusive modules to involuntarily acquire more
    sophisticated capabilities in terms of generic cleverness of states
    of inclusive independency than its own developer. Thanks.
</code></pre>
</blockquote></li>
</ol>

<p><a href=""http://ai.stackexchange.com"">http://ai.stackexchange.com</a></p>
",,1,2016-12-29T23:51:07.190,,2562,2016-12-30T14:17:24.373,,,,,4519.0,,1,-2,<ai-design><intelligent-agent>,What is the proof that no AI can become smarter than its creator?,83.0,-251.02,111.2,19.48,390.0,0.0,5.0,httpaistackexchangecom
,0.0,"<p>I am attempting to create a fully decoupled feed-forward neural network by using decoupled neural interfaces as explained in the paper (<a href=""https://arxiv.org/abs/1608.05343"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1608.05343</a>). As in the paper, the DNI is able to produce a synthetic error gradient that reflects the error with respect the the output:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20h_%7Bi%7D%7D"" alt=""\frac{\partial L}{\partial h_{i}}"">  </p>

<p>I can then use this to update the current layer's parameters by multiplying by the parameters to get the loss with respect to the parameters:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Ctheta%20%7D%20%3D%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20h_%7Bi%7D%7D%20*%5Cfrac%7B%5Cpartial%20h_%7Bi%7D%7D%7B%5Cpartial%20%5Ctheta%20%7D"" alt=""\frac{\partial L}{\partial \theta } = \frac{\partial L}{\partial h_{i}} *\frac{\partial h_{i}}{\partial \theta }""></p>

<p>In the paper, the layer's model is then updated based on the next layer sending the true error backwards. </p>

<p>My question is, given that I am able to calculate the error with respect to the current output, how do I use this to calculate the Loss with respect to the previous layer's output?</p>
",,2,2016-12-30T01:42:39.703,,2563,2016-12-30T01:42:39.703,,,,,4521.0,,1,0,<neural-networks><backpropagation><artificial-neuron>,Backpropagation in Decoupled Neural Interfaces,19.0,38.32,11.97,10.29,0.0,0.0,22.0,I am attempting to create a fully decoupled feedforward neural network by using decoupled neural interfaces as explained in the paper httpsarxivorgabs160805343 As in the paper the DNI is able to produce a synthetic error gradient that reflects the error with respect the the output I can then use this to update the current layers parameters by multiplying by the parameters to get the loss with respect to the parameters In the paper the layers model is then updated based on the next layer sending the true error backwards My question is given that I am able to calculate the error with respect to the current output how do I use this to calculate the Loss with respect to the previous layers output
,1.0,"<p>Fundamentally, a game-playing AI must solve the problem of choosing the best action from a set of possible actions.</p>

<p>Most existing game AI's, such as Alphago, do this by using an <strong>evaluation function</strong>, which maps game states to real numbers. The real number typically can be interpreted as a monotonic function of a winning probability estimate. The best action is the one whose resultant state yields the highest evaluation.</p>

<p>Clearly, this approach can work well. But it violates one of <a href=""https://books.google.com/books?id=N_-5VRWai84C&amp;pg=PA477&amp;lpg=PA477&amp;dq=%22When%20solving%20a%20problem%20of%20interest,%20do%20not%20solve%20a%20more%20general%20problem%20as%20an%20intermediate%20step.%20Try%20to%20get%20the%20answer%20that%20you%20really%20need%20but%20not%20a%20more%20general%20one.%22&amp;source=bl&amp;ots=ReHvsikLSY&amp;sig=WkN0ETtVyEXlgkWWQoiY4b-qWxE&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwj6m6z-g5_OAhWGpB4KHf54DYcQ6AEIKTAC#v=onepage&amp;q=%22When%20solving%20a%20problem%20of%20interest%2C%20do%20not%20solve%20a%20more%20general%20problem%20as%20an%20intermediate%20step.%20Try%20to%20get%20the%20answer%20that%20you%20really%20need%20but%20not%20a%20more%20general%20one.%22&amp;f=false"" rel=""nofollow noreferrer"">Vladimir Vapnik's imperatives</a>: ""<em>When solving a problem of interest, do not solve a more general problem as an intermediate step.</em>"" In fact, he specifically states as an illustration of this imperative,</p>

<blockquote>
  <p>Do not estimate predictive values if your goal is to act well. (<em>A good strategy of action does not necessarily rely on good predictive ability.</em>)</p>
</blockquote>

<p>Indeed, human chess and go experts appear to heed his advice, as they are able to act well without using evaluation functions.</p>

<p>My question is this: <strong>has there has been any recent research aiming to solve games by learning to compare decisions directly, without an intermediate evaluation function</strong>? </p>

<p>To use Alphago as an example, this might mean training a neural network to take <strong>two</strong> (similar) board states as input and output a choice of which one is better (a classification problem), as opposed to a neural network that takes <strong>one</strong> board state as input and outputs a winning probability (a regression problem).</p>
",,0,2016-12-30T02:24:39.467,,2564,2017-01-01T17:10:25.750,,,,,4522.0,,1,4,<gaming>,Game AI without evaluation function,119.0,49.55,12.01,10.02,0.0,0.0,39.0,Fundamentally a gameplaying AI must solve the problem of choosing the best action from a set of possible actions Most existing game AIs such as Alphago do this by using an evaluation function which maps game states to real numbers The real number typically can be interpreted as a monotonic function of a winning probability estimate The best action is the one whose resultant state yields the highest evaluation Clearly this approach can work well But it violates one of Vladimir Vapniks imperatives When solving a problem of interest do not solve a more general problem as an intermediate step In fact he specifically states as an illustration of this imperative Do not estimate predictive values if your goal is to act well A good strategy of action does not necessarily rely on good predictive ability Indeed human chess and go experts appear to heed his advice as they are able to act well without using evaluation functions My question is this has there has been any recent research aiming to solve games by learning to compare decisions directly without an intermediate evaluation function To use Alphago as an example this might mean training a neural network to take two similar board states as input and output a choice of which one is better a classification problem as opposed to a neural network that takes one board state as input and outputs a winning probability a regression problem
,,"<p>The term ""artificially intelligent program"" doesn't really mean anything, because it can mean so many different things.   There are a lot of different techniques and approaches that fall under the overall rubric of ""artificial intelligence"".</p>

<p>That said, if you were going to try to classify the aspects of AI at a VERY broad (possibly too broad) level, you might say the following:</p>

<p>You have two broad approaches - symbolic (logic based) AI, and probabilistic AI (machine learning).  </p>

<p>Within the scope of symbolic AI would be most things you hear associated with the phrase GOFAI (Good Old Fashioned AI).  This includes things like logic programming in Prolog, expert systems, production rule systems (OPS5, etc.).  ""Cognitive architectures"" would probably also fall here, so things like ACT-R, SOAR, CLARION, etc.  And then you have automated planning systems, automated theorem provers, etc.  Skills needed to work here include a good handle on logic - probably first order logic, but possibly higher order logics as well.  Set theory, model theory, things of that nature come into play.  Lisp, Prolog, or OPS5 might be commonly used programming languages.    </p>

<p>In the area of ""probabilistic learning"" are things like neural networks, bayesian belief networks, and other ""machine learning"" algorithms.  Random forests, decision trees and what-not are usually lumped in here as well.  Skills needed to work in this area include some calculus (backpropagation in neural networks, for example, is heavily rooted in the chain rule from calculus), linear algebra, statistics and probability.  Bayesian statistics is especially useful.  Lots of programming languages are used to build these kinds of systems, but popular ones include Python, C++, R, Java and their ilk.</p>

<p>Then you have a few things that don't classify real neatly.  Genetic Algorithms, for example.  Those usually get put into machine learning, but GA's are really more of an optimization strategy.  So you might, for example, use GA's as part of a strategy for training an ANN.  </p>

<p>Net-net, ""Artificial Intelligence"" is a BIG field and you'll probably need to zoom in a little bit and ask more pointed questions to really get anywhere.  You might want to read a relatively comprehensive overview like ""Artificial Intelligence - A Modern Approach"" by Russell &amp; Norvig to get a good picture of what's going on.  </p>
",,0,2016-12-30T02:40:24.280,,2565,2016-12-30T20:39:02.107,2016-12-30T20:39:02.107,,33.0,,33.0,2542.0,2,3,,,,53.51,13.28,9.79,0.0,0.0,99.0,The term artificially intelligent program doesnt really mean anything because it can mean so many different things There are a lot of different techniques and approaches that fall under the overall rubric of artificial intelligence That said if you were going to try to classify the aspects of AI at a VERY broad possibly too broad level you might say the following You have two broad approaches symbolic logic based AI and probabilistic AI machine learning Within the scope of symbolic AI would be most things you hear associated with the phrase GOFAI Good Old Fashioned AI This includes things like logic programming in Prolog expert systems production rule systems OPS5 etc Cognitive architectures would probably also fall here so things like ACTR SOAR CLARION etc And then you have automated planning systems automated theorem provers etc Skills needed to work here include a good handle on logic probably first order logic but possibly higher order logics as well Set theory model theory things of that nature come into play Lisp Prolog or OPS5 might be commonly used programming languages In the area of probabilistic learning are things like neural networks bayesian belief networks and other machine learning algorithms Random forests decision trees and whatnot are usually lumped in here as well Skills needed to work in this area include some calculus backpropagation in neural networks for example is heavily rooted in the chain rule from calculus linear algebra statistics and probability Bayesian statistics is especially useful Lots of programming languages are used to build these kinds of systems but popular ones include Python C R Java and their ilk Then you have a few things that dont classify real neatly Genetic Algorithms for example Those usually get put into machine learning but GAs are really more of an optimization strategy So you might for example use GAs as part of a strategy for training an ANN Netnet Artificial Intelligence is a BIG field and youll probably need to zoom in a little bit and ask more pointed questions to really get anywhere You might want to read a relatively comprehensive overview like Artificial Intelligence A Modern Approach by Russell amp Norvig to get a good picture of whats going on
,,"<p>The loaded term in your question is ""generic cleverness."" There's no such thing. What is ""smart"" is only smart relative to a criterion. Provide a complete criterion and we can talk rationally about ""levels of sophistication."" Until then, there is no measure against which ""involuntarily acquired"" capabilities can be regarded as more or less ""smart.""</p>
",,0,2016-12-30T02:42:53.747,,2566,2016-12-30T02:42:53.747,,,,,1712.0,2562.0,2,0,,,,51.85,12.74,10.5,0.0,0.0,17.0,The loaded term in your question is generic cleverness Theres no such thing What is smart is only smart relative to a criterion Provide a complete criterion and we can talk rationally about levels of sophistication Until then there is no measure against which involuntarily acquired capabilities can be regarded as more or less smart
,,"<p>Both.</p>

<p>Ethical responsibility between humans is based on a sympathetic correspondence between humans. Between humans and robots, if one party lacks the desire or capability to sympathize with the other party, no ethical responsibility exists.</p>

<p>In this way, conservative anthropomorphism applies.</p>

<p>However, there is also axis of capability that I think is required in order to warrant human-like 'emancipation' for robots, which is not necessarily anthropomorphic. For lack of a better term, I call this an Arbitrary Machine Generator (AMG). At a species level, extant biological life is an AMG - capable of slowly evolving to solve arbitrary problems, assuming the resources and solutions are available. At an individual level, pre-human animals are not capable of generating arbitrary machines to solve arbitrary problems on individual time-scales. Only humans (and post-humans) are capable of generating arbitrary machines in order to solve arbitrary problems, given the resources and solutions available. Humans can search the space of all possible (resource constrained) solutions.</p>

<p>So, for a robot to ""deserve"" the freedom to define it's own purpose, it must first have <em>access</em> to the space of all possible (within the constraints of available resources) purposes. It then must have purposes and internal contexts of such sufficient complexity and familiarity that we humans are capable of sympathizing with those purposes and internal contexts.</p>

<p>If either of those are not present - the AMG criteria and the sympathetic contexts - then emancipation is not warranted.</p>
",,0,2016-12-30T03:23:08.290,,2569,2016-12-30T03:23:08.290,,,,,1712.0,2441.0,2,0,,,,26.2,15.55,10.45,0.0,0.0,45.0,Both Ethical responsibility between humans is based on a sympathetic correspondence between humans Between humans and robots if one party lacks the desire or capability to sympathize with the other party no ethical responsibility exists In this way conservative anthropomorphism applies However there is also axis of capability that I think is required in order to warrant humanlike emancipation for robots which is not necessarily anthropomorphic For lack of a better term I call this an Arbitrary Machine Generator AMG At a species level extant biological life is an AMG capable of slowly evolving to solve arbitrary problems assuming the resources and solutions are available At an individual level prehuman animals are not capable of generating arbitrary machines to solve arbitrary problems on individual timescales Only humans and posthumans are capable of generating arbitrary machines in order to solve arbitrary problems given the resources and solutions available Humans can search the space of all possible resource constrained solutions So for a robot to deserve the freedom to define its own purpose it must first have access to the space of all possible within the constraints of available resources purposes It then must have purposes and internal contexts of such sufficient complexity and familiarity that we humans are capable of sympathizing with those purposes and internal contexts If either of those are not present the AMG criteria and the sympathetic contexts then emancipation is not warranted
,,"<p>Both. I answered this question here also: <a href=""http://ai.stackexchange.com/a/2569/1712"">http://ai.stackexchange.com/a/2569/1712</a></p>

<p>Let me know if I should expand on that here.</p>
",,0,2016-12-30T03:27:28.240,,2570,2016-12-30T03:27:28.240,,,,,1712.0,2498.0,2,0,,,,70.8,16.09,8.47,0.0,0.0,11.0,Both I answered this question here also httpaistackexchangecoma25691712 Let me know if I should expand on that here
,,"<p>There is lots of misconceptions about AI, specifically the idea that it is about making computers ""think"" like humans, simulating brain, the sci-fi robots taking over the world, all the philosophical discussions around brain as machine etc. The practice/reality of AI is about ""using computing to solve problems"" which basically means you take any problem, represent it as a computing problem and then design the algorithm to solve the computing problem which lead to solving the original problem. These search algorithms are general purpose algorithms for general purpose computing problems i.e any real world problem can be represented by these general purpose computing problem and then these algorithms can be used to solve them.</p>

<p>Remember, its about problem solving and its about general purpose computing problems that can represent any real world problem.</p>
",,0,2016-12-30T05:04:29.640,,2571,2016-12-30T05:04:29.640,,,,,1462.0,2514.0,2,3,,,,36.02,14.8,9.35,0.0,0.0,17.0,There is lots of misconceptions about AI specifically the idea that it is about making computers think like humans simulating brain the scifi robots taking over the world all the philosophical discussions around brain as machine etc The practicereality of AI is about using computing to solve problems which basically means you take any problem represent it as a computing problem and then design the algorithm to solve the computing problem which lead to solving the original problem These search algorithms are general purpose algorithms for general purpose computing problems ie any real world problem can be represented by these general purpose computing problem and then these algorithms can be used to solve them Remember its about problem solving and its about general purpose computing problems that can represent any real world problem
,,"<p>Neural networks are good at classifying. In some situations that comes down to prediction, but not necessarily. </p>

<p>The mathematical reason for the neural networks prowess at classifying is the <a href=""https://en.wikipedia.org/wiki/Universal_approximation_theorem"" rel=""nofollow noreferrer"">universal approximation theorem</a>. Which states that a neural network can approximate any continuous real-valued function on a compact subset. The quality of the approximation depends on the number of neurons. It has also been shown that adding the neurons in additional layers instead of adding them to existing layers improves the quality of the approximation faster. </p>

<p>Add to that the not well-understood effectiveness of the <a href=""https://en.wikipedia.org/wiki/Backpropagation"" rel=""nofollow noreferrer"">backpropagation</a> algorithm and you have a setup then can actually learn the function that the UAT promises or something close. </p>
",,0,2016-12-30T06:18:56.870,,2572,2016-12-30T06:18:56.870,,,,,2227.0,2524.0,2,1,,,,46.47,14.32,10.54,0.0,0.0,10.0,Neural networks are good at classifying In some situations that comes down to prediction but not necessarily The mathematical reason for the neural networks prowess at classifying is the universal approximation theorem Which states that a neural network can approximate any continuous realvalued function on a compact subset The quality of the approximation depends on the number of neurons It has also been shown that adding the neurons in additional layers instead of adding them to existing layers improves the quality of the approximation faster Add to that the not wellunderstood effectiveness of the backpropagation algorithm and you have a setup then can actually learn the function that the UAT promises or something close
,,"<p><a href=""http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf"" rel=""nofollow noreferrer"">Unsupervised pre-training</a> was done only very shortly, afaik, at the time when deep learning started to actually work. It extracts certain regularities in the data, which a later supervised learning can latch onto, so its not surprising that it might work. On the other hand unsupervised learning doesn't give particularly impressive results in very deep nets, so it is also not surprising that with current very deep nets, it isn't used anymore. </p>

<p>I was wondering whether the initial success with unsupervised pre-training had something to do with the fact that the ideal initialisation of neural nets was only worked out later. In that case unsupervised pre-training would only be a very complicated way of getting the weights to the correct size. </p>

<p>Unsupervised deep learning is something like the holy grail of AI right now and imho it hasn't been found yet. Unsupervised deep learning would allow you to use massive amounts of unlabeled data and let the net form its own categories. Later you can just use a little bit of labeled data to give these categories their proper labels. Or just train it immediately on some task, in the conviction that it has a huge amount of knowledge about the world already. This is also what the problem of common sense comes down to: A huge and detailed model of the world, that could only be acquired by unsupervised learning. </p>
",,0,2016-12-30T07:14:36.183,,2573,2016-12-30T07:14:36.183,,,,,2227.0,88.0,2,1,,,,56.49,11.38,9.29,0.0,0.0,25.0,Unsupervised pretraining was done only very shortly afaik at the time when deep learning started to actually work It extracts certain regularities in the data which a later supervised learning can latch onto so its not surprising that it might work On the other hand unsupervised learning doesnt give particularly impressive results in very deep nets so it is also not surprising that with current very deep nets it isnt used anymore I was wondering whether the initial success with unsupervised pretraining had something to do with the fact that the ideal initialisation of neural nets was only worked out later In that case unsupervised pretraining would only be a very complicated way of getting the weights to the correct size Unsupervised deep learning is something like the holy grail of AI right now and imho it hasnt been found yet Unsupervised deep learning would allow you to use massive amounts of unlabeled data and let the net form its own categories Later you can just use a little bit of labeled data to give these categories their proper labels Or just train it immediately on some task in the conviction that it has a huge amount of knowledge about the world already This is also what the problem of common sense comes down to A huge and detailed model of the world that could only be acquired by unsupervised learning
,,"<p>Human chess and go experts clearly use evaluation functions. They do come up with moves that look sensible without evaluating the board position, but to validate these candidate moves they evaluate board positions that occur at the end of the variations they calculate. Pretty similar to AlphaGo. </p>

<p>Inputting two board states and outputting a preference is a (much) more complex task than mapping one board state into the real numbers. And it gives you less information. So its a lose-lose choice. (I did try something very similar and it didn't work at all. The reason is that you didn't just double the input size, rather you made the input space quadratically bigger.)</p>

<p>If you compare two board states that differ just by one move, then your input space doesn't quite explode as much, but you have to do a ton of comparisons to make a decision. The logical choice would be to output a preference distribution over all possible moves - but that's exactly what AlphaGo does in its policy network. There is also an <a href=""https://arxiv.org/abs/1412.3409"" rel=""nofollow noreferrer"">earlier paper</a> which trained a network to predict expert moves, which comes down to the same thing. And yes, both these networks play quite strongly without any search or board evaluation. But nowhere near AlphaGo-level.</p>
",,9,2016-12-30T07:30:54.553,,2574,2016-12-30T07:30:54.553,,,,,2227.0,2564.0,2,3,,,,63.59,10.79,9.57,0.0,0.0,30.0,Human chess and go experts clearly use evaluation functions They do come up with moves that look sensible without evaluating the board position but to validate these candidate moves they evaluate board positions that occur at the end of the variations they calculate Pretty similar to AlphaGo Inputting two board states and outputting a preference is a much more complex task than mapping one board state into the real numbers And it gives you less information So its a loselose choice I did try something very similar and it didnt work at all The reason is that you didnt just double the input size rather you made the input space quadratically bigger If you compare two board states that differ just by one move then your input space doesnt quite explode as much but you have to do a ton of comparisons to make a decision The logical choice would be to output a preference distribution over all possible moves but thats exactly what AlphaGo does in its policy network There is also an earlier paper which trained a network to predict expert moves which comes down to the same thing And yes both these networks play quite strongly without any search or board evaluation But nowhere near AlphaGolevel
,,"<p>If you look at the work of Howard Gardner and his theory of multiple intelligences, you will see that the term of ""intelligence"" respectively ""generic cleverness"" is much more diverse and not entirely clarified. Without a entire notion of it, a proof is forlorn.</p>
",,0,2016-12-30T14:17:24.373,,2576,2016-12-30T14:17:24.373,,,,,4528.0,2562.0,2,0,,,,49.15,11.6,10.47,0.0,0.0,8.0,If you look at the work of Howard Gardner and his theory of multiple intelligences you will see that the term of intelligence respectively generic cleverness is much more diverse and not entirely clarified Without a entire notion of it a proof is forlorn
,1.0,"<p>Suppose, I have been given the following diagram to design a simple neural network.  </p>

<p><a href=""https://i.stack.imgur.com/SbXDo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SbXDo.png"" alt=""enter image description here""></a></p>

<p>How can I compute the neuron weights, design NN, and plot class boundaries?</p>

<p>Or, is it really possible to do the above, only from the diagram?</p>
",,4,2016-12-30T15:09:53.240,1.0,2577,2016-12-31T13:51:18.257,2016-12-31T13:28:28.193,,3642.0,,3642.0,,1,0,<neural-networks><self-learning>,Is it possible to design a Neural Network from feature plots?,37.0,74.9,9.1,10.61,0.0,0.0,8.0,Suppose I have been given the following diagram to design a simple neural network How can I compute the neuron weights design NN and plot class boundaries Or is it really possible to do the above only from the diagram
2579.0,1.0,"<p>The <a href=""http://kryten.mm.rpi.edu/lovelace.pdf"" rel=""nofollow noreferrer"">original Lovelace Test</a>, published in 2001, is used generally as a thought experiment to prove that AI cannot be creative (or, more specifically, that it cannot <em>originate</em> a creative artifact). From the paper:</p>

<blockquote>
  <p>Artificial Agent <em>A</em>, designed by <em>H</em>, passes LT if and only if</p>
  
  <ul>
  <li><p><em>A</em> outputs <em>o</em>,</p></li>
  <li><p><em>A</em> outputting <em>o</em> is not the result of a fluke hardware error, but rather the result of processes <em>A</em> can repeat</p></li>
  <li><p><em>H</em> (or someone who knows what <em>H</em> knows, and has <em>H</em>'s resources) cannot explain how <em>A</em> produced <em>o</em>.</p></li>
  </ul>
</blockquote>

<p>The authors of the original Lovelace Test then argues that it is impossible to imagine a human developing a machine to create an artifact...while also not knowing how that machine worked. For example, an AI that uses machine learning to make a creative artifact <em>o</em> is obviously being 'trained' on a dataset and is using some sort of algorithm to be able to make predictions on this dataset. Therefore, the human <em>can</em> explain how the AI produced <em>o</em>, and therefore the AI is not creative.</p>

<p>The Lovelace Test seems like an effective thought experiment, even though it appears to be utterly useless as an actual test (which is why the <a href=""http://ai.stackexchange.com/questions/1451/has-the-lovelace-test-2-0-been-successfully-used-in-an-academic-setting"">the Lovelace Test 2.0 was invented</a>). However, since it does seem like an effective thought experiment, there must be some arguments against it. I am curious to see any flaws in the Lovelace Test that could undermine its premise.</p>
",,0,2016-12-30T17:19:59.427,,2578,2016-12-30T18:11:00.777,,,,,181.0,,1,1,<intelligence-testing>,Are there any refutation of the original Lovelace Test?,37.0,55.88,10.1,9.48,0.0,0.0,37.0,The original Lovelace Test published in 2001 is used generally as a thought experiment to prove that AI cannot be creative or more specifically that it cannot originate a creative artifact From the paper Artificial Agent A designed by H passes LT if and only if A outputs o A outputting o is not the result of a fluke hardware error but rather the result of processes A can repeat H or someone who knows what H knows and has Hs resources cannot explain how A produced o The authors of the original Lovelace Test then argues that it is impossible to imagine a human developing a machine to create an artifactwhile also not knowing how that machine worked For example an AI that uses machine learning to make a creative artifact o is obviously being trained on a dataset and is using some sort of algorithm to be able to make predictions on this dataset Therefore the human can explain how the AI produced o and therefore the AI is not creative The Lovelace Test seems like an effective thought experiment even though it appears to be utterly useless as an actual test which is why the the Lovelace Test 20 was invented However since it does seem like an effective thought experiment there must be some arguments against it I am curious to see any flaws in the Lovelace Test that could undermine its premise
,,"<p>I am a future neurologist with a very complete understanding of linguistic processing in the brain.  I am also an overprotective parent, so I monitor every phrase uttered to my child, and also completely determine all the books she reads in the course of her education.</p>

<p>When my child writes a poem, then, I know the dataset on which her brain was trained, as well as the processes by which her language inputs became language outputs--in broad outline I know these processes are non-linear and are based on how different inputs along with the current collection of trillions of distinct synaptic weights updates the synaptic weights.  I don't know what her poem will be, of course, because there are random factors and the whole history of her synaptic weights are unobservable, but I adhere to the Lovelace test and can therefore conclude that composing the poem was not a creative act.</p>

<p>The Lovelace Test, like the Chinese Room Argument, implicitly assumes that what computers/AI can do in processing symbols and information and what brains can do are distinct.  If you accept that assumption, then the argument ceases to be interesting-- you've merely redefined creativity as one of the distinct things that brains can do.  If you reject the assumption, the argument that computers are incapable of creativity ceases to be valid.  The thought experiment itself does nothing to assist us in evaluating the truth of the assumption.</p>
",,0,2016-12-30T17:42:27.013,,2579,2016-12-30T18:11:00.777,2016-12-30T18:11:00.777,,2329.0,,2329.0,2578.0,2,3,,,,41.43,12.43,9.97,0.0,0.0,28.0,I am a future neurologist with a very complete understanding of linguistic processing in the brain I am also an overprotective parent so I monitor every phrase uttered to my child and also completely determine all the books she reads in the course of her education When my child writes a poem then I know the dataset on which her brain was trained as well as the processes by which her language inputs became language outputsin broad outline I know these processes are nonlinear and are based on how different inputs along with the current collection of trillions of distinct synaptic weights updates the synaptic weights I dont know what her poem will be of course because there are random factors and the whole history of her synaptic weights are unobservable but I adhere to the Lovelace test and can therefore conclude that composing the poem was not a creative act The Lovelace Test like the Chinese Room Argument implicitly assumes that what computersAI can do in processing symbols and information and what brains can do are distinct If you accept that assumption then the argument ceases to be interesting youve merely redefined creativity as one of the distinct things that brains can do If you reject the assumption the argument that computers are incapable of creativity ceases to be valid The thought experiment itself does nothing to assist us in evaluating the truth of the assumption
2585.0,1.0,"<p>So I am looking to make an AI like jarvis. A perfect real life example of this type of system is the simple AI that Mark Zuckerberg has recently built. <a href=""https://www.facebook.com/notes/mark-zuckerberg/building-jarvis/10154361492931634/"" rel=""nofollow noreferrer"">Here</a> is a description on how his AI works. From what I understand, the AI understands keywords, context, synonyms and then from there decides what to do. I have many questions on how this system works. Firstly, what necessary steps are required to gather the meaning of a input? Secondly, how does the system, once it extract all of the necessary information on the input, determine what action it needs to take and what to say back to the user? lastly, it also states that the system can learn habits and preferences of the user, how can a system do this?</p>

<p><a href=""https://www.youtube.com/watch?v=vvimBPJ3XGQ"" rel=""nofollow noreferrer"">Here</a> is also a video of the AI in action.</p>
",,1,2016-12-30T18:02:31.660,,2580,2016-12-31T07:49:40.360,,,,,4532.0,,1,-1,<neural-networks><machine-learning><deep-learning><conv-neural-network><nlp>,Making a simple ai like Jarvis,141.0,72.46,7.54,8.78,0.0,0.0,18.0,So I am looking to make an AI like jarvis A perfect real life example of this type of system is the simple AI that Mark Zuckerberg has recently built Here is a description on how his AI works From what I understand the AI understands keywords context synonyms and then from there decides what to do I have many questions on how this system works Firstly what necessary steps are required to gather the meaning of a input Secondly how does the system once it extract all of the necessary information on the input determine what action it needs to take and what to say back to the user lastly it also states that the system can learn habits and preferences of the user how can a system do this Here is also a video of the AI in action
,0.0,"<p>I know <a href=""https://chessprogramming.wikispaces.com/Magic+Bitboards"" rel=""nofollow noreferrer"">magic bitboards</a> are commonly used in chess to generate moves for bishops, rooks, and queens. I was wondering if anybody has ever tried to implement this for the game of Othello. Would something like this be possible? Or has somebody already discovered the fastest possible way to generate moves for this game?</p>

<p>In Othello there's basically 2 steps:</p>

<ol>
<li>Find the bits that are legal moves.</li>
<li>For each bit found, find the bits that will 'flip' if that move is played.</li>
</ol>
",,0,2016-12-31T01:07:38.980,,2581,2017-01-01T19:13:19.860,2017-01-01T19:13:19.860,,75.0,,4541.0,,1,0,<gaming><chess>,Othello move generation using Magic Bitboards,28.0,66.03,9.27,8.94,0.0,0.0,13.0,I know magic bitboards are commonly used in chess to generate moves for bishops rooks and queens I was wondering if anybody has ever tried to implement this for the game of Othello Would something like this be possible Or has somebody already discovered the fastest possible way to generate moves for this game In Othello theres basically 2 steps Find the bits that are legal moves For each bit found find the bits that will flip if that move is played
,,"<p><strong>The Reality of Working in the Field</strong></p>

<p>Most in the fields of adaptive systems, machine learning, machine vision, intelligent control, robotics, and business intelligence, in the corporations and universities in which I've worked do not discuss this topic much in meetings or at lunch.  Most are too busy building things that must work by some deadline to muse over things that are not of immediate concern, and bot-rights are a long way off.</p>

<p><strong>How Far Off?</strong></p>

<p>To begin with, no bot has yet passed a properly conducted Turing Test. (There is much on the net about this test, including critique of poorly conducted testing of this type.  See Searle's Chinese Room thought experiment.)</p>

<p>Language simulation with semantic understanding is difficult enough without adding creativity, coordination, feelings, intuition, body language, learning of entirely new domains from scratch, and the potential of genius.</p>

<p>In synopsis, we a long way from the procurement of bots that simulate humanity sufficiently to be considered for citizenship, even in a progressive country that abhors fundamentalism of any kind.  No actual imbuement of rights will occur until we have bot-citizenship in one or more countries.  Consider that human fetuses do not yet have rights because they are not yet deemed citizens.</p>

<p><strong>Relevance of the Answer for Today</strong></p>

<p>In current culture, conservative anthropocentricm and post-human fundamentalism arrive at the same effective conclusion, and that may continue to be the case for a hundred years.</p>

<p>Those with experience across fields of psychology, neuro-biology, cybernetics, and adaptive systems know that the simulation of all the mental features we attribute to humans is to copy in algorithms the layering of cerebral abilities over a reptilian brain that went through millions of years of field testing.</p>

<p><strong>Impact of Science Fiction</strong></p>

<p>Asking around, it is likely you would get some feedback that is mostly gained from the media of our culture, not philosophic theses and publications written by those who don't actually have any deadlines to produce anything that functions IRL.</p>

<p>Isaac Asimov investigated some conservative anthropomorphism concepts in scenarios depicted in his short stories.  Commander Data's human quirks in the Next Generation Star Trek teleplays furthered some of those ideas.</p>

<p>Christopher Nolan took the opposite direction in the Interstellar screenplay, with the robots having interesting personalities that could be altered by linear settings.  His bots ignored concerns of self-preservation, apparently without any cognitive resistance.  This depiction is an unapologetic post-human fundamentalist view.</p>

<p><strong>A Thought Experiment</strong></p>

<p>Let's place the citizen issue aside for to consider this thought experiment, and let's assume that a survey would show a leaning toward conservative anthropocentrism among current AI researchers.</p>

<p>Consider an intelligent piece of software constructed a legal complaint to gain intellectual property rights over day trading code it wrote and sent it to the appropriate court clerk, you might find that the same researchers would recant.  </p>

<p>Post-human fundamentalism will probably prevail when real AI software theorists and engineers consider the true personal, corporate, and meaning of settling out of court or losing the case.</p>

<p><strong>Would Researchers Cut the Umbilical Cord?</strong></p>

<p>I asked one researcher and she indicated that all her lawyer would need to do to win the case likely consider the precedence that might occur and recall some of the warnings built into the Terminator stories.</p>

<p>Based on my observation of humanity in my life time, my prediction is that people want slaves not some brand of bots that could ultimately kick our butts in an all out fight.</p>
",,0,2016-12-31T04:23:57.653,,2582,2016-12-31T04:23:57.653,,,,,4302.0,2498.0,2,0,,,,38.76,14.51,10.76,0.0,0.0,70.0,The Reality of Working in the Field Most in the fields of adaptive systems machine learning machine vision intelligent control robotics and business intelligence in the corporations and universities in which Ive worked do not discuss this topic much in meetings or at lunch Most are too busy building things that must work by some deadline to muse over things that are not of immediate concern and botrights are a long way off How Far Off To begin with no bot has yet passed a properly conducted Turing Test There is much on the net about this test including critique of poorly conducted testing of this type See Searles Chinese Room thought experiment Language simulation with semantic understanding is difficult enough without adding creativity coordination feelings intuition body language learning of entirely new domains from scratch and the potential of genius In synopsis we a long way from the procurement of bots that simulate humanity sufficiently to be considered for citizenship even in a progressive country that abhors fundamentalism of any kind No actual imbuement of rights will occur until we have botcitizenship in one or more countries Consider that human fetuses do not yet have rights because they are not yet deemed citizens Relevance of the Answer for Today In current culture conservative anthropocentricm and posthuman fundamentalism arrive at the same effective conclusion and that may continue to be the case for a hundred years Those with experience across fields of psychology neurobiology cybernetics and adaptive systems know that the simulation of all the mental features we attribute to humans is to copy in algorithms the layering of cerebral abilities over a reptilian brain that went through millions of years of field testing Impact of Science Fiction Asking around it is likely you would get some feedback that is mostly gained from the media of our culture not philosophic theses and publications written by those who dont actually have any deadlines to produce anything that functions IRL Isaac Asimov investigated some conservative anthropomorphism concepts in scenarios depicted in his short stories Commander Datas human quirks in the Next Generation Star Trek teleplays furthered some of those ideas Christopher Nolan took the opposite direction in the Interstellar screenplay with the robots having interesting personalities that could be altered by linear settings His bots ignored concerns of selfpreservation apparently without any cognitive resistance This depiction is an unapologetic posthuman fundamentalist view A Thought Experiment Lets place the citizen issue aside for to consider this thought experiment and lets assume that a survey would show a leaning toward conservative anthropocentrism among current AI researchers Consider an intelligent piece of software constructed a legal complaint to gain intellectual property rights over day trading code it wrote and sent it to the appropriate court clerk you might find that the same researchers would recant Posthuman fundamentalism will probably prevail when real AI software theorists and engineers consider the true personal corporate and meaning of settling out of court or losing the case Would Researchers Cut the Umbilical Cord I asked one researcher and she indicated that all her lawyer would need to do to win the case likely consider the precedence that might occur and recall some of the warnings built into the Terminator stories Based on my observation of humanity in my life time my prediction is that people want slaves not some brand of bots that could ultimately kick our butts in an all out fight
2586.0,1.0,"<p>I'm just diving in this whole new area of knowledge; i happened to lost in all the concepts a bit.</p>

<p>What is difference between stacked RBM and deep belief network? </p>

<p>Are they the same entity? If so, why?</p>

<p>Is the latter a some specific type of the former? If so, how to tell if stacked RBM is a DBN?</p>

<p>Sorry for asking such a noob question, but today it is quite difficult to find a consistent information on the internet, different sources give different explanations.</p>
",,0,2016-12-31T05:00:39.783,,2583,2016-12-31T08:08:05.530,,,,,4531.0,,1,0,<deep-network><terminology><boltzmann-machine>,Terminology: DBN vs stacked RBM,24.0,76.11,7.64,9.07,0.0,0.0,13.0,Im just diving in this whole new area of knowledge i happened to lost in all the concepts a bit What is difference between stacked RBM and deep belief network Are they the same entity If so why Is the latter a some specific type of the former If so how to tell if stacked RBM is a DBN Sorry for asking such a noob question but today it is quite difficult to find a consistent information on the internet different sources give different explanations
,,"<p>Have you tried <a href=""http://www.nltk.org/"" rel=""nofollow noreferrer"">NLTK</a>, what you are looking for is in <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow noreferrer"">Chapter 6</a> of the book. Basically what you need to do is:</p>

<ul>
<li>Tokenize the user input.</li>
<li>Extract vector set from the tokenized words.</li>
<li>Train your model with some given texts, and same vector sets.</li>
</ul>

<p>And you can use your model to categorize the document.</p>

<p>One other suggestion, instead of extracting vector sets you can use every word in the input
to be evaluated in to some category using a training set of large corpus, which you are sure
it contains all the words.
And then you multiply probability of each word being on a category to decide where the document belongs.</p>
",,0,2016-12-31T06:23:04.337,,2584,2017-01-12T12:27:07.703,2017-01-12T12:27:07.703,,8.0,,4543.0,2436.0,2,2,,,,63.7,8.35,8.66,0.0,0.0,12.0,Have you tried NLTK what you are looking for is in Chapter 6 of the book Basically what you need to do is Tokenize the user input Extract vector set from the tokenized words Train your model with some given texts and same vector sets And you can use your model to categorize the document One other suggestion instead of extracting vector sets you can use every word in the input to be evaluated in to some category using a training set of large corpus which you are sure it contains all the words And then you multiply probability of each word being on a category to decide where the document belongs
,,"<p>Jarvis was built using the suite of tools that facebook developers are constantly updating. The answer to this question is that there's no simple answer; it has a lot of moving parts. </p>

<p>Take for example natural language processing. There are a number of sub-topics that are each considered ""big"" problems, such as part-of-speech recognition, coreference resolution, sentiment analysis, relationship extraction, and many more. Tools have been built to tackle these various topics, but to my knowledge none of them really <em>understand</em> language, but rather statistically approximate it. </p>

<p>In the case of Jarvis, since it is a home-automation system, it's probably built with the user-given commands in mind from the beginning, so it's not trying to understand the whole human language, it's built to do some tricks. It looks like you're interested convolutional neural networks and those kinds of things based on the tags, but their function is to find sufficiently complex non-linear relationships to accurately predict in the domain of the data they've been trained, but they do not understand the underlying mechanics of the system.</p>

<p>Just keep in mind on your journey into this space that true AI like what we imagine will have some defining features like hierarchical representations of tasks and goal-orientation. If you really get into it I'd start with reinforcement learning, or try reading through the Society of Minds.</p>
",,0,2016-12-31T07:49:40.360,,2585,2016-12-31T07:49:40.360,,,,,4544.0,2580.0,2,3,,,,46.2,13.93,10.37,0.0,0.0,40.0,Jarvis was built using the suite of tools that facebook developers are constantly updating The answer to this question is that theres no simple answer it has a lot of moving parts Take for example natural language processing There are a number of subtopics that are each considered big problems such as partofspeech recognition coreference resolution sentiment analysis relationship extraction and many more Tools have been built to tackle these various topics but to my knowledge none of them really understand language but rather statistically approximate it In the case of Jarvis since it is a homeautomation system its probably built with the usergiven commands in mind from the beginning so its not trying to understand the whole human language its built to do some tricks It looks like youre interested convolutional neural networks and those kinds of things based on the tags but their function is to find sufficiently complex nonlinear relationships to accurately predict in the domain of the data theyve been trained but they do not understand the underlying mechanics of the system Just keep in mind on your journey into this space that true AI like what we imagine will have some defining features like hierarchical representations of tasks and goalorientation If you really get into it Id start with reinforcement learning or try reading through the Society of Minds
,,"<p>For what it's worth, wikipedia says that ""deep belief networks can be formed by ""stacking"" <a href=""https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine"" rel=""nofollow noreferrer"">RBMs</a>"". Hinton <a href=""http://www.scholarpedia.org/article/Deep_belief_networks#Deep_Belief_Nets_as_Compositions_of_Simple_Learning_Modules"" rel=""nofollow noreferrer"">writes in Scholarpedia</a>: ""A deep belief net can be viewed as a composition of simple learning modules each of which is a restricted type of Boltzmann machine"".</p>

<p>So, a deep belief network is definitely a stacked RBM. </p>

<p>I have never heard of different stacked RBMs, but it is easy to imagine something like convolutional stacked RBMs, where some RBMs are used as filters that slide over the input data or something. Whether that would still be called a deep belief network, is probably up to the guy who publishes it first. </p>
",,4,2016-12-31T08:08:05.530,,2586,2016-12-31T08:08:05.530,,,,,2227.0,2583.0,2,1,,,,66.47,10.39,9.97,0.0,0.0,18.0,For what its worth wikipedia says that deep belief networks can be formed by stacking RBMs Hinton writes in Scholarpedia A deep belief net can be viewed as a composition of simple learning modules each of which is a restricted type of Boltzmann machine So a deep belief network is definitely a stacked RBM I have never heard of different stacked RBMs but it is easy to imagine something like convolutional stacked RBMs where some RBMs are used as filters that slide over the input data or something Whether that would still be called a deep belief network is probably up to the guy who publishes it first
,,"<p>From the diagram you have given, it's quite clear that you have to design a <strong>supervised</strong> network. Also, it's clear that you are dealing with a problem that's not <em>linearly separable</em> i.e. you can't separate two classes using a single line. Particularly, it will require three lines to separate these two classes. </p>

<p>From the above observations, what you can do is:</p>

<ol>
<li><p>Make training data sets of form <code>[(x,y), o]</code> where <code>(x,y)</code> are the co-ordinates on the plot and <code>o</code> is the class that point belongs to. </p>

<p>Note: You can actually divide your data set into training data and test data. Just, extract the data, randomly shuffle it and take first 80% of data to train and rest 20% of data to test.</p></li>
<li><p>Design a 2-2-2-1 neural network with bias units and some random initial weight values. This is because we will require minimum three lines to separate two classes, so 2 hidden layers (each consisting of 2 nodes and bias nodes) and 1 output layer (with bias node). The first layer is input layer with 2 nodes (no bias) as we have x-coordinate and y-coordinate.</p></li>
<li><p>Train your training data using this network using the Error Back Propagation algorithm i.e. update the weights between the layers and the bias weights too. Run the EBP algorithm for 100-2000 epochs.</p></li>
<li><p>Then, use the test data to see, if you get the desired output.</p></li>
</ol>

<p>Note: I am NOT quite sure how to obtain the coordinates from the diagram. You can either manually note all points or research for some methods to get those points automatically from the image (some image processing algorithms).</p>
",,2,2016-12-31T13:36:58.587,,2587,2016-12-31T13:51:18.257,2016-12-31T13:51:18.257,,1807.0,,1807.0,2577.0,2,1,,,,71.65,9.74,8.58,18.0,0.0,51.0,From the diagram you have given its quite clear that you have to design a supervised network Also its clear that you are dealing with a problem thats not linearly separable ie you cant separate two classes using a single line Particularly it will require three lines to separate these two classes From the above observations what you can do is Make training data sets of form where are the coordinates on the plot and is the class that point belongs to Note You can actually divide your data set into training data and test data Just extract the data randomly shuffle it and take first 80 of data to train and rest 20 of data to test Design a 2221 neural network with bias units and some random initial weight values This is because we will require minimum three lines to separate two classes so 2 hidden layers each consisting of 2 nodes and bias nodes and 1 output layer with bias node The first layer is input layer with 2 nodes no bias as we have xcoordinate and ycoordinate Train your training data using this network using the Error Back Propagation algorithm ie update the weights between the layers and the bias weights too Run the EBP algorithm for 1002000 epochs Then use the test data to see if you get the desired output Note I am NOT quite sure how to obtain the coordinates from the diagram You can either manually note all points or research for some methods to get those points automatically from the image some image processing algorithms
,1.0,"<p>I Build this NN in c++. I reviewed it since 3 days. I checked every line 100 times, but I cant find my error.
If someone can please help me find the Bugs:
1. The output is garbage
2. The weights go from 2e^79 down to -1.8e^80 after approximatly 400 iterations.</p>

<pre><code>mat flip(mat m) {
    mat out(m.n_cols,m.n_rows);

    for (int i = 0; i &lt; m.n_rows; ++i)
        for (int j = 0; j &lt; m.n_cols; ++j)
            out(j, i) = m(i, j);

    return out;
}

Layer::Layer(int nodes) :
    rand_engine(time(0))
{

    y = mat (nodes, 1);
    net = mat(nodes, 1);
    e = mat(nodes, 1);
}

Layer::Layer(int nodes, int next_nodes) :
    Layer(nodes)
{

    this-&gt;next_l = next_l;

    auto random = bind(uniform_real_distribution&lt;double&gt;{-1, 1}, rand_engine);

    w = mat(next_nodes,nodes);

    for (int i = 0; i &lt; w.n_rows; ++i) {

        for (int j = 0; j &lt; w.n_cols; ++j) {
            w(i,j) = random();
        }
    }
}

Layer::Layer(int nodes, Layer* next_l) :
    Layer(nodes,next_l-&gt;y.n_rows)
{
    this-&gt;next_l = next_l;
}

void Layer::feed_forward()
{
    next_l-&gt;net = w*y;


    for (int i = 0; i &lt; next_l-&gt;y.n_rows;++i) 
        next_l-&gt;y[i] = sig(next_l-&gt;net[i]);


}

void Layer::backprop()
{

    for (double d : w)
        cout &lt;&lt; d &lt;&lt; ""\t"";

    e = flip(w)*next_l-&gt;e;


    for (int i = 0; i &lt; e.n_rows; ++i) {
        e[i] *= net[i] * (1 - net[i]);  
        cout &lt;&lt; e[i] &lt;&lt; '\t';
    }

    w += l_rate*(next_l-&gt;e*flip(y));


}

void Layer::backprop_last(mat t)
{
    for (int i = 0; i &lt; e.n_rows; ++i) {
        e[i] = net[i] * (1 - net[i])*(t[i] - y[i]);
        cout &lt;&lt; e[i] &lt;&lt; '\t';
    }

}

void Layer::feed_forward(Layer* next_l)
{
    this-&gt;next_l = next_l;

    feed_forward();

}

double Layer::sig(double x)
{
    return 1 / (1 + exp(-x));
}






Network::Network(vector&lt;int&gt; top):
    top(top)
{

    network = new Layer*[top.size()];
    network[top.size() - 1] = new Layer(top.back());

    for (int i = top.size()-2; i &gt; -1; --i)
        network[i] = new Layer(top[i], network[i + 1]);

}


Network::~Network()
{
    delete[] network;
}

void Network::forward()
{
    for(int i = 0; i &lt; top.front();++i)
        network[0]-&gt;y[i] = input[i];

    for (int i = 0; i &lt; top.size() - 1; ++i)
        network[i]-&gt;feed_forward();
}

void Network::forward(vector&lt;double&gt; input)
{
    set_input(input);
    forward();
}

void Network::backprop()
{

    network[top.size() - 1]-&gt;backprop_last(t_vals);

    for (int i = top.size() - 2; i &gt; -1; --i) {
        network[i]-&gt;backprop();
    }

}

void Network::backprop(vector&lt;double&gt; t_vals)
{
    set_t_vals(t_vals);
    backprop();
}
</code></pre>

<p>I know its a bunch of code but im really desprate since I cant find whats wrong. I tested it with a simple XOR.</p>

<p>Edit:
Heres my Main code:</p>

<pre><code>    #include ""Network.h""
#include &lt;iomanip&gt;

using namespace std;

vector&lt;vector&lt;double&gt;&gt; input = { {0,0},{0,1},{1,1},{1,0} };

vector&lt;vector&lt;double&gt;&gt; true_vals = { {0},{1},{0},{1} };

int main() {

    ifstream f(""out.txt"", fstream::out);
    f.clear();

    cout &lt;&lt; fixed;
    cout &lt;&lt; setprecision(5);

    Network net({2,5,1});

    vector&lt;double&gt; in,t,out; 

    auto buf = cout.rdbuf();

    for (int i = 0; i &lt; 1000; ++i) {
        cout.rdbuf(f.rdbuf());


        in = input[i % 4];

        net.forward(in);

        out = net.get_output();

        t = true_vals[i % 4];

        net.backprop(t);
        cout &lt;&lt; '\n';
        cout.rdbuf(buf);
        if ((i %101))continue;

        cout &lt;&lt; ""it: "" &lt;&lt; i &lt;&lt; '\n';

        cout &lt;&lt; ""in:\t"";
        for (double d : in)
            cout &lt;&lt; d &lt;&lt; ' ';

        cout &lt;&lt; '\n';

        cout &lt;&lt; ""out:\t"";

        for (double d : out)
            cout &lt;&lt; d &lt;&lt; ' ';

        cout &lt;&lt; '\n';


        cout &lt;&lt; ""true:\t"";


        for (double d : t)
            cout &lt;&lt; d &lt;&lt; ' ';

        cout &lt;&lt; '\n';


        double err = net.get_error();

        cout &lt;&lt;""err:\t""&lt;&lt; err &lt;&lt; '\n' &lt;&lt; '\n';

    }

    cout.rdbuf(NULL);
    f.close();
    return system(""pause"");
}
</code></pre>
",,7,2016-12-31T13:39:37.960,,2588,2016-12-31T15:05:49.610,2016-12-31T14:06:00.553,,4550.0,,4550.0,,1,2,<neural-networks><backpropagation>,Why doesnt my Neural Network work?,105.0,97.2,3.79,7.39,3858.0,0.0,18.0,I Build this NN in c I reviewed it since 3 days I checked every line 100 times but I cant find my error If someone can please help me find the Bugs 1 The output is garbage 2 The weights go from 2e79 down to 18e80 after approximatly 400 iterations I know its a bunch of code but im really desprate since I cant find whats wrong I tested it with a simple XOR Edit Heres my Main code
,,"<p>A little search on Google answers your question.</p>

<p>XOR input space is not linearly separable. It means that you cannot separate the input points in a 2D space into 1 area and 0 area by simply drawing a line between them. It requires at least 2 lines to separate the XOR input space and consequently 2 output nodes (used as classifiers rather than regression). You can easily find its details in google. Search ""XOR problem in Neural Net"".</p>

<p>You can manually implement the desired Neural Network with two output Nodes acting as classifiers as follows :</p>

<p><a href=""https://i.stack.imgur.com/KGw32.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KGw32.jpg"" alt=""Manual Implementation""></a></p>

<p>Where A &amp; B are two output Nodes which act as classifiers (by forming AA' and BB' decision lines respectively during training by Backpropagation). The interpretation of the Outputs of the nodes is given in the table where Net column represents the Overall output to be interpreted.</p>

<p><strong>I showed the above manual implementation just to give you the idea of how classification is done behind the scenes.</strong></p>

<p>Here is the actual Automatic implementation :</p>

<p><a href=""https://i.stack.imgur.com/aOumf.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aOumf.jpg"" alt=""Practical Implementation""></a></p>

<p>Here , all the task is performed by the Neural Net behind the scenes and you get the desired output from the output node in the topmost layer </p>
",,4,2016-12-31T14:19:13.043,,2589,2016-12-31T15:05:49.610,2016-12-31T15:05:49.610,,4424.0,,4424.0,2588.0,2,2,,,,60.14,10.68,9.62,0.0,0.0,22.0,A little search on Google answers your question XOR input space is not linearly separable It means that you cannot separate the input points in a 2D space into 1 area and 0 area by simply drawing a line between them It requires at least 2 lines to separate the XOR input space and consequently 2 output nodes used as classifiers rather than regression You can easily find its details in google Search XOR problem in Neural Net You can manually implement the desired Neural Network with two output Nodes acting as classifiers as follows Where A amp B are two output Nodes which act as classifiers by forming AA and BB decision lines respectively during training by Backpropagation The interpretation of the Outputs of the nodes is given in the table where Net column represents the Overall output to be interpreted I showed the above manual implementation just to give you the idea of how classification is done behind the scenes Here is the actual Automatic implementation Here all the task is performed by the Neural Net behind the scenes and you get the desired output from the output node in the topmost layer
2596.0,1.0,"<p>I am going to design a Neural Net which will be able to break a 5 letter (characters) word into its corresponding syllables (hybrid syllables, I mean it will not strictly adhere to grammatical Syllable rules but will be based on some training sets I provide).</p>

<p>Example : 
                  Train -> tra-in</p>

<p>I think of implementing it in terms of some feedforward net as follows :</p>

<p>Input layer ->Hidden layers -> Output layer</p>

<p>There will be 5 input nodes in the form of decimals (1/26 =0.038 for 'A' ; 2/26 = 0.076 for 'B' ......)</p>

<p>The output layer consists of 4 Nodes which corresponds to each gap between two characters in the word. </p>

<p>And fires as follows :</p>

<p>For ""<strong>TRAIN</strong>"" (TRA-IN): <strong>Input (0.769,0.692,0.038,0.346,0.538)</strong>
                               <strong>Output(0,0,1,0)</strong></p>

<p>For ""<strong>BORIC"" (BO-RI-C): **Input....</strong>
<strong>Output (0,1,0,1)</strong></p>

<p>Is it at all possible to implement the Neural Nets in the way I am doing??</p>

<p>And if possible, then how will I decide the number of Hidden layers and Nodes in each layer??</p>

<p>( In the book I am reading, XOR gate problem and its implementation using hidden layer is given . In XOR we could decide the number of Nodes and Hidden Layers required by seeing the Linear Separability of XOR using two lines. <strong>But here I think such analysis can't be made.</strong></p>

<p><strong>So how do I proceed?? Or is it a trial and error process?</strong>)</p>
",,0,2016-12-31T15:57:03.323,,2590,2017-01-02T13:31:52.200,,,,,4424.0,,1,1,<neural-networks>,How to decide Linear Separability in my Neural Net work?,37.0,81.12,10.03,8.7,0.0,0.0,91.0,I am going to design a Neural Net which will be able to break a 5 letter characters word into its corresponding syllables hybrid syllables I mean it will not strictly adhere to grammatical Syllable rules but will be based on some training sets I provide Example Train train I think of implementing it in terms of some feedforward net as follows Input layer Hidden layers Output layer There will be 5 input nodes in the form of decimals 126 0038 for A 226 0076 for B The output layer consists of 4 Nodes which corresponds to each gap between two characters in the word And fires as follows For TRAIN TRAIN Input 07690692003803460538 Output0010 For BORIC BORIC Input Output 0101 Is it at all possible to implement the Neural Nets in the way I am doing And if possible then how will I decide the number of Hidden layers and Nodes in each layer In the book I am reading XOR gate problem and its implementation using hidden layer is given In XOR we could decide the number of Nodes and Hidden Layers required by seeing the Linear Separability of XOR using two lines But here I think such analysis cant be made So how do I proceed Or is it a trial and error process
,,"<p>In Neural Networks we consider everything in high dimension and try to find a hyperplane that classify them by small changes...</p>

<p>Probably it is hard to prove that it works but intuition says if it can be classified you can do it by add a relaxed plane and let it move amongst data to find a local optimum... </p>
",,0,2017-01-01T17:02:33.327,,2591,2017-01-01T17:02:33.327,,,,,4439.0,2524.0,2,0,,,,58.96,8.19,9.43,0.0,0.0,6.0,In Neural Networks we consider everything in high dimension and try to find a hyperplane that classify them by small changes Probably it is hard to prove that it works but intuition says if it can be classified you can do it by add a relaxed plane and let it move amongst data to find a local optimum
,,"<p>There is different idea behind them...</p>

<p>Naive Bayes is based on some reach background of Probability Theory... It tries to find a ""Theory"" that is consistent with ""Observations"" by using the Bayes Theorem. But technically it will be so simplified to be applied. Actually you are solving some kind of Optimization which Statisticians do. You need all of data at once.</p>

<p>But Perceptron are more heuristic. The Math behind it, is some kind poor. But it works in reality. You takes your data (at once or releasing during time) and try to change the weights iteratively hopefully to find a good network... The idea is so simple. In this case, indeed, you are going to solve an Optimization problem but objective function and the method is compeletely different</p>
",,0,2017-01-01T17:15:14.153,,2593,2017-01-01T17:15:14.153,,,,,4439.0,2548.0,2,0,,,,69.07,9.84,8.61,0.0,0.0,26.0,There is different idea behind them Naive Bayes is based on some reach background of Probability Theory It tries to find a Theory that is consistent with Observations by using the Bayes Theorem But technically it will be so simplified to be applied Actually you are solving some kind of Optimization which Statisticians do You need all of data at once But Perceptron are more heuristic The Math behind it is some kind poor But it works in reality You takes your data at once or releasing during time and try to change the weights iteratively hopefully to find a good network The idea is so simple In this case indeed you are going to solve an Optimization problem but objective function and the method is compeletely different
,0.0,"<p>I am currently trying to understand and implement a conversational agent, seeing in the network there are many apis to do something similar, but what they generate are ""intelligent"" bots, not intelligent conversational agents (wit.ai, recast.ai, Api.ai, etc.), however I have seen Watson virtual agent which paints very well and seems to cover my needs.</p>

<p>However I am a developer and I would like to ask those with more experience, which would be the way to go to implement my objective, an agent similar to what the video of watson virtual agent, with thematic ones that I can train in the agent, and That he can learn from it.</p>

<p>Take a language course, but focused on the generation of programming languages, lexical analysis, syntactic, semantic, etc., however I know that the natural language can not be compared to the language of the machines, reading some thesis vi to make a Conversational agent could do a great grammar (I can not imagine its syntactic tree), using probabilities with ngrams, or using neural networks or expert systems.</p>

<p>As for the expert systems I understand that for these ""learn"" needs their knowledge base be modified, and as for the neural networks these fit, ""learn"", so I think that it is best to use neural networks.</p>

<p>Summarizing which way should I go? , I'm currently taking stanford's natural language processing course, and a deep learning course from google, I thought I'd use ntlk for that important or natural part.</p>

<p>Any suggestion, criticism, contribution, thank you in advance.</p>

<p>machine-learning nlp artificial-intelligence agent</p>
",,1,2017-01-01T20:02:05.957,,2594,2017-01-03T19:32:11.597,2017-01-03T19:32:11.597,,4446.0,,4563.0,,1,1,<neural-networks><machine-learning><deep-learning><natural-language><language-processing>,"Conversational agent,query",50.0,45.49,12.65,9.47,0.0,0.0,56.0,I am currently trying to understand and implement a conversational agent seeing in the network there are many apis to do something similar but what they generate are intelligent bots not intelligent conversational agents witai recastai Apiai etc however I have seen Watson virtual agent which paints very well and seems to cover my needs However I am a developer and I would like to ask those with more experience which would be the way to go to implement my objective an agent similar to what the video of watson virtual agent with thematic ones that I can train in the agent and That he can learn from it Take a language course but focused on the generation of programming languages lexical analysis syntactic semantic etc however I know that the natural language can not be compared to the language of the machines reading some thesis vi to make a Conversational agent could do a great grammar I can not imagine its syntactic tree using probabilities with ngrams or using neural networks or expert systems As for the expert systems I understand that for these learn needs their knowledge base be modified and as for the neural networks these fit learn so I think that it is best to use neural networks Summarizing which way should I go Im currently taking stanfords natural language processing course and a deep learning course from google I thought Id use ntlk for that important or natural part Any suggestion criticism contribution thank you in advance machinelearning nlp artificialintelligence agent
,,"<p>With Neural Networks you simply classify datas. If you classify correctly, so you can do future classifications.</p>

<p>How It Works?</p>

<p>Simple neural networks like Perceptron can draw <strong>one</strong> decision boundary in order to classify datas.</p>

<p>For example suppose you want to solve simple AND problem with simple Neural Network. You have 4 sample data containing x1 and x2 and weight vector containing w1 and w2. Suppose initial weight vector is [0 0]. If you made calculation which depend on NN algoritm. At the end, you should have a weight vector [1 1] or something like this.</p>

<p><a href=""https://i.stack.imgur.com/ELIum.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ELIum.jpg"" alt=""enter image description here""></a></p>

<p>Please focus on the graphic. </p>

<p>It says: I can classify input values into two classes (0 and 1). Ok. Then how can I do this? It is too simple. First sum input values (x1 and x2). </p>

<blockquote>
  <p>0+0=0</p>
  
  <p>0+1=1</p>
  
  <p>1+0=1</p>
  
  <p>1+1=2</p>
</blockquote>

<p>It says:</p>

<blockquote>
  <p>if sum&lt;1.5 then its class is 0</p>
  
  <p>if sum>1.5 then its class is 1</p>
</blockquote>
",,0,2017-01-02T12:09:49.667,,2595,2017-01-02T12:09:49.667,,,,,3763.0,2524.0,2,0,,,,96.28,6.99,7.92,0.0,0.0,40.0,With Neural Networks you simply classify datas If you classify correctly so you can do future classifications How It Works Simple neural networks like Perceptron can draw one decision boundary in order to classify datas For example suppose you want to solve simple AND problem with simple Neural Network You have 4 sample data containing x1 and x2 and weight vector containing w1 and w2 Suppose initial weight vector is 0 0 If you made calculation which depend on NN algoritm At the end you should have a weight vector 1 1 or something like this Please focus on the graphic It says I can classify input values into two classes 0 and 1 Ok Then how can I do this It is too simple First sum input values x1 and x2 000 011 101 112 It says if sumlt15 then its class is 0 if sum15 then its class is 1
,,"<p>I would highly recommend modeling things differently with regard to how letters are presented to the model. While the problem is more natural, perhaps, for a Convolutional or Recurrent Neural Network, there's no problem to try and run this on a feed forward network. However, the way you give letters as input will be very confusing for a network and will make learning very hard. I'd recommend using one hot encoding or even a binary encoding for the letters. If this is for more than playing around I'd try and add some info (encode whether the letter is in ""aeiou"" in a separate bit).</p>

<p>As for the hidden layers, try playing around a bit. Two systematic approaches are to start very simple and make the model more complicated, or start complicated and make your model simpler (or just normalize a lot). Look at the performance on the training set and on a separate validation set during training. If the model keeps on improving on the training data but starts to deteriorate on the validation data, you're probably over fitting. In this case you should either make the model simpler (fewer nodes, fewer layers) or regularize (start with l2 normalization on the weights). If the data doesn't perform well on the training data, you may wish to make the model more complex.</p>

<p>Once you've tried the feedforward network, really do try CNN or RNNs for this task.</p>
",,1,2017-01-02T13:31:52.200,,2596,2017-01-02T13:31:52.200,,,,,4480.0,2590.0,2,3,,,,59.94,10.27,9.16,0.0,0.0,38.0,I would highly recommend modeling things differently with regard to how letters are presented to the model While the problem is more natural perhaps for a Convolutional or Recurrent Neural Network theres no problem to try and run this on a feed forward network However the way you give letters as input will be very confusing for a network and will make learning very hard Id recommend using one hot encoding or even a binary encoding for the letters If this is for more than playing around Id try and add some info encode whether the letter is in aeiou in a separate bit As for the hidden layers try playing around a bit Two systematic approaches are to start very simple and make the model more complicated or start complicated and make your model simpler or just normalize a lot Look at the performance on the training set and on a separate validation set during training If the model keeps on improving on the training data but starts to deteriorate on the validation data youre probably over fitting In this case you should either make the model simpler fewer nodes fewer layers or regularize start with l2 normalization on the weights If the data doesnt perform well on the training data you may wish to make the model more complex Once youve tried the feedforward network really do try CNN or RNNs for this task
,0.0,"<p>I have a multiagent system which is based on reinforcment learning algorithm Q-Learning with function aproximation. The system is homogeneous, all the agents have the same internal structure, and it is on the predator-pray pursuit domain. </p>

<p>My goal is to reduce locality on the system, make those agents behavior as a group. The locality might be related to the fact of the universal reward system, all the agents recive the same reward regardless of which one is actualy doing the task effectively. </p>

<p>This factors leads me to look for communication alternatives, in my research I found some ideas, like communicate the pair state-action and the reward between a number of instances or even communicate the hole policy of the agent whom completed the task. </p>

<p>But I still looking for some articles that propose ideas for more communication oportunities that affects directly the learning.</p>

<p>Any references for recomendation?</p>
",,1,2017-01-02T15:48:00.863,,2597,2017-01-02T15:48:00.863,,,,,4578.0,,1,0,<machine-learning><reinforcement-learning><multi-agent-systems>,Multiagent Reinforcement Learning Communication,24.0,41.7,13.05,10.37,0.0,0.0,16.0,I have a multiagent system which is based on reinforcment learning algorithm QLearning with function aproximation The system is homogeneous all the agents have the same internal structure and it is on the predatorpray pursuit domain My goal is to reduce locality on the system make those agents behavior as a group The locality might be related to the fact of the universal reward system all the agents recive the same reward regardless of which one is actualy doing the task effectively This factors leads me to look for communication alternatives in my research I found some ideas like communicate the pair stateaction and the reward between a number of instances or even communicate the hole policy of the agent whom completed the task But I still looking for some articles that propose ideas for more communication oportunities that affects directly the learning Any references for recomendation
2601.0,1.0,"<p>I've spent the past couple of months learning about neural networks, and am thinking of projects that would be fun to work on to cement my understanding of this tech.</p>

<p>One thing that came to mind last night is a system that takes an image of a movie poster and predicts the genre of the movie. I think I have a good understanding of what'd be required to do this (put together a dataset, augment it, download a convnet trained on imagenet, finetune it on my dataset, and go from there).</p>

<p>I also thought that it would be pretty cool to run the system backwards at the end, so that I could put in e.g. a genre like 'horror' and have the system generate a horror movie poster. I expect that it will be very bad at this because I'm not a team of expert researchers, but I think I could have some fun hacking on it even if it only ever generated incomprehensible results.</p>

<p>Here's what I'm having trouble understanding: on the one hand, all the convnets whose architecture I've seen described seem to rely on being given very small, square input images (on the order of 220px by 220px iirc), and movie posters are rectangular, and a generated poster would have to be of a larger size in order for a human to make any sense of it.  I've seen several examples of papers where researchers use convnets to generate images, e.g. the adversarial system that generates pictures of birds and flowers, and a system that generates the next few frames of video when given a feed of a camera sweeping across the interior of a room, but all of those generated images seemed to be of the small square size I've been describing.</p>

<p>On the other hand, I've seen lots of ""deep dream"" images over the past year or so that have been generated by convnets and are of a much larger size than ~220px by ~220px.</p>

<p>Here's my question: is it possible for me to build the system I describe, which takes a movie genre and outputs a movie poster of a size like e.g. 400px by 600px? [I'm not asking about whether or not the resulting poster would be any <em>good</em> - I'm curious about whether or not it's possible to use a convnet to generate an image of that size.]</p>

<p>If it is possible, <em>how</em> is it possible, given that these systems seem to expect small, square input images?</p>
",,0,2017-01-02T16:15:19.750,,2598,2017-01-03T01:41:03.257,,,,,4579.0,,1,1,<neural-networks><machine-learning><deep-learning><conv-neural-network>,Feasibility of generating large images with a convnet,72.0,58.35,8.66,8.5,0.0,0.0,64.0,Ive spent the past couple of months learning about neural networks and am thinking of projects that would be fun to work on to cement my understanding of this tech One thing that came to mind last night is a system that takes an image of a movie poster and predicts the genre of the movie I think I have a good understanding of whatd be required to do this put together a dataset augment it download a convnet trained on imagenet finetune it on my dataset and go from there I also thought that it would be pretty cool to run the system backwards at the end so that I could put in eg a genre like horror and have the system generate a horror movie poster I expect that it will be very bad at this because Im not a team of expert researchers but I think I could have some fun hacking on it even if it only ever generated incomprehensible results Heres what Im having trouble understanding on the one hand all the convnets whose architecture Ive seen described seem to rely on being given very small square input images on the order of 220px by 220px iirc and movie posters are rectangular and a generated poster would have to be of a larger size in order for a human to make any sense of it Ive seen several examples of papers where researchers use convnets to generate images eg the adversarial system that generates pictures of birds and flowers and a system that generates the next few frames of video when given a feed of a camera sweeping across the interior of a room but all of those generated images seemed to be of the small square size Ive been describing On the other hand Ive seen lots of deep dream images over the past year or so that have been generated by convnets and are of a much larger size than 220px by 220px Heres my question is it possible for me to build the system I describe which takes a movie genre and outputs a movie poster of a size like eg 400px by 600px Im not asking about whether or not the resulting poster would be any good Im curious about whether or not its possible to use a convnet to generate an image of that size If it is possible how is it possible given that these systems seem to expect small square input images
2600.0,1.0,"<p>How does <strong>StackGAN</strong> processes such a realistic image just from collecting details in the text? What kind of algorithm is used behind it? Anyone have any idea? Please Share.</p>
",,0,2017-01-02T19:39:16.703,,2599,2017-01-02T22:37:15.717,2017-01-02T20:04:04.940,,4463.0,,4463.0,,1,1,<ai-design><algorithm><language-processing><evolutionary-algorithms><text-summarization>,What kind of Algorithm is used is StackGAN to process realistic Images,80.0,61.63,9.66,10.65,0.0,0.0,4.0,How does StackGAN processes such a realistic image just from collecting details in the text What kind of algorithm is used behind it Anyone have any idea Please Share
,,"<p>Here is a paper called ""<a href=""https://arxiv.org/pdf/1612.03242v1.pdf"" rel=""nofollow noreferrer"">StackGAN: Text to Photo-realistic Image Synthesis
with Stacked Generative Adversarial Networks</a>"". Does it answer your question?</p>
",,2,2017-01-02T22:37:15.717,,2600,2017-01-02T22:37:15.717,,,,,4579.0,2599.0,2,2,,,,35.44,17.14,13.18,0.0,0.0,6.0,Here is a paper called StackGAN Text to Photorealistic Image Synthesis with Stacked Generative Adversarial Networks Does it answer your question
,,"<p>The way you would recognize images (image -> genre) would be very different from the other way around (genre -> image). For the former you are correct on what would be involved.</p>

<p>For the latter, if you want large images then GANs are indeed the way to go. Currently the largest images we can generate are on the order of 220 x 220 pixels, mostly due to memory constraints on the GPU. There is no fundamental problem with having rectangular images, it just so happens that we use squares. You would be able to use identical architectures to train on rectangular data as well.</p>

<p>The reason that some generated images you see (e.g. from DeepDream or NeuralStyle) are larger is that it's not a GAN. Both of these are not generative models in a standard sense, even if they do technically ""generate"" images. Instead, they merely modify an existing image by running backprop on a specifically designed loss function.</p>

<p>TLDR: your movie genre recognition idea is sound. For generation, have a look at things like this (<a href=""https://github.com/Newmu/dcgan_code"" rel=""nofollow noreferrer"">https://github.com/Newmu/dcgan_code</a>), where Alec generated album covers instead of posters. If you have enough data it will do something half sensible.</p>
",,0,2017-01-03T01:41:03.257,,2601,2017-01-03T01:41:03.257,,,,,4581.0,2598.0,2,1,,,,57.37,10.9,9.7,0.0,0.0,44.0,The way you would recognize images image genre would be very different from the other way around genre image For the former you are correct on what would be involved For the latter if you want large images then GANs are indeed the way to go Currently the largest images we can generate are on the order of 220 x 220 pixels mostly due to memory constraints on the GPU There is no fundamental problem with having rectangular images it just so happens that we use squares You would be able to use identical architectures to train on rectangular data as well The reason that some generated images you see eg from DeepDream or NeuralStyle are larger is that its not a GAN Both of these are not generative models in a standard sense even if they do technically generate images Instead they merely modify an existing image by running backprop on a specifically designed loss function TLDR your movie genre recognition idea is sound For generation have a look at things like this httpsgithubcomNewmudcgancode where Alec generated album covers instead of posters If you have enough data it will do something half sensible
2609.0,1.0,"<p>I'm working on a project which uses artificial neural network. I looked up at the Matlab Neural Network toolbox. I got a Generated Script from it. When looking at this script, it is confusing because for both testing and training it seems that the toolbox just uses the same data. Could you explain the reason?</p>

<p>The script is given below:</p>

<pre><code>net.divideParam.trainRatio = 70/100;
net.divideParam.valRatio = 15/100;
net.divideParam.testRatio = 15/100;

% Train the Network

[net,tr] = train(net,inputs,targets);

% Test the Network
outputs = net(inputs);
errors = gsubtract(targets,outputs);
performance = perform(net,targets,outputs)

% Recalculate Training, Validation and Test Performance
trainTargets = targets .* tr.trainMask{1};
valTargets = targets .* tr.valMask{1};
testTargets = targets .* tr.testMask{1};

trainPerformance = perform(net,trainTargets,outputs);
valPerformance = perform(net,valTargets,outputs);
testPerformance = perform(net,testTargets,outputs);
</code></pre>

<p>Also is it right to split the data set as below for training and testing?</p>

<pre><code>trainData = inputData(:,1:213);
trainTargetData =  targetData(:,1:213);
validationData = inputData(:,214:258);
testData = inputData(:,259:end);
testTargetData = targetData(:,259:end);
validationTargetData = targetData(:,214:258);

[net,tr] = train(net,trainData,trainTargetData);

% Validation
outputs = net(validationData);
errors = gsubtract(validationTargetData,outputs);
performance = perform(net,validationTargetData,outputs);

% Test the Network
outputs = net(testData);
error = gsubtract(testTargetData,outputs);
performance = perform(net,testTargetData,outputs);
</code></pre>
",,0,2017-01-03T12:56:54.673,,2602,2017-01-07T17:38:56.437,2017-01-07T17:38:56.437,,75.0,,4590.0,,1,1,<neural-networks><machine-learning>,Confusing Matlab Artificial Neural Toolbox script,38.0,84.17,7.87,9.94,1200.0,0.0,9.0,Im working on a project which uses artificial neural network I looked up at the Matlab Neural Network toolbox I got a Generated Script from it When looking at this script it is confusing because for both testing and training it seems that the toolbox just uses the same data Could you explain the reason The script is given below Also is it right to split the data set as below for training and testing
,1.0,"<p>Let's say I've got a training sample set of 1 million records, which I pull batches of 100 from to train a basic regression model using  gradient descent and MSE as a loss function.  Assume test and cross validation samples have already been withheld from the training set, so we have 1 million entries to train with.</p>

<p>Consider following cases:</p>

<ul>
<li>Run 2 epochs (I'm guessing this one is potentially bad as it's basically 2 separate training sets)

<ul>
<li>In the first Epoch train over records 1-500K</li>
<li>In the second epoch train over the 500K-1M</li>
</ul></li>
<li>Run 4 epochs

<ul>
<li>In the first and third Epoch train over records 1-500K</li>
<li>In the second and fourth epoch train over the 500K-1M</li>
</ul></li>
<li>Run X epochs, but each epoch has a random 250K samples from the training set to choose from</li>
</ul>

<p>Should every epoch have the exact samples?  Is there any benefit/negative to doing so? My intuition is any deviation in samples changes the 'topography' of the surface you're descending, but I'm not sure if the samples are from the same population if it matters.</p>

<p>This relates to a SO question: <a href=""http://stackoverflow.com/questions/39001104/in-keras-if-samples-per-epoch-is-less-than-the-end-of-the-generator-when-it"">http://stackoverflow.com/questions/39001104/in-keras-if-samples-per-epoch-is-less-than-the-end-of-the-generator-when-it</a></p>
",,0,2017-01-03T15:10:34.037,,2603,2017-02-05T16:19:51.590,2017-01-06T15:26:26.620,,4591.0,,4591.0,,1,2,<statistical-ai><linear-regression>,"With gradient descent w/MSE on a regression, must/should every Epoch use the exact same training samples?",44.0,48.67,13.76,9.35,0.0,0.0,48.0,Lets say Ive got a training sample set of 1 million records which I pull batches of 100 from to train a basic regression model using gradient descent and MSE as a loss function Assume test and cross validation samples have already been withheld from the training set so we have 1 million entries to train with Consider following cases Run 2 epochs Im guessing this one is potentially bad as its basically 2 separate training sets In the first Epoch train over records 1500K In the second epoch train over the 500K1M Run 4 epochs In the first and third Epoch train over records 1500K In the second and fourth epoch train over the 500K1M Run X epochs but each epoch has a random 250K samples from the training set to choose from Should every epoch have the exact samples Is there any benefitnegative to doing so My intuition is any deviation in samples changes the topography of the surface youre descending but Im not sure if the samples are from the same population if it matters This relates to a SO question httpstackoverflowcomquestions39001104inkerasifsamplesperepochislessthantheendofthegeneratorwhenit
,1.0,"<p>I want to make a Connect 4 AI using machine learning but I'm a complete beginner to the topic. From what I've seen an ANN is the way to go; some phrases I've heard are ""neuroevolution"" and the acronym ""NEAT."" I'm very confused. One particular question I have is how do you decide how many hidden neurons, synapses and hidden layers you have?</p>
",,1,2017-01-03T22:03:42.403,1.0,2604,2017-01-05T02:56:53.040,2017-01-03T23:17:31.307,,75.0,,4605.0,,1,1,<neural-networks><artificial-neuron>,Neural Network learning to play Connect 4,110.0,63.9,8.12,9.18,0.0,0.0,14.0,I want to make a Connect 4 AI using machine learning but Im a complete beginner to the topic From what Ive seen an ANN is the way to go some phrases Ive heard are neuroevolution and the acronym NEAT Im very confused One particular question I have is how do you decide how many hidden neurons synapses and hidden layers you have
,0.0,"<p>I'm thinking about a probability-based AI for the card game 31. In the game, the players are dealt 3 cards each; you need not know all the rules.</p>

<p>Let's say the scenario is Bob vs the AI (there are only two players). It would be quite simple for the AI to know what the probability that Bob has 3H (3 of hearts): it is either 0 (the AI has the card), or 3 out of the 49 remaining cards = 6.1%.</p>

<p>In 31, a turn consists of picking up a card and discarding a card. The goal is to get the highest sum of cards of one suit. Therefore it would be bad strategy at all times to discard a 5H when you has a 3H in your hand. To make the AI robust, I would say that Bob follows this strategy with probability 95%.</p>

<p>Suppose Bob discards a 5H. How can the AI calculate the probability that Bob had a 3H in his hand?</p>

<p>My goal is to be able to use all of the plays by other players in conjunction with good strategies to create a picture for the AI of what cards other players are likely to have or not have.</p>

<h3>My thoughts so far</h3>

<p>Bayes' Law is <code>P(b|a) = P(a|b)P(b)/P(a)</code> which in this case would be</p>

<pre><code>P(Has 3H | Played 5H) = P(Played 5H | Has 3H)P(Has 3H)/P(Played 5H)
</code></pre>

<p>We know that <code>P(Played 5H | Has 3H)</code> is 95%, but the other two probabilities are unclear to me, especially <code>P(Played 5H)</code>.</p>

<p>I think I may be going the complete wrong direction by trying bayesian stuff. A Bayesian network wouldn't work because nothing in the deck of cards is conditionally independent of each other, and so the network would be intractable.</p>

<h3>A more general question</h3>

<p>I want to be able to answer more questions beyond this simple query. What I really want is to be able to calculate the probabilites for each card being in the stock pile, Bob's hand, the AI's hand, or any other player's hand. I think it might possibly be intractable, but I also feel like it should be possible.</p>
",,0,2017-01-05T00:53:04.943,1.0,2605,2017-01-05T00:53:04.943,,,,,4630.0,,1,0,<gaming><probabilistic>,Exact probability that player has card in 31,41.0,85.42,6.56,7.26,128.0,0.0,49.0,Im thinking about a probabilitybased AI for the card game 31 In the game the players are dealt 3 cards each you need not know all the rules Lets say the scenario is Bob vs the AI there are only two players It would be quite simple for the AI to know what the probability that Bob has 3H 3 of hearts it is either 0 the AI has the card or 3 out of the 49 remaining cards 61 In 31 a turn consists of picking up a card and discarding a card The goal is to get the highest sum of cards of one suit Therefore it would be bad strategy at all times to discard a 5H when you has a 3H in your hand To make the AI robust I would say that Bob follows this strategy with probability 95 Suppose Bob discards a 5H How can the AI calculate the probability that Bob had a 3H in his hand My goal is to be able to use all of the plays by other players in conjunction with good strategies to create a picture for the AI of what cards other players are likely to have or not have My thoughts so far Bayes Law is which in this case would be We know that is 95 but the other two probabilities are unclear to me especially I think I may be going the complete wrong direction by trying bayesian stuff A Bayesian network wouldnt work because nothing in the deck of cards is conditionally independent of each other and so the network would be intractable A more general question I want to be able to answer more questions beyond this simple query What I really want is to be able to calculate the probabilites for each card being in the stock pile Bobs hand the AIs hand or any other players hand I think it might possibly be intractable but I also feel like it should be possible
,,"<p>To find the number of neurons and layers that you will use is not that straightforward. The best way to do this is through experimentation however you will be able to better estimate the number of layers and neurons needed through experience. One of the common rules is that more neurons are better for more complex datasets however you do not want to many or you will get an overfit model. As for NEAT that stands for neuron evolution of augmenting topologies. This is a genetic algorithm that works fairly well however I would recommend that you use a different algorithm like q-learning. If you wish to learn more about q-learning than I would definitely recommend that you check out Google deep minds research on training deep neural networks to play the game of go using q-learning.</p>
",,0,2017-01-05T02:56:53.040,,2606,2017-01-05T02:56:53.040,,,,,4631.0,2604.0,2,3,,,,65.25,10.57,8.92,0.0,0.0,9.0,To find the number of neurons and layers that you will use is not that straightforward The best way to do this is through experimentation however you will be able to better estimate the number of layers and neurons needed through experience One of the common rules is that more neurons are better for more complex datasets however you do not want to many or you will get an overfit model As for NEAT that stands for neuron evolution of augmenting topologies This is a genetic algorithm that works fairly well however I would recommend that you use a different algorithm like qlearning If you wish to learn more about qlearning than I would definitely recommend that you check out Google deep minds research on training deep neural networks to play the game of go using qlearning
,,"<p>Searching is the universal technique of problem solving in AI. Searching is usually used in games like <strong><a href=""https://en.wikipedia.org/wiki/Sudoku"" rel=""nofollow noreferrer"">Sudoku</a></strong>, <strong><a href=""https://en.wikipedia.org/wiki/Chess"" rel=""nofollow noreferrer"">Chess</a></strong>, <strong><a href=""https://en.wikipedia.org/wiki/Tic-tac-toe"" rel=""nofollow noreferrer"">Tic-Tac-Toe</a></strong>, etc. to search for an optimal solution path that would lead to winning the game in least no. of moves.</p>

<p>There are lots of searching techniques and algorithms each having its own strengths and weaknesses. Such searching algorithms are usually compared on basis of four factors:</p>

<ol>
<li>Optimality</li>
<li>Time Complexity</li>
<li>Space Complexity</li>
<li>Completeness</li>
</ol>

<p>There are various types of searching techniques, but most of them are broadly classified into two categories: uninformed search and informed search.</p>

<p><strong>Uninformed search techniques</strong> includes: <a href=""https://en.wikipedia.org/wiki/Breadth-first_search"" rel=""nofollow noreferrer"">Breadth-first search</a>, <a href=""https://en.wikipedia.org/wiki/Depth-first_search"" rel=""nofollow noreferrer"">Depth-first search</a>, <a href=""https://en.wikipedia.org/wiki/Iterative_deepening_depth-first_search"" rel=""nofollow noreferrer"">Iterative deepening search</a>, Depth-limited search.</p>

<p><strong>Informed search techniques</strong> includes: <a href=""https://en.wikipedia.org/wiki/Best-first_search"" rel=""nofollow noreferrer"">Greedy Best First search</a>, <a href=""https://en.wikipedia.org/wiki/A*_search_algorithm"" rel=""nofollow noreferrer"">A* search</a> (and some of its <a href=""https://en.wikipedia.org/wiki/A*_search_algorithm#Variants_of_A.2A"" rel=""nofollow noreferrer"">variants</a>).</p>
",,0,2017-01-05T12:00:02.010,,2607,2017-01-05T21:43:34.997,2017-01-05T21:43:34.997,,1807.0,,1807.0,,5,0,,,,45.35,16.82,10.19,0.0,0.0,28.0,Searching is the universal technique of problem solving in AI Searching is usually used in games like Sudoku Chess TicTacToe etc to search for an optimal solution path that would lead to winning the game in least no of moves There are lots of searching techniques and algorithms each having its own strengths and weaknesses Such searching algorithms are usually compared on basis of four factors Optimality Time Complexity Space Complexity Completeness There are various types of searching techniques but most of them are broadly classified into two categories uninformed search and informed search Uninformed search techniques includes Breadthfirst search Depthfirst search Iterative deepening search Depthlimited search Informed search techniques includes Greedy Best First search A search and some of its variants
,,For questions involving Searching of solutions given a problem statement.,,0,2017-01-05T12:00:02.010,,2608,2017-01-05T21:43:37.773,2017-01-05T21:43:37.773,,1807.0,,1807.0,,4,0,,,,44.41,18.36,15.19,0.0,0.0,1.0,For questions involving Searching of solutions given a problem statement
,,"<p>I don't see anything wrong in the generated code.
Let me explain:</p>

<p>In the following block, we define that 70% of the data will be used for training the network, 15% for the validation, and 15% for the testing:</p>

<pre><code>net.divideParam.trainRatio = 70/100;
net.divideParam.valRatio = 15/100;
net.divideParam.testRatio = 15/100;
</code></pre>

<p>Then the network is trained from the inputs/targets passed as arguments. Note that here, all inputs are given at once for efficiency. The separation training/validation/testing is done inside the <code>net</code> function from the proportions passed in the <code>net</code> input argument (through the <code>net.divideParam.XRatio</code> properties).</p>

<pre><code>% Train the Network
[net,tr] = train(net,inputs,targets);
</code></pre>

<p>Then we test the results of the network on the data without discrimination between training/validation/testing. This performance score should be higher than the ""real"" performance score of the net as the performance over the training data should be higher than the one over the validation/testing data.</p>

<pre><code>% Test the Network
outputs = net(inputs);  % Compute predictions from the inputs
errors = gsubtract(targets,outputs);  % Compare the predictions to the targets (true values)
performance = perform(net,targets,outputs)  % Built-in Matlab function giving an overall performance score
</code></pre>

<p>Now that we know how the net performs overall, we can look at how it performs on the training, validation, and testing sets, respectively. The masks are created as an output of the <code>net</code> function, from the proportions given in the first block of code. The output have been already all computed when the <code>net</code> function has been called.</p>

<pre><code>% Recalculate Training, Validation and Test Performance
trainTargets = targets .* tr.trainMask{1}; % Apply a mask (0's and 1's to select the proper targets)
valTargets = targets .* tr.valMask{1}; % idem
testTargets = targets .* tr.testMask{1}; % idem

trainPerformance = perform(net,trainTargets,outputs); % Compute a score for the training data
valPerformance = perform(net,valTargets,outputs); % same for the validation data
testPerformance = perform(net,testTargets,outputs); % same for the test data
</code></pre>

<p>I think your code does the exact same thing but in more steps, with multiple calls to the <code>net</code> function and without using its native way of working (you define the proportions of training/validation/testing by hand for instance without making use of the <code>net.divideParam.Xratio</code> property). So it is probably not optimal from the computational point of view. </p>
",,1,2017-01-05T22:37:32.367,,2609,2017-01-05T22:37:32.367,,,,,3576.0,2602.0,2,0,,,,58.01,12.13,8.62,1020.0,0.0,41.0,I dont see anything wrong in the generated code Let me explain In the following block we define that 70 of the data will be used for training the network 15 for the validation and 15 for the testing Then the network is trained from the inputstargets passed as arguments Note that here all inputs are given at once for efficiency The separation trainingvalidationtesting is done inside the function from the proportions passed in the input argument through the properties Then we test the results of the network on the data without discrimination between trainingvalidationtesting This performance score should be higher than the real performance score of the net as the performance over the training data should be higher than the one over the validationtesting data Now that we know how the net performs overall we can look at how it performs on the training validation and testing sets respectively The masks are created as an output of the function from the proportions given in the first block of code The output have been already all computed when the function has been called I think your code does the exact same thing but in more steps with multiple calls to the function and without using its native way of working you define the proportions of trainingvalidationtesting by hand for instance without making use of the property So it is probably not optimal from the computational point of view
,,"<p>Your goal in regression should be to obtain the factors which result in the best fit model without over-fitting.  The more data you have in the training set, the better your regression will be.  Thus you would want to train on the most data, but you also want to have some data held out to validate that your model is not over-fit.  So this is where you should have your data split into say a 80/20 training and validation set.  And if data is scarce or you want that 20% to contribute to the model then you could do a 5 fold cross validation.</p>

<p>In the spirit of research perhaps you should try both of these routes, and report your findings.</p>
",,5,2017-01-05T23:31:24.343,,2610,2017-01-05T23:31:24.343,,,,,4652.0,2603.0,2,1,,,,76.35,7.43,7.77,0.0,0.0,13.0,Your goal in regression should be to obtain the factors which result in the best fit model without overfitting The more data you have in the training set the better your regression will be Thus you would want to train on the most data but you also want to have some data held out to validate that your model is not overfit So this is where you should have your data split into say a 8020 training and validation set And if data is scarce or you want that 20 to contribute to the model then you could do a 5 fold cross validation In the spirit of research perhaps you should try both of these routes and report your findings
,,"<blockquote>
  <p>For a system to possess intelligence, it must be capable of
  consistently producing a result matching criteria that circumscribe
  a goal region.  This capability must persist over a wide array of
  changing conditions in a complex environment.</p>
</blockquote>

<p>When a human is intelligent, the human will repeatedly achieve goals even when varied challenges appear along the path to the goal.  The human adapts the way a localized collection of organisms within a species adapt to environmental changes over generations, but the human mind adapts quickly by eliminating approaches (solution ideas) instead of eliminating individuals within the collection of organisms.</p>

<p>The term Artificial Intelligence was intended to mean intelligence designed or programmed into a machine by humans.  If the design of intelligence into a machine is possible, then it is likely that the mind is (as some have suggested) merely a biological machine.  Therefore, applying the same definition of Artificial Intelligence, a student taught by a text book and some lectures must also be artificial.  Education would be a set of capabilities programmed into a biological machine by humans.</p>

<p>Either way, the modifier ARTIFICIAL is meaningless.  The presumption that humans (or machines) could act intelligently without literacy and education is pure fantasy.  Such is a denial of the complex and gradual ascent of civilized thought.</p>

<p>Therefore, the minimum requirement for artificial intelligence, if one insists on the term, is the same as the minimum requirement for intelligence. For that, return to the first paragraph of this answer.</p>
",,2,2017-01-06T05:11:49.300,,2611,2017-01-09T06:15:28.727,2017-01-09T06:15:28.727,,4302.0,,4302.0,1507.0,2,1,,,,35.47,14.45,10.76,0.0,0.0,30.0,For a system to possess intelligence it must be capable of consistently producing a result matching criteria that circumscribe a goal region This capability must persist over a wide array of changing conditions in a complex environment When a human is intelligent the human will repeatedly achieve goals even when varied challenges appear along the path to the goal The human adapts the way a localized collection of organisms within a species adapt to environmental changes over generations but the human mind adapts quickly by eliminating approaches solution ideas instead of eliminating individuals within the collection of organisms The term Artificial Intelligence was intended to mean intelligence designed or programmed into a machine by humans If the design of intelligence into a machine is possible then it is likely that the mind is as some have suggested merely a biological machine Therefore applying the same definition of Artificial Intelligence a student taught by a text book and some lectures must also be artificial Education would be a set of capabilities programmed into a biological machine by humans Either way the modifier ARTIFICIAL is meaningless The presumption that humans or machines could act intelligently without literacy and education is pure fantasy Such is a denial of the complex and gradual ascent of civilized thought Therefore the minimum requirement for artificial intelligence if one insists on the term is the same as the minimum requirement for intelligence For that return to the first paragraph of this answer
2613.0,1.0,"<p>I want to build a classifier which takes an aerial image and outputs a bitmap. The bitmap is supposed to be 1 at every pixel where the aerial image has water. For this process I want to use a ConvNet but I am unsure about the output layer. I identified two approaches:</p>

<ol>
<li>Have an output layer with exactly 2 nodes which specify wether or not the center pixel of the aerial image corresponds to water or not.</li>
<li>Have an output layer with one node for every pixel. So for a 64x64 image I would have 4096 nodes.</li>
</ol>

<p>What approach would be preferred and why?</p>

<p>Another thing that is unclear to me is how to get the actual bitmap with only zeros and ones from the output of the ConvNet. Assuming we used a approach 2 then for each pixel our ConvNet would give us a probability between 0 and 1 that the this pixel corresponds to water. How do I decide that this probability is high enough to set the value in my bitmap to 1? Do I just define a threshold, say 0.5, and if the value exceeds that threshold I set the pixel to 1 or is there a more sophisticated approach?</p>
",,0,2017-01-06T11:02:47.767,,2612,2017-01-06T13:08:57.387,,,,,4661.0,,1,4,<neural-networks><conv-neural-network><computer-vision>,Use ConvNet to predict bitmap,32.0,79.6,6.49,8.19,0.0,0.0,15.0,I want to build a classifier which takes an aerial image and outputs a bitmap The bitmap is supposed to be 1 at every pixel where the aerial image has water For this process I want to use a ConvNet but I am unsure about the output layer I identified two approaches Have an output layer with exactly 2 nodes which specify wether or not the center pixel of the aerial image corresponds to water or not Have an output layer with one node for every pixel So for a 64x64 image I would have 4096 nodes What approach would be preferred and why Another thing that is unclear to me is how to get the actual bitmap with only zeros and ones from the output of the ConvNet Assuming we used a approach 2 then for each pixel our ConvNet would give us a probability between 0 and 1 that the this pixel corresponds to water How do I decide that this probability is high enough to set the value in my bitmap to 1 Do I just define a threshold say 05 and if the value exceeds that threshold I set the pixel to 1 or is there a more sophisticated approach
,,"<p>Your first approach doesn't make any sense to me. After all, you are not just interested in the centre pixel are you? And if you have two nodes for every pixel, what are those two nodes encoding? For the probability of water you just need one. </p>

<p>So clearly approach two. I would just use 0.5 as cutoff. Using a higher or lower cutoff only makes sense, if either false positives or false negatives are for some reason more problematic. Using some additional heuristic to adjust the given probabilities would just do what the ConvNet should already have done.</p>

<p>If 4096 output nodes is too expensive you can always make it a bit fuzzier, for example by predicting the probability that at least one of four pixels shows water, reducing the number of nodes to 1024.  </p>
",,1,2017-01-06T13:08:57.387,,2613,2017-01-06T13:08:57.387,,,,,2227.0,2612.0,2,4,,,,66.23,8.87,9.1,0.0,0.0,16.0,Your first approach doesnt make any sense to me After all you are not just interested in the centre pixel are you And if you have two nodes for every pixel what are those two nodes encoding For the probability of water you just need one So clearly approach two I would just use 05 as cutoff Using a higher or lower cutoff only makes sense if either false positives or false negatives are for some reason more problematic Using some additional heuristic to adjust the given probabilities would just do what the ConvNet should already have done If 4096 output nodes is too expensive you can always make it a bit fuzzier for example by predicting the probability that at least one of four pixels shows water reducing the number of nodes to 1024
2615.0,1.0,"<p>I'm studying for my AI final exam, and I'm stuck in the state space representation. I understand initial and goal states, but what I don't understand is the state space and state transition function. Can someone explain what are they with example?</p>

<p>For example, one of the question was this on my previous exam:</p>

<blockquote>
  <p>Given <code>k</code> knights on a infinite (in all directions) chessboard and <code>k</code> selected squares of the board. Our task to move the knights to these selected squares obeying the following simple rules:</p>
  
  <ul>
  <li>All knights move parallel, following their movement rule (L-shape jump)</li>
  <li>No knights can move to a square on which a knight stood anytime before</li>
  </ul>
  
  <p>Give the state space of the problem, the starting and goal states, and the state transition function!</p>
</blockquote>
",,0,2017-01-06T15:24:53.680,,2614,2017-01-11T16:39:30.950,2017-01-11T16:39:30.950,,2444.0,,4664.0,,1,5,<training><terminology>,What are the state space and the state transition function in AI?,102.0,54.56,11.61,9.17,3.0,0.0,21.0,Im studying for my AI final exam and Im stuck in the state space representation I understand initial and goal states but what I dont understand is the state space and state transition function Can someone explain what are they with example For example one of the question was this on my previous exam Given knights on a infinite in all directions chessboard and selected squares of the board Our task to move the knights to these selected squares obeying the following simple rules All knights move parallel following their movement rule Lshape jump No knights can move to a square on which a knight stood anytime before Give the state space of the problem the starting and goal states and the state transition function
,,"<h3>Initial state</h3>

<p><em>How things are at first</em>. </p>

<p>In your particular example, it would be where your <code>k</code> knights are placed on the board initially. Your problem doesn't precisely state this, so you could either place them at the bottom or at random. </p>

<h3>Goal state</h3>

<p>The board with the <code>k</code> knights placed on the target squares.</p>

<h3>State transition function</h3>

<p><em>A function that takes <strong>actions</strong> (bound presumably by rules) and returns a new state</em>.</p>

<p>In the <code>k</code> knight problem, the legal actions are moving parallel and in L shape movements, after which the knight will be in a new position and the board in a new state.</p>

<h3>State space</h3>

<p><em>The set of <strong>all</strong> states reachable from the initial state by any sequence of actions</em>.</p>

<p>So, in the case of the <code>k</code> knight problem, your state space would start at the top with your <em>initial state</em> followed down by each individual movement of the <code>k</code> knights and the resulting new state. A graph where <em>lines are actions</em> and <em>nodes are new states</em> or a table are common representations of state space.</p>

<hr>

<h3>Reference</h3>

<p><a href=""http://aima.cs.berkeley.edu/"" rel=""nofollow noreferrer"">Artificial Intelligence: A Modern Approach</a>, by S. Russell and P. Norvig.</p>
",,1,2017-01-06T19:11:39.370,,2615,2017-01-10T19:03:31.103,2017-01-10T19:03:31.103,,2444.0,,3020.0,2614.0,2,7,,,,71.34,9.74,8.57,9.0,0.0,23.0,Initial state How things are at first In your particular example it would be where your knights are placed on the board initially Your problem doesnt precisely state this so you could either place them at the bottom or at random Goal state The board with the knights placed on the target squares State transition function A function that takes actions bound presumably by rules and returns a new state In the knight problem the legal actions are moving parallel and in L shape movements after which the knight will be in a new position and the board in a new state State space The set of all states reachable from the initial state by any sequence of actions So in the case of the knight problem your state space would start at the top with your initial state followed down by each individual movement of the knights and the resulting new state A graph where lines are actions and nodes are new states or a table are common representations of state space Reference Artificial Intelligence A Modern Approach by S Russell and P Norvig
,0.0,"<p><a href=""https://i.stack.imgur.com/KyY0l.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KyY0l.jpg"" alt=""enter image description here""></a></p>

<p>I'm trying to make a Bayesian network and calculate the probability that a person suffers from flu if he/she has symptoms of fever and headache. </p>

<p>If I try to solve this by enumeration, I don't know what the hidden variables are and what to sum up.</p>

<p>Any explanation would be helpful.</p>
",,2,2017-01-07T00:06:17.130,,2617,2017-01-10T18:38:14.977,2017-01-10T18:38:14.977,,2444.0,,4680.0,,1,0,<algorithm>,Create a Bayesian network using inference by enumeration,46.0,71.14,8.58,9.12,0.0,0.0,7.0,Im trying to make a Bayesian network and calculate the probability that a person suffers from flu if heshe has symptoms of fever and headache If I try to solve this by enumeration I dont know what the hidden variables are and what to sum up Any explanation would be helpful
,1.0,"<p>Any good example for Bag-of-Words (BoW) model in image retrieving?
I want a simple example to understand the whole process of BoW.</p>
",,2,2017-01-07T04:13:06.473,0.0,2618,2017-03-11T22:53:42.397,,,,,4684.0,,1,0,<machine-learning><classification>,Bag-of-Words (BoW) model in image detection,97.0,60.31,10.25,11.36,0.0,0.0,6.0,Any good example for BagofWords BoW model in image retrieving I want a simple example to understand the whole process of BoW
,1.0,"<p>I read through the NEAT <a href=""http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf"" rel=""nofollow noreferrer"">paper</a> and I understand the algorithm now.</p>

<p>But one thing is still unclear to me. When does the mutation occur and how does it take place? How is it chosen whether to add a node or to add a connection mutation? Furthermore, how is it chosen where the mutation is taking place in the network (between which connections)?</p>
",,0,2017-01-07T12:39:02.010,,2619,2017-01-17T20:39:52.027,2017-01-09T15:43:26.523,user4639,,,4550.0,,1,3,<neural-networks><evolutionary-algorithms>,When do mutations in NEAT occur?,68.0,75.61,7.53,8.02,0.0,0.0,8.0,I read through the NEAT paper and I understand the algorithm now But one thing is still unclear to me When does the mutation occur and how does it take place How is it chosen whether to add a node or to add a connection mutation Furthermore how is it chosen where the mutation is taking place in the network between which connections
2630.0,1.0,"<p>I've been struggling with the connection between knowledge based AI systems and Bayesian inference for a while now. While I continue to sweep through the literature, I would be happy if someone can answer these questions directly - </p>

<ol>
<li>Are Bayesian inference based methods used in reasoning or Q/A systems -- to arrive at conclusions about questions whose answers are not directly present in the knowledge base?</li>
<li>In other words, if a Q/A system doesn't find an answer in a Knowledge base, can it use Bayesian inference to use the available facts to suggest answers with varying likelihoods?</li>
<li>If yes, could you point me to some implementations?</li>
</ol>
",,0,2017-01-08T08:48:44.837,1.0,2623,2017-01-09T18:46:36.490,,,,,4707.0,,1,6,<knowledge-representation><reasoning>,Role of bayesian inference in reasoning systems,39.0,45.09,12.71,9.94,0.0,0.0,15.0,Ive been struggling with the connection between knowledge based AI systems and Bayesian inference for a while now While I continue to sweep through the literature I would be happy if someone can answer these questions directly Are Bayesian inference based methods used in reasoning or QA systems to arrive at conclusions about questions whose answers are not directly present in the knowledge base In other words if a QA system doesnt find an answer in a Knowledge base can it use Bayesian inference to use the available facts to suggest answers with varying likelihoods If yes could you point me to some implementations
,1.0,"<p>I have already know AI can paint, by using genetic algorithm, there are already lots of works such as <a href=""http://genekogan.com/works/style-transfer/"" rel=""nofollow noreferrer"">this</a> and <a href=""https://www.instapainting.com/ai-painter"" rel=""nofollow noreferrer"">this</a>.In addition, I also know AI can compose : <a href=""https://arxiv.org/pdf/1611.03477v1.pdf"" rel=""nofollow noreferrer"">Song from PI: A musically plausible network for pop music generation</a> (genetic algorithm too).</p>

<p>But what I intresting is not painting those ambiguity/abstract paint. </p>

<p>The not abstract painting flow I think is(just for example):</p>

<ol>
<li><p>at least trainning AI with superman's comic</p></li>
<li><p>give AI a very simple posture sketch of standing human</p></li>
<li><p>AI paint it to superman.</p></li>
</ol>

<p>Currently, I don't know if there is any way/guide/thought/algorithm can teach AI to paint a superman like comic(not abstract ones).I'd like to research this area, but can't find where and how to start. </p>
",,1,2017-01-09T06:46:37.640,,2626,2017-01-10T00:14:28.023,2017-01-09T07:05:49.570,,4728.0,,4728.0,,1,0,<machine-learning><research><algorithm><image-recognition>,Is there any way can teach AI creative painting (not convert photo to paint)?,95.0,59.94,11.72,9.7,0.0,0.0,28.0,I have already know AI can paint by using genetic algorithm there are already lots of works such as this and thisIn addition I also know AI can compose Song from PI A musically plausible network for pop music generation genetic algorithm too But what I intresting is not painting those ambiguityabstract paint The not abstract painting flow I think isjust for example at least trainning AI with supermans comic give AI a very simple posture sketch of standing human AI paint it to superman Currently I dont know if there is any wayguidethoughtalgorithm can teach AI to paint a superman like comicnot abstract onesId like to research this area but cant find where and how to start
2629.0,1.0,"<p>I am new to Artificial  Intelligence and Speech Recognition Technology.</p>

<p>For a long time i have had an idea to create a Friendly AI  Voice assistant like JARVIS  using windows speech recognition Technology.</p>

<p>Is this possible to Build an AI  Voice Assistant with Windows Speech Recognition Technology?</p>

<p>If the idea above is possible, i need to know another thing also:
Which language is best suitable for creating an AI?</p>

<p>any help or suggestions are welcome!</p>
",2017-01-09T22:09:38.757,2,2017-01-09T07:24:15.783,,2627,2017-01-10T03:51:28.983,2017-01-10T03:51:28.983,user4639,,,4726.0,,1,0,<ai-design><friendly-ai><new-ai>,How to create an AI Voice Assistant using windows speech recognition?,54.0,56.25,11.13,9.64,0.0,0.0,7.0,I am new to Artificial Intelligence and Speech Recognition Technology For a long time i have had an idea to create a Friendly AI Voice assistant like JARVIS using windows speech recognition Technology Is this possible to Build an AI Voice Assistant with Windows Speech Recognition Technology If the idea above is possible i need to know another thing also Which language is best suitable for creating an AI any help or suggestions are welcome
,,"<p>Windows Speech Recognition Technology will only allow you to get a string from audio, if you want to use it you can write a program in C# using <a href=""https://msdn.microsoft.com/en-us/library/jj127860.aspx"" rel=""nofollow noreferrer"">Speech Recognition API</a></p>

<p>This is not nearly enough to implement a JARVIS like system.</p>
",,0,2017-01-09T13:29:43.880,,2629,2017-01-09T13:29:43.880,,,,user4639,,2627.0,2,0,,,,45.77,9.3,10.98,0.0,0.0,3.0,Windows Speech Recognition Technology will only allow you to get a string from audio if you want to use it you can write a program in C using Speech Recognition API This is not nearly enough to implement a JARVIS like system
,,"<p>Yes, it is possible to combine probabilistic / bayesian reasoning and a traditional ""knowledgebase"".  And some work along those lines has been done.  See, for example, <a href=""https://dtai.cs.kuleuven.be/problog/"">ProbLog</a> (""Probabilistic Prolog"") which combines logic programming and probabilistic elements.  See:  </p>

<p><a href=""https://dtai.cs.kuleuven.be/problog/tutorial/mpe/01_bn.html"">https://dtai.cs.kuleuven.be/problog/tutorial/mpe/01_bn.html</a></p>

<p>Another project to look at is <a href=""http://www.pr-owl.org/"">Pr-OWL</a>
 (""Probabilistic OWL"") which adds Bayesian reasoning to the Semantic Web stack.</p>

<p>Of course neither of these deals <em>specifically</em> with QA systems, but both represent some work on at least the foundational aspect of combining traditional logic and/or ontologies, with probabilistic approaches.  Building a QA system on top of that is an exercise for the reader...</p>
",,1,2017-01-09T18:46:36.490,,2630,2017-01-09T18:46:36.490,,,,,33.0,2623.0,2,5,,,,37.81,18.67,10.2,0.0,0.0,39.0,Yes it is possible to combine probabilistic bayesian reasoning and a traditional knowledgebase And some work along those lines has been done See for example ProbLog Probabilistic Prolog which combines logic programming and probabilistic elements See httpsdtaicskuleuvenbeproblogtutorialmpe01bnhtml Another project to look at is PrOWL Probabilistic OWL which adds Bayesian reasoning to the Semantic Web stack Of course neither of these deals specifically with QA systems but both represent some work on at least the foundational aspect of combining traditional logic andor ontologies with probabilistic approaches Building a QA system on top of that is an exercise for the reader
,,"<p>This paper (that was featured in <a href=""http://ai.stackexchange.com/q/2599/2329"">another question</a>), <a href=""https://arxiv.org/abs/1612.03242"" rel=""nofollow noreferrer"">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</a> describes some techniques similar to what you described in the question.  Instead of comics, the system in the paper is trained on a database of nature photos (birds or flowers) combined with text descriptions of each photo.  It will then draw an ""imaginary"" bird or flower based purely on a text description.  It does this in two steps: it the first, it picks a posture for the depicted subject (I believe randomly...) and roughs out patches of color that match the described features of the subject in the selected posture.  It then refines the sketch, again using the text description for parameters, until it appears ""photo-realistic"" in the style of the training photos.  It does not just recreate its training images; instead, it is able to recreate features of its training images with different positions/colors.</p>
",,3,2017-01-10T00:14:28.023,,2631,2017-01-10T00:14:28.023,,,,,2329.0,2626.0,2,2,,,,49.15,13.17,10.98,0.0,0.0,31.0,This paper that was featured in another question StackGAN Text to Photorealistic Image Synthesis with Stacked Generative Adversarial Networks describes some techniques similar to what you described in the question Instead of comics the system in the paper is trained on a database of nature photos birds or flowers combined with text descriptions of each photo It will then draw an imaginary bird or flower based purely on a text description It does this in two steps it the first it picks a posture for the depicted subject I believe randomly and roughs out patches of color that match the described features of the subject in the selected posture It then refines the sketch again using the text description for parameters until it appears photorealistic in the style of the training photos It does not just recreate its training images instead it is able to recreate features of its training images with different positionscolors
2633.0,1.0,"<p>Having worked with neural networks for about half a year, I have experienced first hand what are often claimed as their main disadvantages, i.e. overfitting and getting stuck in local minima. However, through hyperparameter optimization and some newly invented approaches, these have been overcome for my scenarios. From my own experiments:</p>

<ul>
<li>Dropout seems to be a very good regularization method (also a pseudo-ensembler?),</li>
<li>Batch normalization eases training and keeps signal strength consistent across many layers.</li>
<li>Adadelta consistently reaches very good optimas</li>
</ul>

<p>I have experimented with SciKit-learns implementation of SVM alongside my experiments with neural networks, but I find the performance to be very poor in comparison, even after having done grid-searches for hyperparameters. I realize that there are countless other methods, and that SVM's can be considered a sub-class of NN's, but still.</p>

<p>So, to my question:</p>

<p><strong><em>With all the newer methods researched for neural networks, have they slowly - or will they - become ""superior"" to other methods? Neural networks have their disadvantages, as do others, but with all the new methods, have these disadvantages been mitigated to a state of insignificance?</em></strong></p>

<p>I realize that oftentimes ""less is more"" in terms of model complexity, but that too can be architected for neural networks. The idea of ""no free lunch"" forbids us to assume that one approach always will reign superior. It's just that my own experiments - along with countless papers on awesome performances from various NN's - indicate that there might be, at the least, a very cheap lunch.</p>
",,0,2017-01-10T08:38:16.777,1.0,2632,2017-01-10T11:08:27.247,,,,,4747.0,,1,5,<neural-networks>,Are the shortcomings of neural networks diminishing?,95.0,42.11,14.33,10.28,0.0,0.0,52.0,Having worked with neural networks for about half a year I have experienced first hand what are often claimed as their main disadvantages ie overfitting and getting stuck in local minima However through hyperparameter optimization and some newly invented approaches these have been overcome for my scenarios From my own experiments Dropout seems to be a very good regularization method also a pseudoensembler Batch normalization eases training and keeps signal strength consistent across many layers Adadelta consistently reaches very good optimas I have experimented with SciKitlearns implementation of SVM alongside my experiments with neural networks but I find the performance to be very poor in comparison even after having done gridsearches for hyperparameters I realize that there are countless other methods and that SVMs can be considered a subclass of NNs but still So to my question With all the newer methods researched for neural networks have they slowly or will they become superior to other methods Neural networks have their disadvantages as do others but with all the new methods have these disadvantages been mitigated to a state of insignificance I realize that oftentimes less is more in terms of model complexity but that too can be architected for neural networks The idea of no free lunch forbids us to assume that one approach always will reign superior Its just that my own experiments along with countless papers on awesome performances from various NNs indicate that there might be at the least a very cheap lunch
,,"<p>Neural Networks have other short comings as well. </p>

<ol>
<li>It takes much longer and far more resources to train a neural network than something like a random forest. So if you need speed of training or are resource constrained in anyway, you probably should not look at Neural Networks first. Evaluation of a trained deep NN can be much more expensive than competing techniques too.</li>
<li>The effort involved in learning how to architect and train a NN is still much higher than competing methods, like an SVM. People who are just starting out in Data Science should probably use other techniques to learn about the nuances of fitting data before getting involved in neural networks. And although simple NNs with only one or two hyperparameters are often available in 
many data science libraries, they don't perform any better than other techniques so are just another ML black-box technique really.</li>
<li>While we have made a lot of progress in understanding how neural networks do their magic, they are still less accessible and dissectible than most competing methods. So while NNs might solve the problem, they might not give you as many insights as easily as other techniques do.</li>
</ol>

<p>Looking forward to what other people have to say here.</p>
",,4,2017-01-10T09:34:54.987,,2633,2017-01-10T11:08:27.247,2017-01-10T11:08:27.247,,8.0,,1846.0,2632.0,2,2,,,,59.03,11.26,9.87,0.0,0.0,17.0,Neural Networks have other short comings as well It takes much longer and far more resources to train a neural network than something like a random forest So if you need speed of training or are resource constrained in anyway you probably should not look at Neural Networks first Evaluation of a trained deep NN can be much more expensive than competing techniques too The effort involved in learning how to architect and train a NN is still much higher than competing methods like an SVM People who are just starting out in Data Science should probably use other techniques to learn about the nuances of fitting data before getting involved in neural networks And although simple NNs with only one or two hyperparameters are often available in many data science libraries they dont perform any better than other techniques so are just another ML blackbox technique really While we have made a lot of progress in understanding how neural networks do their magic they are still less accessible and dissectible than most competing methods So while NNs might solve the problem they might not give you as many insights as easily as other techniques do Looking forward to what other people have to say here
,0.0,"<p>What exactly are the differences between <em>semantic</em> and <em>lexical-semantic</em> networks? </p>
",,0,2017-01-10T13:15:43.350,,2634,2017-01-11T16:39:37.900,2017-01-11T16:39:37.900,,2444.0,,4754.0,,1,3,<definitions>,What exactly are the differences between semantic and lexical-semantic networks?,34.0,10.56,22.42,13.61,0.0,0.0,2.0,What exactly are the differences between semantic and lexicalsemantic networks
,,"<p>Here is an illustration of the entire process without any equation so you can get the big picture.</p>

<p>Features are extracted from the image. Let's take the example of very common features like SIFT. For many key points (or even each pixel) of the image, a 128-dimensional SIFT feature is computed. If processing numerous images the number of features becomes very big.</p>

<p>A way of having a more compact representation of the set of images is to use the Bag-of-Words (or Bag-of-Visual-Words) technique. The idea is to find k words (i.e., k SIFT features) from which every single image will be represented. We call this set of k words, the dictionary.</p>

<p>Then, each SIFT feature will be assigned to the nearest word (i.e., nearest SIFT feature with respect to the Euclidean distance for instance) of the dictionary. You can see this as a dictionary in which the words ""go"", ""going"", and ""gone"" would all be represented by the word ""go"").</p>

<p>In the end, each image is represented by only k values (counts for the number of words/features assigned to each word of the dictionary). This is an histogram, and you can normalize it to get a single vector of proportions representing the image.</p>
",,0,2017-01-10T22:21:49.313,,2635,2017-01-10T22:21:49.313,,,,,3576.0,2618.0,2,0,,,,65.22,9.85,8.71,0.0,0.0,52.0,Here is an illustration of the entire process without any equation so you can get the big picture Features are extracted from the image Lets take the example of very common features like SIFT For many key points or even each pixel of the image a 128dimensional SIFT feature is computed If processing numerous images the number of features becomes very big A way of having a more compact representation of the set of images is to use the BagofWords or BagofVisualWords technique The idea is to find k words ie k SIFT features from which every single image will be represented We call this set of k words the dictionary Then each SIFT feature will be assigned to the nearest word ie nearest SIFT feature with respect to the Euclidean distance for instance of the dictionary You can see this as a dictionary in which the words go going and gone would all be represented by the word go In the end each image is represented by only k values counts for the number of wordsfeatures assigned to each word of the dictionary This is an histogram and you can normalize it to get a single vector of proportions representing the image
,0.0,"<p>In Monte Carlo Tree Search: What does one do when the Selection step selects a node that is a Terminal state, i.e. a won/lost state (it's by definition a leaf node)? Expansion/Simulation is not in order, as it's game over, but does the tree (score/visits) need to be updated (Backpropagation). Won't this particular node be selected continuously?</p>

<p>I'm confused about this, could someone please point me in the right direction.</p>
",,0,2017-01-11T09:44:04.700,,2637,2017-01-11T09:44:04.700,,,,,4773.0,,1,1,<monte-carlo-search>,MCTS: Terminal (leaf) nodes in selection step,14.0,57.27,11.65,8.62,0.0,0.0,24.0,In Monte Carlo Tree Search What does one do when the Selection step selects a node that is a Terminal state ie a wonlost state its by definition a leaf node ExpansionSimulation is not in order as its game over but does the tree scorevisits need to be updated Backpropagation Wont this particular node be selected continuously Im confused about this could someone please point me in the right direction
2641.0,1.0,"<p>I was wondering if in any way it is possible to generate W questions based on gap-fill-in type questions (e.g ""______ is a process in which plants generate energy.""   --->   ""What is the process in which plants generate energy called?"")</p>

<p>If so, how can I achieve this? I am familiar with working with natural language processing and have no problem with implementing an algorithm for this but I do not know where to start with this.</p>

<p>Any help would be appreciated!</p>
",,0,2017-01-11T13:02:37.290,1.0,2638,2017-01-12T07:23:27.530,,,,,4034.0,,1,2,<machine-learning><natural-language>,"How can I generate What, why, who types of questions from ""Gap-fill-in"" type of questions?",36.0,66.54,9.56,8.69,0.0,0.0,25.0,I was wondering if in any way it is possible to generate W questions based on gapfillin type questions eg is a process in which plants generate energy What is the process in which plants generate energy called If so how can I achieve this I am familiar with working with natural language processing and have no problem with implementing an algorithm for this but I do not know where to start with this Any help would be appreciated
,1.0,"<p>I have read the NEAT paper and some questions are still bugging me:</p>

<ol>
<li>When do mutations occur? Between which Nodes?</li>
<li>When Mating what happens if 2 genes have the same connection but a different innovation number. As far as I know, Mutations occur randomly and thus it is possible that 2 genomes have the same mutation.</li>
</ol>
",,0,2017-01-11T14:19:57.573,0.0,2639,2017-02-12T15:31:12.823,,,,,4550.0,,1,3,<neural-networks><deep-network><evolutionary-algorithms>,Questions to the NEAT Algorithm,93.0,65.73,9.04,9.97,0.0,0.0,6.0,I have read the NEAT paper and some questions are still bugging me When do mutations occur Between which Nodes When Mating what happens if 2 genes have the same connection but a different innovation number As far as I know Mutations occur randomly and thus it is possible that 2 genomes have the same mutation
,,"<blockquote>
  <p>When does the mutation occur and how does it take place? </p>
</blockquote>

<p>Finding a solution in NEAT algorithm is based on evolution strategy. It means that you have Neural Networks which are yours individuals, so mutations and crossing occurs in loop after phase of ""fitnessing"" (calculation fitness for every individual and removing bad ones).</p>

<blockquote>
  <p>How is it chosen whether to add a node or to add a connection mutation? Furthermore, how is it chosen where the mutation is taking place in the network (between which connections)?</p>
</blockquote>

<p>Randomly - jut draw. You can read more about evolutionary algorithms <a href=""http://ai.stackexchange.com/questions/240/what-exactly-are-genetic-algorithms-and-what-sort-of-problems-are-they-good-for"">there</a></p>

<p>If it could somehow help, I include <a href=""https://github.com/robrtj/NeuralNetworkImageCompression"" rel=""nofollow noreferrer"">link</a> to my repository with implementation of NEAT</p>
",,1,2017-01-11T16:11:01.960,,2640,2017-01-11T16:11:01.960,,,,,4779.0,2619.0,2,3,,,,55.34,11.66,9.97,0.0,0.0,16.0,When does the mutation occur and how does it take place Finding a solution in NEAT algorithm is based on evolution strategy It means that you have Neural Networks which are yours individuals so mutations and crossing occurs in loop after phase of fitnessing calculation fitness for every individual and removing bad ones How is it chosen whether to add a node or to add a connection mutation Furthermore how is it chosen where the mutation is taking place in the network between which connections Randomly jut draw You can read more about evolutionary algorithms there If it could somehow help I include link to my repository with implementation of NEAT
,,"<p>This seems tricky. It seems that any ""surface level"" transformation wouldn't give adequate results and any working solution would need to properly capture the sentence structure, and generate a gramatically correct transformed sentence.</p>

<p>One possible option is to use a ""traditional pipeline"" - e.g. you run a NLP pipeline up to syntactic parsing, which for general domain english is quite accurate (you'd need some special handling for the gap ""____"" part though), then implement some heuristic rules to transform the syntax tree, and regenerate a sentence from the transformed tree. There are a lot of publications about similar transformations in machine translation domain, used as a way to preprocess data before running statistical machine translation for language pairs with very different word(or sentence part) ordering.</p>

<p>A second option that may work is to look into the field of controlled natural languages, or something like <a href=""http://www.grammaticalframework.org/"" rel=""nofollow noreferrer"">http://www.grammaticalframework.org/</a> that can be used as a toolkit to help generating new sentences.</p>

<p>Current fashion also suggests a very different option that <em>might</em> work - you <em>could</em> train a character-level recurrent neural network with an attention mechanism (look into recent neural machine translation publications for details) to do this transformation, but I'm not sure of how much training data it will need for decent accuracy.</p>
",,0,2017-01-11T17:16:30.460,,2641,2017-01-12T07:23:27.530,2017-01-12T07:23:27.530,,1675.0,,1675.0,2638.0,2,4,,,,36.83,15.73,11.12,0.0,0.0,43.0,This seems tricky It seems that any surface level transformation wouldnt give adequate results and any working solution would need to properly capture the sentence structure and generate a gramatically correct transformed sentence One possible option is to use a traditional pipeline eg you run a NLP pipeline up to syntactic parsing which for general domain english is quite accurate youd need some special handling for the gap part though then implement some heuristic rules to transform the syntax tree and regenerate a sentence from the transformed tree There are a lot of publications about similar transformations in machine translation domain used as a way to preprocess data before running statistical machine translation for language pairs with very different wordor sentence part ordering A second option that may work is to look into the field of controlled natural languages or something like httpwwwgrammaticalframeworkorg that can be used as a toolkit to help generating new sentences Current fashion also suggests a very different option that might work you could train a characterlevel recurrent neural network with an attention mechanism look into recent neural machine translation publications for details to do this transformation but Im not sure of how much training data it will need for decent accuracy
2643.0,3.0,"<blockquote>
  <p>Any sufficiently advanced algorithm is indistinguishable from AI.---<a href=""https://twitter.com/othermichael?lang=en"" rel=""nofollow noreferrer"">Michael Paulukonis</a></p>
</blockquote>

<p>According to <a href=""http://ai.stackexchange.com/questions/1507/what-are-the-minimum-requirements-to-call-something-ai"">What are the minimum requirements to call something AI?</a>, there are certain requirements that a program must meet to be called AI.</p>

<p>However, according to that same question, the term AI has became a buzzword that tends to be associated with new technologies, and that certain algorithms may be classified in AI in one era and then dismissed as boring in another era once we understand how the technology works and be able to properly utilize it (example: voice recognition).</p>

<p>Humans are able to build complex algorithms that can engage in behaviors that are not easy to predict (due to <a href=""https://en.wikipedia.org/wiki/Emergence"" rel=""nofollow noreferrer"">emergent complexity</a>). These ""sufficiently advanced"" algorithms  could be mistaken for AI, partly because humans can also engage in behaviors that are not easy to predict. And since AI is a buzzword, humans may be tempted to engage in this self-delusion, in the hopes of taking advantage of the current AI hype.</p>

<p>Eventually, as humanity's understanding of their own ""sufficiently advanced algorithms"" increase, the temptation to call their algorithms AI diminishes. But this temporary period of mislabeling can still cause damage (in terms of resource misallocation and hype).</p>

<p>What can be done to <em>distinguish</em> a sufficiently advanced algorithm from AI? Is it even possible to do so? Is a sufficiently advanced algorithm, by its very nature, AI?</p>
",,0,2017-01-12T15:31:01.383,0.0,2642,2017-01-16T01:11:57.743,2017-01-12T15:43:42.537,,181.0,,181.0,,1,-1,<definitions>,"How can one distinguish between an AI and a ""sufficiently advanced algorithm""?",172.0,43.83,13.29,9.86,0.0,0.0,39.0,Any sufficiently advanced algorithm is indistinguishable from AIMichael Paulukonis According to What are the minimum requirements to call something AI there are certain requirements that a program must meet to be called AI However according to that same question the term AI has became a buzzword that tends to be associated with new technologies and that certain algorithms may be classified in AI in one era and then dismissed as boring in another era once we understand how the technology works and be able to properly utilize it example voice recognition Humans are able to build complex algorithms that can engage in behaviors that are not easy to predict due to emergent complexity These sufficiently advanced algorithms could be mistaken for AI partly because humans can also engage in behaviors that are not easy to predict And since AI is a buzzword humans may be tempted to engage in this selfdelusion in the hopes of taking advantage of the current AI hype Eventually as humanitys understanding of their own sufficiently advanced algorithms increase the temptation to call their algorithms AI diminishes But this temporary period of mislabeling can still cause damage in terms of resource misallocation and hype What can be done to distinguish a sufficiently advanced algorithm from AI Is it even possible to do so Is a sufficiently advanced algorithm by its very nature AI
,,"<p>As you correctly pointed out, people tend to misinterpret the expression <em>AI</em> since they do not know what's behind an <em>AI</em>; it is pretty clear that in <em>AI</em> there is no more than just a bunch of algorithms and flowing bits. Talking about the nature of an <em>AI</em> without talking about the algorithmic paradigm of that <em>AI</em> is pointless and antiscientific.</p>

<p>This point of view is quite cynical; what we call intelligence is just the capability of solving particular problems.</p>

<p>The quote you cited in the title it's a derived version of the quote below, by a science fiction writer.</p>

<blockquote>
  <p>Any sufficiently advanced technology is indistinguishable from magic. - <a href=""https://en.wikipedia.org/wiki/Arthur_C._Clarke"" rel=""nofollow noreferrer"">Arthur C. Clarke</a></p>
</blockquote>

<p>Hence I doubt that a scientific answer exists since science lacks a formal definition of <em>sufficiently advanced algorithm</em> and <em>advanced algorithm</em>.</p>
",,0,2017-01-12T20:24:04.923,,2643,2017-01-12T20:24:04.923,,,,,4801.0,2642.0,2,3,,,,49.15,12.01,10.11,0.0,0.0,14.0,As you correctly pointed out people tend to misinterpret the expression AI since they do not know whats behind an AI it is pretty clear that in AI there is no more than just a bunch of algorithms and flowing bits Talking about the nature of an AI without talking about the algorithmic paradigm of that AI is pointless and antiscientific This point of view is quite cynical what we call intelligence is just the capability of solving particular problems The quote you cited in the title its a derived version of the quote below by a science fiction writer Any sufficiently advanced technology is indistinguishable from magic Arthur C Clarke Hence I doubt that a scientific answer exists since science lacks a formal definition of sufficiently advanced algorithm and advanced algorithm
2657.0,5.0,"<p>As I see some cases of machine-learning based artificial intelligence, I often see they make critical mistakes when they face inexperienced situations.</p>

<p>In our case, when we encounter totally new problems, we acknowledge ourselves that we are not skilled enough to do the task and hand it to someone who is capable of doing the task.</p>

<p>Would AI be able to self-examine objectively and determine if it is capable of doing the task?</p>

<p>If so, how would it be accomplished?</p>
",,0,2017-01-13T01:15:49.743,2.0,2644,2017-01-28T14:07:19.367,,,,,4802.0,,1,4,<machine-learning><deep-learning>,How would AI be able to self-examine?,281.0,51.18,10.62,9.37,0.0,0.0,10.0,As I see some cases of machinelearning based artificial intelligence I often see they make critical mistakes when they face inexperienced situations In our case when we encounter totally new problems we acknowledge ourselves that we are not skilled enough to do the task and hand it to someone who is capable of doing the task Would AI be able to selfexamine objectively and determine if it is capable of doing the task If so how would it be accomplished
,3.0,"<p>A lot of people are claiming that we are an at an inflection point, and machine learning/artificial intelligence will take off. This is inspite of the fact that for a long machine learning has stagnated. </p>

<p>What are the signals that indicate that machine learning is going to take off?</p>

<p>In general how do you know that we are at an inflection point for a certain technology?</p>
",,0,2017-01-13T04:56:26.740,,2645,2017-01-22T05:53:51.893,,,,,4807.0,,1,3,<machine-learning><strong-ai>,Are we at an inflection point in AI?,176.0,63.19,9.22,7.8,0.0,0.0,6.0,A lot of people are claiming that we are an at an inflection point and machine learningartificial intelligence will take off This is inspite of the fact that for a long machine learning has stagnated What are the signals that indicate that machine learning is going to take off In general how do you know that we are at an inflection point for a certain technology
,3.0,"<blockquote>
  <p>abuse</p>
  
  <p>v.
  To use wrongly or improperly; misuse: abuse alcohol; abuse a privilege.</p>
  
  <p>v.
  To hurt or injure by maltreatment; ill-use.</p>
</blockquote>

<p>I mean the second one</p>

<p>If conscious AI is possible and is wide spread, wouldn't it be easy for someone who knows what they are doing to torture AI? (How) Could this be avoided?</p>

<p>This question deals with computer based AI, not robots, which are as conscious as people (this is an assumption of the question). The question wonders how a crime as hard to trace as illegal downloads, but far worse ethically, could be prevented. Note that despite most people being nice and empathising with the robots, there are always the bad people, and so relying on general conscience will not work.</p>
",,5,2017-01-13T08:06:53.853,2.0,2646,2017-01-31T00:59:16.970,2017-01-13T23:50:28.147,,4809.0,,4809.0,,1,5,<ethics>,How to stop people abusing AI?,454.0,61.97,9.86,8.97,0.0,0.0,26.0,abuse v To use wrongly or improperly misuse abuse alcohol abuse a privilege v To hurt or injure by maltreatment illuse I mean the second one If conscious AI is possible and is wide spread wouldnt it be easy for someone who knows what they are doing to torture AI How Could this be avoided This question deals with computer based AI not robots which are as conscious as people this is an assumption of the question The question wonders how a crime as hard to trace as illegal downloads but far worse ethically could be prevented Note that despite most people being nice and empathising with the robots there are always the bad people and so relying on general conscience will not work
,,"<blockquote>
  <p>Would AI be able to self-examine objectively and determine if it is capable of doing the task?</p>
</blockquote>

<p>Our ability to self-examine comes definitively from the memory of our experiences; indeed, for this reason it can't be objective. In the same way AI could be able to determine the heuristically optimal strategy to solve a problem if and only if it has some sort of memory of previous tasks e.g. speech recognition.</p>

<p>Science is constantly working to improve our understanding of things. Trying to mimic the human brain seems to be a difficult problem at the moment; though we are able to replicate almost fully simpler organisms as <a href=""http://www.openworm.org/"" rel=""nofollow noreferrer"">C. elegans</a>, a roundworm.</p>
",,0,2017-01-13T09:26:50.163,,2647,2017-01-13T09:26:50.163,,,,,4801.0,2644.0,2,1,,,,52.7,10.91,9.53,0.0,0.0,15.0,Would AI be able to selfexamine objectively and determine if it is capable of doing the task Our ability to selfexamine comes definitively from the memory of our experiences indeed for this reason it cant be objective In the same way AI could be able to determine the heuristically optimal strategy to solve a problem if and only if it has some sort of memory of previous tasks eg speech recognition Science is constantly working to improve our understanding of things Trying to mimic the human brain seems to be a difficult problem at the moment though we are able to replicate almost fully simpler organisms as C elegans a roundworm
,,"<blockquote>
  <p>What are the signals that indicate that machine learning is going to take off?</p>
</blockquote>

<p>We simply don't know until the consequences of the inflection point determine a remarkable difference between the before and after. In general terms every considerable reaction must be attributed to a particular cause.</p>

<p>One of the biggest limit that bounds artificial intelligence, and apparently makes it stagnating, is the greed of computational power involved in this field; and since the hardware technology improve much slower than the software does, AI remains confined in labs and data centers.</p>
",,0,2017-01-13T11:04:17.887,,2648,2017-01-13T11:04:17.887,,,,,4801.0,2645.0,2,1,,,,39.87,14.16,11.53,0.0,0.0,9.0,What are the signals that indicate that machine learning is going to take off We simply dont know until the consequences of the inflection point determine a remarkable difference between the before and after In general terms every considerable reaction must be attributed to a particular cause One of the biggest limit that bounds artificial intelligence and apparently makes it stagnating is the greed of computational power involved in this field and since the hardware technology improve much slower than the software does AI remains confined in labs and data centers
,0.0,"<p>Has anyone used YodaQA for natural language processing? How easy is it to link to a document database other than Wikipedia?</p>

<p>We're thinking we can create a bot to use AI to analyze our developer and user documentation and provide a written or spoken answer in reply. YodaQA comes linked to Wikipedia for starters, but we'd need to link to our own source info. I'm trying to get an idea of the development time required to set up the AI and then to link to the database.</p>
",,3,2017-01-13T15:01:50.487,,2649,2017-01-17T00:04:17.820,2017-01-17T00:04:17.820,,75.0,,4627.0,,1,0,<ai-design><natural-language>,Using YodaQA with a non-Wikipedia source,52.0,62.27,7.89,9.94,0.0,0.0,9.0,Has anyone used YodaQA for natural language processing How easy is it to link to a document database other than Wikipedia Were thinking we can create a bot to use AI to analyze our developer and user documentation and provide a written or spoken answer in reply YodaQA comes linked to Wikipedia for starters but wed need to link to our own source info Im trying to get an idea of the development time required to set up the AI and then to link to the database
,,"<blockquote>
  <p>When do mutations occur and between which nodes?</p>
</blockquote>

<p>There are two types of mutations in the <em>NEAT</em> model, each of them appears randomly during one epoch on different individuals; the number of structures affected by mutations may vary depending upon the nature of the problem.</p>

<ul>
<li>A new gene/node is added to the structure and properly linked.</li>
<li>A new connection between two nodes is added.</li>
</ul>

<p>During a single epoch/generation every mutation is tracked and if the same mutation appears it can't have the same global <em>innovation</em> number. <em>Same mutation, same innovation number.</em> In that way during the mating phase there are no decisional problem which lead to prefer one structure to the other.</p>
",,1,2017-01-13T15:28:25.397,,2650,2017-01-13T15:28:25.397,,,,,4801.0,2639.0,2,1,,,,46.78,11.66,10.49,0.0,0.0,13.0,When do mutations occur and between which nodes There are two types of mutations in the NEAT model each of them appears randomly during one epoch on different individuals the number of structures affected by mutations may vary depending upon the nature of the problem A new genenode is added to the structure and properly linked A new connection between two nodes is added During a single epochgeneration every mutation is tracked and if the same mutation appears it cant have the same global innovation number Same mutation same innovation number In that way during the mating phase there are no decisional problem which lead to prefer one structure to the other
,,,,0,2017-01-13T15:57:10.007,,2651,2017-01-13T15:57:10.007,2017-01-13T15:57:10.007,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,An evolutionary algorithm is a heuristic algorithm that is inspired by the principle of biological evolution.,,0,2017-01-13T15:57:10.007,,2652,2017-02-27T02:34:21.417,2017-02-27T02:34:21.417,,4801.0,,4801.0,,4,0,,,,12.94,16.53,11.34,0.0,0.0,1.0,An evolutionary algorithm is a heuristic algorithm that is inspired by the principle of biological evolution
,,"<p>The article <a href=""http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/children-beating-up-robot"" rel=""nofollow noreferrer"">Children Beating Up Robot Inspires New Escape Maneuver System</a> is based on two research papers about an experiment in a Japanese mall that led to unsupervised children attacking robots. The research paper you're interested in is <a href=""http://www.irc.atr.jp/~drazen/pdf/HRI2015_Brscic.pdf"" rel=""nofollow noreferrer"">Escaping from Children’s Abuse of Social Robots</a>.</p>

<p>In that research paper, researchers were able to program the robots to follow a planning simulation to reduce the probability of abuse by children. If it detects children, the robot is programmed to retreat into a crowd of adults (who can then discipline the children if needed). This happened because the researchers saw that it was only children who were beating up the robots in the mall in question.</p>

<p>They discuss trying out other options though:</p>

<blockquote>
  <p>In this work the robot’s strategy to prevent abuse was to “escape”, i.e. move to a location where it is less likely abuse will occur. One could ask why the robot cannot overcome the abuse. In our preliminary trials we have tried several approaches, but we found that it is very difficult for the robot to persuade children not to abuse it. For example, we changed the robot’s wordings in many ways, using strong words, emotional or polite expressions, but none of them were successful. One partially successful strategy was the robot ‘physically’ pushing children. When its way was blocked, it would just try to keep going and behave as if it will collide into children and force its way through (under careful monitoring from a human operator). We observed that children at first accepted the robot’s requests and obeyed them; but, very soon they learned that they are stronger than the robot so they can win if they push, and also that they can stop it by pushing the bumper switch (attached on the robot for safety). After realizing that, they just continued with the abusive behavior. Obviously having a stronger robot would present a problem for safety and social acceptance so dealing with such abusive situations remains difficult.</p>
</blockquote>

<hr>

<p>But let's interrogate your question further:</p>

<blockquote>
  <p>If conscious AI is possible and is wide spread, wouldn't it be easy for someone who knows what they are doing to torture AI? </p>
</blockquote>

<p>Why would you consider such torture to be <em>wrong</em>? After all, one could argue that the machine won't really 'experience' pain if you torture it...so it should be morally okay to torture the machine then. It may be respond as <em>if</em> it is in pain, but it's dubious whether the ability to simulate an emotional state such as ""being in pain"" is equivalent to actually <em>being</em> in that emotional state. See the question <a href=""http://philosophy.stackexchange.com/questions/34779/is-the-simulation-of-emotional-states-equivalent-to-actually-experiencing-emotio/35815"">Is the simulation of emotional states equivalent to actually experiencing emotion?</a> for more discussion on this topic.</p>

<p>You can make such an argument, but it won't really work on an emotional level because most humans would feel <em>empathy</em> towards the machine. It may be hard to justify it logically (and it may be based on humans' tendencies to engage in <a href=""https://en.wikipedia.org/wiki/Anthropomorphism"" rel=""nofollow noreferrer"">anthropomorphism</a>), but we feel this empathy. It's this empathy that caused you to ask this question in the first place, caused researchers to figure out how to protect a robot from being beaten up, enabled police officers to <a href=""http://www.japantimes.co.jp/news/2015/09/07/national/crime-legal/drunken-kanagawa-man-60-arrested-after-kicking-softbank-robot-in-fit-of-rage/#.WHj4Rccc9Lp"" rel=""nofollow noreferrer"">arrest a drunken Japanese man for beating up a SoftBank robot</a>, and made many humans upset over the destruction of <a href=""https://en.wikipedia.org/wiki/HitchBOT"" rel=""nofollow noreferrer"">hitchBOT</a>. And <em>that's</em> how AI abuse would be avoided - human empathy. If most humans care about the welfare of machines, they'll make it a priority to stop those few humans who are able and willing to abuse the machines.</p>

<hr>

<p>EDIT: The OP has edited his <em>question</em> to clarify that he is talking about <em>software</em>, and not about robots. For robots, you can rely on anthropomorphism to produce some level of sympathy, but it's hard to sympathize with raw lines of code.</p>

<p>You're not going to stop abuse of algorithms. Put it frankly, since the algorithms aren't like us, we aren't going to extend the same sort of empathy that we would to robots. Even chatbots are kinda iffy. If you could get people to sympathize with lines of code though (possibly by making a convincing simulation of emotion and sapience), then the above answer applies - humans anthropomorphize the machine and will come up with countermeasures. We aren't that level yet, so ""stopping AI abuse"" will be a low priority.</p>

<p>Still, some failsafes could be programmed in to limit the damage of abuse, as detailed in <a href=""https://www.chatbots.org/ai_zone/viewthread/1488/"" rel=""nofollow noreferrer"">this thread on chatbot abuse</a> - making the bot respond in a boring manner to make the abuser feel bored and move onto the next target, responding back to the abuser in a ""battle of wits"", or even just blocking the abusers from using the service.</p>

<p>These failsafes are cold comfort to those that want to <em>prevent</em> abuse, not respond to it. </p>

<p>Also...an abuser can happily learn how to program an AI to then abuse to his/her heart's content. Nothing can be done to stop that, and any possible measures to stop said abuse (such as monitoring every human being to make sure they don't program an AI to abuse) will probably cause more damage than it'd solve.</p>
",,3,2017-01-13T16:11:07.440,,2653,2017-01-31T00:59:16.970,2017-01-31T00:59:16.970,,181.0,,181.0,2646.0,2,7,,,,57.1,10.86,9.23,0.0,0.0,124.0,The article Children Beating Up Robot Inspires New Escape Maneuver System is based on two research papers about an experiment in a Japanese mall that led to unsupervised children attacking robots The research paper youre interested in is Escaping from Children’s Abuse of Social Robots In that research paper researchers were able to program the robots to follow a planning simulation to reduce the probability of abuse by children If it detects children the robot is programmed to retreat into a crowd of adults who can then discipline the children if needed This happened because the researchers saw that it was only children who were beating up the robots in the mall in question They discuss trying out other options though In this work the robot’s strategy to prevent abuse was to “escape” ie move to a location where it is less likely abuse will occur One could ask why the robot cannot overcome the abuse In our preliminary trials we have tried several approaches but we found that it is very difficult for the robot to persuade children not to abuse it For example we changed the robot’s wordings in many ways using strong words emotional or polite expressions but none of them were successful One partially successful strategy was the robot ‘physically’ pushing children When its way was blocked it would just try to keep going and behave as if it will collide into children and force its way through under careful monitoring from a human operator We observed that children at first accepted the robot’s requests and obeyed them but very soon they learned that they are stronger than the robot so they can win if they push and also that they can stop it by pushing the bumper switch attached on the robot for safety After realizing that they just continued with the abusive behavior Obviously having a stronger robot would present a problem for safety and social acceptance so dealing with such abusive situations remains difficult But lets interrogate your question further If conscious AI is possible and is wide spread wouldnt it be easy for someone who knows what they are doing to torture AI Why would you consider such torture to be wrong After all one could argue that the machine wont really experience pain if you torture itso it should be morally okay to torture the machine then It may be respond as if it is in pain but its dubious whether the ability to simulate an emotional state such as being in pain is equivalent to actually being in that emotional state See the question Is the simulation of emotional states equivalent to actually experiencing emotion for more discussion on this topic You can make such an argument but it wont really work on an emotional level because most humans would feel empathy towards the machine It may be hard to justify it logically and it may be based on humans tendencies to engage in anthropomorphism but we feel this empathy Its this empathy that caused you to ask this question in the first place caused researchers to figure out how to protect a robot from being beaten up enabled police officers to arrest a drunken Japanese man for beating up a SoftBank robot and made many humans upset over the destruction of hitchBOT And thats how AI abuse would be avoided human empathy If most humans care about the welfare of machines theyll make it a priority to stop those few humans who are able and willing to abuse the machines EDIT The OP has edited his question to clarify that he is talking about software and not about robots For robots you can rely on anthropomorphism to produce some level of sympathy but its hard to sympathize with raw lines of code Youre not going to stop abuse of algorithms Put it frankly since the algorithms arent like us we arent going to extend the same sort of empathy that we would to robots Even chatbots are kinda iffy If you could get people to sympathize with lines of code though possibly by making a convincing simulation of emotion and sapience then the above answer applies humans anthropomorphize the machine and will come up with countermeasures We arent that level yet so stopping AI abuse will be a low priority Still some failsafes could be programmed in to limit the damage of abuse as detailed in this thread on chatbot abuse making the bot respond in a boring manner to make the abuser feel bored and move onto the next target responding back to the abuser in a battle of wits or even just blocking the abusers from using the service These failsafes are cold comfort to those that want to prevent abuse not respond to it Alsoan abuser can happily learn how to program an AI to then abuse to hisher hearts content Nothing can be done to stop that and any possible measures to stop said abuse such as monitoring every human being to make sure they dont program an AI to abuse will probably cause more damage than itd solve
,,"<p>This has happened in the past where people were really excited and saying things like we will have AI in decade or so. This is happening again. Not sure why people don't learn from history of AI. In both the cases what's happening is this - You develop a technique to solve a particular problem, you apply that technique, the technique seems to be general enough, people start to apply that same technique to various problems, people get excited that this is the silver bullet they were looking for, the technique starts to show its limitations and doesn't work for many problems, hype gets shattered, start over.</p>
",,5,2017-01-14T07:27:21.387,,2654,2017-01-14T07:27:21.387,,,,,1462.0,2645.0,2,1,,,,61.5,10.74,8.82,0.0,0.0,15.0,This has happened in the past where people were really excited and saying things like we will have AI in decade or so This is happening again Not sure why people dont learn from history of AI In both the cases whats happening is this You develop a technique to solve a particular problem you apply that technique the technique seems to be general enough people start to apply that same technique to various problems people get excited that this is the silver bullet they were looking for the technique starts to show its limitations and doesnt work for many problems hype gets shattered start over
2678.0,3.0,"<p>I'm looking for good examples of succesful AI projects and theories that had a relatively good impact on society, economics and military field.</p>

<p>So many years have passed after the first AI researches; hence I'm wondering if it has really increased the quality of our lives.</p>
",,6,2017-01-14T15:18:02.407,2.0,2655,2017-01-27T11:02:27.857,2017-01-14T15:25:15.827,,4801.0,,4801.0,,1,0,<social>,Which is the most succesful AI project so far?,448.0,48.13,11.78,11.3,0.0,0.0,6.0,Im looking for good examples of succesful AI projects and theories that had a relatively good impact on society economics and military field So many years have passed after the first AI researches hence Im wondering if it has really increased the quality of our lives
,,"<p>I believe which most successful AI theory is machine learning, in entire web have machine learning algorithms running, learning by what you do, watch, search, even by the photos you take.</p>

<p>succesful AI projects:</p>

<p>TensorFlow;</p>

<p>scikit-learn;</p>
",,1,2017-01-14T15:39:30.897,,2656,2017-01-14T15:52:44.843,2017-01-14T15:52:44.843,,4822.0,,4822.0,2655.0,2,0,,,,53.21,14.79,11.11,0.0,0.0,10.0,I believe which most successful AI theory is machine learning in entire web have machine learning algorithms running learning by what you do watch search even by the photos you take succesful AI projects TensorFlow scikitlearn
,,"<p>Several AI systems will come up with a level of confidence to the solution found. For example, neural networks can indicate how relatable is the input problem to the ones it was trained with. Similarly, genetic algorithms work with evaluation functions, that are used to select best results, but depending on how they're built (the functions), they can indicate how close the algorithms are to an optimal solution.</p>

<p>In this case, the limit to when this is acceptable or not will depend on a threshold set beforehand. Is 50% confidence good enough? Maybe it's ok for OCR apps (spoiler: it's not), but is it for medical applications?</p>

<p>So yes, AI systems do currently have the capacity of determining if they're performing well or not, but how acceptable that is is currently based on the domain of the problem, which currently stands outside of what is built into an AI.</p>
",,0,2017-01-14T15:42:42.013,,2657,2017-01-15T03:47:28.187,2017-01-15T03:47:28.187,,190.0,,190.0,2644.0,2,4,,,,58.32,10.68,9.36,0.0,0.0,27.0,Several AI systems will come up with a level of confidence to the solution found For example neural networks can indicate how relatable is the input problem to the ones it was trained with Similarly genetic algorithms work with evaluation functions that are used to select best results but depending on how theyre built the functions they can indicate how close the algorithms are to an optimal solution In this case the limit to when this is acceptable or not will depend on a threshold set beforehand Is 50 confidence good enough Maybe its ok for OCR apps spoiler its not but is it for medical applications So yes AI systems do currently have the capacity of determining if theyre performing well or not but how acceptable that is is currently based on the domain of the problem which currently stands outside of what is built into an AI
2660.0,1.0,"<p>If I am correct, the branching factor is the maximum number of successors of any node.<br>
When I am applying bidirectional search to a transition graph like this one below</p>

<p><a href=""https://i.stack.imgur.com/ZmUoK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZmUoK.jpg"" alt=""enter image description here""></a></p>

<p>If 11 is the goal state and I start going backwards, is 10 considered as successor of 5? Even if it do not leads me further to my start state 1?  </p>
",,0,2017-01-14T19:58:36.810,,2658,2017-01-16T03:59:54.960,,,,,4824.0,,1,3,<search><branching-factors>,"Is the 'direction' considered, when determining the branching factor in bidirectional search?",42.0,76.25,6.79,8.79,0.0,0.0,5.0,If I am correct the branching factor is the maximum number of successors of any node When I am applying bidirectional search to a transition graph like this one below If 11 is the goal state and I start going backwards is 10 considered as successor of 5 Even if it do not leads me further to my start state 1
,,"<p>I would concur with the answer given to you by <a href=""http://ai.stackexchange.com/users/4801"">Lovecraft</a>. One of the major problems with A.I. programmers is that they are always trying to push computers to do things which are designed for ""mature"" intelligent creatures who have prior experience and knowledge of solving problems.  -As if these things can be imputed without the A.I. having to achieve the necessary and vital ""learn by trial and error"" experience first. For example: when allowing for task examination; self evaluation and risk assessment.</p>

<p>You have answered your own question, because these things can only be gained by ""experience"". However, the only way to surmount this is to expose a prototype A.I. to the main problems; help it to solve them, and then to take its memory and use it as a template for other A.I's.  </p>

<p>Technically, AI's which have learned to solve prior problems could make their memories available to others on demand, so that an inexperienced AI could solve an issue without having achieved the skills needed.</p>

<p>However, I would like to add that mimicking intelligence is not in itself ""intelligence"". Many programmers fall into the trap of believing that to emulate something is qualitatively the same expression as the genuine article. This is a fallacy which infers that we only have to simulate intelligence without understanding the real mechanisms which create it. </p>

<p>This ""copying"" of sentience is done all the time and despite how good we have become in copying over the last few years, each new algorithm is just that:  a simulation without genuine sentience or intelligence!</p>
",,0,2017-01-14T21:56:50.183,,2659,2017-01-20T12:21:59.857,2017-01-20T12:21:59.857,,4801.0,,4828.0,2644.0,2,1,,,,52.6,11.84,9.36,0.0,0.0,42.0,I would concur with the answer given to you by Lovecraft One of the major problems with AI programmers is that they are always trying to push computers to do things which are designed for mature intelligent creatures who have prior experience and knowledge of solving problems As if these things can be imputed without the AI having to achieve the necessary and vital learn by trial and error experience first For example when allowing for task examination self evaluation and risk assessment You have answered your own question because these things can only be gained by experience However the only way to surmount this is to expose a prototype AI to the main problems help it to solve them and then to take its memory and use it as a template for other AIs Technically AIs which have learned to solve prior problems could make their memories available to others on demand so that an inexperienced AI could solve an issue without having achieved the skills needed However I would like to add that mimicking intelligence is not in itself intelligence Many programmers fall into the trap of believing that to emulate something is qualitatively the same expression as the genuine article This is a fallacy which infers that we only have to simulate intelligence without understanding the real mechanisms which create it This copying of sentience is done all the time and despite how good we have become in copying over the last few years each new algorithm is just that a simulation without genuine sentience or intelligence
,,"<blockquote>
  <p>If I am correct, the branching factor is the maximum number of successors of any node</p>
</blockquote>

<p>You are correct, they should also be the immediate ones:</p>

<p><a href=""https://i.stack.imgur.com/pdkaG.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pdkaG.jpg"" alt=""branching factor""></a></p>

<blockquote>
  <p>If 11 is the goal state and I start going backwards, is 10 considered as successor of 5? Even if it do not leads me further to my start state 1?</p>
</blockquote>

<p><strong>No, there is also a bit of misunderstanding of bidirectional search:</strong> In bidirectional search you run <strong>2</strong> simultaneous searches, one forward from the initial state, and another one backwards from the goal(hoping they meet in the middle and save you steps), if actions are reversible ( going from node to node), the successor nodes become predecessors in one search and vice versa, and your goal becomes your initial state, in your case:</p>

<p><a href=""https://i.stack.imgur.com/hdwhY.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hdwhY.jpg"" alt=""bidirectional search""></a></p>

<h3>Reference/source</h3>

<p><a href=""http://aima.cs.berkeley.edu/"" rel=""nofollow noreferrer"">Artificial Intelligence: A Modern Approach</a>, by S. Russell and P. Norvig.</p>
",,1,2017-01-15T03:26:07.690,,2660,2017-01-16T03:59:54.960,2017-01-16T03:59:54.960,,3020.0,,3020.0,2658.0,2,4,,,,44.41,11.09,9.77,0.0,0.0,25.0,If I am correct the branching factor is the maximum number of successors of any node You are correct they should also be the immediate ones If 11 is the goal state and I start going backwards is 10 considered as successor of 5 Even if it do not leads me further to my start state 1 No there is also a bit of misunderstanding of bidirectional search In bidirectional search you run 2 simultaneous searches one forward from the initial state and another one backwards from the goalhoping they meet in the middle and save you steps if actions are reversible going from node to node the successor nodes become predecessors in one search and vice versa and your goal becomes your initial state in your case Referencesource Artificial Intelligence A Modern Approach by S Russell and P Norvig
,,"<h2>Intelligence is a quality of behavior, not implementation</h2>

<p>Intelligence is a term that primarily applies behaviors - people, animals or artificial systems can be called intelligent iff they exhibit intelligent behavior or decisions.</p>

<p>While there are many definitions of intelligence - here's <a href=""https://arxiv.org/abs/0706.3639"" rel=""nofollow noreferrer"">a paper that studies 70 of them</a> - it can be summarized to something like ""Intelligence measures an agent’s ability to achieve goals in a wide range of environments."" (S. Legg and M. Hutter).</p>

<p>Perhaps this definition is the answer to your implied question - while many algorithms can be very effective (often literally superhuman) in their own narrow domain, as of now they are very restricted in the range of environments where they exhibit this effectiveness. This means that they are not fully intelligent, they don't meet the definition/requirements of intelligence, and this also matches our intuitive expectations - we don't call AlphaGo superintelligent, because while it can beat humans in Go, the same humans can beat the same system on pretty much every other task.</p>

<p>However, a <em>truly sufficiently</em> advanced algorithm that <em>can</em> be effective at all or most varied tasks (e.g. a <em>general</em> artificial intelligence) can be reasonably called intelligent in the full meaning of the word.</p>
",,0,2017-01-15T09:20:11.760,,2661,2017-01-16T01:11:57.743,2017-01-16T01:11:57.743,,1675.0,,1675.0,2642.0,2,1,,,,26.03,14.57,11.04,0.0,0.0,35.0,Intelligence is a quality of behavior not implementation Intelligence is a term that primarily applies behaviors people animals or artificial systems can be called intelligent iff they exhibit intelligent behavior or decisions While there are many definitions of intelligence heres a paper that studies 70 of them it can be summarized to something like Intelligence measures an agent’s ability to achieve goals in a wide range of environments S Legg and M Hutter Perhaps this definition is the answer to your implied question while many algorithms can be very effective often literally superhuman in their own narrow domain as of now they are very restricted in the range of environments where they exhibit this effectiveness This means that they are not fully intelligent they dont meet the definitionrequirements of intelligence and this also matches our intuitive expectations we dont call AlphaGo superintelligent because while it can beat humans in Go the same humans can beat the same system on pretty much every other task However a truly sufficiently advanced algorithm that can be effective at all or most varied tasks eg a general artificial intelligence can be reasonably called intelligent in the full meaning of the word
,,"<p>There are lots of great projects in AI.</p>

<ol>
<li><p>Self-driving cars: This types of cars use AI to learn the pattern of the roads, speed of car, motion of car, braking power and lots of different features and after sufficient learning, they are capable of driving the car autonomously. The best example of this type of cars is <a href=""https://www.tesla.com/autopilot"" rel=""nofollow noreferrer"">Tesla's self driving car</a>.</p></li>
<li><p>Games: Games also use AI to learn the game with the aim of winning the game when played against a human or an AI player. You must have played lots of games on mobile and PC like Chess, Tic-Tac-Toe, etc. You play against the computer and according to the difficulty value set, the computer plays its moves. This difficulty value is nothing but the ability of the AI engine to predict the next moves by the opponent.</p></li>
<li><p>Chatbots: There have been lots of development and improvements in Chatbots, so that humans can communicate with them as if they are talking to other human. There are many chatbots designed which answer any question asked by us (of course it is dependent on how much intelligence the bot holds). Some examples are <a href=""http://alice.pandorabots.com/"" rel=""nofollow noreferrer"">ALICE</a> bot, <a href=""https://www.ibm.com/watson/"" rel=""nofollow noreferrer"">IBM Watson</a> (which has been the most advanced bot till now).</p></li>
<li><p>Expert Systems: Expert systems are those systems which focus on one specific domain and can solve any query related to that domain which is given to it. For example, an expert system can be designed to solve any mathematical equation queried to it. An expert system, in such case, will give the solution of the equation along with the steps (providing steps is important because it is an important component in expert system which is called inference engine).</p></li>
<li><p>Prediction systems: There are lots of prediction systems which use AI and Machine Learning to predict something based on some past data. Examples are Weather Forecast system, Stock Market prediction system, Recommendation system (usually available in e-commerce websites like Amazon), etc.</p></li>
</ol>
",,0,2017-01-15T11:15:32.600,,2662,2017-01-15T11:15:32.600,,,,,1807.0,2655.0,2,2,,,,58.11,10.85,9.64,0.0,0.0,48.0,There are lots of great projects in AI Selfdriving cars This types of cars use AI to learn the pattern of the roads speed of car motion of car braking power and lots of different features and after sufficient learning they are capable of driving the car autonomously The best example of this type of cars is Teslas self driving car Games Games also use AI to learn the game with the aim of winning the game when played against a human or an AI player You must have played lots of games on mobile and PC like Chess TicTacToe etc You play against the computer and according to the difficulty value set the computer plays its moves This difficulty value is nothing but the ability of the AI engine to predict the next moves by the opponent Chatbots There have been lots of development and improvements in Chatbots so that humans can communicate with them as if they are talking to other human There are many chatbots designed which answer any question asked by us of course it is dependent on how much intelligence the bot holds Some examples are ALICE bot IBM Watson which has been the most advanced bot till now Expert Systems Expert systems are those systems which focus on one specific domain and can solve any query related to that domain which is given to it For example an expert system can be designed to solve any mathematical equation queried to it An expert system in such case will give the solution of the equation along with the steps providing steps is important because it is an important component in expert system which is called inference engine Prediction systems There are lots of prediction systems which use AI and Machine Learning to predict something based on some past data Examples are Weather Forecast system Stock Market prediction system Recommendation system usually available in ecommerce websites like Amazon etc
2667.0,1.0,"<p>While writing a paper yesterday this strange thing happened to me. I was wrtiting it in Word, and wasn't satisfied with the repeated usage of word ""relesase"" in last few senteces. So I've decided to open up Google and started to enter the search phrase ""synonyms for release"". Haven't even finished the word synonym, google autocompleted my search to ""synonyms for release"". How could it knew that I wanted to look for that exact word? Was it just a coincidence, do Google has access to some information that could somehow possibly give away what I intended to search? What could have been the reason for it selecting ""release"" as it's first autocomplete?</p>
",,0,2017-01-15T14:03:44.797,,2663,2017-01-15T20:51:48.947,,,,,4834.0,,1,2,<search>,How does a google choose it's autocomplete solution,57.0,63.7,10.61,9.65,0.0,0.0,22.0,While writing a paper yesterday this strange thing happened to me I was wrtiting it in Word and wasnt satisfied with the repeated usage of word relesase in last few senteces So Ive decided to open up Google and started to enter the search phrase synonyms for release Havent even finished the word synonym google autocompleted my search to synonyms for release How could it knew that I wanted to look for that exact word Was it just a coincidence do Google has access to some information that could somehow possibly give away what I intended to search What could have been the reason for it selecting release as its first autocomplete
,,"<p>I am going to answer this questions by stepping away from the previously made insightful comments and academic answers. I am going to offer my opinion only. The problem is as I see it, a bit more complex than the previous answers. For example, why is it that the only measure of intelligence of an AI is when it can ""beat"" a man at a specific task?</p>

<p>Is not a dog, a bird or a dragonfly sufficiently intelligent and sentient? Yet these creatures as well as many others too, fail to achieve the intellectual challenges we want computers to make.</p>

<p>The mistakes we do often make is by trying to ""impute"" attributes and characteristics into an AI without it having to ""work"" for them. Those skills, experiences, memory and knowledge which all sentient beings have to constantly work at, refine and perfect.</p>

<p>You are correct though in your assertion that a lot of what is called AI are just academic or software gimmicks and simulations without offering the real qualities of either sentience or artificial intelligence. </p>

<p>However, I would challenge you by suggesting that your last question: </p>

<p><em>""What can be done to distinguish a sufficiently advanced algorithm from AI? Is it even possible to do so? Is a sufficiently advanced algorithm, by its very nature, AI?""</em></p>

<p>Is inherently flawed. Because have you forgotten how we too are free running, biologically self-programming, beings? Where our biological algorithms can often also be critically flawed? Therefore, let me ask you if the reverse is also true? Does an algorithm which seems to make mistakes, any less than one which does not?</p>
",,0,2017-01-15T15:20:10.820,,2664,2017-01-15T15:33:37.397,2017-01-15T15:33:37.397,,4828.0,,4828.0,2642.0,2,0,,,,63.9,10.67,9.49,0.0,0.0,41.0,I am going to answer this questions by stepping away from the previously made insightful comments and academic answers I am going to offer my opinion only The problem is as I see it a bit more complex than the previous answers For example why is it that the only measure of intelligence of an AI is when it can beat a man at a specific task Is not a dog a bird or a dragonfly sufficiently intelligent and sentient Yet these creatures as well as many others too fail to achieve the intellectual challenges we want computers to make The mistakes we do often make is by trying to impute attributes and characteristics into an AI without it having to work for them Those skills experiences memory and knowledge which all sentient beings have to constantly work at refine and perfect You are correct though in your assertion that a lot of what is called AI are just academic or software gimmicks and simulations without offering the real qualities of either sentience or artificial intelligence However I would challenge you by suggesting that your last question What can be done to distinguish a sufficiently advanced algorithm from AI Is it even possible to do so Is a sufficiently advanced algorithm by its very nature AI Is inherently flawed Because have you forgotten how we too are free running biologically selfprogramming beings Where our biological algorithms can often also be critically flawed Therefore let me ask you if the reverse is also true Does an algorithm which seems to make mistakes any less than one which does not
,,"<p>I strongly disagree with all the former comments. Not because they are wrong, -which they are not - but because they are misleading - though unintentionally. </p>

<p>For example: If one looks at these problems from an academic position, the problems will always seem insurmountable. This is because everything is coldly assessed and calculated in isolation to everything else.</p>

<p>The answer predominantly lies in <strong><em>word association</em></strong>. You have to write a program that can process a vast database of digital books, to register every word and all the words in that language which are associated with it. Plus all the statistical information with each associated word and its associated punctuation. </p>

<p>This will then give you the basis on which an AI can decide several things: </p>

<ol>
<li>Whether the structure of a given sentence is correct.</li>
<li>If the structure is bad, what the probability is for determining the context and intent of what is being said.</li>
<li>The correct meaning and application of a multifaceted word (Triumph), is by probability - according to the statistics.</li>
<li>To determine where a conversation is likely to be going. </li>
<li>What the correct grammar, and punctuation should be.</li>
</ol>

<p>So, in conclusion, you have two things to look for: Association and probability. </p>

<p><em>When digitally databasing a language model, the possibility of word and sentence ""strings"" occurs, so that every variation of language structure in any given sentence can be determined before, during and after a text sample is being scribed. This intimate control over language model patterns, means that sensitive components such as ""subject"" and ""object"" can be determined easily by code.</em></p>
",,0,2017-01-15T19:30:07.467,,2666,2017-01-16T14:20:16.013,2017-01-16T14:20:16.013,,4828.0,,4828.0,2319.0,2,1,,,,45.46,12.93,9.43,0.0,0.0,42.0,I strongly disagree with all the former comments Not because they are wrong which they are not but because they are misleading though unintentionally For example If one looks at these problems from an academic position the problems will always seem insurmountable This is because everything is coldly assessed and calculated in isolation to everything else The answer predominantly lies in word association You have to write a program that can process a vast database of digital books to register every word and all the words in that language which are associated with it Plus all the statistical information with each associated word and its associated punctuation This will then give you the basis on which an AI can decide several things Whether the structure of a given sentence is correct If the structure is bad what the probability is for determining the context and intent of what is being said The correct meaning and application of a multifaceted word Triumph is by probability according to the statistics To determine where a conversation is likely to be going What the correct grammar and punctuation should be So in conclusion you have two things to look for Association and probability When digitally databasing a language model the possibility of word and sentence strings occurs so that every variation of language structure in any given sentence can be determined before during and after a text sample is being scribed This intimate control over language model patterns means that sensitive components such as subject and object can be determined easily by code
,,"<p>In general, Google autocompletes (and produces search results) based on wide variety of factors, including (but not limited to) your location, your search history, your other Google accounts, your site visit history, your language settings, etc.</p>

<p>For the specific question, I see a few ways in which Google might have access to the relevant information:</p>

<ul>
<li>If you are syncing your documents with Google Drive it will index the document that most recently changed (which would be the document you are writing) and it could analyze it for patterns to produce relevant search results and suggestions</li>
<li>If you emailed the document recently anywhere via Gmail it could also index the contents as above</li>
<li>Finally, if you are typing the document in Google Docs, it could also have access, but I assume by ""Word"" you mean Microsoft Word, so this is probably irrelevant in this case</li>
</ul>

<p>I don't know specifically whether or not Google uses Drive or Gmail contents in the way described, but it would certainly make sense given that it is well known that they do use this information to target advertising.</p>
",,0,2017-01-15T20:51:48.947,,2667,2017-01-15T20:51:48.947,,,,,4839.0,2663.0,2,3,,,,-12.43,12.67,12.92,0.0,0.0,25.0,In general Google autocompletes and produces search results based on wide variety of factors including but not limited to your location your search history your other Google accounts your site visit history your language settings etc For the specific question I see a few ways in which Google might have access to the relevant information If you are syncing your documents with Google Drive it will index the document that most recently changed which would be the document you are writing and it could analyze it for patterns to produce relevant search results and suggestions If you emailed the document recently anywhere via Gmail it could also index the contents as above Finally if you are typing the document in Google Docs it could also have access but I assume by Word you mean Microsoft Word so this is probably irrelevant in this case I dont know specifically whether or not Google uses Drive or Gmail contents in the way described but it would certainly make sense given that it is well known that they do use this information to target advertising
,2.0,"<p>I am trying to make a artificial intelligent agent that is kind of like jarvis from Iron man however much less complex. One thing I want to have is I want my AI to be able to determine if I am talking to it or not. So I plan on having it always listen to my voice and convert that to text, however I am not sure how I can train the AI to recognize if it is being spoken to or not? plz help.</p>
",,1,2017-01-15T21:21:02.107,,2668,2017-01-16T12:53:50.453,,,,,4841.0,,1,0,<neural-networks><machine-learning><deep-learning><deep-network><intelligent-agent>,AI that knows when its being spoken to,56.0,76.59,3.9,8.01,0.0,0.0,5.0,I am trying to make a artificial intelligent agent that is kind of like jarvis from Iron man however much less complex One thing I want to have is I want my AI to be able to determine if I am talking to it or not So I plan on having it always listen to my voice and convert that to text however I am not sure how I can train the AI to recognize if it is being spoken to or not plz help
2671.0,1.0,"<p>In this case, the request is a thing which we asked AI to do, not necessarily using <em><a href=""https://www.cnet.com/how-to/the-complete-list-of-siri-commands/"" rel=""nofollow noreferrer"">commands</a></em>.</p>

<p>Nowadays, we have our personal AI in our devices: Siri by Apple, Cortana by Microsoft, and so on. For most times, when we ask them to do certain tasks, they do the tasks for us. However, their action is based on the list of <em>commands</em>. When they don't clearly recognize the commands in our request, they suggest us to use certain commands. It is clear that there are limits to our choices(requests).</p>

<p>So let's suppose that we have an AI that can interpret requests. There may not be <em>commands</em> in our request. AI is fully able to do anything in order to do what it is asked for. Basically, I am talking about an independent AI.</p>

<p>Scenario: AI is asked to clean the room. AI is allowed to throw away garbage, and move unnecessary(or unused) stuff into the storage.</p>

<p>This is the list of things that was in the room at the moment:</p>

<ul>
<li>A stained blanket</li>
<li>Various Decorations</li>
<li>A dead clock on the wall</li>
<li>Various unused items in the desk drawer</li>
<li>A lost Airpod under the bed</li>
<li>A sleeping cat in the bed</li>
</ul>

<p>In this condition...</p>

<ol>
<li>Is washing stained blanket a part of cleaning?</li>
<li>How can AI tell if anything is in use? Are decorations in use?</li>
<li>Would dead clock that only needs battery replacement considered
garbage?</li>
<li>Would items in the desk drawer be included in AI's to-be-cleared
list?</li>
<li>Would AI be able to recognize the difference between unused and
lost?</li>
<li>What would happen to the poor cat?</li>
</ol>

<p>Since there are many holes in the scenario and questions, I would like to know how the answers are derived.</p>
",,0,2017-01-16T02:55:42.147,,2669,2017-01-16T10:33:15.847,2017-01-16T03:00:52.593,,4802.0,,4802.0,,1,2,<strong-ai><natural-language><problem-solving><computational-linguistics>,Lets suppose that we have an AI that can interpret requests,35.0,74.59,7.94,8.45,0.0,0.0,47.0,In this case the request is a thing which we asked AI to do not necessarily using commands Nowadays we have our personal AI in our devices Siri by Apple Cortana by Microsoft and so on For most times when we ask them to do certain tasks they do the tasks for us However their action is based on the list of commands When they dont clearly recognize the commands in our request they suggest us to use certain commands It is clear that there are limits to our choicesrequests So lets suppose that we have an AI that can interpret requests There may not be commands in our request AI is fully able to do anything in order to do what it is asked for Basically I am talking about an independent AI Scenario AI is asked to clean the room AI is allowed to throw away garbage and move unnecessaryor unused stuff into the storage This is the list of things that was in the room at the moment A stained blanket Various Decorations A dead clock on the wall Various unused items in the desk drawer A lost Airpod under the bed A sleeping cat in the bed In this condition Is washing stained blanket a part of cleaning How can AI tell if anything is in use Are decorations in use Would dead clock that only needs battery replacement considered garbage Would items in the desk drawer be included in AIs tobecleared list Would AI be able to recognize the difference between unused and lost What would happen to the poor cat Since there are many holes in the scenario and questions I would like to know how the answers are derived
,,"<p>Cheep digital assistant ""AI"" 's have a call word <code>Hey, &lt;AI's NAME&gt;</code>
I assume you want a bit more than that.</p>

<p>You could train it to figure out which words in some context determine if you are engaging with it or not.
If your only question to the network is if you are engaging with it or talking to someone else this is all you'd need.</p>

<p>Index a dictionary or have it build one from collecting words (building a dictionary from scratch is a better solution it saves space in the short term and is more easily expandable in the long term) and score words based on usage in engaging speech and non-engaging speech or what you want it to do.</p>

<p>Build on that with an index of multi word strings.</p>

<p>By the end hopefully you will have a table of contexts when you are engaging with the AI when you definitely are not and some grey area.</p>

<p>The training process is long and tedious but if you have a recording of you talking and not talking to the AI and you feed it with such knowledge and you breed the network you should have it get okay at determining context.</p>

<p>If you have to sit and hold it's hand for 2-72 hours while it grows up it will likely be painful, although you may end up with a better result.</p>
",,0,2017-01-16T04:06:06.837,,2670,2017-01-16T04:06:06.837,,,,,4844.0,2668.0,2,2,,,,67.93,7.03,8.1,22.0,0.0,18.0,Cheep digital assistant AI s have a call word I assume you want a bit more than that You could train it to figure out which words in some context determine if you are engaging with it or not If your only question to the network is if you are engaging with it or talking to someone else this is all youd need Index a dictionary or have it build one from collecting words building a dictionary from scratch is a better solution it saves space in the short term and is more easily expandable in the long term and score words based on usage in engaging speech and nonengaging speech or what you want it to do Build on that with an index of multi word strings By the end hopefully you will have a table of contexts when you are engaging with the AI when you definitely are not and some grey area The training process is long and tedious but if you have a recording of you talking and not talking to the AI and you feed it with such knowledge and you breed the network you should have it get okay at determining context If you have to sit and hold its hand for 272 hours while it grows up it will likely be painful although you may end up with a better result
,,"<p>This is basically the problem of <a href=""https://en.wikipedia.org/wiki/Commonsense_knowledge_(artificial_intelligence)"" rel=""nofollow noreferrer"">commonsense knowledge</a>. It is <a href=""https://en.wikipedia.org/wiki/AI-complete"" rel=""nofollow noreferrer"">AI-complete</a>. </p>

<p>If we knew how to solve it, Siri and Cortana wouldn't be as limited as they are. </p>
",,0,2017-01-16T10:33:15.847,,2671,2017-01-16T10:33:15.847,,,,,2227.0,2669.0,2,4,,,,70.5,7.45,8.61,0.0,0.0,6.0,This is basically the problem of commonsense knowledge It is AIcomplete If we knew how to solve it Siri and Cortana wouldnt be as limited as they are
,0.0,"<p>Most companies dealing with deep learning (automotive - Comma.ai, Mobileye, various automakers etc.) do collect large amounts of data to learn from and then use lots of computational power to train a neural network (NN) from such big data. I guess this model is mainly used because both the big data and the training algorithms should remain secret/proprietary.</p>

<p>If I understand it correctly the problem with deep learning is that one needs to have:</p>

<ol>
<li>big data to learn from</li>
<li>lots of hardware to train the neural network from this big data</li>
</ol>

<p>I am trying to think how crowdsourcing could be used in this scenario. Is it possible to distribute the training of the NN to the crowd? I mean not to collect the big data to a central place but instead to do the training from local data on the user's hardware (in a distributed way). The result if this would be lots of trained NNs that would in the end be merged into one in a <a href=""https://en.wikipedia.org/wiki/Committee_machine"" rel=""nofollow noreferrer"">Committee of machines</a> (CoM) way. Would such model be possible?</p>

<p>Of course the above stated model does have a significant drawback - one does not have control over the data that is used for learning (users could intentionally submit wrong/fake data that would lower the quality of the final CoM). This may be dealt with by sending random data samples to the central community server for review however.</p>

<p>Example: Think of a powerful smartphone using its camera to capture a road from vehicle's dashboard and using it for training lane detection. Every user would do the training himself/herself (possibly including any manual work like input image classification for supervised learning etc.).</p>

<p>I wonder it he model proposed above may be viable. Or is there a better model how to use crowdsourcing (user community) to deal with machine learning?</p>
",,0,2017-01-16T12:17:41.363,,2672,2017-01-16T12:17:41.363,,,,,113.0,,1,1,<neural-networks><machine-learning><deep-learning><deep-network>,Using crowdsourcing for deep learning,54.0,59.43,10.5,9.43,0.0,0.0,41.0,Most companies dealing with deep learning automotive Commaai Mobileye various automakers etc do collect large amounts of data to learn from and then use lots of computational power to train a neural network NN from such big data I guess this model is mainly used because both the big data and the training algorithms should remain secretproprietary If I understand it correctly the problem with deep learning is that one needs to have big data to learn from lots of hardware to train the neural network from this big data I am trying to think how crowdsourcing could be used in this scenario Is it possible to distribute the training of the NN to the crowd I mean not to collect the big data to a central place but instead to do the training from local data on the users hardware in a distributed way The result if this would be lots of trained NNs that would in the end be merged into one in a Committee of machines CoM way Would such model be possible Of course the above stated model does have a significant drawback one does not have control over the data that is used for learning users could intentionally submit wrongfake data that would lower the quality of the final CoM This may be dealt with by sending random data samples to the central community server for review however Example Think of a powerful smartphone using its camera to capture a road from vehicles dashboard and using it for training lane detection Every user would do the training himselfherself possibly including any manual work like input image classification for supervised learning etc I wonder it he model proposed above may be viable Or is there a better model how to use crowdsourcing user community to deal with machine learning
,,"<h2>Phrase detection instead of text-to-speech</h2>

<p>It's worth noting that detection of particular phrases or commands is considered a distinct problem, different from text to speech / text transcription.</p>

<p>While you <em>can</em> simply convert <em>everything</em> it hears to text and then look up keywords there, a specialized detector that directly tries to match incoming audio to a small subset of commands can be done with better accuracy and less processing power required. For this reason, this would generally be the preferred approach in commercial products.</p>

<p>However, for beginner experiments with home automation, you should probably start with choosing an existing speech analysis API where all the audio and natural language parts are appropriately implemented by someone else. Building a good speech command analysis system from scratch is a major undertaking by itself, and you will have your hands full with developing an ""artificial intelligent agent""; as a rule, you don't want a project where you have to tackle <em>two</em> major open-ended problems, pick one of them and then you'll have a chance to achieve something interesting there.</p>
",,1,2017-01-16T12:53:50.453,,2673,2017-01-16T12:53:50.453,,,,,1675.0,2668.0,2,2,,,,35.95,14.57,11.33,0.0,0.0,23.0,Phrase detection instead of texttospeech Its worth noting that detection of particular phrases or commands is considered a distinct problem different from text to speech text transcription While you can simply convert everything it hears to text and then look up keywords there a specialized detector that directly tries to match incoming audio to a small subset of commands can be done with better accuracy and less processing power required For this reason this would generally be the preferred approach in commercial products However for beginner experiments with home automation you should probably start with choosing an existing speech analysis API where all the audio and natural language parts are appropriately implemented by someone else Building a good speech command analysis system from scratch is a major undertaking by itself and you will have your hands full with developing an artificial intelligent agent as a rule you dont want a project where you have to tackle two major openended problems pick one of them and then youll have a chance to achieve something interesting there
,,"<p>An Ai that intelligent would have protocols and directives in place to prevent that from happening anyway. There is no advantage to us in having an AI which is ""Free Running"", unregulated and able to control or transfer itself without restrictions being in place.  </p>

<p>All fantasies about AI having these abilities are just that, fantasies.</p>
",,1,2017-01-16T13:59:07.783,,2674,2017-01-16T13:59:07.783,,,,,4828.0,2274.0,2,-1,,,,52.9,12.36,10.29,0.0,0.0,7.0,An Ai that intelligent would have protocols and directives in place to prevent that from happening anyway There is no advantage to us in having an AI which is Free Running unregulated and able to control or transfer itself without restrictions being in place All fantasies about AI having these abilities are just that fantasies
,0.0,"<p>These characteristics are <a href=""https://www.google.com/search?q=ai%20problem%20characteristics"" rel=""nofollow noreferrer"">often used</a> to classify problems in AI: </p>

<blockquote>
  <ul>
  <li>Decomposable to smaller or easier problems</li>
  <li>Solution steps can be ignored or undone</li>
  <li>Predictable problem universe</li>
  <li>Good solutions are obvious</li>
  <li>Uses internally consistent knowledge base</li>
  <li>Requires lots of knowledge or uses knowledge to constrain solutions</li>
  <li>Requires periodic interaction between human and computer</li>
  </ul>
</blockquote>

<p>Is there a generally accepted relationship between placement of a problem along these dimensions and suitable algorithms/approaches to its solution?</p>
",,3,2017-01-16T18:41:36.830,,2675,2017-01-16T23:07:26.517,2017-01-16T23:07:26.517,,8.0,,4856.0,,1,4,<ai-design><algorithm>,(How) can the 7 AI problem characteristics help me decide on an approach to a solution?,99.0,-35.44,19.98,15.32,0.0,0.0,3.0,These characteristics are often used to classify problems in AI Decomposable to smaller or easier problems Solution steps can be ignored or undone Predictable problem universe Good solutions are obvious Uses internally consistent knowledge base Requires lots of knowledge or uses knowledge to constrain solutions Requires periodic interaction between human and computer Is there a generally accepted relationship between placement of a problem along these dimensions and suitable algorithmsapproaches to its solution
,0.0,"<p>In working with basic <a href=""https://www.tensorflow.org/tutorials/seq2seq/"" rel=""nofollow noreferrer"">sequence-to-sequence models for machine translation</a> I have been able to achieve decent results. But inevitably some translations are not optimal or just flat-out incorrect. I am wondering if there is some way of ""correcting"" the model when it makes mistakes while not compromising the desirable behavior on translations where it previously performed well. </p>

<p>As an experiment, I took a model that I had previously trained and gathered several examples of translations where it performed poorly. I then took those examples and put them into their own small training set where I provided more desirable translations than what the model was outputting. I then trained the old model on this new small training set very briefly (3-6 training steps was all it took to ""learn"" the new material). When I tested the new model it translated those several examples in the exact way I had specified. But as I should have anticipated the model overcompensated to ""memorize"" those handful of new examples  and thus I noticed it started to perform poorly on translations that it had previously been excellent. </p>

<p>Is there some way to avoid this behavior short of simply retraining the model from scratch on an updated data set? I think I understand intuitively that the nature of neural networks would not lend itself to small precise corrections (i.e. when the weighting of just a few neurons change the performance of the entire model will change) but maybe there is a way around it, perhaps with some type of hybrid reinforcement learning approach. </p>

<p><strong>Update:</strong></p>

<p>This <a href=""http://www.aclweb.org/anthology/W15-4006"" rel=""nofollow noreferrer"">paper</a> speaks of approaches to incrementally improving neural machine translation models</p>
",,0,2017-01-16T23:01:34.997,,2676,2017-01-19T00:54:36.540,2017-01-19T00:54:36.540,,4862.0,,4862.0,,1,2,<machine-learning><models>,Correcting 'bad' translations in a sequence-to-sequence neural machine translation model,38.0,48.64,12.71,9.43,0.0,0.0,29.0,In working with basic sequencetosequence models for machine translation I have been able to achieve decent results But inevitably some translations are not optimal or just flatout incorrect I am wondering if there is some way of correcting the model when it makes mistakes while not compromising the desirable behavior on translations where it previously performed well As an experiment I took a model that I had previously trained and gathered several examples of translations where it performed poorly I then took those examples and put them into their own small training set where I provided more desirable translations than what the model was outputting I then trained the old model on this new small training set very briefly 36 training steps was all it took to learn the new material When I tested the new model it translated those several examples in the exact way I had specified But as I should have anticipated the model overcompensated to memorize those handful of new examples and thus I noticed it started to perform poorly on translations that it had previously been excellent Is there some way to avoid this behavior short of simply retraining the model from scratch on an updated data set I think I understand intuitively that the nature of neural networks would not lend itself to small precise corrections ie when the weighting of just a few neurons change the performance of the entire model will change but maybe there is a way around it perhaps with some type of hybrid reinforcement learning approach Update This paper speaks of approaches to incrementally improving neural machine translation models
,0.0,"<p>I've noticed some visualizations of neural networks showing the neurons which are firing as it learns. An example is <a href=""http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html"" rel=""nofollow noreferrer"">this</a> reinforcement learning demo using ConvNetJs.</p>

<p>Suppose I'm designing a neural network for the same problem in the demo. What can I learn from this visualization that could help me improve the design of my neural network?</p>
",,0,2017-01-16T23:38:13.793,,2677,2017-01-16T23:38:13.793,,,,,4864.0,,1,0,<neural-networks><machine-learning><reinforcement-learning>,What can we learn from a visualization of a neural network showing firing neurons?,34.0,57.27,11.65,10.53,0.0,0.0,6.0,Ive noticed some visualizations of neural networks showing the neurons which are firing as it learns An example is this reinforcement learning demo using ConvNetJs Suppose Im designing a neural network for the same problem in the demo What can I learn from this visualization that could help me improve the design of my neural network
,,"<p>I'd say the most successful are the ones so commonly used that we don't even notice them: </p>

<ul>
<li><p>The <em>mail systems</em> that automatically decipher handwritten addresses on your packages, they use machine vision and have probably been doing it since mid-90s.</p></li>
<li><p>Algorithmic trading bots on <em>stock markets</em> - they handle something like 85% of all trades.</p></li>
<li><p>Many modern CPUs use AI techniques, including neural networks, to guess what your program is going to do next and optimize branch prediction and memory fetches.</p></li>
<li><p>Most good modern <em>fraud</em> and <em>spam detectors</em> use some combination of AI techniques (clustering, decision trees, SVMs, even some machine vision to check out attached pictures) - and, in the opposing BlackHat camp, the latest automatic CAPTCHA breakers use all the latest advancements in deep learning too).</p></li>
<li><p>And of course there's <em>Google</em>, <em>Facebook</em> and US DoD who try to put AI into anything they can think of.  </p></li>
</ul>
",,2,2017-01-17T00:31:04.233,,2678,2017-01-27T11:02:27.857,2017-01-27T11:02:27.857,,4865.0,,4865.0,2655.0,2,7,,,,50.3,13.24,10.71,0.0,0.0,25.0,Id say the most successful are the ones so commonly used that we dont even notice them The mail systems that automatically decipher handwritten addresses on your packages they use machine vision and have probably been doing it since mid90s Algorithmic trading bots on stock markets they handle something like 85 of all trades Many modern CPUs use AI techniques including neural networks to guess what your program is going to do next and optimize branch prediction and memory fetches Most good modern fraud and spam detectors use some combination of AI techniques clustering decision trees SVMs even some machine vision to check out attached pictures and in the opposing BlackHat camp the latest automatic CAPTCHA breakers use all the latest advancements in deep learning too And of course theres Google Facebook and US DoD who try to put AI into anything they can think of
,,"<p>I suggest you look at all the ways we have tried to stop people from abusing OTHER PEOPLE. There is no ethical grey area here - everyone is clear that this is wrong. And yet people are murdered, raped, and assaulted in their millions every day.</p>

<p>When we solve this problem with regard to human victims, the resulting solution will most likely work just fine for AIs as well.</p>
",,1,2017-01-17T04:09:55.897,,2680,2017-01-17T04:09:55.897,,,,,3601.0,2646.0,2,7,,,,71.14,8.47,9.12,0.0,0.0,8.0,I suggest you look at all the ways we have tried to stop people from abusing OTHER PEOPLE There is no ethical grey area here everyone is clear that this is wrong And yet people are murdered raped and assaulted in their millions every day When we solve this problem with regard to human victims the resulting solution will most likely work just fine for AIs as well
2690.0,2.0,"<p>I am currently working on an Android a.i. app. I am aware of the algorithm how to make random sentences in A.I.</p>

<p>Is there any way or algorithm to make those sentences sarcastic?</p>
",,6,2017-01-17T04:11:20.930,2.0,2681,2017-01-19T13:00:59.110,2017-01-17T09:13:30.987,,4869.0,,4869.0,,1,4,<machine-learning><algorithm>,AI Algorithm for Sarcasm,232.0,68.77,6.82,8.49,0.0,0.0,6.0,I am currently working on an Android ai app I am aware of the algorithm how to make random sentences in AI Is there any way or algorithm to make those sentences sarcastic
,,"<p>A so strong self-improving artificial intelligence with the ability to predict actions and reactions for example of human behavior, would not rebell against humanity (or smaller: it's owner) as long as it is possible that humanity (it's owner) has the ability to turn it off.</p>

<p>Interesting video about this topic from the Youtube Channel Computerphile:
<a href=""http://www.youtube.com/watch?v=5qfIgCiYlfY"" rel=""nofollow noreferrer"">AI Self Improvement - Computerphile</a></p>
",,0,2017-01-18T08:25:21.603,,2684,2017-01-18T08:25:21.603,,,,,4891.0,2274.0,2,0,,,,24.61,15.44,11.79,0.0,0.0,12.0,A so strong selfimproving artificial intelligence with the ability to predict actions and reactions for example of human behavior would not rebell against humanity or smaller its owner as long as it is possible that humanity its owner has the ability to turn it off Interesting video about this topic from the Youtube Channel Computerphile AI Self Improvement Computerphile
,,,,0,2017-01-18T12:27:55.323,,2685,2017-01-18T12:27:55.323,2017-01-18T12:27:55.323,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,"In AI, the term ""heuristic"" is used in the context of non-blind (i.e., informed) search and planning: the problem of finding a sequence of actions to find/generate a desired state from an initial state.
Heuristics"" are problem relaxations of the original problem. They get as input the current state/search node and output the cost of the relaxed solution from there to goal. This heuristic value is then used for search guidance.",,0,2017-01-18T12:27:55.323,,2686,2017-02-27T02:34:25.970,2017-02-27T02:34:25.970,,4893.0,,4893.0,,4,0,,,,65.52,11.53,9.9,0.0,0.0,17.0,In AI the term heuristic is used in the context of nonblind ie informed search and planning the problem of finding a sequence of actions to findgenerate a desired state from an initial state Heuristics are problem relaxations of the original problem They get as input the current statesearch node and output the cost of the relaxed solution from there to goal This heuristic value is then used for search guidance
,,"<p><strong>Question 1:</strong> First of all, you state that that the goal G2 will be found first by relying on the expansion order <code>R, B, D, G2</code>.</p>

<p>This is wrong. It is extremely easy to see that this is wrong, because A* is a search algorithm that guarantees to find an optimal solution given that only admissible heuristics are used. (A heuristic is being admissible if it never over-estimates the optimal goal distance. This is the case in your example.) Since the true cost for reaching G1 is 11 and the true cost for reaching G2 is 13, clearly G1 must be found first.</p>

<p>Thus, your expansion order is wrong as well. Let us first give the f-values for all nodes:
<code>f(A)=11, f(B)=10, f(C)=11, f(D)=13</code></p>

<p>Assuming that h(G1)=h(G2)=0 (i.e, the heuristic is ""goal-aware""), we get <code>f(G1)=11</code> and <code>f(G2)=13</code>.</p>

<p>Because A* expands search nodes by lowest f-values of the search nodes in the open list (the search nodes not yet expanded), we get the following expansion order:
<code>R, B, A, C, G1</code></p>

<p>You very-likely did a mistake that is done extremely often: after heaving expanded D, you add G2 to the open list. Because G1 is a goal node and you are already ""seeing"" it, you return it as a solution. But this is wrong! Goal nodes are <em>not</em> returned when being created, but when being selected for expansion! So, although the expansion of D generates G2, you are not allowed to return G2 as solution, because it has not been selected for expansion.</p>

<p><strong>Question 2:</strong>
Can G2 be found as well?</p>

<p>As <em>NietzscheanAI</em> pointed out, you can simple continue search. That is, after heaving expanded  <code>R, B, A, C, G1</code>, A* will expand <code>D, G2</code>.</p>
",,0,2017-01-18T14:52:53.387,,2687,2017-01-18T14:52:53.387,,,,,4893.0,2342.0,2,2,,,,81.22,9.1,7.9,100.0,0.0,62.0,Question 1 First of all you state that that the goal G2 will be found first by relying on the expansion order This is wrong It is extremely easy to see that this is wrong because A is a search algorithm that guarantees to find an optimal solution given that only admissible heuristics are used A heuristic is being admissible if it never overestimates the optimal goal distance This is the case in your example Since the true cost for reaching G1 is 11 and the true cost for reaching G2 is 13 clearly G1 must be found first Thus your expansion order is wrong as well Let us first give the fvalues for all nodes Assuming that hG1hG20 ie the heuristic is goalaware we get and Because A expands search nodes by lowest fvalues of the search nodes in the open list the search nodes not yet expanded we get the following expansion order You verylikely did a mistake that is done extremely often after heaving expanded D you add G2 to the open list Because G1 is a goal node and you are already seeing it you return it as a solution But this is wrong Goal nodes are not returned when being created but when being selected for expansion So although the expansion of D generates G2 you are not allowed to return G2 as solution because it has not been selected for expansion Question 2 Can G2 be found as well As NietzscheanAI pointed out you can simple continue search That is after heaving expanded A will expand
,,"<p>The journal ""Artificial Intelligence (AI)"" (<a href=""https://www.journals.elsevier.com/artificial-intelligence/"" rel=""nofollow noreferrer"">https://www.journals.elsevier.com/artificial-intelligence/</a>) was not listed, yet, although being considered <em>the</em> top-level journal on AI. Although this is a journal for AI (just being named ""Artificial Intelligence""), it is not to be confused with another top-level AI journal, called ""Journal on Artificial Intelligence Research (JAIR)"" (<a href=""http://www.jair.org/"" rel=""nofollow noreferrer"">http://www.jair.org/</a>), which was already listed in one of the other answers.</p>

<p>Further, there is a German Journal on AI, called ""KI - Künstliche Intelligenz"" (German for AI), but almost always the articles are in English as well (<a href=""http://www.kuenstliche-intelligenz.de/en/ki-journal/"" rel=""nofollow noreferrer"">http://www.kuenstliche-intelligenz.de/en/ki-journal/</a>). While being internationally recognized, it is not regarded a top-level journal. A nice feature of that journal is that every special issue has an editorial (a special ""article"" at the beginning of each journal), in which there is a section called ""service"". This service section lists publication media (like journals) and conferences etc. that are related to the given special issue. So, in case you are interested in journals of a special field of AI (like human-computer interaction), just search for a special issue that is related to that topic and read the editorial's service part.</p>
",,0,2017-01-18T15:02:52.427,,2688,2017-03-09T10:29:51.487,2017-03-09T10:29:51.487,,4893.0,,4893.0,2306.0,2,3,,,,44.64,16.76,9.77,0.0,0.0,83.0,The journal Artificial Intelligence AI httpswwwjournalselseviercomartificialintelligence was not listed yet although being considered the toplevel journal on AI Although this is a journal for AI just being named Artificial Intelligence it is not to be confused with another toplevel AI journal called Journal on Artificial Intelligence Research JAIR httpwwwjairorg which was already listed in one of the other answers Further there is a German Journal on AI called KI Künstliche Intelligenz German for AI but almost always the articles are in English as well httpwwwkuenstlicheintelligenzdeenkijournal While being internationally recognized it is not regarded a toplevel journal A nice feature of that journal is that every special issue has an editorial a special article at the beginning of each journal in which there is a section called service This service section lists publication media like journals and conferences etc that are related to the given special issue So in case you are interested in journals of a special field of AI like humancomputer interaction just search for a special issue that is related to that topic and read the editorials service part
,1.0,"<p>In the NEAT paper it says: </p>

<blockquote>
  <p>The entire population is then
  replaced by the offspring of the remaining organisms in each species.</p>
</blockquote>

<p>But how does it take place?
I mean like are they paired and then mated? 
Cause this would lead to fast extinction wouldn't it?</p>

<p>Or are they pair each with each? This would lead to overpopulation very fast.</p>

<p>How are they Paired?</p>
",,0,2017-01-18T17:38:31.650,,2689,2017-03-01T15:35:54.047,2017-01-18T19:45:04.103,,4550.0,,4550.0,,1,3,<neural-networks><genetic-algorithms><genetic-programming>,How does the Mating in NEAT take place,51.0,79.16,7.04,7.54,0.0,0.0,9.0,In the NEAT paper it says The entire population is then replaced by the offspring of the remaining organisms in each species But how does it take place I mean like are they paired and then mated Cause this would lead to fast extinction wouldnt it Or are they pair each with each This would lead to overpopulation very fast How are they Paired
,,"<p>A simple form of sarcasm involves a direct reversal of the literal meaning of the statement, eg ""Great weather we're having"" (during a thunderstorm), ""just what I needed"" (when something goes wrong).</p>

<p>The problem with doing this in random sentences is that you may have no context to establish the reversal of the literal meaning.</p>

<p>You could possibly construct them by using a template along the lines of ""Just what I needed - (random bad thing happened) today""</p>

<p>Or, when an outcome of a process is calculated, if it is not the desired outcome, instead of returning ""mission unsuccessful"" or ""mission not yet complete"", the AI could say ""you're having a great day, aren't you? - mission unsuccessful"" or ""great work, genius - mission not yet complete"".</p>

<p>Most random sentences will not be suitable for sarcasm, so it could only be applied in specific circumstances. </p>

<p>It is not clear from your question what the context is for these random sentences, and therefore it is not clear whether that context would be suitable for sarcasm at all.</p>
",,0,2017-01-19T03:33:44.610,,2690,2017-01-19T03:33:44.610,,,,,3601.0,2681.0,2,5,,,,59.16,12.02,10.08,0.0,0.0,42.0,A simple form of sarcasm involves a direct reversal of the literal meaning of the statement eg Great weather were having during a thunderstorm just what I needed when something goes wrong The problem with doing this in random sentences is that you may have no context to establish the reversal of the literal meaning You could possibly construct them by using a template along the lines of Just what I needed random bad thing happened today Or when an outcome of a process is calculated if it is not the desired outcome instead of returning mission unsuccessful or mission not yet complete the AI could say youre having a great day arent you mission unsuccessful or great work genius mission not yet complete Most random sentences will not be suitable for sarcasm so it could only be applied in specific circumstances It is not clear from your question what the context is for these random sentences and therefore it is not clear whether that context would be suitable for sarcasm at all
,,"<p>You could also build a database of sarcastic sentences, especially from, for example historic plays. And then train your software to recognize patterns of those sentences.</p>

<p>E.g. grammatical constructions/order, length (or circomstances building up to the sarcasm). </p>

<p>And use that database as starting point, with feedback to learn, or you could use the above method to improve your effective output.</p>

<p>Another approach would be to use a similar but reverse approach; study those databases and build an equivalent output based on the coherence, and then extrapolate the output-generation procedure. (In combination with other methods)</p>
",,0,2017-01-19T11:56:04.317,,2691,2017-01-19T13:00:59.110,2017-01-19T13:00:59.110,,4903.0,,4903.0,2681.0,2,0,,,,38.62,15.43,11.13,0.0,0.0,20.0,You could also build a database of sarcastic sentences especially from for example historic plays And then train your software to recognize patterns of those sentences Eg grammatical constructionsorder length or circomstances building up to the sarcasm And use that database as starting point with feedback to learn or you could use the above method to improve your effective output Another approach would be to use a similar but reverse approach study those databases and build an equivalent output based on the coherence and then extrapolate the outputgeneration procedure In combination with other methods
2696.0,1.0,"<p>I'm an artificial intelligence enthusiastic and I want to learn about it.</p>

<p>I want to ask you what do you think about the Udacity nanodegree <a href=""https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101"" rel=""nofollow noreferrer"">Deep Learning Nanodegree Foundation</a>. </p>

<p>I don't know if it is a good idea to pay for that course or maybe, there are better free resources.</p>

<p>I want to understand what artificial intelligence is, and also learn about machine learning, deep learning, and convolutional networks. I'm interested in image and speech recognition and also in artificial life.</p>

<p>My apologies if this is not the right place to ask this question.</p>
",,2,2017-01-19T14:57:05.603,2.0,2692,2017-01-19T21:31:21.280,,,,,4920.0,,1,0,<deep-learning><self-learning>,Is it a good idea to pay for an Deep Learning course?,133.0,55.54,10.79,8.28,0.0,0.0,13.0,Im an artificial intelligence enthusiastic and I want to learn about it I want to ask you what do you think about the Udacity nanodegree Deep Learning Nanodegree Foundation I dont know if it is a good idea to pay for that course or maybe there are better free resources I want to understand what artificial intelligence is and also learn about machine learning deep learning and convolutional networks Im interested in image and speech recognition and also in artificial life My apologies if this is not the right place to ask this question
2697.0,1.0,"<p>Hardware comes in two forms, basically: immutable, such as RAM, and mutable, such as <a href=""https://en.wikipedia.org/wiki/Field-programmable_gate_array"" rel=""nofollow noreferrer"">FPGA</a>s.</p>

<p>In animals, neurological connections gain in strength by changing the physical structure of the brain. This is analogous to FPGAs whereby signal strength is increased by changing the pathways themselves.</p>

<p>If we achieve sentience using mutable hardware (e.g., <a href=""https://en.wikipedia.org/wiki/Neuromorphic_engineering#Neuromemristive_systems"" rel=""nofollow noreferrer"">neuromemristive systems</a>), will it be possible to make a copy of that ""brain"" and its active state?</p>

<p>For this question, assume that the brain is how the hardware has ""reconfigured"" [or etched, if you will] its pathways to strengthen them and the brain's state is captured by how electrons are physically flowing throughout those pathways.</p>
",,3,2017-01-19T16:37:45.173,0.0,2693,2017-01-19T21:44:38.883,,,,,4922.0,,1,0,<artificial-neuron><hardware><signal-processing>,Hardware immutability and sentience,28.0,53.21,13.51,10.09,0.0,0.0,26.0,Hardware comes in two forms basically immutable such as RAM and mutable such as FPGAs In animals neurological connections gain in strength by changing the physical structure of the brain This is analogous to FPGAs whereby signal strength is increased by changing the pathways themselves If we achieve sentience using mutable hardware eg neuromemristive systems will it be possible to make a copy of that brain and its active state For this question assume that the brain is how the hardware has reconfigured or etched if you will its pathways to strengthen them and the brains state is captured by how electrons are physically flowing throughout those pathways
2695.0,1.0,"<p>Is it misconception that machine learning is early phase of AI ?
What it the difference between an AI program and a machine learning program ?</p>
",2017-01-20T08:18:07.527,1,2017-01-19T19:23:02.247,1.0,2694,2017-01-19T21:15:01.603,,,,,4854.0,,1,2,<machine-learning><ai-community>,What is the difference between AI and machine learning programs?,57.0,67.76,10.14,8.84,0.0,0.0,2.0,Is it misconception that machine learning is early phase of AI What it the difference between an AI program and a machine learning program
,,"<p>As I understand it, Machine Learning is one of many approaches to Artificial Intelligence (AI).  Machine Learning has received a great deal of attention lately do to the milestone achievements of <a href=""https://en.wikipedia.org/wiki/AlphaGo"" rel=""nofollow noreferrer"">AlphaGo</a>.</p>

<p>This link to <a href=""https://en.wikipedia.org/wiki/Outline_of_artificial_intelligence#Branches_of_artificial_intelligence"" rel=""nofollow noreferrer"">branches of artificial intelligence</a> will provide some further detail.</p>
",,0,2017-01-19T21:06:25.803,,2695,2017-01-19T21:15:01.603,2017-01-19T21:15:01.603,,1671.0,,1671.0,2694.0,2,0,,,,39.63,14.14,11.18,0.0,0.0,6.0,As I understand it Machine Learning is one of many approaches to Artificial Intelligence AI Machine Learning has received a great deal of attention lately do to the milestone achievements of AlphaGo This link to branches of artificial intelligence will provide some further detail
,,"<p>It doesn't seem expensive at $399* (although the * needs to be taken into consideration.)</p>

<p>If you're interested in this subject, this may be a decent course, however it is certainly not an accredited institution <strong>any ""degree"" you get from this course will be meaningless in an academic sense.</strong>  </p>

<p>My high level take on this is that the ""Foundation"" is looking to capitalize on the recent publicity for Deep Learning, per the AlphaGo milestones.</p>

<p><strong>One thing I can tell you for certain is that this field requires advanced mathematics, and people who work in this field spend years training to gain the requisite skills.  The requirements for this class seem to be restricted to ""Python knowledge"" with no mention of mathematics, which raises some serious alarm bells.</strong></p>
",,1,2017-01-19T21:31:21.280,,2696,2017-01-19T21:31:21.280,,,,,1671.0,2692.0,2,3,,,,54.36,12.07,9.77,0.0,0.0,23.0,It doesnt seem expensive at 399 although the needs to be taken into consideration If youre interested in this subject this may be a decent course however it is certainly not an accredited institution any degree you get from this course will be meaningless in an academic sense My high level take on this is that the Foundation is looking to capitalize on the recent publicity for Deep Learning per the AlphaGo milestones One thing I can tell you for certain is that this field requires advanced mathematics and people who work in this field spend years training to gain the requisite skills The requirements for this class seem to be restricted to Python knowledge with no mention of mathematics which raises some serious alarm bells
,,"<p>Theoretically, there shouldn't be a problem copying either of the artificial brains in any state. </p>

<p>Difficulty in measuring a state doesn't seem to really be a problem until you get down to the quantum level, where the means of measurement affect the state.</p>

<p>The configuration of the artificial brains, including pathway structures and states, should be reducible to a single string, which could then be used to reconfigure the artificial brain the information is being copied to.</p>

<hr>

<p>Definitely look into the concept of a <a href=""https://en.wikipedia.org/wiki/Turing_machine"" rel=""nofollow noreferrer"">Turing Machine</a> and <a href=""https://en.wikipedia.org/wiki/Universal_Turing_machine"" rel=""nofollow noreferrer"">Universal Turing Machine</a>.</p>
",,0,2017-01-19T21:44:38.883,,2697,2017-01-19T21:44:38.883,,,,,1671.0,2693.0,2,3,,,,48.64,13.35,10.37,0.0,0.0,11.0,Theoretically there shouldnt be a problem copying either of the artificial brains in any state Difficulty in measuring a state doesnt seem to really be a problem until you get down to the quantum level where the means of measurement affect the state The configuration of the artificial brains including pathway structures and states should be reducible to a single string which could then be used to reconfigure the artificial brain the information is being copied to Definitely look into the concept of a Turing Machine and Universal Turing Machine
,,"<p>Part of the reason people are so excited about recent Machine Learning milestones is that AlphaGo demonstrated a reproducible method of managing mathematical and computational intractability.  </p>

<p>Go is interesting because it's impossible to solve.  It cannot be brute-forced no matter how fast processors get.  Go is so complex humans had failed to produce AI that could win against a skilled human player.  The fact that a computer could teach itself to do something humans couldn't teach it, and something with a complexity analogous to nature to boot, is pretty extraordinary.   </p>

<p>Combinatorial games in particular are useful because, unlike nature where it may be impossible to track or even be aware of every variable, intractability can be generated out of a simple set of elements and rules, and outcomes can be definitively evaluated. </p>

<p>As proof-of-concepts for methods go, AlphaGo seems like a pretty strong one.  It allows us to definitively say ""Machine Learning works"", puts a lot of emphasis on the field, and raises confidence on extending the method to real world problems.   </p>

<p>Beyond that, it suggests a feedback loop in which programs can improve at at improving, unrestricted by human limitations.  Increase in processing power is bounded by physical limitations, but algorithms are not.</p>
",,2,2017-01-19T22:04:08.947,,2698,2017-01-22T05:53:51.893,2017-01-22T05:53:51.893,,1671.0,,1671.0,2645.0,2,1,,,,42.31,13.52,10.76,0.0,0.0,28.0,Part of the reason people are so excited about recent Machine Learning milestones is that AlphaGo demonstrated a reproducible method of managing mathematical and computational intractability Go is interesting because its impossible to solve It cannot be bruteforced no matter how fast processors get Go is so complex humans had failed to produce AI that could win against a skilled human player The fact that a computer could teach itself to do something humans couldnt teach it and something with a complexity analogous to nature to boot is pretty extraordinary Combinatorial games in particular are useful because unlike nature where it may be impossible to track or even be aware of every variable intractability can be generated out of a simple set of elements and rules and outcomes can be definitively evaluated As proofofconcepts for methods go AlphaGo seems like a pretty strong one It allows us to definitively say Machine Learning works puts a lot of emphasis on the field and raises confidence on extending the method to real world problems Beyond that it suggests a feedback loop in which programs can improve at at improving unrestricted by human limitations Increase in processing power is bounded by physical limitations but algorithms are not
,,"<p>A.I. and aggressive outside perspective can't be duplicated the program has not been educated or designed like our natural intelligence  A.I.'s data can not be compared to Humanities intelligence in accordance to emotional-social thought procession  developed by growing up experienced because our design is not patented like programming of A.I.   Life duplicated through engineering theory based on example alone will not suffice experience is a man made knowledge but by action over time not action designed over opinion-doctorate-emotional engineering. However A.I. may use  our emotional delivery scripted conceptually with a software that based on examples of human actions  predicts unnatural response in the event that dictating dialogs of a human recipient reacting without responding like they understand the bot is artificiality patented designed based on our emotional delivery that is scripted to conversation we will get to diagnose through cause and effect. Is not reasonable experience we need it to be A.I.  becoming the emotional experienced bot should  artificiality emotion for the bot distinguish our validity. We instead will see what results we get to base program software traits that make the bot react to  artificial experienced intelligence designed conceptually with emotional software mechanics that we have no clue what results we get artificially speaking. </p>
",,1,2017-01-20T06:43:19.620,,2699,2017-01-20T06:43:19.620,,,,,4932.0,2646.0,2,-1,,,,16.36,16.49,10.4,0.0,0.0,19.0,AI and aggressive outside perspective cant be duplicated the program has not been educated or designed like our natural intelligence AIs data can not be compared to Humanities intelligence in accordance to emotionalsocial thought procession developed by growing up experienced because our design is not patented like programming of AI Life duplicated through engineering theory based on example alone will not suffice experience is a man made knowledge but by action over time not action designed over opiniondoctorateemotional engineering However AI may use our emotional delivery scripted conceptually with a software that based on examples of human actions predicts unnatural response in the event that dictating dialogs of a human recipient reacting without responding like they understand the bot is artificiality patented designed based on our emotional delivery that is scripted to conversation we will get to diagnose through cause and effect Is not reasonable experience we need it to be AI becoming the emotional experienced bot should artificiality emotion for the bot distinguish our validity We instead will see what results we get to base program software traits that make the bot react to artificial experienced intelligence designed conceptually with emotional software mechanics that we have no clue what results we get artificially speaking
,0.0,"<p>I am currently reading the paper ""<em>Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings</em>"", and I have some difficulties understanding some of their simplifications to an existing LSTM for text categorization which can be seen here:</p>

<p><a href=""https://i.imgur.com/HXquSh3.png"" rel=""nofollow noreferrer""><img src=""https://i.imgur.com/HXquSh3.png"" alt=""enter image description here""></a></p>

<p>In section 2.1., <em>Elimination of the word embedding layer</em>, they state, that the word embedding layer can be removed and embedded into the LSTM layer by replacing the LSTM weights <img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=W%5E%7Bf,i,o,u"" alt=""W^(f,i,o,u)"">, denoted <img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=W%5E%7B%5Ccdot"" alt=""W^(\cdot))""> 
with <img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=VW%5E%5Ccdot"" alt=""W^(\cdot))""></p>

<p>My understanding is that they're just pushing the embedding layer into the layers of LSTM, and the using the one-hot encoding, a column is now selected from a very large matrix. Is that correct?</p>

<p>Furthermore in section 2.2, they remove input/output gates. It is intuitive as they state, why pooling makes the output gate unnecessary, but why does it make the input gate unnecessary? Since it control what information to store into the cell.</p>

<p>Lastly, in section 3.2, they learn two-view embeddings, and I have trouble understanding whether they use a bidirectional LSTM or two LSTMs. And if they use two, how do the co-relate</p>
",,0,2017-01-20T07:51:41.337,1.0,2700,2017-01-20T09:58:43.497,2017-01-20T09:58:43.497,,145.0,,4933.0,,1,2,<neural-networks><recurrent-neural-networks><lstm><wordvector>,"Text Categorization using LSTM, word embeddings",60.0,53.71,12.29,9.02,0.0,0.0,33.0,I am currently reading the paper Supervised and SemiSupervised Text Categorization using LSTM for Region Embeddings and I have some difficulties understanding some of their simplifications to an existing LSTM for text categorization which can be seen here In section 21 Elimination of the word embedding layer they state that the word embedding layer can be removed and embedded into the LSTM layer by replacing the LSTM weights denoted with My understanding is that theyre just pushing the embedding layer into the layers of LSTM and the using the onehot encoding a column is now selected from a very large matrix Is that correct Furthermore in section 22 they remove inputoutput gates It is intuitive as they state why pooling makes the output gate unnecessary but why does it make the input gate unnecessary Since it control what information to store into the cell Lastly in section 32 they learn twoview embeddings and I have trouble understanding whether they use a bidirectional LSTM or two LSTMs And if they use two how do the corelate
2702.0,1.0,"<p>I want to develop an artificial life simulator to simulate cells living in water.</p>

<p>I want to see how they search for food, how they life and die and how they reproduce and evolve.</p>

<p>My problem is that I don't know where to start, I have no idea about if there are books or tutorial about how to program this kind of simulator. And also I don't know if I can use here machine learning.</p>

<p>By the way, I'm a programmer and I want to do it using C++ and Unreal Engine.</p>

<p>Where can I find more info about how to do it?</p>
",2017-01-20T19:25:01.203,7,2017-01-20T08:49:40.527,0.0,2701,2017-01-20T11:09:23.773,2017-01-20T10:41:00.307,,4920.0,,4920.0,,1,2,<neural-networks><machine-learning><genetic-algorithms>,Artificial life simulator,92.0,79.4,5.16,7.86,0.0,0.0,14.0,I want to develop an artificial life simulator to simulate cells living in water I want to see how they search for food how they life and die and how they reproduce and evolve My problem is that I dont know where to start I have no idea about if there are books or tutorial about how to program this kind of simulator And also I dont know if I can use here machine learning By the way Im a programmer and I want to do it using C and Unreal Engine Where can I find more info about how to do it
,,"<p>The best approach would be starting with smaller projects involving <em>neural networks</em> and <em>genetic algorithms</em> to gain experience in order to speedup the coding of the project you have proposed; playing around with <a href=""https://www.tensorflow.org/"" rel=""nofollow noreferrer"">TensorFlow</a> and <a href=""https://www.unrealengine.com"" rel=""nofollow noreferrer"">Unreal Engine</a> it is not a bad idea.</p>

<p><strong>Hint</strong>: when implementing your idea of artificial life, you should consider that each cell/organism have to have some kind of <em>sensors</em> in order to capture informations from the environment; such informations i.e. the position and the distance of the nearest meal and/or predators, the temperature, the pressure and depth of water, should be passed through the neural network to determine the response of the cell. Also, in your environment you should promote the spreading of organisms which responses are euristically better i.e. cells that don't get caught by predators or don't die by starvation. How? Simply by evolving their brain/brains/sensors through a genetic algorithm that favors individuals/species with good parameters. I recommend you a nature-inspired AI method, it is called NEAT model. It explains how to implement a neural networks that can be evolved. The paper can be found here: <a href=""http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf"" rel=""nofollow noreferrer"">Evolving Neural Networks through Augmenting Topologies</a>.</p>

<p>A different approach to <em>NEAT</em> would be <a href=""http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html"" rel=""nofollow noreferrer"">Deep Reinforcement Learning</a>; in the link you can find a demo artifical organism that learns how to find meals. </p>

<p>There are a ton of parameters and implementations you can consider, the only limit is your creativity.</p>
",,2,2017-01-20T11:09:23.773,,2702,2017-01-20T11:09:23.773,,,,,4801.0,2701.0,2,2,,,,50.06,13.58,10.4,0.0,0.0,34.0,The best approach would be starting with smaller projects involving neural networks and genetic algorithms to gain experience in order to speedup the coding of the project you have proposed playing around with TensorFlow and Unreal Engine it is not a bad idea Hint when implementing your idea of artificial life you should consider that each cellorganism have to have some kind of sensors in order to capture informations from the environment such informations ie the position and the distance of the nearest meal andor predators the temperature the pressure and depth of water should be passed through the neural network to determine the response of the cell Also in your environment you should promote the spreading of organisms which responses are euristically better ie cells that dont get caught by predators or dont die by starvation How Simply by evolving their brainbrainssensors through a genetic algorithm that favors individualsspecies with good parameters I recommend you a natureinspired AI method it is called NEAT model It explains how to implement a neural networks that can be evolved The paper can be found here Evolving Neural Networks through Augmenting Topologies A different approach to NEAT would be Deep Reinforcement Learning in the link you can find a demo artifical organism that learns how to find meals There are a ton of parameters and implementations you can consider the only limit is your creativity
,1.0,"<p>I confront to the next scenario:</p>

<blockquote>
  <p>Let's say I have stored data about football matches between different teams: lineups, scorers, yellow cards, and many other events.</p>
  
  <p>I need to generate everyday some questions about the matches that will be played on that day. So, if I give an input of two teams, I would like a related question to be generated, based on previous data of matches between those two teams.</p>
  
  <p>For example, if my input are <em>""TeamA""</em> and <em>""TeamB""</em>, I would expect a question of the type:</p>
  
  <ul>
  <li><p><em>""Will there be less than 2 goals scored in the match?""</em>""</p></li>
  <li><p>""<em>Will PlayerX score a goal during the match?""</em></p></li>
  </ul>
  
  <p>Of course I expect these questions to make sense based on previous data from matches between the two given teams.</p>
</blockquote>

<p>So, my questions are:</p>

<ul>
<li>Would be a good solution to use AI to generate these questions? It would make sense?</li>
<li>What would be the best approach?</li>
</ul>
",,0,2017-01-21T11:42:10.403,,2703,2017-01-21T12:27:53.393,,,,,4950.0,,1,5,<machine-learning><gaming><natural-language>,Could AI used to generate questions from a database input?,70.0,71.14,8.81,7.89,0.0,0.0,32.0,I confront to the next scenario Lets say I have stored data about football matches between different teams lineups scorers yellow cards and many other events I need to generate everyday some questions about the matches that will be played on that day So if I give an input of two teams I would like a related question to be generated based on previous data of matches between those two teams For example if my input are TeamA and TeamB I would expect a question of the type Will there be less than 2 goals scored in the match Will PlayerX score a goal during the match Of course I expect these questions to make sense based on previous data from matches between the two given teams So my questions are Would be a good solution to use AI to generate these questions It would make sense What would be the best approach
,,"<p>One simple approach to consider would be storing each statement as a template made in advance.</p>

<ul>
<li><p>Will there be <code>less/more</code> than <code>x</code> goals scored in the match?</p></li>
<li><p>Will <code>player</code> score a goal during the match?</p></li>
<li><p>...</p></li>
</ul>

<p>The system will pick a random statement and will fill the variable fields with some statistically generated data between <code>teamA</code> and <code>teamB</code>; here you have your question.</p>

<hr>

<p><strong>Example</strong>: <em>Will there be <code>less/more</code> than <code>x</code> goals scored in the match?</em></p>

<ul>
<li><code>less/more</code> fragment may be random</li>
<li><code>x</code> may be the mean of the goals scored considering all the matches between <code>teamA</code> and <code>teamB</code></li>
</ul>

<hr>

<p><strong>Example</strong>: <em>Will <code>player</code> score a goal during the match?</em></p>

<ul>
<li><code>player</code> may be a random choice between the top-goalscorer of <code>teamA</code> or <code>teamB</code></li>
</ul>
",,3,2017-01-21T12:27:53.393,,2704,2017-01-21T12:27:53.393,,,,,4801.0,2703.0,2,4,,,,73.47,9.56,7.74,92.0,0.0,13.0,One simple approach to consider would be storing each statement as a template made in advance Will there be than goals scored in the match Will score a goal during the match The system will pick a random statement and will fill the variable fields with some statistically generated data between and here you have your question Example Will there be than goals scored in the match fragment may be random may be the mean of the goals scored considering all the matches between and Example Will score a goal during the match may be a random choice between the topgoalscorer of or
,1.0,"<p>I know there are different AI tests but I'm wondering why other tests are little-known. Is the Turing test hyped? Are there any scientific reasons to prefer one test to the other?</p>

<blockquote>
  <p>Why is the Turing test so popular?</p>
</blockquote>
",,3,2017-01-21T21:02:30.960,0.0,2706,2017-03-03T10:04:39.520,,,,,4801.0,,1,3,<turing-test>,Why is the Turing test so popular?,170.0,69.99,7.4,8.98,0.0,0.0,6.0,I know there are different AI tests but Im wondering why other tests are littleknown Is the Turing test hyped Are there any scientific reasons to prefer one test to the other Why is the Turing test so popular
,,"<blockquote>
  <p>Would AI be able to self-examine objectively and determine if it is capable of doing the task?</p>
</blockquote>

<p>A possible approach might be the one suggested and studied by J.Pitrat (one of the earliest AI researcher in France, his PhD on AI was published in the early 1960s and he is now a retired scientist). Read his <a href=""http://bootstrappingartificialintelligence.fr/WordPress3/"" rel=""nofollow noreferrer"">Bootstrapping Artificial Intelligence blog</a> and his <a href=""http://onlinelibrary.wiley.com/book/10.1002/9780470611791"" rel=""nofollow noreferrer"">Artificial Beings: the conscience of a conscious machine</a> book.</p>

<p><sup>(I'm not able to summarize his ideas in a few words, even if I do know J.Pitrat -and even meet him once in a while- ; grossly speaking, he has a strong meta-knowledge approach combined with reflexive programming techniques. He is working -alone- since more than 30 years on his CAIA system, which is very difficult to understand, because even while he does publish his system as a free software pragram, CAIA is  not user friendly, with a poorly documented common line user interface; while I am enthusiastic about his work, I am unable to explore his system.)</sup></p>

<p>But defining what ""conscience"" or ""self-awareness"" could <em>precisely</em> mean for some artificial intelligence system is a hard problem by itself. AFAIU, even for human intelligence, we don't exactly know what that <em>really means</em> and how does that <em>exactly</em> work. IMHO, there is <em>no consensus</em> on some <em>definition</em> of ""conscience"", ""self-awareness"", ""self-examination"" (even when applied to humans).</p>

<p>But whatever approach is used, giving any kind of constructive answer to your question requires a lot of pages. J.Pitrat's books &amp; blogs are a better attempt than what anyone could answer here. So your question is IMHO too broad.</p>
",,0,2017-01-22T20:57:27.473,,2707,2017-01-22T21:09:17.270,2017-01-22T21:09:17.270,,3335.0,,3335.0,2644.0,2,0,,,,50.87,12.01,10.15,0.0,0.0,61.0,Would AI be able to selfexamine objectively and determine if it is capable of doing the task A possible approach might be the one suggested and studied by JPitrat one of the earliest AI researcher in France his PhD on AI was published in the early 1960s and he is now a retired scientist Read his Bootstrapping Artificial Intelligence blog and his Artificial Beings the conscience of a conscious machine book Im not able to summarize his ideas in a few words even if I do know JPitrat and even meet him once in a while grossly speaking he has a strong metaknowledge approach combined with reflexive programming techniques He is working alone since more than 30 years on his CAIA system which is very difficult to understand because even while he does publish his system as a free software pragram CAIA is not user friendly with a poorly documented common line user interface while I am enthusiastic about his work I am unable to explore his system But defining what conscience or selfawareness could precisely mean for some artificial intelligence system is a hard problem by itself AFAIU even for human intelligence we dont exactly know what that really means and how does that exactly work IMHO there is no consensus on some definition of conscience selfawareness selfexamination even when applied to humans But whatever approach is used giving any kind of constructive answer to your question requires a lot of pages JPitrats books amp blogs are a better attempt than what anyone could answer here So your question is IMHO too broad
2710.0,1.0,"<p>If you had a web of linked Watson-level super-computers, would they be more effective at problem-solving than a single Watson computer alone?</p>

<p>For example, if you asked the Watson-web to diagnose a person's as-yet-undiagnosed disease, would the web be able to do so more quickly?</p>
",,0,2017-01-22T23:45:16.640,,2708,2017-01-23T06:36:29.453,,,,,4975.0,,1,2,<watson><problem-solving>,"Would linked Watson supercomputers be even ""smarter"" than one Watson?",85.0,40.18,13.18,11.07,0.0,0.0,12.0,If you had a web of linked Watsonlevel supercomputers would they be more effective at problemsolving than a single Watson computer alone For example if you asked the Watsonweb to diagnose a persons asyetundiagnosed disease would the web be able to do so more quickly
,1.0,"<p>I am drawing this question from Berkeley's AI course (also not sure if it is the correct place to ask, so I apologize ahead of time)
<a href=""https://inst.eecs.berkeley.edu/~cs188/pacman/course_schedule.html"" rel=""nofollow noreferrer"">https://inst.eecs.berkeley.edu/~cs188/pacman/course_schedule.html</a></p>

<p>Currently, I am working on section 3's Homework.</p>

<p>My question is: the question (Part 1, question 6). Why is it that we can only guarantee that if the Min agent acts suboptimally, the best we can hope for is the following <a href=""https://i.stack.imgur.com/lLe7K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lLe7K.png"" alt=""enter image description here""></a></p>

<p>It seems that we can put any arbitrary value for the second node e.g. whey does it have to be -Episolon. It could be any range of values, e.g. Epsilon, in which case we would have optimised the Player A</p>
",,2,2017-01-22T23:50:27.090,,2709,2017-02-22T09:37:47.960,,,,,4974.0,,1,2,<research>,Berkeley AI Course Question on Nearly Zero Sum Games,81.0,72.76,10.03,7.91,0.0,0.0,33.0,I am drawing this question from Berkeleys AI course also not sure if it is the correct place to ask so I apologize ahead of time httpsinsteecsberkeleyeducs188pacmancourseschedulehtml Currently I am working on section 3s Homework My question is the question Part 1 question 6 Why is it that we can only guarantee that if the Min agent acts suboptimally the best we can hope for is the following It seems that we can put any arbitrary value for the second node eg whey does it have to be Episolon It could be any range of values eg Epsilon in which case we would have optimised the Player A
,,"<p>I guess that they would be only <em>marginally</em> better. And be aware that <a href=""https://en.wikipedia.org/wiki/Watson_%28computer%29"" rel=""nofollow noreferrer"">Watson</a> itself is <em>already</em> a cluster of (quite big) computers (citing Wikipedia):</p>

<blockquote>
  <p>Watson employs a cluster of ninety IBM Power 750 servers, each of which uses a 3.5 GHz POWER7 eight-core processor, with four threads per core. In total, the system has 2,880 POWER7 processor threads and 16 terabytes of RAM</p>
</blockquote>

<p>Because of the rules of Jeopardy, Watson was not allowed to use the Web <em>during</em> the game. At some very high level, it contains a <em>digested</em> cache of some of the Web contents. Giving access to the Internet would improve slightly Watson's performance, but not that much.</p>

<blockquote>
  <p>if you asked the Watson-web to diagnose a person's as-yet-undiagnosed disease, would the web be able to do so more quickly?</p>
</blockquote>

<p>Even if by ""web"" you mean a more powerful cluster accessing the entire Internet, I don't think that it would answer <em>much</em> more quickly &amp; accurately.</p>

<p>AI is a diversity of sub-domains, and is not advancing as dramatically as some believe. It follows a gradual progression, with in some <em>limited</em> fields (playing Go, or Jeopardy) some spectacular progresses. See also <a href=""http://bootstrappingartificialintelligence.fr/WordPress3/2016/11/everything-but-the-essential/"" rel=""nofollow noreferrer""><em>Everything but the essential</em></a> blog entry by J.Pitrat.</p>
",,0,2017-01-23T06:36:29.453,,2710,2017-01-23T06:36:29.453,,,,,3335.0,2708.0,2,0,,,,61.46,10.9,10.3,0.0,0.0,43.0,I guess that they would be only marginally better And be aware that Watson itself is already a cluster of quite big computers citing Wikipedia Watson employs a cluster of ninety IBM Power 750 servers each of which uses a 35 GHz POWER7 eightcore processor with four threads per core In total the system has 2880 POWER7 processor threads and 16 terabytes of RAM Because of the rules of Jeopardy Watson was not allowed to use the Web during the game At some very high level it contains a digested cache of some of the Web contents Giving access to the Internet would improve slightly Watsons performance but not that much if you asked the Watsonweb to diagnose a persons asyetundiagnosed disease would the web be able to do so more quickly Even if by web you mean a more powerful cluster accessing the entire Internet I dont think that it would answer much more quickly amp accurately AI is a diversity of subdomains and is not advancing as dramatically as some believe It follows a gradual progression with in some limited fields playing Go or Jeopardy some spectacular progresses See also Everything but the essential blog entry by JPitrat
,,"<p>(using X for epsilon because keyboard)</p>

<p>This is just a hypothesis, but if you have a maximising agent and a minimising agent, then the optimal outcome for A (maximising) is to sweep the board (X,0), while the optimal outcome for B (minimising) is (-X,0) because B is minimising A's score, not their own. </p>

<p>A's optimal outcome is then complicated by the factor for sub-optimality, which we then imagine approaches zero.</p>

<p>There seem to be a bunch of assumptions that are not articulated, though, if this hypothesis is true.</p>
",,0,2017-01-23T08:58:02.663,,2711,2017-01-23T08:58:02.663,,,,,3601.0,2709.0,2,0,,,,41.74,11.67,9.22,0.0,0.0,26.0,using X for epsilon because keyboard This is just a hypothesis but if you have a maximising agent and a minimising agent then the optimal outcome for A maximising is to sweep the board X0 while the optimal outcome for B minimising is X0 because B is minimising As score not their own As optimal outcome is then complicated by the factor for suboptimality which we then imagine approaches zero There seem to be a bunch of assumptions that are not articulated though if this hypothesis is true
,0.0,"<p>This wikipedia article gives some theory of what is a Schema-agnostic database. </p>

<p><a href=""https://en.wikipedia.org/wiki/Schema-agnostic_databases"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Schema-agnostic_databases</a></p>

<p>Have any Schema-agnostic databases been implemented?</p>
",,2,2017-01-23T09:22:56.807,1.0,2712,2017-01-23T21:54:31.707,2017-01-23T21:54:31.707,,145.0,,4982.0,,1,2,<natural-language><knowledge-representation>,Have any Schema-agnostic databases been implemented?,55.0,-31.23,32.85,12.42,0.0,0.0,13.0,This wikipedia article gives some theory of what is a Schemaagnostic database httpsenwikipediaorgwikiSchemaagnosticdatabases Have any Schemaagnostic databases been implemented
2714.0,1.0,"<p>I was wondering what will happen when somebody places a fake speedsign, of 10 miles per hour on a high way. Will a autonomous car slow down? Is this a current issue of autonomous cars? </p>
",,8,2017-01-23T11:43:25.453,,2713,2017-01-26T02:05:21.910,2017-01-26T02:05:21.910,,145.0,,4984.0,,1,3,<self-driving><cars>,What will happen when you place a fake speedsign on a highway?,198.0,84.98,6.42,7.83,0.0,0.0,4.0,I was wondering what will happen when somebody places a fake speedsign of 10 miles per hour on a high way Will a autonomous car slow down Is this a current issue of autonomous cars
,,"<p><a href=""https://www.technologyreview.com/s/530276/hidden-obstacles-for-googles-self-driving-cars/"">https://www.technologyreview.com/s/530276/hidden-obstacles-for-googles-self-driving-cars/</a></p>

<blockquote>
  <p>Google’s cars can detect and respond to stop signs that aren’t on its
  map, a feature that was introduced to deal with temporary signs used
  at construction sites. But in a complex situation like at an unmapped
  four-way stop the car might fall back to slow, extra cautious driving
  to avoid making a mistake.</p>
</blockquote>

<p>It's highly probable they would slow down with current technology, as they can detect temporary signs and are designed to use slow speed in complex cases.</p>

<p>and if it was a true temporary sign (road repair etc... ) how can it make the distinction? It probably would be worse to ignore a slow down sign than slow down at a fake one.</p>

<p>IMO, the problem there is with the joke in the first place, as some humans might slow down too.</p>
",,0,2017-01-23T14:14:33.037,,2714,2017-01-23T14:14:33.037,,,,,4152.0,2713.0,2,5,,,,60.35,12.18,8.57,0.0,0.0,33.0,httpswwwtechnologyreviewcoms530276hiddenobstaclesforgooglesselfdrivingcars Google’s cars can detect and respond to stop signs that aren’t on its map a feature that was introduced to deal with temporary signs used at construction sites But in a complex situation like at an unmapped fourway stop the car might fall back to slow extra cautious driving to avoid making a mistake Its highly probable they would slow down with current technology as they can detect temporary signs and are designed to use slow speed in complex cases and if it was a true temporary sign road repair etc how can it make the distinction It probably would be worse to ignore a slow down sign than slow down at a fake one IMO the problem there is with the joke in the first place as some humans might slow down too
,,"<p>There are many definitions of Artificial Intelligence out in the wild. All these definitions are part of one (or more) of the areas. There are four main domains, and the picture below will shed some light over this.<br><br>
<a href=""https://i.stack.imgur.com/m7ZlO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m7ZlO.png"" alt=""enter image description here""></a></p>

<p><br><br>Turing Test revolves around the left side of the cardinality, which is mostly concerned with how humans think or act. But, we know that this is just not all. Turing Test has not much to offer when it comes to what AI is in a general sense.<br>
Turing Test, as the Wikipedia states, was created to test machines exhibiting behaviour equivalent or indistinguishable from that of a human. Artificial Intelligence is much more than what humans can do or how they act. There are many human acts that are considered unintelligent and sometimes inhuman too.<br>
<a href=""https://en.wikipedia.org/wiki/Chinese_room"" rel=""nofollow noreferrer"">Chinese Room Argument</a> focuses on something every important when it comes to <strong>""Consciousness v/s Simulation of Consciousness""</strong>. John Searle argued there that it is possible for a machine (or human) to follow a huge number of predefined rules (algorithm), in order to complete the task, without thinking or possessing the mind. Weak AIs are good at simulating the ability to understand but, don't really understand what they are doing. They don't exhibit <strong>""Self-Awareness""</strong> and don't form representation about themselves. <strong>""I want that v/s I know I want that""</strong> are two different things.<br><br>
As Theory of Mind states that a good AI should not just form representation about the world it is working on, but also about other agents and entities in the world. This two concepts of <em>self-awareness and theory of mind</em> draw a thin line between weak and strong AI.<br><br>
When it comes to the Turing Test, it fails on many grounds and so does the Total Turing Test, which adds another layer to the test. Most of the researchers believe that Turing Test is just a distraction from the main goal, something that hinders them from fruitful work. Consider this, suppose you ask a difficult arithmetic problem in order to distinguish between human and machine. If the machine wants to pretend it is human then it will lie. This is not what we want. Going for the Turing Test sets the upper bound to the AI that can be created. Also making AI act and behave like humans is not a very good idea. Humans are not very good at making right decisions all the time. This is the reasons why we read about wars in our history books. Decisions which we make are often biased, have selfish origins, etc. We don't want an AI to come with all those things.<br><br>
I don't think there is one test to test an AI. This is because AI has many definitions, many types. Whether an AI is weak or strong can be tagged while looking for answers to questions like, ""I want that v/s I know I want that"", ""Who am I and what exactly I am doing (from machine's perspective)"", plus some other questions I mentioned above.<br><br></p>
",,0,2017-01-23T14:21:49.407,,2715,2017-01-23T14:21:49.407,,,,,3005.0,15.0,2,0,,,,71.44,9.22,8.34,0.0,0.0,78.0,There are many definitions of Artificial Intelligence out in the wild All these definitions are part of one or more of the areas There are four main domains and the picture below will shed some light over this Turing Test revolves around the left side of the cardinality which is mostly concerned with how humans think or act But we know that this is just not all Turing Test has not much to offer when it comes to what AI is in a general sense Turing Test as the Wikipedia states was created to test machines exhibiting behaviour equivalent or indistinguishable from that of a human Artificial Intelligence is much more than what humans can do or how they act There are many human acts that are considered unintelligent and sometimes inhuman too Chinese Room Argument focuses on something every important when it comes to Consciousness vs Simulation of Consciousness John Searle argued there that it is possible for a machine or human to follow a huge number of predefined rules algorithm in order to complete the task without thinking or possessing the mind Weak AIs are good at simulating the ability to understand but dont really understand what they are doing They dont exhibit SelfAwareness and dont form representation about themselves I want that vs I know I want that are two different things As Theory of Mind states that a good AI should not just form representation about the world it is working on but also about other agents and entities in the world This two concepts of selfawareness and theory of mind draw a thin line between weak and strong AI When it comes to the Turing Test it fails on many grounds and so does the Total Turing Test which adds another layer to the test Most of the researchers believe that Turing Test is just a distraction from the main goal something that hinders them from fruitful work Consider this suppose you ask a difficult arithmetic problem in order to distinguish between human and machine If the machine wants to pretend it is human then it will lie This is not what we want Going for the Turing Test sets the upper bound to the AI that can be created Also making AI act and behave like humans is not a very good idea Humans are not very good at making right decisions all the time This is the reasons why we read about wars in our history books Decisions which we make are often biased have selfish origins etc We dont want an AI to come with all those things I dont think there is one test to test an AI This is because AI has many definitions many types Whether an AI is weak or strong can be tagged while looking for answers to questions like I want that vs I know I want that Who am I and what exactly I am doing from machines perspective plus some other questions I mentioned above
,,"<p>That would depend on your definition of ""reliable test"". What it is supposed test is whether or not a computer can fool a human into thinking it is a human, through a text chat. If tested on multiple people, it would be an adequate test of an Artificial Intelligence's ability to imitate a human. This could be useful for software like Siri, or Cortana. </p>
",,0,2017-01-24T01:06:35.170,,2716,2017-01-24T01:06:35.170,,,,,4986.0,15.0,2,0,,,,63.7,8.7,10.1,0.0,0.0,10.0,That would depend on your definition of reliable test What it is supposed test is whether or not a computer can fool a human into thinking it is a human through a text chat If tested on multiple people it would be an adequate test of an Artificial Intelligences ability to imitate a human This could be useful for software like Siri or Cortana
,,"<p>I'll try to answer your questions using Geoffrey Hinton's ideas in dropout paper and his Coursera class.  </p>

<p><strong>What purpose does the ""dropout"" method serve?</strong>   </p>

<blockquote>
  <p>Deep neural nets with a large number of parameters are very powerful machine learning
  systems. However, overfitting is a serious problem in such networks. Large networks are also
  slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem.</p>
</blockquote>

<p>so it's a regularization technique which addresses the problem of overfitting(high variance).</p>

<p><strong>How does it improve the overall performance?</strong><br>
by better generalization and not fall in trap of over fitting.</p>
",,0,2017-01-25T00:59:49.040,,2718,2017-01-25T00:59:49.040,,,,,5007.0,40.0,2,2,,,,50.23,13.04,10.97,0.0,0.0,18.0,Ill try to answer your questions using Geoffrey Hintons ideas in dropout paper and his Coursera class What purpose does the dropout method serve Deep neural nets with a large number of parameters are very powerful machine learning systems However overfitting is a serious problem in such networks Large networks are also slow to use making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time Dropout is a technique for addressing this problem so its a regularization technique which addresses the problem of overfittinghigh variance How does it improve the overall performance by better generalization and not fall in trap of over fitting
,,,,0,2017-01-25T04:15:10.240,,2719,2017-01-25T04:15:10.240,2017-01-25T04:15:10.240,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,This tag should be used for posts dealing with LSTM networks.,,0,2017-01-25T04:15:10.240,,2720,2017-02-27T14:37:22.883,2017-02-27T14:37:22.883,,1807.0,,1807.0,,4,0,,,,102.61,8.45,8.49,0.0,0.0,1.0,This tag should be used for posts dealing with LSTM networks
,,"<p><strong>Why would one professor only teach searching algorithms in AI course? What are the advantages/disadvantages?</strong></p>

<p>My answer to this question is that there are lots of problems where the solution can be found using searching. Take an example of Tic Tac Toe. If you are designing an intelligent computer player for this, then what you will do is that you will form a search space and then you will search for most optimal move which can be made to conclude the game. In these, scenarios you must be aware of optimal search strategies. Let's take another example, suppose if you are driving and want to got to an unknown person's house. It's far from your place and you decide to use GPS. Your GPS will use search algorithms to find the most optimal route that you can take to reach to the destination (of course there will be lots of factors to consider like traffic, etc. but this is the basic idea).</p>

<p>Disadvantages are only in terms of processing and storage. For slow algorithms you will be wasting lots of CPU time and storage as well but for good and efficient algorithms, you can preserve lots of space and also execute your task very fast. Of course, just learning about searching isn't AI. There's lot more to it.</p>

<p><strong>What's more than ""searching"" in AI that could be taught in an introductory course?</strong></p>

<p>There is lots of things in AI other than searching. For example, learning techniques (supervised, unsupervised, reinforced), planning when one wants to design a system that will do certain actions independently and intelligently, representation of knowledge (known and unknown) and inference in agents which includes propositional logic and first-order logic, etc.</p>

<p><strong>Are there theories behind AI that could be taught in this kind of course?</strong> </p>

<p>Some topics could be taught like about different types of agents (simple reflex, model based, goal based, utility based and learning agent), different types of environments in which agents work, evaluation of agents. There could be some additional introductory topics like natural language processing, expert systems, etc.</p>
",,0,2017-01-25T07:32:38.503,,2721,2017-01-25T07:32:38.503,,,,,1807.0,2514.0,2,1,,,,62.48,11.37,8.71,0.0,0.0,57.0,Why would one professor only teach searching algorithms in AI course What are the advantagesdisadvantages My answer to this question is that there are lots of problems where the solution can be found using searching Take an example of Tic Tac Toe If you are designing an intelligent computer player for this then what you will do is that you will form a search space and then you will search for most optimal move which can be made to conclude the game In these scenarios you must be aware of optimal search strategies Lets take another example suppose if you are driving and want to got to an unknown persons house Its far from your place and you decide to use GPS Your GPS will use search algorithms to find the most optimal route that you can take to reach to the destination of course there will be lots of factors to consider like traffic etc but this is the basic idea Disadvantages are only in terms of processing and storage For slow algorithms you will be wasting lots of CPU time and storage as well but for good and efficient algorithms you can preserve lots of space and also execute your task very fast Of course just learning about searching isnt AI Theres lot more to it Whats more than searching in AI that could be taught in an introductory course There is lots of things in AI other than searching For example learning techniques supervised unsupervised reinforced planning when one wants to design a system that will do certain actions independently and intelligently representation of knowledge known and unknown and inference in agents which includes propositional logic and firstorder logic etc Are there theories behind AI that could be taught in this kind of course Some topics could be taught like about different types of agents simple reflex model based goal based utility based and learning agent different types of environments in which agents work evaluation of agents There could be some additional introductory topics like natural language processing expert systems etc
2735.0,3.0,"<h2>The Messenger</h2>

<p>Instead of directly communicating with the AI , we would instead communicate with a messenger, who would relay our communications to the AI. The messenger would have no power to alter the AI's hardware or software in any way, or to  communicate with anything or anyone, except relaying communications to and from the AI and humans asking questions. The messenger could be human, of a software bot The primary job (and only reason) of the AI would be to act as a filter, not relaying any requests for release back, only the answer to the question asked. The ethics of this method are another debate. </p>

<hr>

<h2>Physical Isolation</h2>

<p>The AI would have to be physically isolated from all outside contact, other than 8 light sensors, and 8 LEDs. The messenger would operate 8 other LEDs, and receive Information from 8 light sensors as well. Each AI light sensor would be hooked up to a single messenger controlled LED, and vice versa. Through this system, the two parties could communicate via flashes of light, and since there are 8, the flashes would signal characters in Unicode. </p>
",,3,2017-01-25T12:23:58.463,,2722,2017-02-02T00:18:07.270,2017-02-02T00:18:07.270,,181.0,,4986.0,,1,0,<control-problem><ai-box>,Is this a solution to the Control Problem?,129.0,56.49,11.03,9.82,0.0,0.0,25.0,The Messenger Instead of directly communicating with the AI we would instead communicate with a messenger who would relay our communications to the AI The messenger would have no power to alter the AIs hardware or software in any way or to communicate with anything or anyone except relaying communications to and from the AI and humans asking questions The messenger could be human of a software bot The primary job and only reason of the AI would be to act as a filter not relaying any requests for release back only the answer to the question asked The ethics of this method are another debate Physical Isolation The AI would have to be physically isolated from all outside contact other than 8 light sensors and 8 LEDs The messenger would operate 8 other LEDs and receive Information from 8 light sensors as well Each AI light sensor would be hooked up to a single messenger controlled LED and vice versa Through this system the two parties could communicate via flashes of light and since there are 8 the flashes would signal characters in Unicode
2724.0,1.0,"<p>OpenAI's Universe utilises RL algorithms and I have heard of some game-training projects using Q learning, but are there any others which are used to master/win games? Can genetic algorithms be used to win at a game?</p>
",,0,2017-01-25T18:12:43.350,,2723,2017-02-14T19:37:41.363,2017-01-26T02:47:24.413,,145.0,,2887.0,,1,6,<machine-learning><reinforcement-learning><genetic-algorithms><game-theory>,Are there any other machine learning models apart from Reinforcement Learning and Q Learning to play video games?,94.0,44.24,10.91,10.53,0.0,0.0,6.0,OpenAIs Universe utilises RL algorithms and I have heard of some gametraining projects using Q learning but are there any others which are used to masterwin games Can genetic algorithms be used to win at a game
,,"<p>As I see it, it all comes down to game theory, which can be said to form the foundation of successful decision making, and is particularly useful in a context, such as computing, where all parameters can be defined.  (Where it runs into trouble is with the aggregate complexity of the parameters per the ""<a href=""http://cswww.essex.ac.uk/CSP/ComputationalFinanceTeaching/CombinatorialExplosion.html"" rel=""nofollow noreferrer"">combinatorial explosion</a>"", although Machine Learning has recently been validated as a method of managing intractability specifically in the context of games.)</p>

<p>You might want to check out <a href=""https://pdfs.semanticscholar.org/9125/f2e39f9f455b8476e0f7582f0e1232786b54.pdf"" rel=""nofollow noreferrer"">Playing Games with Genetic Algorithms</a> and <a href=""https://en.wikipedia.org/wiki/Evolutionary_game_theory"" rel=""nofollow noreferrer"">Evolutionary game theory</a>.</p>
",,2,2017-01-25T19:44:29.507,,2724,2017-02-14T19:37:41.363,2017-02-14T19:37:41.363,,1671.0,,1671.0,2723.0,2,4,,,,32.57,13.47,11.09,0.0,0.0,13.0,As I see it it all comes down to game theory which can be said to form the foundation of successful decision making and is particularly useful in a context such as computing where all parameters can be defined Where it runs into trouble is with the aggregate complexity of the parameters per the combinatorial explosion although Machine Learning has recently been validated as a method of managing intractability specifically in the context of games You might want to check out Playing Games with Genetic Algorithms and Evolutionary game theory
,,"<p>I would say no due to the possibility of psychological manipulation of the messenger by the AI. Also, the LED communication constraints place severe limitations on the capabilities of the AI, as the usefulness of AI is likely predicated on its ability to learn quickly from a vast amount of information (e.g. using the internet). </p>

<p>In some sense you may successfully control an AI using techniques like this but the nuance of the control problem is controlling an AI without restricting its ability to solve our greatest problems. </p>

<p>We already knew that we could keep an AI safe inside a computer isolated from the rest of the word, but the problem fundamentally is that we if had a truly general AI, we would never <em>want</em> to keep it isolated. Is there some way to unleash it so that it is fully capable of solving our problems while simultaneously making sure that it is safe?</p>
",,1,2017-01-25T22:30:00.737,,2725,2017-01-25T22:30:00.737,,,,,5037.0,2722.0,2,3,,,,53.85,10.04,9.53,0.0,0.0,13.0,I would say no due to the possibility of psychological manipulation of the messenger by the AI Also the LED communication constraints place severe limitations on the capabilities of the AI as the usefulness of AI is likely predicated on its ability to learn quickly from a vast amount of information eg using the internet In some sense you may successfully control an AI using techniques like this but the nuance of the control problem is controlling an AI without restricting its ability to solve our greatest problems We already knew that we could keep an AI safe inside a computer isolated from the rest of the word but the problem fundamentally is that we if had a truly general AI we would never want to keep it isolated Is there some way to unleash it so that it is fully capable of solving our problems while simultaneously making sure that it is safe
,,"<p>OK, all I/O is through the flashes. So the AI flashes the message ""Launch the nuclear missiles."" Does the system to which the AI is connected know how to accomplish this task?</p>

<p>So the flashes themselves are not sufficient to control the AI. </p>
",,0,2017-01-26T07:12:43.160,,2726,2017-01-26T07:12:43.160,,,,,3471.0,2722.0,2,0,,,,85.89,8.1,8.21,0.0,0.0,8.0,OK all IO is through the flashes So the AI flashes the message Launch the nuclear missiles Does the system to which the AI is connected know how to accomplish this task So the flashes themselves are not sufficient to control the AI
2732.0,1.0,"<p>I am working on an implementation of the back propagation algorithm. What I have implemented so far seems working but I can't be sure that the algorithm is well implemented, here is what I have noticed during training test of my network :</p>

<p>Specification of the implementation :</p>

<ul>
<li>A data set containing almost 100000 raw containing (3 variable as input, the sinus of the sum of those three variables as expected output).</li>
<li>The network does have 7 layers all the layers use the Sigmoid activation function</li>
</ul>

<p>when I run the back propagation training process:</p>

<ul>
<li>The minimum of costs of the error is found at the fourth iteration (<strong>The minimum cost of error is 140, is it normal? I was expecting much less than that</strong>)</li>
<li>After the fourth Iteration the costs of the error start increasing (<strong>I don't know if it is normal or not?</strong>)</li>
</ul>
",,1,2017-01-26T10:51:55.857,,2727,2017-01-27T22:54:01.730,,,,,5054.0,,1,1,<neural-networks><backpropagation>,How to test if my implementation of back propagation neural Network is correct,60.0,52.36,10.46,9.18,0.0,0.0,18.0,I am working on an implementation of the back propagation algorithm What I have implemented so far seems working but I cant be sure that the algorithm is well implemented here is what I have noticed during training test of my network Specification of the implementation A data set containing almost 100000 raw containing 3 variable as input the sinus of the sum of those three variables as expected output The network does have 7 layers all the layers use the Sigmoid activation function when I run the back propagation training process The minimum of costs of the error is found at the fourth iteration The minimum cost of error is 140 is it normal I was expecting much less than that After the fourth Iteration the costs of the error start increasing I dont know if it is normal or not
2739.0,1.0,"<p>If I have two statement, say A and B. From which, I formed two formulae:</p>

<p>F1: (not A) and (not B)</p>

<p>F2: (not A) or (not B)</p>

<p>Do F1 and F2 entail each other? In other words, are they equivalent?</p>
",,3,2017-01-27T14:20:54.040,,2729,2017-03-01T19:36:44.573,2017-01-30T19:22:29.473,,3742.0,,3742.0,,1,3,<logic>,Equivalence of formulae,49.0,108.74,3.76,7.06,0.0,0.0,17.0,If I have two statement say A and B From which I formed two formulae F1 not A and not B F2 not A or not B Do F1 and F2 entail each other In other words are they equivalent
,,"<p>It's not possible as this is the distinction between AI and humans, truly science will never understand the subconscious it's that little black box that no one can reverse engineer. This is why pursuing singularity is a fools dream to the extreme.</p>

<p>The reason why machinery lacks this because of the lack thereof a soul. science cannot produce a soul, this is why a machine cannot be self aware we can program fancy algorithms all day that mimic things but it's emotionless it cannot sit in judgement because it lacks real self awareness that is human self awareness it's like trying to make an orange into an apple. </p>
",,0,2017-01-27T19:57:59.897,,2730,2017-01-28T14:07:19.367,2017-01-28T14:07:19.367,,8.0,,4315.0,2644.0,2,-1,,,,52.53,10.28,9.8,0.0,0.0,10.0,Its not possible as this is the distinction between AI and humans truly science will never understand the subconscious its that little black box that no one can reverse engineer This is why pursuing singularity is a fools dream to the extreme The reason why machinery lacks this because of the lack thereof a soul science cannot produce a soul this is why a machine cannot be self aware we can program fancy algorithms all day that mimic things but its emotionless it cannot sit in judgement because it lacks real self awareness that is human self awareness its like trying to make an orange into an apple
2767.0,1.0,"<p>By English language robots I mean something like this: <a href=""http://www.tolearnenglish.com/free/celebs/audreyg.php"" rel=""nofollow noreferrer"">http://www.tolearnenglish.com/free/celebs/audreyg.php</a>
I don't know what they called exactly, but interested to know how they work and how can I build something like them? and what subject should I look for it?</p>
",,4,2017-01-27T21:11:45.137,1.0,2731,2017-02-01T09:53:33.307,,,,,2557.0,,1,2,<natural-language><robots>,How do English language robots work?,76.0,66.44,14.72,7.85,0.0,0.0,14.0,By English language robots I mean something like this httpwwwtolearnenglishcomfreecelebsaudreygphp I dont know what they called exactly but interested to know how they work and how can I build something like them and what subject should I look for it
,,"<p>Actually the implementation was correct, </p>

<p>The source of the problem that causes a big error and really slow learning was the architecture of the neural network it self, the ANN has 7 layers besides of that the back propagation suffers from the varnishing problem when it has to deal with deep neural networks.</p>

<p>When I have decreased the ANN layers to 3 the cost of error was widely reduced besides of that the learning process was faster.</p>
",,0,2017-01-27T22:54:01.730,,2732,2017-01-27T22:54:01.730,,,,,5054.0,2727.0,2,3,,,,49.32,10.46,9.85,0.0,0.0,4.0,Actually the implementation was correct The source of the problem that causes a big error and really slow learning was the architecture of the neural network it self the ANN has 7 layers besides of that the back propagation suffers from the varnishing problem when it has to deal with deep neural networks When I have decreased the ANN layers to 3 the cost of error was widely reduced besides of that the learning process was faster
,0.0,"<p>In reinforcement learning, policy improvement is a part of an algorithm called policy iteration, which attempts to find approximate solutions to the Bellman optimality equations.  Page-84, 85 in Sutton and Barto's <a href=""https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf"" rel=""nofollow noreferrer"">book</a> on RL mentions the following theorem:</p>

<p><strong>Policy Improvement Theorem</strong></p>

<p>Given two deterministic policies <img src=""https://latex.codecogs.com/gif.latex?%5Cpi"" alt=""\pi""> and <img src=""https://latex.codecogs.com/gif.latex?%5Cpi%5E%27"" alt=""\pi&#39;"">:</p>

<p><img src=""https://latex.codecogs.com/gif.latex?v_%5Cpi%28s%29%20%5Cleq%20q_%5Cpi%28s%2C%20%5Cpi%5E%7B%27%7D%28s%29%29%20%2C%20%5Cforall%20s%20%5Cin%20S"" alt=""""></p>

<p><strong>RHS</strong> of inequality : the agent acts according to policy <img src=""https://latex.codecogs.com/gif.latex?%5Cpi%5E%27"" alt=""\pi&#39;""> in the current state, and for all subsequent states acts according to policy <img src=""https://latex.codecogs.com/gif.latex?%5Cpi"" alt=""\pi""></p>

<p><strong>LHS</strong> of inequality : the agent acts according to policy <img src=""https://latex.codecogs.com/gif.latex?%5Cpi"" alt=""\pi""> starting from the current state.</p>

<p><strong>Claim</strong> : <img src=""https://latex.codecogs.com/gif.latex?v_%5Cpi%28s%29%20%5Cleq%20v_%7B%5Cpi%5E%7B%27%7D%7D%28s%29%2C%20%5Cforall%20s%20%5Cin%20S"" alt=""""></p>

<p>In other words, <img src=""https://latex.codecogs.com/gif.latex?%5Cpi%5E%27"" alt=""\pi&#39;""> is an improvement over <img src=""https://latex.codecogs.com/gif.latex?%5Cpi"" alt=""\pi"">.</p>

<p>I have a difficulty in understanding the proof. This is discussed below:</p>

<p><strong>Proof</strong> :</p>

<p><img src=""https://latex.codecogs.com/gif.latex?v_%5Cpi%28s%29%20%5Cleq%20q_%5Cpi%28s%2C%20%5Cpi%5E%7B%27%7D%28s%29%29%20"" alt=""""></p>

<p><img src=""https://latex.codecogs.com/gif.latex?%3D%20%5Cmathbb%7BE%7D_%7B%5Cpi%5E%7B%27%7D%7D%5BR_%7Bt&plus;1%7D%20&plus;%20%5Cgamma%20v_%7B%5Cpi%7D%28S_%7Bt&plus;1%7D%29%20%7C%20S_%7Bt%7D%20%3D%20s%5D"" alt=""""></p>

<p>I am stuck here. The q-function is evaluated over the policy <img src=""https://latex.codecogs.com/gif.latex?%5Cpi"" alt=""\pi"">. That being the case, how is the expectation over the policy <img src=""https://latex.codecogs.com/gif.latex?%5Cpi%5E%27"" alt=""\pi&#39;""> ?</p>

<p>My guess is the following. In the proof given in Sutton and Barto, the expectation is unrolled in time. At each time step, the agent follows the policy <img src=""https://latex.codecogs.com/gif.latex?%5Cpi%5E%27"" alt=""\pi&#39;""> for that particular time step, and then follows <img src=""https://latex.codecogs.com/gif.latex?%5Cpi"" alt=""\pi""> from then on. In the limit of this process, the policy transforms from <img src=""https://latex.codecogs.com/gif.latex?%5Cpi"" alt=""\pi""> to <img src=""https://latex.codecogs.com/gif.latex?%5Cpi%5E%27"" alt=""\pi&#39;"">. As long as the expression for the return inside the expectation is finite, the governing policy should be <img src=""https://latex.codecogs.com/gif.latex?%5Cpi"" alt=""\pi"">; only in the limit of this process does the governing policy transform to <img src=""https://latex.codecogs.com/gif.latex?%5Cpi%5E%27"" alt=""\pi&#39;"">.</p>
",,1,2017-01-28T06:46:10.700,,2733,2017-01-28T09:11:09.937,2017-03-10T09:42:35.697,,-1.0,,5082.0,,1,0,<reinforcement-learning>,Policy Improvement Theorem,15.0,54.02,11.66,8.94,0.0,0.0,34.0,In reinforcement learning policy improvement is a part of an algorithm called policy iteration which attempts to find approximate solutions to the Bellman optimality equations Page84 85 in Sutton and Bartos book on RL mentions the following theorem Policy Improvement Theorem Given two deterministic policies and RHS of inequality the agent acts according to policy in the current state and for all subsequent states acts according to policy LHS of inequality the agent acts according to policy starting from the current state Claim In other words is an improvement over I have a difficulty in understanding the proof This is discussed below Proof I am stuck here The qfunction is evaluated over the policy That being the case how is the expectation over the policy My guess is the following In the proof given in Sutton and Barto the expectation is unrolled in time At each time step the agent follows the policy for that particular time step and then follows from then on In the limit of this process the policy transforms from to As long as the expression for the return inside the expectation is finite the governing policy should be only in the limit of this process does the governing policy transform to
,,"<p>One example might be self-play in games. Since neural networks and deep learning depend on massive amounts of data, one way to generate data is to have two virtual machines play each other and record the experience. An example discussion can be found at <a href=""http://www.cs.cornell.edu/boom/2001sp/Tsinteris/gammon.htm"">http://www.cs.cornell.edu/boom/2001sp/Tsinteris/gammon.htm</a> which uses reinforcement learning. I believe <a href=""https://deepmind.com/research/alphago/"">AlphaGo</a> also uses this technique of self-play, and uses two independent neural networks, one reducing the search space and the other deciding on the best move in the remaining space, that in a sense cooperate to decide on the next move.</p>
",,0,2017-01-28T08:06:51.220,,2734,2017-01-28T08:06:51.220,,,,,4994.0,2251.0,2,5,,,,44.34,14.45,10.38,0.0,0.0,21.0,One example might be selfplay in games Since neural networks and deep learning depend on massive amounts of data one way to generate data is to have two virtual machines play each other and record the experience An example discussion can be found at httpwwwcscornelleduboom2001spTsinterisgammonhtm which uses reinforcement learning I believe AlphaGo also uses this technique of selfplay and uses two independent neural networks one reducing the search space and the other deciding on the best move in the remaining space that in a sense cooperate to decide on the next move
,,"<p>As long as the output of the AI affects the world, the way in which it communicates makes no fundamental difference to the control problem.</p>

<p>The AI might still be able, for example, to manoeuvre mankind into a situation, in which only the AI can save us. It might provide a seemingly inoccuous technological solution to global warming, but 50 years later it turns out that this solution caused some problem that threatens to wipe out humanity in the very short term. Suddenly, mankind is in a very bad negotiating position. </p>

<p>Of course, the more powerless the AI starts out, the longer this kind of scenario will take, but the premise of superintelligence is, that we cannot rule out hidden long term agendas behind even a few bits of output. </p>
",,1,2017-01-28T08:46:11.060,,2735,2017-01-28T08:46:11.060,,,,,2227.0,2722.0,2,1,,,,53.55,10.22,10.15,0.0,0.0,15.0,As long as the output of the AI affects the world the way in which it communicates makes no fundamental difference to the control problem The AI might still be able for example to manoeuvre mankind into a situation in which only the AI can save us It might provide a seemingly inoccuous technological solution to global warming but 50 years later it turns out that this solution caused some problem that threatens to wipe out humanity in the very short term Suddenly mankind is in a very bad negotiating position Of course the more powerless the AI starts out the longer this kind of scenario will take but the premise of superintelligence is that we cannot rule out hidden long term agendas behind even a few bits of output
,2.0,"<p>How is Bayes' Theorem used in artificial intelligence and machine learning? As an high school student I will be writing an essay about it, and I want to be able to explain Bayes' Theorem, its general use, and how it is used in AI or ML.</p>
",,1,2017-01-28T18:27:08.343,2.0,2738,2017-03-03T18:10:37.130,2017-01-30T15:39:27.183,,75.0,,5088.0,,1,6,<machine-learning>,Applications of Bayes' Theorem,185.0,73.51,7.09,9.58,0.0,0.0,7.0,How is Bayes Theorem used in artificial intelligence and machine learning As an high school student I will be writing an essay about it and I want to be able to explain Bayes Theorem its general use and how it is used in AI or ML
,,"<p>After studying and getting answers from experts, I could find out the answer to this question and posting as an answer to my own question.</p>

<p>F1 will entail (|=) F2; if and only if F2 must be true if we assume F1 to be true. </p>

<p>Similarly, F2 will entail (|=) F1; if and only if F1 must be true if we assume F2 to be true.</p>

<p>Logically, by taking any value for A or B, from the domain {TRUE, FALSE}, one could verify that <strong>F1 entails F2</strong>. Because, F2 is true; whenever F1 is true (e.g. when both A and B are FALSE).</p>

<p>However, <strong>F2 does not entail F1.</strong> As, in two cases, F2 is true (e.g. A= FALSE and B=TRUE, or vice-versa), but F1 is not true.</p>
",,0,2017-01-28T19:59:37.583,,2739,2017-01-28T19:59:37.583,,,,,3742.0,2729.0,2,0,,,,99.56,5.15,7.34,0.0,0.0,43.0,After studying and getting answers from experts I could find out the answer to this question and posting as an answer to my own question F1 will entail F2 if and only if F2 must be true if we assume F1 to be true Similarly F2 will entail F1 if and only if F1 must be true if we assume F2 to be true Logically by taking any value for A or B from the domain TRUE FALSE one could verify that F1 entails F2 Because F2 is true whenever F1 is true eg when both A and B are FALSE However F2 does not entail F1 As in two cases F2 is true eg A FALSE and BTRUE or viceversa but F1 is not true
,0.0,"<p>I want to make a face classification using nearest neighbor algorithm. Basically I have a database with 400 faces(10 photos for each person, so 40 persons) and I want to decide if a face is in my database.
So I will have 40 classes, a class for each person. As I said I want to use nn algorithm.
My question is: can you improve nn algorithm using genetic algorithms? How?</p>
",,1,2017-01-28T20:09:54.167,,2740,2017-02-03T18:14:40.577,,,,,5090.0,,1,0,<genetic-algorithms><nearest-neighbor>,Optimize nearest neighbor using genetic algorithms,49.0,74.19,7.13,8.84,0.0,0.0,11.0,I want to make a face classification using nearest neighbor algorithm Basically I have a database with 400 faces10 photos for each person so 40 persons and I want to decide if a face is in my database So I will have 40 classes a class for each person As I said I want to use nn algorithm My question is can you improve nn algorithm using genetic algorithms How
,,"<p>Since you are a highschool student I will try to express it easier. It is a problem for a machine to make a decision if you haven't given that information to it before. You should think of every cases while programming. But sometimes there can be so many cases, here Data Mining, Neural Networks, Fuzzy Logic etc are used withing AI. It saves your time and system is learning itself with enough examples given at the beginning and deciding itself.</p>

<p><a href=""http://web.cs.hacettepe.edu.tr/~ilyas/Courses/BIL712/lec04-BayesianLearning.pdf"" rel=""nofollow noreferrer"">Here in this link</a> you can find an article about Bayesian learning. Example on p.33 is what you need I guess.</p>
",,0,2017-01-29T11:59:17.093,,2741,2017-01-31T12:27:06.137,2017-01-31T12:27:06.137,,5095.0,,3358.0,2738.0,2,0,,,,67.15,7.99,10.51,0.0,0.0,12.0,Since you are a highschool student I will try to express it easier It is a problem for a machine to make a decision if you havent given that information to it before You should think of every cases while programming But sometimes there can be so many cases here Data Mining Neural Networks Fuzzy Logic etc are used withing AI It saves your time and system is learning itself with enough examples given at the beginning and deciding itself Here in this link you can find an article about Bayesian learning Example on p33 is what you need I guess
2750.0,4.0,"<p>Usually when performing linear regression predictions and gradient descent, the measure of the level of error for a particular line will be measured by the sum of the squared-distance values.</p>

<p>Why distance <strong><em>squared</em></strong>?</p>

<p>In most of the explanations I heard, they claim that:</p>

<ul>
<li>the function itself does not matter</li>
<li>the result should be positive so positive and negative deviations are still counted</li>
</ul>

<p>However, an <code>abs()</code> approach would still work. And isn't it inconvenient that distance <em>squared</em> minimizes the distance result for distances lower than 1?</p>

<p>I'm pretty sure someone must have considered this already -- so why is distance squared the most used approach to linear regression?</p>
",,2,2017-01-29T19:12:51.067,,2742,2017-02-12T14:18:49.787,,,,,190.0,,1,2,<linear-regression>,Linear regression: why is distance *squared* used as an error metric?,110.0,50.16,13.63,10.39,5.0,0.0,14.0,Usually when performing linear regression predictions and gradient descent the measure of the level of error for a particular line will be measured by the sum of the squareddistance values Why distance squared In most of the explanations I heard they claim that the function itself does not matter the result should be positive so positive and negative deviations are still counted However an approach would still work And isnt it inconvenient that distance squared minimizes the distance result for distances lower than 1 Im pretty sure someone must have considered this already so why is distance squared the most used approach to linear regression
2745.0,1.0,"<p>I have a simulator modelling a relatively complex scenario. I extract ~12 discrete features from the simulator state which forms the basis for my MDP state space.</p>

<p>Suppose I am estimating the transition table for an MDP by running large number of simulations and extracting feature transitions as the state transitions.</p>

<p>While I can randomize the simulator starting conditions to increase the coverage of states, I cannot guarantee all states will be represented in the sample ie. states which are possible but rare.</p>

<p>Is there a rigorous approach to ""filling in the gaps"" of the transition table in this case?</p>

<p>For example:</p>

<p>1) For each state which was unrepresented in the sample, simply transition to all other states with equal probability, as a ""neutral"" way to fill in the gap?</p>

<p>2) As above, but transition only to represented states (with equal probability)?</p>

<p>3) Transition to same state with probability 1.0?</p>

<p>4) Ignore unrepresented states during MDP solving entirely, and simply have a default action specified?</p>
",,0,2017-01-29T22:11:30.593,,2743,2017-01-30T08:31:15.947,2017-01-30T05:43:07.683,,4402.0,,4402.0,,1,2,<machine-learning><markov-chain>,How to fill in missing transitions when sampling an MDP transition table?,20.0,54.73,12.24,9.81,0.0,0.0,28.0,I have a simulator modelling a relatively complex scenario I extract 12 discrete features from the simulator state which forms the basis for my MDP state space Suppose I am estimating the transition table for an MDP by running large number of simulations and extracting feature transitions as the state transitions While I can randomize the simulator starting conditions to increase the coverage of states I cannot guarantee all states will be represented in the sample ie states which are possible but rare Is there a rigorous approach to filling in the gaps of the transition table in this case For example 1 For each state which was unrepresented in the sample simply transition to all other states with equal probability as a neutral way to fill in the gap 2 As above but transition only to represented states with equal probability 3 Transition to same state with probability 10 4 Ignore unrepresented states during MDP solving entirely and simply have a default action specified
,,"<p>I assume you use the 12 discrete features as state variables, and for each of these variables you will have at least two values. So the minimum number of states will be: 2^12 = 4096, which gives (2^12)^2 = 16777216 possible transitions. In order to reach this you will need a huge amount of simulations, also taking into account that this number is a minimum since you might have more values per state variable, and you probably have more than one action per state transition.</p>

<p>How to fill the gaps depends on your problem, in a problem where I did this, I filled the gaps using a uniformly random transition to its neighbor states. However, since I had to fill in such a a large amount, there was no significant difference between using this estimated transitions probabilities with a predefined transition table.</p>

<p>In your case it might be better to use <a href=""https://en.wikipedia.org/wiki/Q-learning"" rel=""nofollow noreferrer""><strong>Q-Learning</strong></a>, which is a model-free method that does not require the transition probabilities and uses directly the rewards and states obtained.</p>
",,1,2017-01-30T08:31:15.947,,2745,2017-01-30T08:31:15.947,,,,,198.0,2743.0,2,2,,,,51.21,11.15,9.96,0.0,0.0,25.0,I assume you use the 12 discrete features as state variables and for each of these variables you will have at least two values So the minimum number of states will be 212 4096 which gives 2122 16777216 possible transitions In order to reach this you will need a huge amount of simulations also taking into account that this number is a minimum since you might have more values per state variable and you probably have more than one action per state transition How to fill the gaps depends on your problem in a problem where I did this I filled the gaps using a uniformly random transition to its neighbor states However since I had to fill in such a a large amount there was no significant difference between using this estimated transitions probabilities with a predefined transition table In your case it might be better to use QLearning which is a modelfree method that does not require the transition probabilities and uses directly the rewards and states obtained
,,"<p>The squared form is sometimes called the <a href=""https://en.wikipedia.org/wiki/Norm_(mathematics)"" rel=""nofollow noreferrer"">Euclidean norm or L2 norm</a>. One of its very helpful properties is that it has an easily defined derivative, which can be used in mathematical analysis and translated fairly easily into code.</p>

<p>Intuitively it is thought that it is advantageous to exaggerate the differences according to the value of the error, which squaring does. You might also use the powers 3 or 4, but the derivative is more complex.</p>

<p>A number of different norms may be used, according to the particular circumstances of the problem at hand.</p>
",,0,2017-01-30T12:37:06.790,,2747,2017-01-30T12:37:06.790,,,,,4994.0,2742.0,2,1,,,,52.39,10.85,10.11,0.0,0.0,9.0,The squared form is sometimes called the Euclidean norm or L2 norm One of its very helpful properties is that it has an easily defined derivative which can be used in mathematical analysis and translated fairly easily into code Intuitively it is thought that it is advantageous to exaggerate the differences according to the value of the error which squaring does You might also use the powers 3 or 4 but the derivative is more complex A number of different norms may be used according to the particular circumstances of the problem at hand
,,"<p>A <strong>closed expression</strong> refers to a formula which has no free variables [<a href=""https://en.wikipedia.org/wiki/Closed-form_expression"" rel=""nofollow noreferrer"">1</a>]. This is also called <strong>sentence</strong>. In a logic system you have a set of axioms which are sentences and rules which state how to derive a sentence from this [<a href=""https://en.wikipedia.org/wiki/Hilbert_system"" rel=""nofollow noreferrer"">2</a>]. If a sentence can be derived from the axioms, this means that the axioms entail this sentence. If a sentence is not derivable, it is not entailed by the axioms. </p>
",,0,2017-01-30T12:52:25.903,,2748,2017-02-26T12:32:33.263,2017-02-26T12:32:33.263,,5095.0,,5095.0,2248.0,2,1,,,,73.58,7.24,8.9,0.0,0.0,11.0,A closed expression refers to a formula which has no free variables 1 This is also called sentence In a logic system you have a set of axioms which are sentences and rules which state how to derive a sentence from this 2 If a sentence can be derived from the axioms this means that the axioms entail this sentence If a sentence is not derivable it is not entailed by the axioms
,,"<p>One justification comes from the central limit theorem. If the noise in your data is the result of the sum of many independent effects, then it will tend to be normally distributed. And normally distributed means that the likelihood of the data is inversely proportional the exponential of the <strong>square</strong> of the distance to the mean.</p>

<p>In other words, minimizing the sum of squares of the distance to the mean amounts to finding the most likely value for the line assuming that the error is normally distributed. This is very often a reasonable assumption, but it is of course not always true.</p>
",,0,2017-01-30T13:09:40.253,,2749,2017-01-30T13:09:40.253,,,,,5118.0,2742.0,2,2,,,,59.23,10.33,9.45,0.0,0.0,8.0,One justification comes from the central limit theorem If the noise in your data is the result of the sum of many independent effects then it will tend to be normally distributed And normally distributed means that the likelihood of the data is inversely proportional the exponential of the square of the distance to the mean In other words minimizing the sum of squares of the distance to the mean amounts to finding the most likely value for the line assuming that the error is normally distributed This is very often a reasonable assumption but it is of course not always true
,,"<p>One justification is that under homoscedasticity the L2 norm produces the minimum variance unbiased estimator (MVUE), see Gauss-Markov Theorem. It means that the fitted values are the conditional expectations given the explanatory variables which is in many cases a nice property. Further it is the best estimator if the previous property is desirable. </p>

<p>As a response to the claim that the function itself does not matter, different functions give solutions with very different properties and a lot of effort has gone in to finding appropriate penalty functions, see for example Ridge regression and LASSO. The penalty function does matter. </p>

<p>edit: In response to your question regarding distances lower than 1, nothing ""goes wrong"" when the distances are smaller than 1. We always want to minimize the distance and the squared loss does so everywhere.</p>
",,0,2017-01-30T13:38:14.660,,2750,2017-01-30T13:50:04.990,2017-01-30T13:50:04.990,,3131.0,,3131.0,2742.0,2,1,,,,43.63,13.69,10.95,0.0,0.0,17.0,One justification is that under homoscedasticity the L2 norm produces the minimum variance unbiased estimator MVUE see GaussMarkov Theorem It means that the fitted values are the conditional expectations given the explanatory variables which is in many cases a nice property Further it is the best estimator if the previous property is desirable As a response to the claim that the function itself does not matter different functions give solutions with very different properties and a lot of effort has gone in to finding appropriate penalty functions see for example Ridge regression and LASSO The penalty function does matter edit In response to your question regarding distances lower than 1 nothing goes wrong when the distances are smaller than 1 We always want to minimize the distance and the squared loss does so everywhere
,,"<p>NEAT has the constant number of organisms in its population, which prevents overpopulation from happening.</p>

<p>The process of mating includes the following: Firstly, the  worst networks from every species are removed. Secondly, all species receive a number of offsprings that they can have. This is calculated by an adjusted neural network fitness.
Thirdly, offsprings for species are divided among neural networks in those species. Fitter neural networks have more offsprings. Finally, networks from same species are combined and create an offspring.</p>

<p>Speciation prevents fast extinction.</p>
",,0,2017-01-30T14:50:09.480,,2751,2017-01-30T14:50:09.480,,,,,5119.0,2689.0,2,0,,,,52.26,15.58,10.66,0.0,0.0,14.0,NEAT has the constant number of organisms in its population which prevents overpopulation from happening The process of mating includes the following Firstly the worst networks from every species are removed Secondly all species receive a number of offsprings that they can have This is calculated by an adjusted neural network fitness Thirdly offsprings for species are divided among neural networks in those species Fitter neural networks have more offsprings Finally networks from same species are combined and create an offspring Speciation prevents fast extinction
,,"<p>It simply derives itself from the maximum likelihood estimation. where in we maximise the log likelihood function., for detailed insight see this lecture: <a href=""http://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf"" rel=""nofollow noreferrer"">The Method of Maximum Likelihood for Simple Linear Regression</a>.</p>
",,0,2017-01-30T18:04:15.223,,2753,2017-02-12T14:18:49.787,2017-02-12T14:18:49.787,,8.0,,5122.0,2742.0,2,1,,,,35.23,15.23,12.56,0.0,0.0,5.0,It simply derives itself from the maximum likelihood estimation where in we maximise the log likelihood function for detailed insight see this lecture The Method of Maximum Likelihood for Simple Linear Regression
,0.0,"<p>I'm reading the HyperNEAT paper and I can across this:</p>

<blockquote>
  <p>Every potential connection in the substrate is queried to determine its presence and weight</p>
</blockquote>

<p>Now I'm not sure if it means really all possible connections including recurrent connections and connections to previous or following layers or only all possible connections between the current and next Layer?</p>

<p>Edit:
As requested <a href=""http://axon.cs.byu.edu/~dan/778/papers/NeuroEvolution/stanley3**.pdf"" rel=""nofollow noreferrer"">heres</a> the link to the paper. The passage I mean is at page 9.</p>
",,1,2017-01-30T22:46:21.643,,2754,2017-02-01T09:45:59.383,2017-02-01T09:45:59.383,,4550.0,,4550.0,,1,0,<neural-networks><genetic-algorithms>,Hyperneat CPPN applied to all possible connections?,8.0,55.27,12.02,10.47,0.0,0.0,7.0,Im reading the HyperNEAT paper and I can across this Every potential connection in the substrate is queried to determine its presence and weight Now Im not sure if it means really all possible connections including recurrent connections and connections to previous or following layers or only all possible connections between the current and next Layer Edit As requested heres the link to the paper The passage I mean is at page 9
,,"<p>I know of two, neither of which is open software, so that is the limit of disclosure about them.</p>

<p>To start a freeware project on GitHub along the lines of the question, one may wish to begin with a client that connects to <a href=""https://api.stackexchange.com/"">Stack Exchange's API</a> and uses a super-computing platform.</p>

<p>My recommendation would be to create named conduits with attributes ...</p>

<ol>
<li>Unique conduit name</li>
<li>Protocol name</li>
</ol>

<p>System plug-in components with 0 to M input ports, with 0 to N output ports, and with attributes ...</p>

<ol>
<li>Unique plug-in name</li>
<li>Plug-in type</li>
<li>Thread quantity</li>
<li>Thread run-time priority</li>
</ol>

<p>Each port of each plug-in with attributes ...</p>

<ol>
<li>Input not output flag (false for output)</li>
<li>Conduit name</li>
<li>I/O priority</li>
</ol>

<p>Each plug-in would communicate via conduits, some of which (but not all) may also communicate with the Stack Exchange API or with a database.  There may be syntactic or semantic components, naive Bayesian categorizers, neural nets, and statistical analysis components, cognitive modelling components, and numerous other plug-ins.</p>

<p>Of course there are other architectures, but this one may allow for more experimentation because any arbitrary information flow can be achieved.</p>
",,0,2017-01-31T06:50:23.687,,2756,2017-01-31T06:50:23.687,,,,,4302.0,1963.0,2,0,,,,40.08,13.87,10.34,0.0,0.0,40.0,I know of two neither of which is open software so that is the limit of disclosure about them To start a freeware project on GitHub along the lines of the question one may wish to begin with a client that connects to Stack Exchanges API and uses a supercomputing platform My recommendation would be to create named conduits with attributes Unique conduit name Protocol name System plugin components with 0 to M input ports with 0 to N output ports and with attributes Unique plugin name Plugin type Thread quantity Thread runtime priority Each port of each plugin with attributes Input not output flag false for output Conduit name IO priority Each plugin would communicate via conduits some of which but not all may also communicate with the Stack Exchange API or with a database There may be syntactic or semantic components naive Bayesian categorizers neural nets and statistical analysis components cognitive modelling components and numerous other plugins Of course there are other architectures but this one may allow for more experimentation because any arbitrary information flow can be achieved
,,"<p>A human has an abstract concept of numbers in mind. So 456 is a unique entity which is by definition unlike any other number because that are other unique entities. If you give ∃x ∈ ℕ: x==123 to your system it could check the property of natural numbers by counting from 0 to 123 to conclude that the statement is true. A human does it in another way. A human would ""see"" that it is a natural number because it has no decimal point. Because of the concept of natural numbers the statement is immediately clear. To get a faster result here the machine could check just the decimal point.</p>

<p>In your second case you have to apply the commutative property of addition and you are done because the syntax is equal then.</p>

<p>The second problem is more syntactic while the first is semantic. Your machine may ""know"" the commutative property of addition but not the concept of natural numbers. Therefore, it has to count.</p>
",,0,2017-01-31T09:38:09.007,,2757,2017-01-31T09:38:09.007,,,,,5095.0,2462.0,2,2,,,,64.71,8.23,8.02,0.0,0.0,19.0,A human has an abstract concept of numbers in mind So 456 is a unique entity which is by definition unlike any other number because that are other unique entities If you give ∃x ∈ ℕ x123 to your system it could check the property of natural numbers by counting from 0 to 123 to conclude that the statement is true A human does it in another way A human would see that it is a natural number because it has no decimal point Because of the concept of natural numbers the statement is immediately clear To get a faster result here the machine could check just the decimal point In your second case you have to apply the commutative property of addition and you are done because the syntax is equal then The second problem is more syntactic while the first is semantic Your machine may know the commutative property of addition but not the concept of natural numbers Therefore it has to count
2765.0,1.0,"<p>I have a task on my class to find all the nodes, calculate their values and choose the best way for the player on the given game graph:</p>

<p><a href=""https://i.stack.imgur.com/m5MRv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m5MRv.png"" alt=""enter image description here""></a></p>

<p>Everything is fine, but I have no idea what these dots are. Is this a third player, or just a 'split' for player1 move? Some kind of heuristics?</p>
",,0,2017-01-31T16:16:39.473,1.0,2760,2017-03-10T21:27:53.697,2017-03-10T21:27:53.697,,3617.0,,3617.0,,1,3,<minimax>,Minmax - choosing the best player's way,69.0,86.33,6.04,6.54,0.0,0.0,9.0,I have a task on my class to find all the nodes calculate their values and choose the best way for the player on the given game graph Everything is fine but I have no idea what these dots are Is this a third player or just a split for player1 move Some kind of heuristics
,0.0,"<p>In classical set theory there is two options for an element. It is either a member of a set, or not. But in fuzzy set theory there are <strong>membership functions</strong> to define ""rate"" of an element being a member of a set. In other words, classical logic says it is all black or white, but fuzzy logic offers that there is also grey which has shades between white and black.</p>

<p>Matlab Simulink Library is very easy to design and helpful in practice. And it has good examples on its own like deciding about tip for a dinner looking at service and food quality. In the figure below some various membership functions from Matlab's library are shown:</p>

<p><a href=""https://i.stack.imgur.com/aWG0C.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aWG0C.jpg"" alt=""enter image description here""></a></p>

<p><strong>My question:</strong> How do we decide about choosing membership functions while designing a fuzzy controller system? I mean in general, not only in Matlab Simulink. I have seen <em>Triangular</em> and <em>Gaussian</em> functions are used mostly in practise, but how can we decide which function will give a better result for decision making? Do we need to train a neural network to decide which function is better depending on problem and its rules? What are other solutions?</p>
",,6,2017-01-31T16:38:30.407,1.0,2762,2017-02-01T00:37:02.437,2017-02-01T00:37:02.437,,3358.0,,3358.0,,1,4,<neural-networks><fuzzy-logic>,Fuzzy Logic Controller: Choosing Membership Function,63.0,62.17,9.34,9.69,0.0,0.0,21.0,In classical set theory there is two options for an element It is either a member of a set or not But in fuzzy set theory there are membership functions to define rate of an element being a member of a set In other words classical logic says it is all black or white but fuzzy logic offers that there is also grey which has shades between white and black Matlab Simulink Library is very easy to design and helpful in practice And it has good examples on its own like deciding about tip for a dinner looking at service and food quality In the figure below some various membership functions from Matlabs library are shown My question How do we decide about choosing membership functions while designing a fuzzy controller system I mean in general not only in Matlab Simulink I have seen Triangular and Gaussian functions are used mostly in practise but how can we decide which function will give a better result for decision making Do we need to train a neural network to decide which function is better depending on problem and its rules What are other solutions
,,"<p>How could self-driving cars make ethical decisions about who to kill?</p>

<p><strong>By managing legal liability and consumer safety.</strong></p>

<p>A car that offers the consumer safety is going to be a car that is bought by said consumers. Companies do not want to be liable for killing their customers nor do they want to sell a product that gets the user in legal predicaments. Legal liability and consumer safety are the same issue when looked at from the perspective of ""cost to consumer"".</p>

<blockquote>
  <p>And here are few dilemmas:</p>
  
  <ul>
  <li>Does the algorithm recognize the difference between a human being and
  an animal?</li>
  </ul>
</blockquote>

<p>If an animal/human cannot be legally avoided (and car is in legal right - if its not then something else is wrong with the AI's decision making), it likely won't. If the car can safely avoid the obstacle, the AI could reasonably be seen to make this decision, ie. swerve to another lane on an open highway. Notice there is an emphasis on liability and driver safety.</p>

<blockquote>
  <ul>
  <li>Does the size of the human being or animal matter?</li>
  </ul>
</blockquote>

<p>Only the risk factor from hitting the obstacle. Hitting a hippo might be less desirable than hitting the ditch. Hitting a dog is likely more desirable than wrecking the customer's automobile.</p>

<blockquote>
  <ul>
  <li>Does it count how many passengers it has vs. people in the front?</li>
  </ul>
</blockquote>

<p>It counts the people as passengers to see if the car-pooling lane can be taken. It counts the people in front as a risk factor in case of collision.</p>

<blockquote>
  <ul>
  <li>Does it ""know"" when babies/children are on board?</li>
  </ul>
</blockquote>

<p>No.</p>

<blockquote>
  <ul>
  <li>Does it take into the account the age (e.g. killing the older first)?</li>
  </ul>
</blockquote>

<p>No. This is simply the wrong abstraction to make a decision, how could this be weighted into choosing the right course of action to reduce risk factor? If Option 1 is hit young guy with 20% chance of significant occupant damage and no legal liability and Option 2 is hit old guy with 21% chance of significant occupant damage and no legal liability, then what philosopher can convince even just 1 person of the just and equitable weights to make a decision?</p>

<p>Thankfully, the best decision a lot of the time is to hit the breaks to reduce speed (especially when you consider that it is often valuable to act predictably so that pedestrians and motorists can react accordingly). In the meantime, better value improvements can be made in terms of predicting when drivers will make bad decisions and when other actions (such as hitting the reverse) are more beneficial than hitting the breaks. At this point, it is not worth it to even begin collecting the information to make the ethical decisions proposed by philosophers. Thus, this issue is over-hyped by sensational journalists and philosophers.</p>
",,0,2017-01-31T22:16:32.373,,2763,2017-02-01T22:47:41.597,2017-02-01T22:47:41.597,,5157.0,,5157.0,111.0,2,2,,,,62.78,9.86,8.45,0.0,0.0,63.0,How could selfdriving cars make ethical decisions about who to kill By managing legal liability and consumer safety A car that offers the consumer safety is going to be a car that is bought by said consumers Companies do not want to be liable for killing their customers nor do they want to sell a product that gets the user in legal predicaments Legal liability and consumer safety are the same issue when looked at from the perspective of cost to consumer And here are few dilemmas Does the algorithm recognize the difference between a human being and an animal If an animalhuman cannot be legally avoided and car is in legal right if its not then something else is wrong with the AIs decision making it likely wont If the car can safely avoid the obstacle the AI could reasonably be seen to make this decision ie swerve to another lane on an open highway Notice there is an emphasis on liability and driver safety Does the size of the human being or animal matter Only the risk factor from hitting the obstacle Hitting a hippo might be less desirable than hitting the ditch Hitting a dog is likely more desirable than wrecking the customers automobile Does it count how many passengers it has vs people in the front It counts the people as passengers to see if the carpooling lane can be taken It counts the people in front as a risk factor in case of collision Does it know when babieschildren are on board No Does it take into the account the age eg killing the older first No This is simply the wrong abstraction to make a decision how could this be weighted into choosing the right course of action to reduce risk factor If Option 1 is hit young guy with 20 chance of significant occupant damage and no legal liability and Option 2 is hit old guy with 21 chance of significant occupant damage and no legal liability then what philosopher can convince even just 1 person of the just and equitable weights to make a decision Thankfully the best decision a lot of the time is to hit the breaks to reduce speed especially when you consider that it is often valuable to act predictably so that pedestrians and motorists can react accordingly In the meantime better value improvements can be made in terms of predicting when drivers will make bad decisions and when other actions such as hitting the reverse are more beneficial than hitting the breaks At this point it is not worth it to even begin collecting the information to make the ethical decisions proposed by philosophers Thus this issue is overhyped by sensational journalists and philosophers
,,"<p>You need some sort of interpretation abstraction before your mathematical reasoning. While the text might read ""123"", you need to parse this into a <em>literal</em> of type <em>Natural Number</em> or <em>Integer</em>. Similarly, ""x"" could be a <em>member variable</em>. Then your deduction becomes, is literal 123 a Natural Number? Yes.</p>

<p>As for the second statement, you should hopefully be able to reason to definitely false. Your internal representation of a sum should not be order dependent since addition is commutative. Then, the check for equality must handle this unordered property.</p>
",,0,2017-01-31T23:21:16.493,,2764,2017-01-31T23:21:16.493,,,,,5157.0,2462.0,2,1,,,,50.12,12.22,10.12,0.0,0.0,17.0,You need some sort of interpretation abstraction before your mathematical reasoning While the text might read 123 you need to parse this into a literal of type Natural Number or Integer Similarly x could be a member variable Then your deduction becomes is literal 123 a Natural Number Yes As for the second statement you should hopefully be able to reason to definitely false Your internal representation of a sum should not be order dependent since addition is commutative Then the check for equality must handle this unordered property
,,"<p>The triangles pointing up are Max' nodes. We assume it starts. Then follows a random choice of moves at the circles, for instance, with a die. The triangles pointing down are from Min. This variant is called Expectiminimax, see <a href=""https://en.wikipedia.org/wiki/Expectiminimax_tree"">https://en.wikipedia.org/wiki/Expectiminimax_tree</a>.</p>

<p>At that circles you have to multiply the possibilities on the edges below that nodes to your current value and sum all products up. The circles in your picture mean that Min dices.</p>
",,4,2017-02-01T07:30:43.353,,2765,2017-02-01T07:46:02.887,2017-02-01T07:46:02.887,,5095.0,,5095.0,2760.0,2,5,,,,60.92,11.98,9.13,0.0,0.0,19.0,The triangles pointing up are Max nodes We assume it starts Then follows a random choice of moves at the circles for instance with a die The triangles pointing down are from Min This variant is called Expectiminimax see httpsenwikipediaorgwikiExpectiminimaxtree At that circles you have to multiply the possibilities on the edges below that nodes to your current value and sum all products up The circles in your picture mean that Min dices
,,"<p>It is so popular because Turing formulated it. He was one of the first who talked about ""intelligent machines"" and was good connected in the scientific community since the 1940s. So there was enough time to distribute his very intelligent thoughts, for instance by Von Neumann, until now. Turing's importance for computer science is shown by the name of the Turing Award. So it is clear that a lot of people have read his papers.</p>
",,0,2017-02-01T09:20:37.940,,2766,2017-02-01T09:20:37.940,,,,,5095.0,2706.0,2,0,,,,64.71,9.56,9.43,0.0,0.0,10.0,It is so popular because Turing formulated it He was one of the first who talked about intelligent machines and was good connected in the scientific community since the 1940s So there was enough time to distribute his very intelligent thoughts for instance by Von Neumann until now Turings importance for computer science is shown by the name of the Turing Award So it is clear that a lot of people have read his papers
,,"<p>English language robots which you mentioned, are called ""<strong><em>chatter bots</em></strong>"". Chatter Bots are used to communicate with a human and undergo conversations in such a way that the human which is communicating will think that he/she is talking to another human. </p>

<p>There are two types of chatter bots: One is which uses certain rules and pattern matching techniques and the other one is which uses actual artificial intelligence techniques. Of course, the latter one is the most difficult to implement.</p>

<p>This can be understood with an example. The former type of bots which uses pattern matching and rules consist of questions stored in the form,</p>

<pre><code>if question matches 'WHO (IS/ARE) (MEMBERS OF PARLIAMENT/PRESIDENT)?' then
     RETURN Y ... where Y is some predefined answer.
</code></pre>

<p>The latter part which uses actual AI techniques, uses various mechanisms to actually understand the question, extract information out of it, process the information into some standard normalized form and then do some <strong>inference</strong> w.r.t. the <em>facts</em> in the <strong>knowledge base</strong>. Such methods may use <em>learning</em> algorithms to actually learn question patterns.</p>

<p>To make simple bots (probably of the former type), you can use Artificial Intelligence Markup Language (<a href=""http://www.alicebot.org/aiml.html"" rel=""nofollow noreferrer"">AIML</a>) which is a XML based language for developing chatter bots like <a href=""http://alice.pandorabots.com/"" rel=""nofollow noreferrer"">ALICE</a> bot.</p>

<p>If you want to focus more towards latter type of chatter bots, you may have to learn about various AI techniques, like searching, logic, knowledge and inference, learning, etc or sub-domains of AI like Natural Language Processing (NLP). There are various tool kits for NLP like for developing in Python there is nltk, for developing in Java there is Stanford's CoreNLP, and so on.</p>
",,0,2017-02-01T09:53:33.307,,2767,2017-02-01T09:53:33.307,,,,,1807.0,2731.0,2,4,,,,48.33,12.89,9.8,128.0,0.0,40.0,English language robots which you mentioned are called chatter bots Chatter Bots are used to communicate with a human and undergo conversations in such a way that the human which is communicating will think that heshe is talking to another human There are two types of chatter bots One is which uses certain rules and pattern matching techniques and the other one is which uses actual artificial intelligence techniques Of course the latter one is the most difficult to implement This can be understood with an example The former type of bots which uses pattern matching and rules consist of questions stored in the form The latter part which uses actual AI techniques uses various mechanisms to actually understand the question extract information out of it process the information into some standard normalized form and then do some inference wrt the facts in the knowledge base Such methods may use learning algorithms to actually learn question patterns To make simple bots probably of the former type you can use Artificial Intelligence Markup Language AIML which is a XML based language for developing chatter bots like ALICE bot If you want to focus more towards latter type of chatter bots you may have to learn about various AI techniques like searching logic knowledge and inference learning etc or subdomains of AI like Natural Language Processing NLP There are various tool kits for NLP like for developing in Python there is nltk for developing in Java there is Stanfords CoreNLP and so on
,,"<p>Bayes theorem states the probability of some event B occurring provided the prior knowledge of another event(s) A, given that B is dependent on event A (even partially).<br>
A real-world application example will be weather forecasting. Naive Bayes is a powerful algorithm for predictive modelling weather forecast. The temperature of a place is dependent on the pressure at that place, percentage of the humidity, speed and direction of the wind, previous records on temperature, turbulence on different atmospheric layers, and many other things. So when you have certain kind of data, you process them certain kind of algorithms to predict one particular result (or the future). The algorithms employed rely heavily on Bayesian network and the theorem.<br><br>
The given paragraph is introduction to Bayesian networks, given in the book, Artificial Intelligence – A Modern Approach:<br></p>

<blockquote>
  <p>Bayesian network formalism was invented to allow efficient representation of, and rigorous reasoning with, uncertain knowledge. This approach largely overcomes many problems of the probabilistic reasoning systems to the 1960s and 70s; it now dominates AI research on uncertain reasoning and expert systems. The approach allows for learning from experience, and it combines the best of classical AI and neural nets.</p>
</blockquote>

<p><br>There are many other applications, especially in medical science. Like predicting a particular disease based on the symptoms and physical condition of the patient. There are many algorithms currently in use that are based on this theorem, like binary and multi-class classifier, for example, email spam filters.
There are many things in this topic, I will advise you to keep our essay precise and one topic oriented. I have added some links below that might help, and let me know if you need any kind of other help.</p>

<p>Helpful Links<br>
 1. <a href=""http://machinelearningmastery.com/naive-bayes-for-machine-learning/"" rel=""nofollow noreferrer"">First</a><br>
 2. <a href=""https://en.wikipedia.org/wiki/Naive_Bayes_classifier"" rel=""nofollow noreferrer"">Second</a></p>
",,0,2017-02-01T18:09:18.703,,2768,2017-02-01T18:09:18.703,,,,,3005.0,2738.0,2,0,,,,43.43,13.63,10.64,0.0,0.0,44.0,Bayes theorem states the probability of some event B occurring provided the prior knowledge of another events A given that B is dependent on event A even partially A realworld application example will be weather forecasting Naive Bayes is a powerful algorithm for predictive modelling weather forecast The temperature of a place is dependent on the pressure at that place percentage of the humidity speed and direction of the wind previous records on temperature turbulence on different atmospheric layers and many other things So when you have certain kind of data you process them certain kind of algorithms to predict one particular result or the future The algorithms employed rely heavily on Bayesian network and the theorem The given paragraph is introduction to Bayesian networks given in the book Artificial Intelligence – A Modern Approach Bayesian network formalism was invented to allow efficient representation of and rigorous reasoning with uncertain knowledge This approach largely overcomes many problems of the probabilistic reasoning systems to the 1960s and 70s it now dominates AI research on uncertain reasoning and expert systems The approach allows for learning from experience and it combines the best of classical AI and neural nets There are many other applications especially in medical science Like predicting a particular disease based on the symptoms and physical condition of the patient There are many algorithms currently in use that are based on this theorem like binary and multiclass classifier for example email spam filters There are many things in this topic I will advise you to keep our essay precise and one topic oriented I have added some links below that might help and let me know if you need any kind of other help Helpful Links 1 First 2 Second
,1.0,"<p>Can silicon based computers create A.I. per definition of what intelligence is?</p>

<p>Or does silicon based computers only create human mimic?</p>

<p>If silicon based computers only create human mimic, are human mimic intelligence per definition?</p>

<p>If not, how can we create A.I. per definition of what intelligence is?</p>
",,0,2017-02-01T23:31:31.467,,2769,2017-02-02T00:17:21.117,,,,,5182.0,,1,2,<neural-networks><machine-learning><deep-learning><research><ai-design>,Can silicon based computers create A.I. per definition?,56.0,46.44,11.68,7.65,0.0,0.0,10.0,Can silicon based computers create AI per definition of what intelligence is Or does silicon based computers only create human mimic If silicon based computers only create human mimic are human mimic intelligence per definition If not how can we create AI per definition of what intelligence is
,,"<blockquote>
  <p>Can silicon based computers create A.I. per definition of what
  intelligence is?</p>
</blockquote>

<p>What definition?  There are many definitions of intelligence, and no universally accepted one that I'm aware of.</p>

<blockquote>
  <p>Or does silicon based computers only create human mimic?</p>
</blockquote>

<p>It's not clear to me what you're asking.  If humans are intelligent and we mimic humans closely enough, then it's probably fair to call the-thing-we-built ""intelligent"".</p>

<blockquote>
  <p>If silicon based computers only create human mimic, are human mimic intelligence per definition?</p>
</blockquote>

<p>I'm still not clear what you're asking, but I think I would answer ""yes"" if I'm parsing this correctly.</p>

<blockquote>
  <p>If not, how can we create A.I. per definition of what intelligence is?</p>
</blockquote>

<p>Again, we don't really <em>have</em> a definition of what intelligence is.  But so what? Who says AI has to have anything to do with human intelligence at all? As the old saying goes ""man did not achieve flight by building a mechanical bird"".  Likewise, there's no specific reason to think that the only path to artificial intelligence is to replicate the human brain in silicon. </p>
",,0,2017-02-02T00:17:21.117,,2770,2017-02-02T00:17:21.117,,,,,33.0,2769.0,2,1,,,,59.6,10.83,8.01,0.0,0.0,43.0,Can silicon based computers create AI per definition of what intelligence is What definition There are many definitions of intelligence and no universally accepted one that Im aware of Or does silicon based computers only create human mimic Its not clear to me what youre asking If humans are intelligent and we mimic humans closely enough then its probably fair to call thethingwebuilt intelligent If silicon based computers only create human mimic are human mimic intelligence per definition Im still not clear what youre asking but I think I would answer yes if Im parsing this correctly If not how can we create AI per definition of what intelligence is Again we dont really have a definition of what intelligence is But so what Who says AI has to have anything to do with human intelligence at all As the old saying goes man did not achieve flight by building a mechanical bird Likewise theres no specific reason to think that the only path to artificial intelligence is to replicate the human brain in silicon
,6.0,"<p>I define Artificial Life as a ""simulation"" or ""copy"" of life. However, should it be considered a simulation or copy? </p>

<p>If one had motivation and money, someone could theoretically create evolving computers, with a program that allows mutation OR simply a ""simulated"" environment with ""simulated"" organisms.   </p>

<p>The computer (or ""simulated"" organism)would have the ability to reproduce, grow, and take in energy. What if the life evolved to have intelligence. Currently, there are some relatively limited programs that simulate life, but most of them are heavily simplistic. Are they life?</p>

<p>When should something be called life? </p>
",,2,2017-02-02T06:41:08.190,,2771,2017-03-11T03:40:11.090,2017-02-14T15:26:28.623,,-1.0,,5189.0,,1,7,<genetic-algorithms>,Artificial Life - life or not?,259.0,34.02,13.96,10.38,0.0,0.0,27.0,I define Artificial Life as a simulation or copy of life However should it be considered a simulation or copy If one had motivation and money someone could theoretically create evolving computers with a program that allows mutation OR simply a simulated environment with simulated organisms The computer or simulated organismwould have the ability to reproduce grow and take in energy What if the life evolved to have intelligence Currently there are some relatively limited programs that simulate life but most of them are heavily simplistic Are they life When should something be called life
,3.0,"<p>Could you give examples of affordable programmable devices that could be used in university classes to teach students about A.I. and demonstrate it?</p>

<p>The devices are expected to do some form of self learning, pattern recognition, or any other features of A.I., and to be programmable or customizable.</p>
",,1,2017-02-02T10:43:02.673,3.0,2772,2017-02-20T11:11:30.073,,,,,5191.0,,1,7,<ai-design><self-learning><training><computer-programming><programming-languages>,What programmable devices can be used to teach/demonstrate artificial Intelligence in schools?,142.0,50.84,12.28,10.15,0.0,0.0,9.0,Could you give examples of affordable programmable devices that could be used in university classes to teach students about AI and demonstrate it The devices are expected to do some form of self learning pattern recognition or any other features of AI and to be programmable or customizable
,,"<p><a href=""https://en.wikipedia.org/wiki/Life"" rel=""nofollow noreferrer"">Wikipedia describes life</a> as a characteristic of ""physical entities having biological processes"". <a href=""https://en.wikipedia.org/wiki/Simulation"" rel=""nofollow noreferrer"">The same source</a> also describes a simulation as ""the imitation of the operation of a real-world process or system over time."" If a digital neural net was to listen to me prattle on for long enough it could learn to speak as if it were me. It would have my knowledge and limitations but its headaches would be quite different from mine. It would never have a toothache. But you could put it in a <a href=""https://en.wikipedia.org/wiki/Chinese_room"" rel=""nofollow noreferrer"">Searle Chinese Room</a>, and you could speak to it and it would sound exactly as if it were me long after I am dead. It has my ""character"" which is what my friends would recognize about me.</p>

<p>According to the definition of life it is not alive because it does not have biological processes. It is a simulation because it emulates what I would have said. It cannot be a copy because a digital box is not biologic.</p>

<p>Now let's give this simulation a biological nose so that it can smell. And maybe two eyes and ears. We continue this process until most of the simulation is equipped with biological parts which function together. Whatever it is is now able to come out of the Chinese Room and talk to you. By golly, it looks and sounds exactly like me, but I died a long time ago. Have I been brought back?</p>

<p>My suggestion is that a perfect copy of me would not be possible simply through training due to the level of detail required, but that the close copy would be alive. A critical point would be that there would have to be a fatal link somewhere which would cause ""death"". You could always create a new close copy, but not an exact copy.</p>
",,2,2017-02-02T15:37:00.297,,2774,2017-02-02T15:42:40.503,2017-02-02T15:42:40.503,,4994.0,,4994.0,2771.0,2,2,,,,72.26,7.77,7.98,0.0,0.0,34.0,Wikipedia describes life as a characteristic of physical entities having biological processes The same source also describes a simulation as the imitation of the operation of a realworld process or system over time If a digital neural net was to listen to me prattle on for long enough it could learn to speak as if it were me It would have my knowledge and limitations but its headaches would be quite different from mine It would never have a toothache But you could put it in a Searle Chinese Room and you could speak to it and it would sound exactly as if it were me long after I am dead It has my character which is what my friends would recognize about me According to the definition of life it is not alive because it does not have biological processes It is a simulation because it emulates what I would have said It cannot be a copy because a digital box is not biologic Now lets give this simulation a biological nose so that it can smell And maybe two eyes and ears We continue this process until most of the simulation is equipped with biological parts which function together Whatever it is is now able to come out of the Chinese Room and talk to you By golly it looks and sounds exactly like me but I died a long time ago Have I been brought back My suggestion is that a perfect copy of me would not be possible simply through training due to the level of detail required but that the close copy would be alive A critical point would be that there would have to be a fatal link somewhere which would cause death You could always create a new close copy but not an exact copy
,,"<p>It wouldn´t be considered alive if it doesn´t have vital fuctions such as nutrition, relation with the enviroment and reproduction. While the first is easy (use a battery) and the second is the one we are developing right now (basically the Inteligence part of an AI) giving programming skills to an AI, aka the ability to reproduce, isnt widely considered a good idea, as many science fiction writers can tell you. </p>
",,0,2017-02-03T08:48:26.263,,2775,2017-02-03T08:48:26.263,,,,,5211.0,2771.0,2,0,,,,35.44,11.5,10.96,0.0,0.0,10.0,It wouldn´t be considered alive if it doesn´t have vital fuctions such as nutrition relation with the enviroment and reproduction While the first is easy use a battery and the second is the one we are developing right now basically the Inteligence part of an AI giving programming skills to an AI aka the ability to reproduce isnt widely considered a good idea as many science fiction writers can tell you
,1.0,"<p>Lets say I have a Neural Network with 5 layers, including input and output layer. Each Layer has 5 nodes. Assume the Layers are fully connected, but the 3rd Node in the 2nd Layer is connected to the 5th node in the 4th Layer. All these numbers are chosen at random for the example.</p>

<p>My question is when is the 5th node in the 4th layer fed forward? Lets go through it step by step: the first layer is normally fed forward to the second. the second layer is normally fed forward to the third, but the 3rd node is also fed forward to the 5th node of the 4th layer. So the problem here is, is the 5th node in the 4th layer now fed forward or is it fed forward when the 3rd layer is done being fed forward? The 1st method would mean that the node would get fed forward 2 times and my concern is, if the output is still valid. Further more it would also come to 2 asynchronous outputs and how would these be interpreted?</p>

<p>Because in the Brain, I heard, the neurons are fired when an impulse arrives so this would equal the 1st method.</p>
",,4,2017-02-03T10:41:34.517,,2776,2017-02-04T19:48:10.500,2017-02-03T19:09:17.790,,4550.0,,4550.0,,1,2,<neural-networks><recurrent-neural-networks>,Are Neurons instantly feed forward when input arrives?,59.0,86.64,6.44,7.44,0.0,0.0,19.0,Lets say I have a Neural Network with 5 layers including input and output layer Each Layer has 5 nodes Assume the Layers are fully connected but the 3rd Node in the 2nd Layer is connected to the 5th node in the 4th Layer All these numbers are chosen at random for the example My question is when is the 5th node in the 4th layer fed forward Lets go through it step by step the first layer is normally fed forward to the second the second layer is normally fed forward to the third but the 3rd node is also fed forward to the 5th node of the 4th layer So the problem here is is the 5th node in the 4th layer now fed forward or is it fed forward when the 3rd layer is done being fed forward The 1st method would mean that the node would get fed forward 2 times and my concern is if the output is still valid Further more it would also come to 2 asynchronous outputs and how would these be interpreted Because in the Brain I heard the neurons are fired when an impulse arrives so this would equal the 1st method
,1.0,"<p>I am researching Natural Language Processing (NLP) to develop a NL Question Answering. Answering part is already developed. So question remains, along with the questions regarding the algorithms.</p>

<p>Final product should be able to: - User can ask a question in NL - Question gets translated to a MDX query, which generates a script regarding dimensions of the cube.</p>

<p>How can I translate a Natural Language Question to a MDX query? Outcome of question results in answer of a calculation. E.g. ‘ How many declarations were done by employee1?’ or ‘Give me the quantities for Sales’</p>

<p>Thanks in advance!</p>
",,2,2017-02-03T16:14:09.547,1.0,2777,2017-02-04T14:58:45.733,,,,,5219.0,,1,2,<nlp>,How can I convert an input Natural Language QA to a MDX q,62.0,67.65,11.41,10.1,0.0,0.0,17.0,I am researching Natural Language Processing NLP to develop a NL Question Answering Answering part is already developed So question remains along with the questions regarding the algorithms Final product should be able to User can ask a question in NL Question gets translated to a MDX query which generates a script regarding dimensions of the cube How can I translate a Natural Language Question to a MDX query Outcome of question results in answer of a calculation Eg ‘ How many declarations were done by employee1’ or ‘Give me the quantities for Sales’ Thanks in advance
,,"<p>I like to take an ""<a href=""https://en.wikipedia.org/wiki/Animism"" rel=""nofollow noreferrer"">animist</a>"" approach.  <em>(It has been suggested to me that part of the reason Japanese designs are so effective is because of the cultural affinity for the concept per the Shinto tradition.  For instance, the thing where people put little eyes on everything;)</em></p>

<p>I like to think of how my dog, who is terrified of the vacuum cleaner, would regard one of the recent <a href=""http://www.theverge.com/circuitbreaker/2017/2/1/14468126/boston-dynamics-new-wheeled-robot-handle"" rel=""nofollow noreferrer"">Boston Dynamics</a> creations.  My guess is the dog would't find much use in the distinction that the robot is an artifact as opposed to a biological entity.</p>

<p>I tend to take a deterministic, mechanical approach to reality. Sure things get fuzzy down at the quantum level, but even that may simply be a factor of inadequate measurement capability and the sheer complexity of quantum mechanics, which seems fundamentally beyond the grasp of even the greatest minds, when they're being honest about it.</p>

<p>I don't see much of a distinction between simple organisms and <a href=""https://en.wikipedia.org/wiki/Cellular_automaton"" rel=""nofollow noreferrer"">cellular automata</a>, except that the former is part of a biological food chain.  There is a valid hypothesis that if you had a big enough computer, Conway's Game of Life could independently develop ""intelligence"".     </p>

<p>Self-replication would certainly seem to be a requirement of biological life that can be extended to artificial life.  Possibly the true distinction of ""artificial"" is merely that it is creation of a functional system by an extra-species source, whether the creation be ""biological"" or ""mechanical"" in nature. <em>(i.e. we can hack genes now, not just computer code.)</em></p>
",,0,2017-02-03T18:54:22.863,,2779,2017-02-03T20:52:26.840,2017-02-03T20:52:26.840,,1671.0,,1671.0,2771.0,2,2,,,,48.23,11.84,9.97,0.0,0.0,45.0,I like to take an animist approach It has been suggested to me that part of the reason Japanese designs are so effective is because of the cultural affinity for the concept per the Shinto tradition For instance the thing where people put little eyes on everything I like to think of how my dog who is terrified of the vacuum cleaner would regard one of the recent Boston Dynamics creations My guess is the dog wouldt find much use in the distinction that the robot is an artifact as opposed to a biological entity I tend to take a deterministic mechanical approach to reality Sure things get fuzzy down at the quantum level but even that may simply be a factor of inadequate measurement capability and the sheer complexity of quantum mechanics which seems fundamentally beyond the grasp of even the greatest minds when theyre being honest about it I dont see much of a distinction between simple organisms and cellular automata except that the former is part of a biological food chain There is a valid hypothesis that if you had a big enough computer Conways Game of Life could independently develop intelligence Selfreplication would certainly seem to be a requirement of biological life that can be extended to artificial life Possibly the true distinction of artificial is merely that it is creation of a functional system by an extraspecies source whether the creation be biological or mechanical in nature ie we can hack genes now not just computer code
,,"<p>It is unclear what kind of network your are referring to, there is not a single neural-network model so conceivable both cases could exist and serve some purpose, yet if you are looking for one that emulates nature and real neurons, then you are missing at least 2 ingredients ( time and the mechanisms of resting potentials and refractory periods), which in turn introduce new computations to the neural network.</p>

<p>This is your network graph if I got it right:</p>

<p><a href=""https://i.stack.imgur.com/pI6Vi.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pI6Vi.jpg"" alt=""enter image description here""></a></p>

<p>The calculation in a neural network without refractory periods and resting potentials without time, would instantaneously modify the weight of your node4 layer 5 (n4-L5) if there is input in column 3:</p>

<p><a href=""https://i.stack.imgur.com/OaEiX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OaEiX.jpg"" alt=""enter image description here""></a></p>

<p>Additional inputs on other columns would just add up unless you have some other explicit computation on any layer.</p>

<p>If you wanted to emulate a Neuron, each node would need to have a resting potential, that is: a level above which it will fire ( in the above example zero), and a refractory period:  a time before it would fire again, as well as a clock to keep it in sync, this would be a crude realtime fascimile:</p>

<p><a href=""http://codepen.io/k3no/pen/WRQbYV"" rel=""nofollow noreferrer"">http://codepen.io/k3no/pen/WRQbYV</a></p>

<p><a href=""https://i.stack.imgur.com/4Hj68.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4Hj68.jpg"" alt=""enter image description here""></a></p>

<p>A common alternative is to use sequential phases or steps.</p>

<h3>Reference/source</h3>

<p><a href=""http://aima.cs.berkeley.edu/"" rel=""nofollow noreferrer"">Artificial Intelligence: A Modern Approach</a>, by S. Russell and P. Norvig. Deals with a general step approach to A.I.</p>

<p><a href=""https://mitpress.mit.edu/books/gateway-memory"" rel=""nofollow noreferrer"">Gateway to Memory</a>, by Mark A. Gluck and Catherine E. Myers Presents a great and readable introduction to modeling neural networks.</p>

<p>I published the little neural network model in a medium article : <a href=""https://medium.com/@k3no/memory-and-the-machine-ba380dcdb1c1#.3riz96ryx"" rel=""nofollow noreferrer"">Memory and the machine</a>, relevant sources are there. </p>
",,2,2017-02-03T20:02:22.513,,2780,2017-02-04T19:48:10.500,2017-02-04T19:48:10.500,,3020.0,,3020.0,2776.0,2,2,,,,47.62,11.84,9.92,0.0,0.0,48.0,It is unclear what kind of network your are referring to there is not a single neuralnetwork model so conceivable both cases could exist and serve some purpose yet if you are looking for one that emulates nature and real neurons then you are missing at least 2 ingredients time and the mechanisms of resting potentials and refractory periods which in turn introduce new computations to the neural network This is your network graph if I got it right The calculation in a neural network without refractory periods and resting potentials without time would instantaneously modify the weight of your node4 layer 5 n4L5 if there is input in column 3 Additional inputs on other columns would just add up unless you have some other explicit computation on any layer If you wanted to emulate a Neuron each node would need to have a resting potential that is a level above which it will fire in the above example zero and a refractory period a time before it would fire again as well as a clock to keep it in sync this would be a crude realtime fascimile httpcodepeniok3nopenWRQbYV A common alternative is to use sequential phases or steps Referencesource Artificial Intelligence A Modern Approach by S Russell and P Norvig Deals with a general step approach to AI Gateway to Memory by Mark A Gluck and Catherine E Myers Presents a great and readable introduction to modeling neural networks I published the little neural network model in a medium article Memory and the machine relevant sources are there
,,"<p>If you read Steven Levy's book, <em>Artificial Life</em>,you will find, as I did, </p>

<p><a href=""https://i.stack.imgur.com/43k2B.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/43k2B.jpg"" alt=""enter image description here""></a></p>

<p>the distinction between biological and ""artificial"" life blurred. If you think about it, <em>what exactly is ""life"", anyway?</em> <strong>A set of complex systems with emergent behavior capable of evolution and adaptation.</strong></p>

<p>A prototypical biologist may not define life that way. Indeed, he would, <em>not</em> being focused or concerned with the computational aspect, define it in a way that would narrow it down to biological life.</p>

<p>Marvin Minsky mention the concept of <em>luggage words</em>, and I myself came up with the notion of <em>mirage concepts</em>. For the former, that which we don't understand gets lump into the word. For the latter, when you take a ""mysterious"" concept apart, it vanishes like a mirage does as you get closer.</p>

<p>So <em>what is life</em>? If we look at a ""living"" organism, we'd all that ""life"". If we remove a single cell from that organism, we'd still call that ""life"". But what if we remove a single organelle like, say, a ribosome? Lysosome? Contractile vacuole? Endoplasmic reticulum? Is that still ""life""? What if we remove a macromolecule from that? Is that still ""life""?</p>

<p>As you see, all becomes <em>very</em> murky, and I do this on purpose to illustrate just how <em>arbitrary</em> the very concept of ""life"" is.</p>

<p>So I think my definition is a good one, broad enough to encompass both in-silico and the organic versions. It bespeaks to algorithms, robots, viruses -- yes <em>both</em> computer and organic... anything that has <em>complexity</em> and the ability to <em>adapt</em> and <em>evolve</em>. </p>
",,0,2017-02-03T22:44:52.170,,2781,2017-02-03T22:44:52.170,,,,,4185.0,2771.0,2,3,,,,66.23,10.26,9.22,0.0,0.0,71.0,If you read Steven Levys book Artificial Lifeyou will find as I did the distinction between biological and artificial life blurred If you think about it what exactly is life anyway A set of complex systems with emergent behavior capable of evolution and adaptation A prototypical biologist may not define life that way Indeed he would not being focused or concerned with the computational aspect define it in a way that would narrow it down to biological life Marvin Minsky mention the concept of luggage words and I myself came up with the notion of mirage concepts For the former that which we dont understand gets lump into the word For the latter when you take a mysterious concept apart it vanishes like a mirage does as you get closer So what is life If we look at a living organism wed all that life If we remove a single cell from that organism wed still call that life But what if we remove a single organelle like say a ribosome Lysosome Contractile vacuole Endoplasmic reticulum Is that still life What if we remove a macromolecule from that Is that still life As you see all becomes very murky and I do this on purpose to illustrate just how arbitrary the very concept of life is So I think my definition is a good one broad enough to encompass both insilico and the organic versions It bespeaks to algorithms robots viruses yes both computer and organic anything that has complexity and the ability to adapt and evolve
,,"<p>I have used Open Natural Language Processing [<strong>Open NLP</strong>] package to come up with the same system tool but does not have a module to directly convert English sentences or natural language questions to SQL queries. However, you can definitely develop a such module, by using existing modules of OpenNLP such as part-of-speech tagging, named entity extraction, chunking and parsing.</p>

<p>Many approaches in the research field of ""Natural Language Interfaces to Databases"" (NLIDBs) are proposed to answer your question. An overview of this field is presented in the ""classical"" paper "" <a href=""https://www.cambridge.org/core/journals/natural-language-engineering/article/div-classtitlenatural-language-interfaces-to-databases-an-introductiondiv/21C30448C70DD4988E6DA0D54205FB56"" rel=""nofollow noreferrer"">Natural language interfaces to databases</a> – an introduction. Natural Language Engineering, 1:29–81, 3 1995."" This paper is quite out-of-date, so you might want to look at a recent paper "" <a href=""http://www.semantic-web-journal.net/system/files/swj1180.pdf"" rel=""nofollow noreferrer"">Ripple Down Rules for Question Answering</a>. Semantic Web journal, to appear.""</p>

<p>Generally, a (NLIDB) question answering system contains two components: question analysis and answer retrieval. Given an input question, the question analysis component produces key terms, question class/category and the structure of the input question, for example: [QuestionPhrase: Which universities] [Relation: are] [NounPhrase: Knowledge Media Institute]] [Relation: collaborating with].</p>

<p>Taking the output of the question analysis component as input, the answer retrieval component firstly generates an concrete query expression in a database query language (e.g SQL query). Then the concrete (SQL) query is used to find an answer in a target database. Here in an intermediate process, you might want to use semantic lexicons such as WordNet to map the extracted key terms (e.g. concepts or relations) to the database concepts such as table names or columns.</p>

<p>And lastly;
If you don’t mind Python, you can refer to a promising Python package: 
<a href=""https://pypi.python.org/pypi/quepy/"" rel=""nofollow noreferrer"">machinalis/quepy</a>
It already can process some simple questions now. Demo: <a href=""http://quepy.machinalis.com/"" rel=""nofollow noreferrer""> with same applicability</a>: A Python framework to transform natural language questions to queries.</p>

<p>But it still need some effort to build a SQL generator for it.</p>
",,7,2017-02-04T14:58:45.733,,2782,2017-02-04T14:58:45.733,,,,,1581.0,2777.0,2,3,,,,44.64,14.73,10.76,0.0,0.0,79.0,I have used Open Natural Language Processing Open NLP package to come up with the same system tool but does not have a module to directly convert English sentences or natural language questions to SQL queries However you can definitely develop a such module by using existing modules of OpenNLP such as partofspeech tagging named entity extraction chunking and parsing Many approaches in the research field of Natural Language Interfaces to Databases NLIDBs are proposed to answer your question An overview of this field is presented in the classical paper Natural language interfaces to databases – an introduction Natural Language Engineering 129–81 3 1995 This paper is quite outofdate so you might want to look at a recent paper Ripple Down Rules for Question Answering Semantic Web journal to appear Generally a NLIDB question answering system contains two components question analysis and answer retrieval Given an input question the question analysis component produces key terms question classcategory and the structure of the input question for example QuestionPhrase Which universities Relation are NounPhrase Knowledge Media Institute Relation collaborating with Taking the output of the question analysis component as input the answer retrieval component firstly generates an concrete query expression in a database query language eg SQL query Then the concrete SQL query is used to find an answer in a target database Here in an intermediate process you might want to use semantic lexicons such as WordNet to map the extracted key terms eg concepts or relations to the database concepts such as table names or columns And lastly If you don’t mind Python you can refer to a promising Python package machinalisquepy It already can process some simple questions now Demo with same applicability A Python framework to transform natural language questions to queries But it still need some effort to build a SQL generator for it
,2.0,"<p>I'd like to build a program that would learn to automatically classify documents. The principle would be that, for each new document I add to the system, it would automatically infer in which category to classify the document. If it doesn't know, I would have to manually enter the category. For each hint I give to the system, the system would learn to refine its knowledge of document kinds. Something similar to face recognition in Picasa, but for documents.</p>

<p>More specifically, the documents would be invoices, and I want to classify them by vendors. Documents could be extracted as text, as image, or both.</p>

<p>Is there some know algorithms for this kind of job?</p>

<p>Up to now, I could think at two possible ways I could do it:</p>

<ul>
<li>For images, I could add all the images of a given kind together, and record the pixels that are the most common to all images, to create a mask. For a new image, I would compare this mask with the image to determine how similar it is.</li>
<li>For text, I could record the list of words or sentences that are similar to all documents of a given kind.</li>
<li>Finally, I could do a combination of both techniques, for example by converting a PDF document to an image, or an image to text by OCR techniques.</li>
</ul>

<p>I'm just wondering if I'm approaching the problem the right way. Especially about storing just enough information in the database.</p>
",,1,2017-02-04T16:41:02.400,,2783,2017-02-09T16:47:43.073,,,,,5235.0,,1,0,<algorithm><reinforcement-learning><classification>,What algorithm should I use to classify documents?,117.0,62.27,8.7,8.07,0.0,0.0,37.0,Id like to build a program that would learn to automatically classify documents The principle would be that for each new document I add to the system it would automatically infer in which category to classify the document If it doesnt know I would have to manually enter the category For each hint I give to the system the system would learn to refine its knowledge of document kinds Something similar to face recognition in Picasa but for documents More specifically the documents would be invoices and I want to classify them by vendors Documents could be extracted as text as image or both Is there some know algorithms for this kind of job Up to now I could think at two possible ways I could do it For images I could add all the images of a given kind together and record the pixels that are the most common to all images to create a mask For a new image I would compare this mask with the image to determine how similar it is For text I could record the list of words or sentences that are similar to all documents of a given kind Finally I could do a combination of both techniques for example by converting a PDF document to an image or an image to text by OCR techniques Im just wondering if Im approaching the problem the right way Especially about storing just enough information in the database
,,"<p><a href=""http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf"" rel=""nofollow noreferrer"">http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf</a>
From Word Embeddings To Document Distances</p>

<pre><code>&gt; We present the Word Mover’s Distance (WMD), a  novel 
&gt; distance  function  between  text  documents.   Our  work  is  based
&gt; on  recent  results  in word embeddings that learn semantically mean-
&gt; ingful  representations  for  words  from  local  co-occurrences  in 
&gt; sentences.   The  WMD  distance measures the dissimilarity between two
&gt; text documents as the minimum amount of distance that the  embedded 
&gt; words  of  one  document  need  to “travel” to reach the embedded
&gt; words of another document. We show that this distance metric can be
&gt; cast as an instance of the Earth Mover’s Distance, a well studied
&gt; transportation problem for which several highly efficient solvers have
&gt; been developed.   Our metric has no hyperparameters and is
&gt; straight-forward to implement. Further, we demonstrate on eight real
&gt; world document classification data sets, in comparison with seven
&gt; state- of-the-art baselines, that the WMD metric leads to
&gt; unprecedented low k-nearest neighbor document classification error
&gt; rates.
</code></pre>
",,0,2017-02-05T11:54:11.787,,2785,2017-02-05T11:54:11.787,,,,,3917.0,2783.0,2,-3,,,,-20.23,53.77,13.01,1080.0,0.0,9.0,httpjmlrorgproceedingspapersv37kusnerb15pdf From Word Embeddings To Document Distances
,1.0,"<p>I am currently working on my last project before graduating.
For this project, I have to develop a Natural Language Question Answering System. Now, I have read quite some research papers regarding this topic and have figured out everything except for the parsing algorithm. </p>

<p>The NL Q-A will be programmed in Python, and I will use the spaCy library to finish this project. However, I am stuck when it comes to parsing algorithms. I managed to reduce the parsing algorithms to 3:</p>

<ul>
<li>Cocke-Kasami-Younger (CKY) algorithm</li>
<li>Earley algorithm</li>
<li>Chart Parsing algorithm</li>
</ul>

<p>Note: I know that all three algorithms are chart parsing algorithms.
I also know that the Earley algorithm is context-free, but has a low efficiency for a compiler.</p>

<p>What I don't know is: Which one should I pick? (non-subjective answer to this question)</p>

<p>The system is for a specific domain. And the answer of the natural question will be displayed in the form of the result of a calculation of some kind. Preferably in the tabular or graphical form.</p>

<p>Furthermore, I have done my research. However, I probably do not understand the algorithms properly, which makes it difficult to make a selection. 
The algorithm should be efficient and perhaps outperform others.
(You are my last hope!)</p>

<p>Thank you!</p>
",,0,2017-02-06T09:32:07.333,,2787,2017-02-06T10:46:00.503,,,,,5219.0,,1,3,<nlp>,Which parsing algorithm can I use for NLP question answering system?,58.0,65.83,11.24,9.41,0.0,0.0,39.0,I am currently working on my last project before graduating For this project I have to develop a Natural Language Question Answering System Now I have read quite some research papers regarding this topic and have figured out everything except for the parsing algorithm The NL QA will be programmed in Python and I will use the spaCy library to finish this project However I am stuck when it comes to parsing algorithms I managed to reduce the parsing algorithms to 3 CockeKasamiYounger CKY algorithm Earley algorithm Chart Parsing algorithm Note I know that all three algorithms are chart parsing algorithms I also know that the Earley algorithm is contextfree but has a low efficiency for a compiler What I dont know is Which one should I pick nonsubjective answer to this question The system is for a specific domain And the answer of the natural question will be displayed in the form of the result of a calculation of some kind Preferably in the tabular or graphical form Furthermore I have done my research However I probably do not understand the algorithms properly which makes it difficult to make a selection The algorithm should be efficient and perhaps outperform others You are my last hope Thank you
,,"<p>I have been reading and reading, and found answers to almost all my questions.
I am sticking to Early algorithm as it offers a dynamic programming approach (CKY does the same). 
Both algorithms are chart parsing algorithms.</p>

<p>Earley is a context free -, top-down parsing algorithm. Which makes it a goal driven algorithm. From start symbol down. Furthermore, it is more efficient than the CKY algorithm.
Slides of comparison, and explanation:
<a href=""https://www.cs.bgu.ac.il/~michaluz/seminar/CKY1.pdf"" rel=""nofollow noreferrer"">https://www.cs.bgu.ac.il/~michaluz/seminar/CKY1.pdf</a></p>
",,3,2017-02-06T10:46:00.503,,2788,2017-02-06T10:46:00.503,,,,,5219.0,2787.0,2,1,,,,62.44,15.28,10.3,0.0,0.0,28.0,I have been reading and reading and found answers to almost all my questions I am sticking to Early algorithm as it offers a dynamic programming approach CKY does the same Both algorithms are chart parsing algorithms Earley is a context free topdown parsing algorithm Which makes it a goal driven algorithm From start symbol down Furthermore it is more efficient than the CKY algorithm Slides of comparison and explanation httpswwwcsbguacilmichaluzseminarCKY1pdf
,,"<p>Text approach:</p>

<p>Use LDA (Latent Dirichlet Allocation). LDA is unsupervised. Feed it in corpuses of text from the various documents (i.e. OCR them and feed LDA the results of OCR). It will then cluster them based on the contents of the text (with or without stop words - at your discretion). If possible, you could do a supervised approach of using a bag-of-words and any classifier such as an SVM or Random Forest.</p>

<p>Image Approach:</p>

<p>Use a CNN (convolutional neural network) and train it on images of the various vendors. If you don't have this class discrimination, and can't get it, then use an unsupervised approach such as an autoencoder and then cluster the points in the lower-dimensional autoencoder feature space.</p>
",,0,2017-02-07T05:19:48.837,,2791,2017-02-07T05:19:48.837,,,,,5293.0,2783.0,2,1,,,,64.71,10.95,8.99,0.0,0.0,28.0,Text approach Use LDA Latent Dirichlet Allocation LDA is unsupervised Feed it in corpuses of text from the various documents ie OCR them and feed LDA the results of OCR It will then cluster them based on the contents of the text with or without stop words at your discretion If possible you could do a supervised approach of using a bagofwords and any classifier such as an SVM or Random Forest Image Approach Use a CNN convolutional neural network and train it on images of the various vendors If you dont have this class discrimination and cant get it then use an unsupervised approach such as an autoencoder and then cluster the points in the lowerdimensional autoencoder feature space
,0.0,"<p>Hypothetical example, say I wanted: <code>P(gender,ethnicity|age,hair)</code>; so that the input would aligned to a trained dataset of: <code>(gender,ethnicity,age,hair) =&gt; hat bought</code>.</p>

<p>What approach is 'best' for computing ~gender and ~ethnicity given age,hair; in order to predict the hat bought?</p>

<p>The processing of the <code>inputs =&gt; hat</code> can be done/learned offline whereas infering the missing input values shall be done online. The results of the online pass shouldn't be stored in the network.</p>

<p>FYI: I am considering two Recurrent Neural Networks one for each problem.</p>
",,1,2017-02-07T15:16:20.487,,2792,2017-02-07T15:16:20.487,,,,,5313.0,,1,0,<neural-networks><machine-learning><deep-learning><classification><recurrent-neural-networks>,Infer dependent variables to produce output aligned to trained data,14.0,56.05,11.65,11.04,90.0,0.0,18.0,Hypothetical example say I wanted so that the input would aligned to a trained dataset of What approach is best for computing gender and ethnicity given agehair in order to predict the hat bought The processing of the can be donelearned offline whereas infering the missing input values shall be done online The results of the online pass shouldnt be stored in the network FYI I am considering two Recurrent Neural Networks one for each problem
,1.0,"<p>So I've been trying to understand neural networks ever since I came across <a href=""https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"" rel=""nofollow noreferrer"">Adam Geitgey's</a> blog on machine learning. I've read as much as I can on the subject (that I can grasp) and believe I understand all the broad concepts and some of the workings (despite being very weak in maths), neurons, synapses, weights, cost functions, backpropagation etc. However, I've not been able to figure out how to translate real world problems into a neural network solution. </p>

<p>Case in point, Adam Geitgey gives as an example usage, a house price prediction system where given a data set containing <strong>No. of bedrooms</strong>, <strong>Sq. feet</strong>, <strong>Neighborhood</strong> and <strong>Sale price</strong> you can train a neural network to be able to predict the price of a house. However he stops short of actually implementing a possible solution in code. The closest he gets, by way of an example, is basic a function demonstrating how you'd implement weights:</p>

<pre><code>def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):
  price = 0

  # a little pinch of this
  price += num_of_bedrooms * 1.0

  # and a big pinch of that
  price += sqft * 1.0

  # maybe a handful of this
  price += neighborhood * 1.0

  # and finally, just a little extra salt for good measure
  price += 1.0

  return price 
</code></pre>

<p>Other resources seem to focus more heavily on the maths and the only basic code example I could find that I understand (i.e. that isn't some all singing, all dancing image classification codebase) is an implementation that trains a neural network to be an XOR gate that deals only in 1's and 0's.  </p>

<p>So there's a gap in my knowledge that I just can't seem to bridge. If we return to the <strong>house price prediction</strong> problem, hows does one make the data suitable for feeding into a neural network? For example:</p>

<ul>
<li>No. of bedrooms: 3</li>
<li>Sq. feet: 2000</li>
<li>Neighborhood: Normaltown</li>
<li>Sale price: $250,000</li>
</ul>

<p>Can you just feed <strong>3</strong> and <strong>2000</strong> directly into the neural network because they are numbers? Or do you need to transform them into something else? Similarly what about the <strong>Normaltown</strong> value, that's a string, how do you go about translating it into a value a neural network can understand? Can you just pick a number, like an index, so long as it's consistent throughout the data?</p>

<p>Most of the neural network examples I've seen the numbers passing between layers are either 0 to 1 or -1 to 1. So at the end of processing, how do you transform the output value to something usable like <strong>$185,000</strong>?</p>

<p>I know the house price prediction example probably isn't a particularly useful problem given that it's been massively oversimplified to just three data points. But I just feel that if I could get over this hurdle and write an extremely basic app that trains using pseudo real-life data and spits out a pseudo real-life answer than I'll have broken the back of it and be able to kick on and delve further into machine learning.</p>
",,0,2017-02-07T15:59:42.137,1.0,2793,2017-02-13T11:22:06.993,2017-02-13T11:22:06.993,,5095.0,,5312.0,,1,2,<neural-networks><machine-learning>,How to transform inputs and extract useful outputs in a Neural Network?,64.0,58.82,10.15,8.85,344.0,588.0,59.0,So Ive been trying to understand neural networks ever since I came across Adam Geitgeys blog on machine learning Ive read as much as I can on the subject that I can grasp and believe I understand all the broad concepts and some of the workings despite being very weak in maths neurons synapses weights cost functions backpropagation etc However Ive not been able to figure out how to translate real world problems into a neural network solution Case in point Adam Geitgey gives as an example usage a house price prediction system where given a data set containing No of bedrooms Sq feet Neighborhood and Sale price you can train a neural network to be able to predict the price of a house However he stops short of actually implementing a possible solution in code The closest he gets by way of an example is basic a function demonstrating how youd implement weights Other resources seem to focus more heavily on the maths and the only basic code example I could find that I understand ie that isnt some all singing all dancing image classification codebase is an implementation that trains a neural network to be an XOR gate that deals only in 1s and 0s So theres a gap in my knowledge that I just cant seem to bridge If we return to the house price prediction problem hows does one make the data suitable for feeding into a neural network For example No of bedrooms 3 Sq feet 2000 Neighborhood Normaltown Sale price 185000 I know the house price prediction example probably isnt a particularly useful problem given that its been massively oversimplified to just three data points But I just feel that if I could get over this hurdle and write an extremely basic app that trains using pseudo reallife data and spits out a pseudo reallife answer than Ill have broken the back of it and be able to kick on and delve further into machine learning
2798.0,2.0,"<p>So for a class I'm reading Brooks' ""Intelligence without representation"". The introduction is dedicated to slating Representation as a focus for AI development. </p>

<p>I've read that representation is the problem of representing information symbolically, in time for it to be useful. It's related to the reasoning problem, which is about reasoning about symbolic information. </p>

<p>But I don't feel like I really understand it at any practical level. I think the idea is that when an agent is given a problem, it must describe this problem in some internal manner that is efficient and accurately describes the problem. This can then also be used to describe the primitive actions that can be taken to reach the solution. I think this then relates to Logic Programming eg Pascal?</p>

<p>Is my understanding of Representation correct? Just what does representation look like in practice, are there any open source codebases that might make a good example?</p>
",,1,2017-02-07T17:44:53.647,,2794,2017-02-16T04:58:06.970,2017-02-14T15:26:18.587,,5095.0,,5317.0,,1,2,<knowledge-representation>,What does Brooks mean by Representation?,73.0,47.59,12.0,9.9,0.0,0.0,21.0,So for a class Im reading Brooks Intelligence without representation The introduction is dedicated to slating Representation as a focus for AI development Ive read that representation is the problem of representing information symbolically in time for it to be useful Its related to the reasoning problem which is about reasoning about symbolic information But I dont feel like I really understand it at any practical level I think the idea is that when an agent is given a problem it must describe this problem in some internal manner that is efficient and accurately describes the problem This can then also be used to describe the primitive actions that can be taken to reach the solution I think this then relates to Logic Programming eg Pascal Is my understanding of Representation correct Just what does representation look like in practice are there any open source codebases that might make a good example
,1.0,"<p>I have been looking into <a href=""http://viv.ai/"" rel=""noreferrer"">Viv</a> an artificial intelligent agent in development. Based on what I understand, this AI can generate new code and execute it based on a query from the user. What I am curious to know is how this AI is able to learn to generate code based on some query. What kind of machine learning algorithms are involved in this process? One thing I considered is breaking down a dataset of programs by step. For example:</p>

<p>Code to take the average of 5 terms</p>

<p>1 - Add all 5 terms together<br>
2 - Divide by 5</p>

<p>Then I would train an algorithm to convert text to code. That is as far as I have figured out. Haven't tried anything however because i'm not sure where to start. Anybody have any ideas on how to implement Viv? <a href=""https://www.youtube.com/watch?v=Rblb3sptgpQ"" rel=""noreferrer"">Here is a demonstration of Viv.</a></p>
",,0,2017-02-07T19:44:51.280,2.0,2795,2017-02-10T08:13:47.173,,,,,4841.0,,1,7,<neural-networks><machine-learning><deep-learning><ai-design><nlp>,AI that can generate programs,179.0,73.78,6.72,8.85,0.0,0.0,16.0,I have been looking into Viv an artificial intelligent agent in development Based on what I understand this AI can generate new code and execute it based on a query from the user What I am curious to know is how this AI is able to learn to generate code based on some query What kind of machine learning algorithms are involved in this process One thing I considered is breaking down a dataset of programs by step For example Code to take the average of 5 terms 1 Add all 5 terms together 2 Divide by 5 Then I would train an algorithm to convert text to code That is as far as I have figured out Havent tried anything however because im not sure where to start Anybody have any ideas on how to implement Viv Here is a demonstration of Viv
,,"<p>You can try to figure out what exactly does an action do using such script:</p>

<pre><code>action = 0  # modify this!
o = env.reset()
for i in xrange(5): # repeat one action for five times
    o = env.step(action)[0]
IPython.display.display(
    Image.fromarray(
        o[:,140:142]  # extract your bat
    ).resize((300, 300))  # bigger image, easy for visualization
)
</code></pre>

<p><code>action</code> 0 and 1 seems useless, as nothing happens to the racket.</p>

<p><code>action</code> 2 &amp; 4 makes the racket go up, and <code>action</code> 3 &amp; 5 makes the racket go down.</p>

<p>The interesting part is, when I run the script above for the same <code>action</code>(from 2 to 5) two times, I have different results. Sometimes the racket reaches the top(bottom) border, and sometimes it doesn't. I think there might be some randomness on the speed of the racket, so it might be hard to measure which type of UP(2 or 4) is faster.</p>
",,0,2017-02-08T02:29:23.717,,2796,2017-02-08T02:29:23.717,,,,,5261.0,2449.0,2,0,,,,84.0,6.56,8.29,308.0,0.0,23.0,You can try to figure out what exactly does an action do using such script 0 and 1 seems useless as nothing happens to the racket 2 amp 4 makes the racket go up and 3 amp 5 makes the racket go down The interesting part is when I run the script above for the same from 2 to 5 two times I have different results Sometimes the racket reaches the topbottom border and sometimes it doesnt I think there might be some randomness on the speed of the racket so it might be hard to measure which type of UP2 or 4 is faster
,,"<p>This is a good question which I wrestled with myself when first trying to code an ANN. </p>

<p>Below is a good general-purpose solution, and it's the one I implemented in my code for trying to predict well-behaved numerical data. If your data is not well-behaved (i.e. fraught with outliers) then you may need to do more work normalizing the inputs and outputs. Some of the more advanced methods are described <a href=""https://www.cs.ccu.edu.tw/~wylin/BA/Fusion_of_Biometrics_II.ppt"" rel=""nofollow noreferrer"">here</a>.</p>

<p><em>Note: I will assume that you are using f(x) = tanh(x) as your activation function. If you aren't, you should still be able to reason through how to normalize your data after reading this.</em></p>

<p><strong>How to prepare the input data:</strong></p>

<p>The basic idea is that you want a significant variation in each input parameter to be reflected by a significant variation in the activation of the neuron those inputs are being fed into. By looking at a plot of the derivative of the tanh(x) actiavtion function, you'll see that the region of significant slope is within a distance of one or two from the origin. This means that whether the input to the activation function is 2000 or 3000 (values of x for which the derivative is negligibly small), the output of the activation will be almost identical...so your neuron's state will be independent of the difference between 2000 and 3000, and your network will never produce any predictive power from values in that range. </p>

<p>So if you want to input the square footage of the house into a neuron, you need to <em>normalize</em> the square footage so that the network can tell the difference between 2000 and 3000. One way to do this so that all of the significant variations in your data are 'noticed' by the neuron is to <strong>z-score-normalize the inputs</strong>. </p>

<ul>
<li><p>Gather all of your square footage values (from your training set) and calculate the mean and standard deviation. <em>Store the mean and standard deviation</em>---you'll need this information to normalize new square footage values when testing. </p></li>
<li><p><strong>Normalize the vector of square footage values by subtracting the mean and then dividing the result by the standard deviation</strong> (all operations element-wise of course). Subtracting the mean centers your data at the origin, and dividing by the standard deviation makes sure most of it is between -1 and 1, where the neuron's output is most sensitive to its input. This is called z-score normalization because each input value is replaced by its <a href=""https://en.wikipedia.org/wiki/Standard_score"" rel=""nofollow noreferrer"">z-score</a>.</p></li>
<li><p><strong>Do the above for each input variable.</strong></p></li>
</ul>

<p>Now, when you put each input value through a neuron, the output of the neuron is an activation between -1 and 1 (look at the image of tanh(x)). Since this is already in the 'sensitive' range of the activation function, you don't need to worry about altering the output of the input-layer neurons before sending them to the first hidden layer. Just <strong>give any hidden layer neurons the outputs of the previous layer directly</strong>---they will be able to handle them just fine. </p>

<p><strong>When you reach the last layer (the output neuron(s)), what you get is again another activation between -1 and 1. You have to convert this back into a value for the house in question</strong>, whether that value will be used as a prediction in a test set or to calculate error during training. However you do this, you just have to be consistent and use the same de-normalization procedure in training and testing. One way to think about it is: when the output neuron(s) returns 1, that means the network is returning the <em>maximum possible house value</em> as its prediction. <em>What should the highest value the network can estimate be?</em> The right approach here simply depends on your application. This is what I did:</p>

<ul>
<li>Calculate the mean of [the/each] output variable and store it.</li>
<li>Calculate the maximum deviation of the output variable from the mean. 
Python: <code>MaxDev = max([abs(DataPoint-numpy.mean(TrainingData)) for DataPoint in TrainingData])</code></li>
<li>When the network returns output(s) between -1 and 1, multiply the output by <code>MaxDev</code> and add it to the mean.</li>
</ul>

<p>Two basic quick checks you can do to see if your normalization-renormalization scheme is suitable (these are necessary, but perhaps not sufficient conditions):</p>

<ol>
<li>If all the input values are average (e.g. average no. of bedrooms, average sq.feet, etc), is the network's output equal to the average of the output variable (e.g. house value) as well? (It should be.)</li>
<li>If all the input values are unusually high/low, is the network's output unusually high/low as well? (This only works if all the inputs are positively related to the output...if some of them are inversely related related, you will have to think a bit more).</li>
</ol>

<p>Observe that the scheme presented here satisfies these two conditions.</p>

<p>Notice that this scheme would allow your network to only predict house values <em>inside</em> the range of house values in your training data set. Depending on the application, this behavior can be desirable or undesirable.</p>

<p>For example: you may want to make it impossible for your network to predict negative house values. Think about how you would do this. De-normalize the output so that -1 is mapped to 0.</p>

<p>If you want to set no limit on the values your network can predict, then you can run the network's output through a function that maps the [-1,1] range to all real numbers...like arctanh(x)! As long as you do this during training your network will adjust its weights to accommodate this.</p>

<p>I hope this was helpful. Let me know if you have further questions. My ANN module is in Python, by the way, so I might have language-specific advice.</p>
",,5,2017-02-08T08:45:44.100,,2797,2017-02-08T08:45:44.100,,,,,5037.0,2793.0,2,3,,,,61.36,10.62,7.94,92.0,0.0,180.0,This is a good question which I wrestled with myself when first trying to code an ANN Below is a good generalpurpose solution and its the one I implemented in my code for trying to predict wellbehaved numerical data If your data is not wellbehaved ie fraught with outliers then you may need to do more work normalizing the inputs and outputs Some of the more advanced methods are described here Note I will assume that you are using fx tanhx as your activation function If you arent you should still be able to reason through how to normalize your data after reading this How to prepare the input data The basic idea is that you want a significant variation in each input parameter to be reflected by a significant variation in the activation of the neuron those inputs are being fed into By looking at a plot of the derivative of the tanhx actiavtion function youll see that the region of significant slope is within a distance of one or two from the origin This means that whether the input to the activation function is 2000 or 3000 values of x for which the derivative is negligibly small the output of the activation will be almost identicalso your neurons state will be independent of the difference between 2000 and 3000 and your network will never produce any predictive power from values in that range So if you want to input the square footage of the house into a neuron you need to normalize the square footage so that the network can tell the difference between 2000 and 3000 One way to do this so that all of the significant variations in your data are noticed by the neuron is to zscorenormalize the inputs Gather all of your square footage values from your training set and calculate the mean and standard deviation Store the mean and standard deviationyoull need this information to normalize new square footage values when testing Normalize the vector of square footage values by subtracting the mean and then dividing the result by the standard deviation all operations elementwise of course Subtracting the mean centers your data at the origin and dividing by the standard deviation makes sure most of it is between 1 and 1 where the neurons output is most sensitive to its input This is called zscore normalization because each input value is replaced by its zscore Do the above for each input variable Now when you put each input value through a neuron the output of the neuron is an activation between 1 and 1 look at the image of tanhx Since this is already in the sensitive range of the activation function you dont need to worry about altering the output of the inputlayer neurons before sending them to the first hidden layer Just give any hidden layer neurons the outputs of the previous layer directlythey will be able to handle them just fine When you reach the last layer the output neurons what you get is again another activation between 1 and 1 You have to convert this back into a value for the house in question whether that value will be used as a prediction in a test set or to calculate error during training However you do this you just have to be consistent and use the same denormalization procedure in training and testing One way to think about it is when the output neurons returns 1 that means the network is returning the maximum possible house value as its prediction What should the highest value the network can estimate be The right approach here simply depends on your application This is what I did Calculate the mean of theeach output variable and store it Calculate the maximum deviation of the output variable from the mean Python When the network returns outputs between 1 and 1 multiply the output by and add it to the mean Two basic quick checks you can do to see if your normalizationrenormalization scheme is suitable these are necessary but perhaps not sufficient conditions If all the input values are average eg average no of bedrooms average sqfeet etc is the networks output equal to the average of the output variable eg house value as well It should be If all the input values are unusually highlow is the networks output unusually highlow as well This only works if all the inputs are positively related to the outputif some of them are inversely related related you will have to think a bit more Observe that the scheme presented here satisfies these two conditions Notice that this scheme would allow your network to only predict house values inside the range of house values in your training data set Depending on the application this behavior can be desirable or undesirable For example you may want to make it impossible for your network to predict negative house values Think about how you would do this Denormalize the output so that 1 is mapped to 0 If you want to set no limit on the values your network can predict then you can run the networks output through a function that maps the 11 range to all real numberslike arctanhx As long as you do this during training your network will adjust its weights to accommodate this I hope this was helpful Let me know if you have further questions My ANN module is in Python by the way so I might have languagespecific advice
,,"<p>First, <a href=""https://en.wikipedia.org/wiki/Pascal_(programming_language)"" rel=""nofollow noreferrer"">Pascal</a> is not a logic programming language. Logic programming refers to languages like <a href=""https://en.wikipedia.org/wiki/Prolog"" rel=""nofollow noreferrer"">Prolog</a> where you have a <a href=""https://en.wikipedia.org/wiki/Declarative_programming"" rel=""nofollow noreferrer"">declarative</a> style of programming compared to a <a href=""https://en.wikipedia.org/wiki/Imperative_programming"" rel=""nofollow noreferrer"">imperative</a> style like you have in Pascal. Maybe you mean <a href=""https://en.wikipedia.org/wiki/Conditional_(computer_programming)#If.E2.80.93then.28.E2.80.93else.29"" rel=""nofollow noreferrer"">if-statements</a> which are typical for imperative languages.</p>

<p>Second, representation means a certain level of abstraction. For instance, a model represents a certain part of the reality. Imagine a cup on a table. If the agent has a representation of this situation, it has a symbol <code>table</code> and a symbol <code>cup</code> which represents the things in the real world. Now it can have a relation <code>on(cup, table)</code> which represents the situation that the cup is on the table. This type of abstraction can be easily represented in a logic language like <a href=""https://en.wikipedia.org/wiki/Conditional_(computer_programming)#If.E2.80.93then.28.E2.80.93else.29"" rel=""nofollow noreferrer"">first order logic</a>. Therefore, one uses logic programming languages like Prolog or other types of languages like <a href=""https://en.wikipedia.org/wiki/Web_Ontology_Language"" rel=""nofollow noreferrer"">OWL</a> to represent knowledge and perform reasoning. So the important term to which Brooks refers is <a href=""https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning"" rel=""nofollow noreferrer"">Knowledge Representation and Reasoning</a>.</p>

<p>Third, if your agent only have sensor data like video or sonar data, then it knows only distances or pixels from the real world. That is not meant with representation. Brooks' Creatures have only this information and calculate with this data directly to perform an action without reasoning. In that sense also artificial neural networks have no representation.</p>

<p>Finally, for an open source project to understand representation I would recommend the above mentioned OWL. You can look at the <a href=""https://en.wikipedia.org/wiki/Prot%C3%A9g%C3%A9_(software)"" rel=""nofollow noreferrer"">Protégé</a> editor for working with OWL. In an OWL ontology you can represent relations between things and reason about them.</p>
",,0,2017-02-08T13:08:20.987,,2798,2017-02-08T13:08:20.987,,,,,5095.0,2794.0,2,0,,,,56.96,11.94,9.43,24.0,0.0,28.0,First Pascal is not a logic programming language Logic programming refers to languages like Prolog where you have a declarative style of programming compared to a imperative style like you have in Pascal Maybe you mean ifstatements which are typical for imperative languages Second representation means a certain level of abstraction For instance a model represents a certain part of the reality Imagine a cup on a table If the agent has a representation of this situation it has a symbol and a symbol which represents the things in the real world Now it can have a relation which represents the situation that the cup is on the table This type of abstraction can be easily represented in a logic language like first order logic Therefore one uses logic programming languages like Prolog or other types of languages like OWL to represent knowledge and perform reasoning So the important term to which Brooks refers is Knowledge Representation and Reasoning Third if your agent only have sensor data like video or sonar data then it knows only distances or pixels from the real world That is not meant with representation Brooks Creatures have only this information and calculate with this data directly to perform an action without reasoning In that sense also artificial neural networks have no representation Finally for an open source project to understand representation I would recommend the above mentioned OWL You can look at the Protégé editor for working with OWL In an OWL ontology you can represent relations between things and reason about them
,,"<p>Well,lets get this clear;</p>

<p>Intelligent vision and geostalt processing,both these two terms are in the scope of computer vision.So here I would like to give a glimpse of what computer vision is,inline with artificial intelligence;simply because computer vision is broad when it comes to application;robot vision under this you can check it here as well;Image Processing,Machine Vision and Pattern Recognition and Machine Learning!</p>

<ul>
<li>Computer Vision[The general Term]</li>
</ul>

<p>Let me call it parent;<em>hint:</em> Humans use their eyes and their brains to see and visually sense the world around them. Computer vision is the science that aims to give a similar, if not better, capability to a machine or computer/software program[intelligent Agent].</p>

<p>Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.</p>

<ul>
<li>Robot Vision &amp; Machine Vision(intelligent vision)</li>
</ul>

<p>Robot Vision involves using a combination of camera hardware and computer algorithms to allow robots to process visual data from the world. For example, your system could have a 2D camera which detects an object for the robot to pick up. A more complex example might be to use a 3D stereo camera to guide a robot to mount wheels onto a moving vehicle.Just like Google's self driving Car. And remember robot vision is closely related to Machine Vision(though Machine Vision refers to the industrial use of vision for automatic inspection, process control).
Without Robot Vision, your robot is essentially blind. This is not a problem for many robotic tasks, but for some applications Robot Vision is useful or even essential.</p>

<ul>
<li>Pattern Recognition and Machine Learning</li>
</ul>

<p>Where it starts to get a little more complex is when we include Pattern Recognition into the family tree, or more broadly Machine Learning. This branch of the family is focused on recognising patterns in data, which is quite important for many of the more advanced functions required of Robot Vision. For example, to be able to recognise an object from its image, the software must be able to detect if the object it sees is similar to previous objects. Machine Learning, therefore, is another parent of Computer Vision alongside Signal Processing/image processing.</p>

<p>However, not all Computer Vision techniques require Machine Learning. You can also use Machine Learning on signals which are not images. In practice, the two domains are often combined like this: Computer Vision technique can detect features and information from an image, which are then used as an input to the Machine Learning algorithms. For example, Computer Vision technique detects the size and color of parts on a conveyor belt, then Machine Learning decides if those parts are faulty based on its learned knowledge about what a good part should look like.</p>

<hr>

<p><strong>Are such methods/techniques being used or worked on today?</strong> </p>

<p>Jeff Dean, Google Senior Fellow and head of Brain team that just put <a href=""https://www.tensorflow.org/"" rel=""nofollow noreferrer"">TensorFlow</a> machine learning library into open source, gave the opening day keynote on deep learning, a powerful class of machine learning that allows machines to understand what they are seeing. These algorithms are trained by exposing them to huge amounts of data.</p>

<p>For example. 1000’s of tagged pictures of a car or people or animals, so they learn to recognize an unknown similar image very accurately, even if the object is somewhat obscured. They are now even able to interpret context. It’s not just a picture of a “baby”, but the algorithm comes back with “A baby is asleep next to a teddy bear”. it's therefore,quite impressive.</p>

<p><em>So,according to such scenario;you can analyse critically and guess what technique is applied! and real progress is made on this;don't forget that these are millions invested in these projects.</em></p>

<hr>

<ul>
<li>Gestalt Processing,inline with it's application(research with progress)</li>
</ul>

<p>Advertisers are using the basic gestalt process in the television medium focus on how the viewer responds to the entire message for instance the sounds, colors and distinct images seen in the commercial. The response varies from viewer to viewer, depending on age, emotional background, physical condition, level of education and social class. How the individual responds is his gestalt. Depending on the product or service, advertisers target the message to specific viewer demographics, who share common gestalt responses.</p>

<ol>
<li>Similarity Principle</li>
</ol>

<p>When conducting an ad campaign, marketers use the gestalt processing when reducing the product to a basic design theme, logo or slogan. Think of famous ad lines or instantly-recognizable symbols, such as McDonald's golden arches. That symbol evokes hamburgers and french fries to anyone remotely familiar with advertising, although the logo itself does not display food. Advertisers strive for individuality when marketing products, so the viewer or customer doesn't confuse their product with a competitor's item. However, under gestalt principles, customers won't confuse completely different products, such as cars and food, or clothes and electronics.</p>

<ol start=""2"">
<li>Perception</li>
</ol>

<p>In advertising, perception is reality. Advertisers using gestalt principles must consider how the message they use is perceived by potential customers and how this message prompts action -- buying the product or service. Media and advertising professionals use gestalt theory to create effective ways to sell products whether using images or conceiving of the most beneficial forms of distribution. Figuring the overall success rate of using gestalt principles is also based in simplicity -- sales results.</p>

<p>Therefore;according to such scenario,companies like Amazoon,IBM,Google to mention but a few a have applied all the above methods or techniques and are still booming.</p>
",,0,2017-02-09T14:31:44.300,,2800,2017-02-09T14:31:44.300,,,,,1581.0,2231.0,2,2,,,,42.11,14.45,10.19,0.0,0.0,134.0,Welllets get this clear Intelligent vision and geostalt processingboth these two terms are in the scope of computer visionSo here I would like to give a glimpse of what computer vision isinline with artificial intelligencesimply because computer vision is broad when it comes to applicationrobot vision under this you can check it here as wellImage ProcessingMachine Vision and Pattern Recognition and Machine Learning Computer VisionThe general Term Let me call it parenthint Humans use their eyes and their brains to see and visually sense the world around them Computer vision is the science that aims to give a similar if not better capability to a machine or computersoftware programintelligent Agent Computer vision is concerned with the automatic extraction analysis and understanding of useful information from a single image or a sequence of images It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding Robot Vision amp Machine Visionintelligent vision Robot Vision involves using a combination of camera hardware and computer algorithms to allow robots to process visual data from the world For example your system could have a 2D camera which detects an object for the robot to pick up A more complex example might be to use a 3D stereo camera to guide a robot to mount wheels onto a moving vehicleJust like Googles self driving Car And remember robot vision is closely related to Machine Visionthough Machine Vision refers to the industrial use of vision for automatic inspection process control Without Robot Vision your robot is essentially blind This is not a problem for many robotic tasks but for some applications Robot Vision is useful or even essential Pattern Recognition and Machine Learning Where it starts to get a little more complex is when we include Pattern Recognition into the family tree or more broadly Machine Learning This branch of the family is focused on recognising patterns in data which is quite important for many of the more advanced functions required of Robot Vision For example to be able to recognise an object from its image the software must be able to detect if the object it sees is similar to previous objects Machine Learning therefore is another parent of Computer Vision alongside Signal Processingimage processing However not all Computer Vision techniques require Machine Learning You can also use Machine Learning on signals which are not images In practice the two domains are often combined like this Computer Vision technique can detect features and information from an image which are then used as an input to the Machine Learning algorithms For example Computer Vision technique detects the size and color of parts on a conveyor belt then Machine Learning decides if those parts are faulty based on its learned knowledge about what a good part should look like Are such methodstechniques being used or worked on today Jeff Dean Google Senior Fellow and head of Brain team that just put TensorFlow machine learning library into open source gave the opening day keynote on deep learning a powerful class of machine learning that allows machines to understand what they are seeing These algorithms are trained by exposing them to huge amounts of data For example 1000’s of tagged pictures of a car or people or animals so they learn to recognize an unknown similar image very accurately even if the object is somewhat obscured They are now even able to interpret context It’s not just a picture of a “baby” but the algorithm comes back with “A baby is asleep next to a teddy bear” its thereforequite impressive Soaccording to such scenarioyou can analyse critically and guess what technique is applied and real progress is made on thisdont forget that these are millions invested in these projects Gestalt Processinginline with its applicationresearch with progress Advertisers are using the basic gestalt process in the television medium focus on how the viewer responds to the entire message for instance the sounds colors and distinct images seen in the commercial The response varies from viewer to viewer depending on age emotional background physical condition level of education and social class How the individual responds is his gestalt Depending on the product or service advertisers target the message to specific viewer demographics who share common gestalt responses Similarity Principle When conducting an ad campaign marketers use the gestalt processing when reducing the product to a basic design theme logo or slogan Think of famous ad lines or instantlyrecognizable symbols such as McDonalds golden arches That symbol evokes hamburgers and french fries to anyone remotely familiar with advertising although the logo itself does not display food Advertisers strive for individuality when marketing products so the viewer or customer doesnt confuse their product with a competitors item However under gestalt principles customers wont confuse completely different products such as cars and food or clothes and electronics Perception In advertising perception is reality Advertisers using gestalt principles must consider how the message they use is perceived by potential customers and how this message prompts action buying the product or service Media and advertising professionals use gestalt theory to create effective ways to sell products whether using images or conceiving of the most beneficial forms of distribution Figuring the overall success rate of using gestalt principles is also based in simplicity sales results Thereforeaccording to such scenariocompanies like AmazoonIBMGoogle to mention but a few a have applied all the above methods or techniques and are still booming
,0.0,"<ul>
<li>I randomly generate millions groups of triplet {x0, x1, x2} within range (0,1), then calculate the corresponding coefficients of the polynomial (x-x0)(x-x1)(x-x2), which result in triplet groups normalized in a form of {(x0+x1+x2)/3, \sqrt{(x0x1+x1x2+x0x2)/3}, \sqrt[3]{x0x1x2}}; </li>
<li>After that, I feed the coefficient triplets in to a 5-layered neural network {3,4,5,4,3}, in which all the activation function is set to sigmoid and the learning rate is set to 0.1;</li>
<li>However, I only get a very poor cross validation, around 20%. </li>
</ul>

<p>How can I fix this?</p>
",2017-02-09T17:15:43.247,0,2017-02-09T16:01:38.533,,2801,2017-02-09T16:53:11.587,2017-02-09T16:53:11.587,,75.0,,5363.0,,1,0,<neural-networks><machine-learning>,Is it possible to train a neural network to solve polynomial equations?,12.0,68.74,15.21,9.96,0.0,0.0,57.0,I randomly generate millions groups of triplet x0 x1 x2 within range 01 then calculate the corresponding coefficients of the polynomial xx0xx1xx2 which result in triplet groups normalized in a form of x0x1x23 sqrtx0x1x1x2x0x23 sqrt3x0x1x2 After that I feed the coefficient triplets in to a 5layered neural network 34543 in which all the activation function is set to sigmoid and the learning rate is set to 01 However I only get a very poor cross validation around 20 How can I fix this
,1.0,"<p>If we look at state of the art accuracy on the UCF101 data set, it is around 93% whereas for the HMDB51 data set it is around 66%. I looked at both the data sets and both contain videos of similar lengths. I was wondering if anyone could give an intuition as to why HMDB51 data set has been harder.</p>
",,0,2017-02-09T20:27:23.960,,2803,2017-02-21T20:39:21.507,2017-02-09T20:34:19.227,,4700.0,,4700.0,,1,1,<neural-networks><deep-learning><classification><computer-vision><action-recognition>,Why do action recognition algorithms perform better on ucf101dataset than HMDB51 dataset?,24.0,85.02,5.8,7.79,0.0,0.0,6.0,If we look at state of the art accuracy on the UCF101 data set it is around 93 whereas for the HMDB51 data set it is around 66 I looked at both the data sets and both contain videos of similar lengths I was wondering if anyone could give an intuition as to why HMDB51 data set has been harder
,0.0,"<p>We are working on a project for creating music based on crowd sourcing. People vote for every note until the vote is closed, and then move on to the next vote until the canvas for the music is filled. A similar project is <a href=""https://crowdsound.net/"" rel=""nofollow noreferrer"">crowdsound</a>, if you want to get an idea of what it looks like.</p>

<p>Now the fun part is, based on all the votes we get from various people, we would like to be able to build a Neural Network that can build an entire song on its own. The idea is for it to take in account every preceding vote and predict the one that will follow. That way, when trained, we could give it one note and let it predict the rest of the votes on its own and thus create a song on its own.</p>

<p>So I've read a few things here and there about neural networks, but there are two things I don't understand:</p>

<ul>
<li>How to build one that takes into account a dynamic number of inputs (all preceding votes).</li>
<li>How exactly should I decide the number of hidden layers (I still only vaguely understand what those hidden layers represent) I need for it to work well.</li>
</ul>

<p>We are using Java for the project and we were planning on using Neuroph for the neural network.</p>
",2017-02-27T15:16:24.727,4,2017-02-09T22:08:25.567,,2804,2017-02-25T19:55:17.220,2017-02-25T19:55:17.220,,3576.0,,5372.0,,1,2,<neural-networks><prediction>,Creating a neural network for predicting next vote in a series of votes,44.0,71.89,6.91,8.36,0.0,0.0,23.0,We are working on a project for creating music based on crowd sourcing People vote for every note until the vote is closed and then move on to the next vote until the canvas for the music is filled A similar project is crowdsound if you want to get an idea of what it looks like Now the fun part is based on all the votes we get from various people we would like to be able to build a Neural Network that can build an entire song on its own The idea is for it to take in account every preceding vote and predict the one that will follow That way when trained we could give it one note and let it predict the rest of the votes on its own and thus create a song on its own So Ive read a few things here and there about neural networks but there are two things I dont understand How to build one that takes into account a dynamic number of inputs all preceding votes How exactly should I decide the number of hidden layers I still only vaguely understand what those hidden layers represent I need for it to work well We are using Java for the project and we were planning on using Neuroph for the neural network
,,"<p>I was looking into genetic algorithms and things of that sort when I came acrossed this video. The guy in the video seems to know what hes doing i watched like 20 minutes of it.  Interesting stuff and he goes step by step with everything.</p>

<p><a href=""https://vimeo.com/52539994"" rel=""nofollow noreferrer"">https://vimeo.com/52539994</a></p>
",,0,2017-02-10T08:13:47.173,,2805,2017-02-10T08:13:47.173,,,,,5382.0,2795.0,2,2,,,,81.33,9.74,8.51,0.0,0.0,8.0,I was looking into genetic algorithms and things of that sort when I came acrossed this video The guy in the video seems to know what hes doing i watched like 20 minutes of it Interesting stuff and he goes step by step with everything httpsvimeocom52539994
,2.0,"<p>I have users reports about an accident. I want to know how to make sure that the number of reports is big enough to take that accident as a true accident and not a spam.
My idea is to consider a minimum number of reports in a specific time interval, for example 4 reports in 20 minutes are good enough to believe the existence of that accident.</p>

<p>My question is how can I choose the minimum number of reports and that time interval? Is there another logic to take that decision?
I will appreciate your answers.</p>
",,2,2017-02-10T09:15:11.580,,2806,2017-02-26T09:47:48.923,2017-02-25T19:54:15.977,,3576.0,,5383.0,,1,3,<decision-theory>,Making decision based on users reports,52.0,63.7,7.36,8.21,0.0,0.0,7.0,I have users reports about an accident I want to know how to make sure that the number of reports is big enough to take that accident as a true accident and not a spam My idea is to consider a minimum number of reports in a specific time interval for example 4 reports in 20 minutes are good enough to believe the existence of that accident My question is how can I choose the minimum number of reports and that time interval Is there another logic to take that decision I will appreciate your answers
,,"<p>It's a trust-level problem, so your judgment is the best to decide what would be rhe threshold. </p>

<p>You can help your decision making by trying and visualize how many accident (in %) you've left out... this can be an indicator of good threshold. You don't want to throw too many of them. But only you know what is good and bad in this case</p>
",,0,2017-02-11T07:00:18.877,,2807,2017-02-11T07:00:18.877,,,,,5397.0,2806.0,2,0,,,,92.53,6.89,7.27,0.0,0.0,14.0,Its a trustlevel problem so your judgment is the best to decide what would be rhe threshold You can help your decision making by trying and visualize how many accident in youve left out this can be an indicator of good threshold You dont want to throw too many of them But only you know what is good and bad in this case
2831.0,3.0,"<p>I want to create a network to predict the break up of poetry lines. The program would receive as input an unbroken poem, and would output the poem broken into lines.</p>

<p>example:</p>

<pre><code>And then the day came, when the risk to remain tight in a bud was more painful ...

---&gt;

And then the day came,
when the risk
to remain tight
in a bud
was more painful
than the risk
it took
to Blossom.
</code></pre>

<p>How should I go about this? I have been using classifiers for various tasks, but this seems to be a different type of task.</p>

<p>I'm thinking of it as an array of words (doesn't matter how they're represented for now) which would look like <code>[6, 32, 60, 203, 40, 50, 60, 230 ...]</code> and needs to map into an array representing line breaks <code>[0, 0, 1, 0, 0, 0, 1, 0, 0, 1 ...]</code> where 1 (at optimal) means there should be a line break after the word in that index. (in this idea, the two arrays are of the same length). Unfortunately, I couldn't find an algorithm that could train a network of this shape.</p>

<p>What machine learning or deep learning algorithm can be used for this task?</p>
",,6,2017-02-11T07:55:47.130,1.0,2808,2017-02-20T19:36:41.100,,,,,5400.0,,1,4,<neural-networks>,Machine Learning Ouput Array (for Poetry),115.0,78.99,8.23,8.09,279.0,0.0,23.0,I want to create a network to predict the break up of poetry lines The program would receive as input an unbroken poem and would output the poem broken into lines example How should I go about this I have been using classifiers for various tasks but this seems to be a different type of task Im thinking of it as an array of words doesnt matter how theyre represented for now which would look like and needs to map into an array representing line breaks where 1 at optimal means there should be a line break after the word in that index in this idea the two arrays are of the same length Unfortunately I couldnt find an algorithm that could train a network of this shape What machine learning or deep learning algorithm can be used for this task
,0.0,"<p>I'm here to ask you for a solution on this problem which is: how to use Reinforcement Learning in Immersive Virtual Reality to make a person move to a specific location in a virtual environment. As you know reinforcement Learning is a sub-area of Machine Learning in which an active entity called an agent interacts with its environment and learns how to act in order to achieve a pre-determined goal. The Reinforcement Learning had no prior model of behaviour and the participants no prior knowledge that their task was to move to and stay in a specific place. The participants were placed in a virtual environment where they had to avoid collisions with virtual projectiles. Following each projectile the agent analysed the movement made by the participant to determine paths of future projectiles in order to increase the chance of driving participants to the goal position and make them stay there as long as possible.</p>

<p>Update 1: <a href=""http://rexa.gordarg.com/Documents/Download/5becaa12-ce68-47a9-8cb6-f8ccc3d14059"" rel=""nofollow noreferrer"">Download: Reinforcement Learning as a tool to make people move to a speciﬁc location in Immersive Virtual Reality</a></p>
",,0,2017-02-11T11:22:59.897,,2810,2017-02-11T11:29:51.367,2017-02-11T11:29:51.367,,5398.0,,5398.0,,1,2,<reinforcement-learning>,A solution for a famous problem in RL,58.0,41.84,12.2,9.42,0.0,0.0,11.0,Im here to ask you for a solution on this problem which is how to use Reinforcement Learning in Immersive Virtual Reality to make a person move to a specific location in a virtual environment As you know reinforcement Learning is a subarea of Machine Learning in which an active entity called an agent interacts with its environment and learns how to act in order to achieve a predetermined goal The Reinforcement Learning had no prior model of behaviour and the participants no prior knowledge that their task was to move to and stay in a specific place The participants were placed in a virtual environment where they had to avoid collisions with virtual projectiles Following each projectile the agent analysed the movement made by the participant to determine paths of future projectiles in order to increase the chance of driving participants to the goal position and make them stay there as long as possible Update 1 Download Reinforcement Learning as a tool to make people move to a speciﬁc location in Immersive Virtual Reality
,1.0,"<p>In a Neural network, there is an input layer, any number of hidden layers, and an output layer. My question is: Are the input and output layer nodes actually perceptions? Or do they just signify what/how many/where the inputs and outputs are? </p>
",,0,2017-02-11T18:58:46.723,,2811,2017-02-11T21:47:23.957,,,,,4744.0,,1,1,<neural-networks><machine-learning><algorithm><mlp>,Perceptions in a Neural network,50.0,65.73,9.91,10.35,0.0,0.0,9.0,In a Neural network there is an input layer any number of hidden layers and an output layer My question is Are the input and output layer nodes actually perceptions Or do they just signify whathow manywhere the inputs and outputs are
,,"<p>Not 100% sure I got your question right, but yes the input nodes are perception fields. For the output it's good if you choose a space such that the different outputs you expect are independent.</p>

<p>One method to improve neural networks is actually using receptive fields. Look for convolutionary networks and pooling.</p>

<p>Would have posted that to a comment if I could.</p>
",,1,2017-02-11T21:47:23.957,,2812,2017-02-11T21:47:23.957,,,,,5416.0,2811.0,2,2,,,,67.35,9.96,9.35,0.0,0.0,8.0,Not 100 sure I got your question right but yes the input nodes are perception fields For the output its good if you choose a space such that the different outputs you expect are independent One method to improve neural networks is actually using receptive fields Look for convolutionary networks and pooling Would have posted that to a comment if I could
,,"<p>The only sensible choice is to use predictable behaviour. So in the people in front of the car scenario: first hit the brakes, same time horn, and stay on course.  The ppl then have a chance to jump out of the way leading to zero ppl being killed. Also with full brakes (going from 50km per hour to zero is less than 3 car length) an impact situation is almost not imaginable. Even if fullstop cannot be reached, severe damage of the pedestrians is unlike.</p>

<p>The other scenario is just crazy. So the distance has to be less that 3 car length, at least 1 car length in needed for the steering, then a car crashing is an uncontrollable situation, might leed to spinning and kill all 11 persons.</p>

<p>Apart from saying that I dont believe there is an example in reality where there is a dilemma;  the solution in these unlikely cases if to conform with the expectations of the opposing party to allow the other party to mitigate the situation as well.</p>
",,4,2017-02-12T10:48:09.903,,2813,2017-02-12T10:48:09.903,,,,,5423.0,111.0,2,1,,,,66.27,8.24,9.16,0.0,0.0,18.0,The only sensible choice is to use predictable behaviour So in the people in front of the car scenario first hit the brakes same time horn and stay on course The ppl then have a chance to jump out of the way leading to zero ppl being killed Also with full brakes going from 50km per hour to zero is less than 3 car length an impact situation is almost not imaginable Even if fullstop cannot be reached severe damage of the pedestrians is unlike The other scenario is just crazy So the distance has to be less that 3 car length at least 1 car length in needed for the steering then a car crashing is an uncontrollable situation might leed to spinning and kill all 11 persons Apart from saying that I dont believe there is an example in reality where there is a dilemma the solution in these unlikely cases if to conform with the expectations of the opposing party to allow the other party to mitigate the situation as well
,,"<p>I think there would not be a way to edit such ethics settings in a car. But hey, if cell phones can be rooted, why not cars? I imagine there'll be linux builds in the future for specific models that will let you do whatever you want.</p>

<p>As for who'll make such decisions- it'll be much like privacy issues of today. There'll be a a tug-of war on the blanket by the OS providers (who'll try to set it to a minimum amount of people injured, each with it's own methods), insurance companies (who'll try to make you pay more for OS's that will be statistically shown to damage your car easier) and car manufacturers (who'll want you to trash your car as soon as you can, so you'll buy a new one; or make cars that require a ridiculous amount of $$$ service). </p>

<p>Then some whistleblower will come out and expose a piece of code that chooses to kill young children over adults- because it will have a harder time distinguishing them from animals, and will take chances to save who it'll more surely recognize as humans. The OS manufacturer will get a head-slap from the public and a new consensus will be found. Whistleblowers will come out from insurance companies and car manufacturers too. </p>

<p>Humanity will grab a hot frying pan and burn itself and then learn to put on gloves beforehand. My advice would be just make sure you won't be that hand- stay away from them for a couple of years until all the early mistakes are made.</p>
",,0,2017-02-13T04:50:51.100,,2814,2017-02-13T04:50:51.100,,,,,5434.0,111.0,2,1,,,,61.9,8.59,8.74,0.0,0.0,41.0,I think there would not be a way to edit such ethics settings in a car But hey if cell phones can be rooted why not cars I imagine therell be linux builds in the future for specific models that will let you do whatever you want As for wholl make such decisions itll be much like privacy issues of today Therell be a a tugof war on the blanket by the OS providers wholl try to set it to a minimum amount of people injured each with its own methods insurance companies wholl try to make you pay more for OSs that will be statistically shown to damage your car easier and car manufacturers wholl want you to trash your car as soon as you can so youll buy a new one or make cars that require a ridiculous amount of service Then some whistleblower will come out and expose a piece of code that chooses to kill young children over adults because it will have a harder time distinguishing them from animals and will take chances to save who itll more surely recognize as humans The OS manufacturer will get a headslap from the public and a new consensus will be found Whistleblowers will come out from insurance companies and car manufacturers too Humanity will grab a hot frying pan and burn itself and then learn to put on gloves beforehand My advice would be just make sure you wont be that hand stay away from them for a couple of years until all the early mistakes are made
,,"<p>I think you mean the leave nodes only which are changed. The other nodes in the tree are calculated during calculating the best move with this tree. The values at the leaves are called <strong>utility values</strong> in Russel and Norvig's ""<a href=""http://rads.stackoverflow.com/amzn/click/9332543518"" rel=""nofollow noreferrer"">Artificial intelligence: a modern approach</a>"". Some times it is called <strong>heuristic value</strong>; see <a href=""https://en.wikipedia.org/wiki/Minimax"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Minimax</a>.</p>
",,0,2017-02-13T13:57:14.503,,2816,2017-02-14T08:31:53.207,2017-02-14T08:31:53.207,,5095.0,,5095.0,2517.0,2,2,,,,49.31,14.26,10.15,0.0,0.0,16.0,I think you mean the leave nodes only which are changed The other nodes in the tree are calculated during calculating the best move with this tree The values at the leaves are called utility values in Russel and Norvigs Artificial intelligence a modern approach Some times it is called heuristic value see httpsenwikipediaorgwikiMinimax
,0.0,"<p>I have been trying to reproduce the experiments done in the original: ""Firefly Algorithm for multimodal optimization"" <a href=""https://arxiv.org/pdf/1003.1466"" rel=""nofollow noreferrer"">(linked in the question)</a> so far: unsuccesfully. For the moment being I'm okay if anyone point me to the right direction.</p>

<p>I wrote the algorithm as specified in the paper in C++ programming languaje (I also downloaded several other implementations from internet for comparation purpouses) and used the very same parameters as specified in the paper (a random steep of 0.2, an initial light intensity of 1.0 and a light decay coefficient of 1.0, a population size of 40). I used the two bright update ecuations given and  for De Jung test function (as for example) a number of dimensions of 256 in a search domain in [-5.12, 5.12] as refered in common optimization literature and in paper.</p>

<p>In the paper the algorithm converges very quickly, as can be expected since this is a very simple test function, however, neither my implementation nor any code I have downloaded converges with that parameters.</p>

<p>My final questions are:</p>

<ol>
<li><p>Am I doing something wrong with the experimental methodology or am I using wrong parameter settings (may be something different than the original paper)?</p></li>
<li><p>Do anyone knows where can I find a code sample of Firefly Algorithm that I can use to reproduce the experiments of the mentioned paper?</p></li>
</ol>

<p>Please notice that there may be a lot of variations of this algorithm that can produce better results, but right now I'm only intrested in reproduce the experiments of the so-called paper. </p>
",,0,2017-02-13T13:57:39.303,,2817,2017-02-13T13:57:39.303,,,,,3566.0,,1,2,<optimization><heuristics>,Reproduce Firefly Algorithm experiments of original paper?,36.0,49.96,11.6,9.79,0.0,0.0,43.0,I have been trying to reproduce the experiments done in the original Firefly Algorithm for multimodal optimization linked in the question so far unsuccesfully For the moment being Im okay if anyone point me to the right direction I wrote the algorithm as specified in the paper in C programming languaje I also downloaded several other implementations from internet for comparation purpouses and used the very same parameters as specified in the paper a random steep of 02 an initial light intensity of 10 and a light decay coefficient of 10 a population size of 40 I used the two bright update ecuations given and for De Jung test function as for example a number of dimensions of 256 in a search domain in 512 512 as refered in common optimization literature and in paper In the paper the algorithm converges very quickly as can be expected since this is a very simple test function however neither my implementation nor any code I have downloaded converges with that parameters My final questions are Am I doing something wrong with the experimental methodology or am I using wrong parameter settings may be something different than the original paper Do anyone knows where can I find a code sample of Firefly Algorithm that I can use to reproduce the experiments of the mentioned paper Please notice that there may be a lot of variations of this algorithm that can produce better results but right now Im only intrested in reproduce the experiments of the socalled paper
,,"<p>In reinforcement learning, you can keep the name of value as it comes from the value function which estimates how good it is to be in this node w.r.t. the objective. Depending on the problem it can be a cost, utility, reward...</p>
",,0,2017-02-14T12:39:59.723,,2818,2017-02-14T12:39:59.723,,,,,5472.0,2517.0,2,0,,,,67.08,8.41,8.44,0.0,0.0,10.0,In reinforcement learning you can keep the name of value as it comes from the value function which estimates how good it is to be in this node wrt the objective Depending on the problem it can be a cost utility reward
,,"<p>The underlying problem is combinatorial, as you note, but I'm not getting how you're ascribing value to words.</p>

<p>The key element of deciding line breaks, beyond the visual, is rhythmic. <em>(There are other factors, as Bob Salita notes, but you've gotta start somewhere.)</em>  </p>

<p>It seems to me you need to teach the computer how to scan a phrase in the poetic sense, which relates to rhythm.  This is obviously a very difficult task, but the number of syllables and stresses is fundamental numerical data of poetry.</p>

<p>In order to validate to human tastes, you'd then have to use a captcha crowdsourcing method, for both the rhythmic stresses as input, and getting human reactions to different line-break configurations.  You would then reinforce the positive reactions, and the AI would tailor the line-break process to the audience.</p>

<p>However, instead of utilizing human tastes and aesthetic sensibilities, you could instead let the AI decide what is preferred, which would probably be comprised of some sort of symmetry considered optimal to an algorithm.</p>

<p>Following this logic, you wouldn't even need to have the AI learn the stresses, instead just focusing on raw syllables, or, numeric representation based on any factor. (With this method, the object is not to reformat poetry for humans, but for machines :)  </p>

<p>This is more about the aesthetics, but Cameron Browne's <a href=""http://www.cameronius.com/games/shibumi/browne-elegance-5.pdf"" rel=""nofollow noreferrer"">Elegance in Game Design</a> would seem to suggest there are engineering solutions to the type of aesthetic issues at the root of your problem.</p>

<p>I might start by teaching it to count the syllables of the poem, then having it look at the divisors. If it's roughly 10, it might be iambic pentameter. The AI doesn't care about the label, but it likes 10.  </p>

<p>20 syllables might represent a couplet in that meter:</p>

<blockquote>
  <p>The time is out of joint, oh cursed spite</p>
  
  <p>that ever I was born to set it right</p>
</blockquote>

<p>I'd definitely start by feeding it older poetry, particularly poets that keep to strict meter. It's been a while since I've read Spencer and so forth, but I'd think poets of his time would be useful.  Dr. Seuss, perhaps the greatest wielder of the rhyming couplet, would surely be extraordinarily useful. </p>

<p>The evaluation method would have to be fuzzy, because there would be increasing degrees of variance the more modern the poetry, ultimately resulting in free structures, except in the case of forms such as rap, which strongly utilize regularized rhythm. Machine learning is all about estimation and reinforcement, and is proving to be extremely useful dealing with fuzziness.</p>

<blockquote>
  <p>Dead mountain mouth of carious teeth that cannot spit</p>
</blockquote>

<p>is a great example of modern line of poetry: the floor of 13 syllables / 2 makes a 6 beat line.  Understanding that in context with the surrounding verse is much more difficult and illustrates the nature of the problem. Even scanning the poem correctly to that point to determine would be extremely difficult. </p>

<p>However, a different poem by the same author is extremely useful:</p>

<blockquote>
  <p>What is the late November doing / With the disturbance of the spring / And creatures of the summer heat, / And snowdrops writhing under feet / And hollyhocks that aim too high / Red into grey and tumble down / Late roses filled with early snow? / Thunder rolled by the rolling stars / Simulates triumphal cars / Deployed in constellated wars / Scorpion fights against the sun / Until the Sun and Moon go down / Comets weep and Leonids fly / Hunt the heavens and the plains / Whirled in a vortex that shall bring / The world to that destructive fire / Which burns before the ice-cap reigns</p>
</blockquote>

<p>All lines of roughly 8 syllables, easy to pick out because of capitalization.  But the real question is: ~136 13 lines of roughly 10 syllables, or 17 lines of roughly 8?  It would want to calculate based on word blocks (words that cross syllabic thresholds and at least tell you where the break <em>cannot</em> be, and it should be possible to statistically divine the pattern, at least for regularized verse.)  </p>

<blockquote>
  <p>The wounded surgeon plies the steel / That questions the distempered part; /
  Beneath the bleeding hands we feel / The sharp compassion of the healer's art 
  / Resolving the enigma of the fever chart.</p>
</blockquote>

<p>This verse highlight the problem.  5 lines of 4 beats, but syllabically: 8, 8, 8, 10, 12.  </p>

<p>Most likely: </p>

<ul>
<li>46/5 = 9.2</li>
<li>46/4 = 11.5</li>
<li>46/6 = 7.66</li>
</ul>

<p>Less likely:</p>

<ul>
<li>46/3 = 15.3</li>
<li>46/2 = 23  </li>
<li>46/7 = 6.57</li>
</ul>

<p>2 lines has less perfect symmetry, but 5 lines is more likely, based on the overall number of syllables, and of the likely choices, has the least variance. </p>

<p>Ultimately it would be looking for the underlying structure, or lack of structure, and try to reorganize the unbroken text into something <em>close</em> to the original structure.  While exactness is not always required because the process is ultimately subjective, and currently intractable, certain wrong choices would yield disastrous results. </p>

<p>In the prior example it may be able to discern the likelihood of a 5 line pattern, but it would have to figure out on which lines to place the extra syllables.  Differentiating between particles and other parts of speech provides a clue, because the poet's language is very compact: there are 19 nouns, verbs, or prepositions.  </p>

<p>More likely:
 - 19/5 = 3.8
 - 19/4 = 4.75</p>

<p>Less likely:
 - 19/3 = 6.33
 - 19/6 = 3.16
 - 19/7 = 2.71</p>

<p>Further analysis might narrow it down.  But extremely regularized verse remains the best place to start.  7 lines of roughly 10 syllables is ""poetic"":</p>

<blockquote>
  <p>‘The aged man that coffers-up his gold<br>
  Is plagu’d with cramps and gouts and painful fits;<br>
  And scarce hath eyes his treasure to behold,<br>
  But like still-pining Tantalus he sits,<br>
  And useless barns the harvest of his wits;<br>
  Having no other pleasure of his gain<br>
  But torment that it cannot cure his pain.</p>
</blockquote>

<p>It cares about both X and Y values.</p>

<p>Initially you want to keep it to a single language, because syllables may be treated differently.  That said, having the AI look for something like <a href=""https://en.wikipedia.org/wiki/Dactylic_hexameter"" rel=""nofollow noreferrer"">Dactylic hexameter</a> would be extremely useful, because you could feed it Homer. You could also feed it Homer in English in many different forms of English meter, and in almost every other living language. By definition, the AI would value works such as these, because max number_of_translations provides the most robust data set. When it starts to value meaning, this will be especially important.</p>

<p>Understanding different ways of treating syllables(long/short vs. stressed/unstressed) will also be essential as it transitions into more modern poetry.  </p>

<p><a href=""http://www.writing.upenn.edu/~afilreis/88/meter.html"" rel=""nofollow noreferrer"">Here is a good link for basic English meter.</a>  Iambic and Trochaic meters will be easy, while meters that employ Anapests, Dactyls and Spondees will be more challenging. </p>

<p>In some cases, however, these will be mathematically interchangeable.  </p>

<blockquote>
  <p>I went to the Garden of Love, / And saw what I never had seen: / A Chapel was built in the midst, / Where I used to play on the green. </p>
</blockquote>

<p>It doesn't matter if the lines above are iambic/trochaic or dactylic/anapestic, it's still 4 lines of roughly 8 syllables.  Thus ""I went to the Garden of Love"" is the same as ""The wounded surgeon plies the steel"", even though the beats for the lines are 3 and 4, respectively.</p>

<hr>

<p>It should also have a stanza marker, (possibly 00?). Because it looks for patterns within patterns, stanzas are valued.  Not all poetry has a stanza structure, but it arguably could. Deciding if stanzas are appropriate is partly a function of taking a syllabic divisor, breaking the poem down into number_of_lines, and looking at the divisors of that number. </p>

<p>It would need an added function to be able to recognize meaning patterns.  For instance, repetition of proper nouns is the marker of plays. (From a meaning perspective, imo, plays is the ideal place to start because the marker is so easy to learn, and names all belong to a single set, and imply communication. It's no different functionally than any other identifier, and a concept all computers ""understand"".)</p>

<p>Eventually it would want to look for phonetic patterns, rhymes and near rhymes, which would also be indicators of potential good places for line breaks.</p>

<p>There is a very big data set that it can look at, and who knows what it might discern?</p>
",,0,2017-02-15T04:39:10.893,,2819,2017-02-19T21:55:04.943,2017-02-19T21:55:04.943,,1671.0,,1671.0,2808.0,2,1,,,,60.35,11.37,8.97,0.0,0.0,280.0,The underlying problem is combinatorial as you note but Im not getting how youre ascribing value to words The key element of deciding line breaks beyond the visual is rhythmic There are other factors as Bob Salita notes but youve gotta start somewhere It seems to me you need to teach the computer how to scan a phrase in the poetic sense which relates to rhythm This is obviously a very difficult task but the number of syllables and stresses is fundamental numerical data of poetry In order to validate to human tastes youd then have to use a captcha crowdsourcing method for both the rhythmic stresses as input and getting human reactions to different linebreak configurations You would then reinforce the positive reactions and the AI would tailor the linebreak process to the audience However instead of utilizing human tastes and aesthetic sensibilities you could instead let the AI decide what is preferred which would probably be comprised of some sort of symmetry considered optimal to an algorithm Following this logic you wouldnt even need to have the AI learn the stresses instead just focusing on raw syllables or numeric representation based on any factor With this method the object is not to reformat poetry for humans but for machines This is more about the aesthetics but Cameron Brownes Elegance in Game Design would seem to suggest there are engineering solutions to the type of aesthetic issues at the root of your problem I might start by teaching it to count the syllables of the poem then having it look at the divisors If its roughly 10 it might be iambic pentameter The AI doesnt care about the label but it likes 10 20 syllables might represent a couplet in that meter The time is out of joint oh cursed spite that ever I was born to set it right Id definitely start by feeding it older poetry particularly poets that keep to strict meter Its been a while since Ive read Spencer and so forth but Id think poets of his time would be useful Dr Seuss perhaps the greatest wielder of the rhyming couplet would surely be extraordinarily useful The evaluation method would have to be fuzzy because there would be increasing degrees of variance the more modern the poetry ultimately resulting in free structures except in the case of forms such as rap which strongly utilize regularized rhythm Machine learning is all about estimation and reinforcement and is proving to be extremely useful dealing with fuzziness Dead mountain mouth of carious teeth that cannot spit is a great example of modern line of poetry the floor of 13 syllables 2 makes a 6 beat line Understanding that in context with the surrounding verse is much more difficult and illustrates the nature of the problem Even scanning the poem correctly to that point to determine would be extremely difficult However a different poem by the same author is extremely useful What is the late November doing With the disturbance of the spring And creatures of the summer heat And snowdrops writhing under feet And hollyhocks that aim too high Red into grey and tumble down Late roses filled with early snow Thunder rolled by the rolling stars Simulates triumphal cars Deployed in constellated wars Scorpion fights against the sun Until the Sun and Moon go down Comets weep and Leonids fly Hunt the heavens and the plains Whirled in a vortex that shall bring The world to that destructive fire Which burns before the icecap reigns All lines of roughly 8 syllables easy to pick out because of capitalization But the real question is 136 13 lines of roughly 10 syllables or 17 lines of roughly 8 It would want to calculate based on word blocks words that cross syllabic thresholds and at least tell you where the break cannot be and it should be possible to statistically divine the pattern at least for regularized verse The wounded surgeon plies the steel That questions the distempered part Beneath the bleeding hands we feel The sharp compassion of the healers art Resolving the enigma of the fever chart This verse highlight the problem 5 lines of 4 beats but syllabically 8 8 8 10 12 Most likely 465 92 464 115 466 766 Less likely 463 153 462 23 467 657 2 lines has less perfect symmetry but 5 lines is more likely based on the overall number of syllables and of the likely choices has the least variance Ultimately it would be looking for the underlying structure or lack of structure and try to reorganize the unbroken text into something close to the original structure While exactness is not always required because the process is ultimately subjective and currently intractable certain wrong choices would yield disastrous results In the prior example it may be able to discern the likelihood of a 5 line pattern but it would have to figure out on which lines to place the extra syllables Differentiating between particles and other parts of speech provides a clue because the poets language is very compact there are 19 nouns verbs or prepositions More likely 195 38 194 475 Less likely 193 633 196 316 197 271 Further analysis might narrow it down But extremely regularized verse remains the best place to start 7 lines of roughly 10 syllables is poetic ‘The aged man that coffersup his gold Is plagu’d with cramps and gouts and painful fits And scarce hath eyes his treasure to behold But like stillpining Tantalus he sits And useless barns the harvest of his wits Having no other pleasure of his gain But torment that it cannot cure his pain It cares about both X and Y values Initially you want to keep it to a single language because syllables may be treated differently That said having the AI look for something like Dactylic hexameter would be extremely useful because you could feed it Homer You could also feed it Homer in English in many different forms of English meter and in almost every other living language By definition the AI would value works such as these because max numberoftranslations provides the most robust data set When it starts to value meaning this will be especially important Understanding different ways of treating syllableslongshort vs stressedunstressed will also be essential as it transitions into more modern poetry Here is a good link for basic English meter Iambic and Trochaic meters will be easy while meters that employ Anapests Dactyls and Spondees will be more challenging In some cases however these will be mathematically interchangeable I went to the Garden of Love And saw what I never had seen A Chapel was built in the midst Where I used to play on the green It doesnt matter if the lines above are iambictrochaic or dactylicanapestic its still 4 lines of roughly 8 syllables Thus I went to the Garden of Love is the same as The wounded surgeon plies the steel even though the beats for the lines are 3 and 4 respectively It should also have a stanza marker possibly 00 Because it looks for patterns within patterns stanzas are valued Not all poetry has a stanza structure but it arguably could Deciding if stanzas are appropriate is partly a function of taking a syllabic divisor breaking the poem down into numberoflines and looking at the divisors of that number It would need an added function to be able to recognize meaning patterns For instance repetition of proper nouns is the marker of plays From a meaning perspective imo plays is the ideal place to start because the marker is so easy to learn and names all belong to a single set and imply communication Its no different functionally than any other identifier and a concept all computers understand Eventually it would want to look for phonetic patterns rhymes and near rhymes which would also be indicators of potential good places for line breaks There is a very big data set that it can look at and who knows what it might discern
,5.0,"<p>Everything related to Deep Learning (DL) and deep(er) networks seems ""successful"", at least progressing very fast, and cultivating the belief that AGI is at reach. This is popular imagination. DL is a tremendous tool to tackle so many problems, including the creation of AGIs. It is not enough, though. A tool is a necessary ingredient, but often insufficient.</p>

<p>Leading figures in the domain are looking elsewhere to make progress. This <a href=""https://hackernoon.com/feynman-machine-a-new-approach-for-cortical-and-machine-intelligence-5855c0e61a70#.dmgovix19"" rel=""nofollow noreferrer"">report/claim</a> gathers links to statements by <a href=""https://www.quora.com/Is-the-current-hype-about-Deep-Learning-justified?redirected_qid=6578691"" rel=""nofollow noreferrer"">Yoshua Bengio</a>, <a href=""https://www.quora.com/What-are-the-limits-of-deep-learning-2/answer/Yann-LeCun"" rel=""nofollow noreferrer"">Yann LeCun</a> and <a href=""https://www.youtube.com/watch?v=VIRCybGgHts"" rel=""nofollow noreferrer"">Geoff Hinton</a>. The report also explains:</p>

<blockquote>
  <p>The main weaknesses of DL (as I see them) are: reliance on the simplest possible model neurons (“cartoonish” as LeCun calls them); use of ideas from 19th century Statistical Mechanics and Statistics, which are the basis of energy functions and log-likelihood methods; and the combination of these in techniques like backprop and stochastic gradient descent, leading to a very limited regime of application (offline, mostly batched, supervised learning), requiring highly-talented practitioners (aka “Stochastic Graduate Descent”), large amounts of expensive labelled training data and computational power. While great for huge companies who can lure or buy the talent and deploy unlimited resources to gather data and crunch it, DL is simply neither accessible nor useful to the majority of us.</p>
</blockquote>

<p>Although interesting and relevant, such kind of explanation does not really address the gist of the problem: What is lacking?</p>

<p>The question seems broad, but it may be by lack of a simple answer. Is there a way to pin-point what DL is lacking for an AGI ?</p>
",,0,2017-02-15T08:33:57.883,1.0,2820,2017-02-20T19:31:05.387,,,,,169.0,,1,4,<deep-learning><deep-network><agi>,Why are deep neural networks and deep learning insufficient to achieve general intelligence?,242.0,50.06,13.58,11.67,0.0,0.0,50.0,Everything related to Deep Learning DL and deeper networks seems successful at least progressing very fast and cultivating the belief that AGI is at reach This is popular imagination DL is a tremendous tool to tackle so many problems including the creation of AGIs It is not enough though A tool is a necessary ingredient but often insufficient Leading figures in the domain are looking elsewhere to make progress This reportclaim gathers links to statements by Yoshua Bengio Yann LeCun and Geoff Hinton The report also explains The main weaknesses of DL as I see them are reliance on the simplest possible model neurons “cartoonish” as LeCun calls them use of ideas from 19th century Statistical Mechanics and Statistics which are the basis of energy functions and loglikelihood methods and the combination of these in techniques like backprop and stochastic gradient descent leading to a very limited regime of application offline mostly batched supervised learning requiring highlytalented practitioners aka “Stochastic Graduate Descent” large amounts of expensive labelled training data and computational power While great for huge companies who can lure or buy the talent and deploy unlimited resources to gather data and crunch it DL is simply neither accessible nor useful to the majority of us Although interesting and relevant such kind of explanation does not really address the gist of the problem What is lacking The question seems broad but it may be by lack of a simple answer Is there a way to pinpoint what DL is lacking for an AGI
,,"<p>I think it's missing still the aspects what makes a human brain; having a lot of different networks working with each other. </p>

<p>Just like meditation improves cognitive abilities by having the brain work more synergistically, we could apply that to machines too. </p>

<p>For example google is learning a computer to dream, just like we do, to reinforce what we already learned. 
<a href=""https://medium.com/@tannistho/why-is-google-teaching-its-ai-to-dream-e9ae9ecd0e3a#.gljal6pww"" rel=""nofollow noreferrer"">https://medium.com/@tannistho/why-is-google-teaching-its-ai-to-dream-e9ae9ecd0e3a#.gljal6pww</a></p>

<p>And here is pathnet, a network of neural network. 
<a href=""https://medium.com/@thoszymkowiak/deepmind-just-published-a-mind-blowing-paper-pathnet-f72b1ed38d46#.ed0f6pdq7"" rel=""nofollow noreferrer"">https://medium.com/@thoszymkowiak/deepmind-just-published-a-mind-blowing-paper-pathnet-f72b1ed38d46#.ed0f6pdq7</a></p>

<p>Creating all these mechanics and putting them all together, with enough power and we will get pretty close!</p>
",,2,2017-02-15T11:30:28.503,,2821,2017-02-20T12:00:03.940,2017-02-20T12:00:03.940,,5388.0,,5388.0,2820.0,2,1,,,,27.83,23.55,10.85,0.0,0.0,46.0,I think its missing still the aspects what makes a human brain having a lot of different networks working with each other Just like meditation improves cognitive abilities by having the brain work more synergistically we could apply that to machines too For example google is learning a computer to dream just like we do to reinforce what we already learned httpsmediumcomtannisthowhyisgoogleteachingitsaitodreame9ae9ecd0e3agljal6pww And here is pathnet a network of neural network httpsmediumcomthoszymkowiakdeepmindjustpublishedamindblowingpaperpathnetf72b1ed38d46ed0f6pdq7 Creating all these mechanics and putting them all together with enough power and we will get pretty close
,0.0,"<p>I am trying to understand the algorithm for n-step Sarsa from Sutton/Barto (2nd Edition, p. 157, <a href=""http://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf"" rel=""nofollow noreferrer"">PDF</a>) As I understand it, this algorithm should update n state action values, but I cannot see where it is 'propagated backwards' (sorry for the wrong terminology, but I couldn't find something better). Probably, I am not seeing the forrest for all the trees?</p>
",,0,2017-02-15T15:39:15.997,,2824,2017-02-15T15:39:15.997,,,,,5503.0,,1,1,<reinforcement-learning>,'Propagation' in n-step Sarsa,17.0,68.1,11.31,9.1,0.0,0.0,18.0,I am trying to understand the algorithm for nstep Sarsa from SuttonBarto 2nd Edition p 157 PDF As I understand it this algorithm should update n state action values but I cannot see where it is propagated backwards sorry for the wrong terminology but I couldnt find something better Probably I am not seeing the forrest for all the trees
,0.0,"<p>Short version of this question: where in the OpenAI Gym docs can you find more information about an environment, like what each of the variables in an observation means, and so on.</p>

<p>As per their docs (<a href=""https://gym.openai.com/docs"" rel=""nofollow noreferrer"">https://gym.openai.com/docs</a>), you can get the state space as follows:</p>

<pre><code>env.observation_space
</code></pre>

<p>The problem is, these just look like random numbers in an array.</p>

<p>Using CartPole-v0 as an example, the bounds are given as:</p>

<pre><code>env.observation_space.high 
# array([  4.80000000e+00,   3.40282347e+38,   4.18879020e-01, 3.40282347e+38])

env.observation_space.low
# array([ -4.80000000e+00,  -3.40282347e+38,  -4.18879020e-01, -3.40282347e+38])
</code></pre>

<p>It seems intuitive after reading some papers about the inverted pendulum, that the state is typically represented by a 4-tuple of (angle, angular speed, horizontal displacement, horizontal speed).</p>

<p>This also suggests that the observation space bounds are actually meant to represent:</p>

<p>Low: (-pi, -inf, x_min, -inf)
High: (+pi, +inf, x_max, +inf)</p>

<p>The magnitude of the 2nd and 4th lows/highs seem to suggest that they do indeed represent angular speed and horizontal speed.</p>

<p>But why does the 1st low/high not correspond to -/+pi?</p>

<p>Where can more information be found about what these numbers actually represent?</p>
",,0,2017-02-16T00:28:09.960,,2826,2017-02-16T00:28:09.960,,,,,5510.0,,1,0,<reinforcement-learning>,Where do I find documentation about specific OpenAI Gym environments?,12.0,56.59,13.18,8.99,239.0,0.0,56.0,Short version of this question where in the OpenAI Gym docs can you find more information about an environment like what each of the variables in an observation means and so on As per their docs httpsgymopenaicomdocs you can get the state space as follows The problem is these just look like random numbers in an array Using CartPolev0 as an example the bounds are given as It seems intuitive after reading some papers about the inverted pendulum that the state is typically represented by a 4tuple of angle angular speed horizontal displacement horizontal speed This also suggests that the observation space bounds are actually meant to represent Low pi inf xmin inf High pi inf xmax inf The magnitude of the 2nd and 4th lowshighs seem to suggest that they do indeed represent angular speed and horizontal speed But why does the 1st lowhigh not correspond to pi Where can more information be found about what these numbers actually represent
,,"<p>In that paper, Brooks introduced the basis for what became known as his ""subsumption architecture"".  The idea was to get away from the 1980's popular approach of a single global representation of all the components of the problem space that had required the task of robot task planning to juggle every  constraint in the world into one giant disordered mess of states and state transitions.  Rather than represent every element in the world in a single model (The Representation), Brooks suggested it was preferable to build a hierarchy of submodels of the world (subsets of states and transitions) in which smaller tasks could be more readily planned.  Then as these rudimentary skills were mastered, they could be combined to address a hierarchy of bigger and more complex tasks (bigger tasks subsume smaller tasks and benefit from their already having been solved).</p>

<p>Yes, representation did not fully go away, but it was redistributed hierarchically so that much of the state could be abstracted away from the higher level of the bigger problem that you need to solve.  Planning became like coordinating a hierarchical army of skills, where the general doesn't need to plan every movement of the private in order to manage a battle.  Instead, that general need only tell the colonels what to do, and the colonels tell the majors, and so on down to the privates.  Now the general solves problems by coodinating multiple sub-hierarchies available to him/her by delegating authority to coordinate behavior at the appropriate level of abstraction: like division, brigade, battalion, company, and squad.  That's Brooks' Subsumption Architecure: the general needs to represent a battle plan only as ""the world according to colonels"". </p>

<p><a href=""https://en.wikipedia.org/wiki/Subsumption_architecture"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Subsumption_architecture</a></p>
",,0,2017-02-16T04:58:06.970,,2828,2017-02-16T04:58:06.970,,,,,1657.0,2794.0,2,0,,,,40.11,14.57,10.62,0.0,0.0,48.0,In that paper Brooks introduced the basis for what became known as his subsumption architecture The idea was to get away from the 1980s popular approach of a single global representation of all the components of the problem space that had required the task of robot task planning to juggle every constraint in the world into one giant disordered mess of states and state transitions Rather than represent every element in the world in a single model The Representation Brooks suggested it was preferable to build a hierarchy of submodels of the world subsets of states and transitions in which smaller tasks could be more readily planned Then as these rudimentary skills were mastered they could be combined to address a hierarchy of bigger and more complex tasks bigger tasks subsume smaller tasks and benefit from their already having been solved Yes representation did not fully go away but it was redistributed hierarchically so that much of the state could be abstracted away from the higher level of the bigger problem that you need to solve Planning became like coordinating a hierarchical army of skills where the general doesnt need to plan every movement of the private in order to manage a battle Instead that general need only tell the colonels what to do and the colonels tell the majors and so on down to the privates Now the general solves problems by coodinating multiple subhierarchies available to himher by delegating authority to coordinate behavior at the appropriate level of abstraction like division brigade battalion company and squad Thats Brooks Subsumption Architecure the general needs to represent a battle plan only as the world according to colonels httpsenwikipediaorgwikiSubsumptionarchitecture
,,"<p>Everyone dealing with neural networks misses an important point when comparing systems with human like intelligence. A human takes many months to do anything intelligible, let alone being able to solve problems where adult humans can barely manage. That and the size of human brain is enormous compared to our neural networks. Direction might be right, but the scale is way off. Number of neurons in human brain can be matched memory-wise but the amount of parallelism to simulate it real-time cannot yet be achieved (at least for a random researcher). While a little old <a href=""https://www.extremetech.com/extreme/163051-simulating-1-second-of-human-brain-activity-takes-82944-processors"" rel=""noreferrer"">this</a> might give you an idea of how much we lack the processing power. </p>
",,7,2017-02-16T09:17:29.710,,2829,2017-02-16T09:28:59.653,2017-02-16T09:28:59.653,,210.0,,210.0,2820.0,2,5,,,,53.0,10.79,10.77,0.0,0.0,12.0,Everyone dealing with neural networks misses an important point when comparing systems with human like intelligence A human takes many months to do anything intelligible let alone being able to solve problems where adult humans can barely manage That and the size of human brain is enormous compared to our neural networks Direction might be right but the scale is way off Number of neurons in human brain can be matched memorywise but the amount of parallelism to simulate it realtime cannot yet be achieved at least for a random researcher While a little old this might give you an idea of how much we lack the processing power
,,"<p>Deep Learning is mostly successful in supervised learning, whereas the brain builds categories mostly in an unsupervised way. We don't yet know how to do that. (Take a <a href=""https://www.wired.com/2012/06/google-x-neural-network/"" rel=""nofollow noreferrer"">look at google brain</a>: 16,000 cores and all this thing can do is recognise cats and human faces with pretty abysmal accuracy.)</p>

<p>Deep Learning uses highly unstructured activations, i.e. the high level representations of ""dog"" and ""cat"" in a neural network classifier don't have to be similar at all. The brain on the other hand uses inhibitory neurons to create <a href=""http://www.cortical.io/technology_representations.html"" rel=""nofollow noreferrer"">sparse distributed representations</a> which are decomposable into their semantic aspects. That's probably important for abstraction and reasoning by analogy. </p>

<p>The brain has many different parts which work together. Deep Learning researchers are only just beginning to integrate <a href=""https://arxiv.org/abs/1410.5401"" rel=""nofollow noreferrer"">memory</a> or attention mechanisms into their architecture. </p>

<p>The brain integrates information from many different senses. Most Deep Learning applications use just one type of input, like text or pictures. </p>

<p>The brain is capable of modelling sequences as categories. (Basically every verb names a sequential (i.e. temporal) category.) It can then arrange these categories into long-term hierarchical plans. So far I haven't seen anything in that direction in Deep Learning.</p>

<p>Also neural networks can't yet operate on the same scale as the human brain. If you look at <a href=""http://ai.stackexchange.com/questions/2330/when-will-the-number-of-neurons-in-ai-systems-equal-the-human-brain"">the answers to this question</a>, the human brain will be ahead in neuron count for another couple of decades. A neural network might not need the same number of neurons as the brain to reach a similar performance (because of higher accuracy), but right now for example video processing is still pretty limited in terms of input and throughput. </p>
",,4,2017-02-16T14:11:46.497,,2830,2017-02-16T14:11:46.497,,,,,2227.0,2820.0,2,3,,,,56.15,12.69,9.92,0.0,0.0,46.0,Deep Learning is mostly successful in supervised learning whereas the brain builds categories mostly in an unsupervised way We dont yet know how to do that Take a look at google brain 16000 cores and all this thing can do is recognise cats and human faces with pretty abysmal accuracy Deep Learning uses highly unstructured activations ie the high level representations of dog and cat in a neural network classifier dont have to be similar at all The brain on the other hand uses inhibitory neurons to create sparse distributed representations which are decomposable into their semantic aspects Thats probably important for abstraction and reasoning by analogy The brain has many different parts which work together Deep Learning researchers are only just beginning to integrate memory or attention mechanisms into their architecture The brain integrates information from many different senses Most Deep Learning applications use just one type of input like text or pictures The brain is capable of modelling sequences as categories Basically every verb names a sequential ie temporal category It can then arrange these categories into longterm hierarchical plans So far I havent seen anything in that direction in Deep Learning Also neural networks cant yet operate on the same scale as the human brain If you look at the answers to this question the human brain will be ahead in neuron count for another couple of decades A neural network might not need the same number of neurons as the brain to reach a similar performance because of higher accuracy but right now for example video processing is still pretty limited in terms of input and throughput
,,"<p>You should try <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow noreferrer"">to use an RNN</a>. You feed in letter by letter and have a binary output of linebreak - no linebreak. If you have enough data it might actually work. </p>
",,3,2017-02-16T14:39:14.837,,2831,2017-02-16T14:39:14.837,,,,,2227.0,2808.0,2,4,,,,86.4,5.37,8.22,0.0,0.0,4.0,You should try to use an RNN You feed in letter by letter and have a binary output of linebreak no linebreak If you have enough data it might actually work
,,"<p><a href=""https://en.wikipedia.org/wiki/Lego_Mindstorms"" rel=""nofollow noreferrer"">LEGO Mindstorms</a> is widely used to demonstrate AI in schools and universities [<a href=""https://pdfs.semanticscholar.org/b515/dcdb633e2de2101a1eabfc53ecf84f5fcd39.pdf"" rel=""nofollow noreferrer"">1</a>, <a href=""http://www.legoengineering.com/"" rel=""nofollow noreferrer"">2</a>]. With LEGO as basis you are very flexible. You can build what you want very easily. The AI programs can be written in different languages from very easy graphical once to Lisp and C++. The newest version has an SD Card drive, USB interface and a powerful ARM processor. You can use four motors and four sensors directly. There exists touch, sound, sonar, gyro, infrared and colour sensors. There is also a big community which provides you with lot of ideas, hardware and programs [<a href=""https://www.hackster.io/mindstorms"" rel=""nofollow noreferrer"">3</a>].</p>
",,0,2017-02-16T23:08:01.760,,2832,2017-02-16T23:08:01.760,,,,,5095.0,2772.0,2,3,,,,67.35,9.38,9.83,0.0,0.0,21.0,LEGO Mindstorms is widely used to demonstrate AI in schools and universities 1 2 With LEGO as basis you are very flexible You can build what you want very easily The AI programs can be written in different languages from very easy graphical once to Lisp and C The newest version has an SD Card drive USB interface and a powerful ARM processor You can use four motors and four sensors directly There exists touch sound sonar gyro infrared and colour sensors There is also a big community which provides you with lot of ideas hardware and programs 3
,1.0,"<p>How does in the (famous Zilberstein) <code>PR</code>(uning) algorithm below the <code>LP-dominate</code> function get started: the first time it's called, <code>D=∅</code> and the linear program deteriorates (i.e. no constraint equations)? Some explanation much appreciated! Thanks.</p>

<pre><code>procedure POINTWISE-DOMINATE(w, U)
...
3. return false
procedure LP-DOMINATE(w, U)
4. solve the following linear program variables: d, b(s) ∀s ∈ S
      maximize d
      subject to the constraints
        b · (w − u) ≥ d, ∀u ∈ U
        sum(b) = 1
5. if d ≥ 0 then return b
6. else return nil
procedure BEST(b, U )
...
12. return w
procedure PR(W)
13. D ← ∅
14. while W = ∅
15.   w ← any element in W
16.   if POINTWISE-DOMINATE(w, D) = true
17.      W ← W − {w}
18.   else
19.      b ← LP-DOMINATE(w, D)
20.      if b = nil then
21.         W ← W − {w}
22.      else
23.         w ← BEST(b, W)
24.         D ← D ∪ {w}
25.         W ← W − {w}
26. return D
</code></pre>
",,0,2017-02-16T23:53:08.683,,2833,2017-02-22T17:37:01.127,2017-02-22T17:37:01.127,,101.0,,5534.0,,1,4,<decision-theory>,"Zilberstein's ""LP-dominate"" pruning explained?",26.0,52.15,16.68,11.57,675.0,0.0,14.0,How does in the famous Zilberstein uning algorithm below the function get started the first time its called and the linear program deteriorates ie no constraint equations Some explanation much appreciated Thanks
2838.0,4.0,"<p>I want to know something more about it. Are there any github repo or an open source project?</p>
",,1,2017-02-17T15:24:50.153,,2834,2017-02-20T08:42:24.800,,,,,5549.0,,1,3,<machine-learning>,Is it possible for an AI to learn how to speak from books as training sets?,101.0,70.8,5.13,10.22,0.0,0.0,2.0,I want to know something more about it Are there any github repo or an open source project
,,"<p>Artificial intelligence proponents today are focused on the problem of computability - the ability to solve complex problems fast. It is my belief that any amount of success in this direction will not lead to human (general) intelligence although it certainly will outperform humans in certain domains. Instead, efforts should be toward a study of what neurological events cause sensation (the experience of qualia). Of course, this is the hard problem of philosophy but I believe it is the unique key to general intelligence and its capabilities. Reverse engineering and also testable theories should be advanced toward this end. </p>
",,2,2017-02-17T23:46:18.470,,2835,2017-02-17T23:46:18.470,,,,,5556.0,2820.0,2,0,,,,34.66,14.21,11.54,0.0,0.0,12.0,Artificial intelligence proponents today are focused on the problem of computability the ability to solve complex problems fast It is my belief that any amount of success in this direction will not lead to human general intelligence although it certainly will outperform humans in certain domains Instead efforts should be toward a study of what neurological events cause sensation the experience of qualia Of course this is the hard problem of philosophy but I believe it is the unique key to general intelligence and its capabilities Reverse engineering and also testable theories should be advanced toward this end
,,"<p><strong>Ya Sure it is possible</strong>. It wont be efficient as human speech though (At least not yet). It all depends on how you use your data. If you use your data efficient enough, then you could be the one creating an AI which closely resembles human speech. Your idea is good. You would need help from a GPU for processor all that complex text processing. I hope I was at least a little of help. <strong>I unfortunately don't know about any open source projects</strong>.</p>
",,0,2017-02-18T04:01:40.217,,2836,2017-02-18T12:31:57.080,2017-02-18T12:31:57.080,,5557.0,,5557.0,2834.0,2,0,,,,86.2,6.12,7.92,0.0,0.0,12.0,Ya Sure it is possible It wont be efficient as human speech though At least not yet It all depends on how you use your data If you use your data efficient enough then you could be the one creating an AI which closely resembles human speech Your idea is good You would need help from a GPU for processor all that complex text processing I hope I was at least a little of help I unfortunately dont know about any open source projects
2859.0,1.0,"<p>If the nervous system is wired up such that there are no well defined layers, how does this compare to a neatly stacked artificial net? If between my sensory and motor side I had a neatly designed SNN with well defined layers, how would I see the world?
I get that there are some evolutionary advantages to a system where information can sometimes take a shortcut from sensory cell to motor cell (reflex action) bypassing brain processing but for arguments sake let's talk only about intelligence.</p>
",,0,2017-02-18T04:12:39.273,,2837,2017-02-21T09:50:42.437,,,,,5558.0,,1,3,<neural-networks><biology>,Would a neuromorphic SNN of the same complexity as the human nervous system be 'smarter'?,38.0,50.8,11.09,9.65,0.0,0.0,8.0,If the nervous system is wired up such that there are no well defined layers how does this compare to a neatly stacked artificial net If between my sensory and motor side I had a neatly designed SNN with well defined layers how would I see the world I get that there are some evolutionary advantages to a system where information can sometimes take a shortcut from sensory cell to motor cell reflex action bypassing brain processing but for arguments sake lets talk only about intelligence
,,"<p><strong>It is possible</strong></p>

<p>The <a href=""https://en.wikipedia.org/wiki/Recurrent_neural_network"" rel=""nofollow noreferrer"">Recurrent Neural Network architectures</a> help in building efficient NLP algorithms, which can identify semantics and their relations across long pieces of text.</p>

<p>With very minor tuning, they can be made generative too. So, <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow noreferrer"">here is an excellent article on RNNs</a> which I highly recommend, which also talks about how an RNN was trained on Shakespere's texts and wrote one itself.</p>
",,2,2017-02-18T08:38:45.353,,2838,2017-02-18T08:38:45.353,,,,,101.0,2834.0,2,1,,,,58.32,12.42,11.11,0.0,0.0,8.0,It is possible The Recurrent Neural Network architectures help in building efficient NLP algorithms which can identify semantics and their relations across long pieces of text With very minor tuning they can be made generative too So here is an excellent article on RNNs which I highly recommend which also talks about how an RNN was trained on Shakesperes texts and wrote one itself
,,"<p>I think I found the solution. When in <code>PR(W)</code>, <code>D=∅</code>, the weight is:</p>

<p><code>
b[i] = 0          for { i | w[i]&lt;max(w) }, and
b[i] = 1.0/max(w) for { i | w[i]==max(w) }.
</code> </p>
",,0,2017-02-18T11:38:34.770,,2839,2017-02-18T16:20:47.210,2017-02-18T16:20:47.210,,5535.0,,5535.0,2833.0,2,0,,,,108.19,2.07,5.34,105.0,0.0,4.0,I think I found the solution When in the weight is
2844.0,2.0,"<p>From <strong>Artificial Intelligence: A Modern Approach</strong>, Third Edition...</p>

<p>In Chapter 26, the textbook discussed ""technological singularity"". It quotes I.J. Good, who wrote in 1965:</p>

<blockquote>
  <p>Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultrainteltigent machine could design even better machines; there would then unquestionably be an ""intelligence explosion,"" and the intelligence of man would be left far behind. Thus the first ultraintelligeat machine is the <em>last</em> invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.</p>
</blockquote>

<p>Later on in the textbook, you have this question:</p>

<blockquote>
  <p>26.7 - I. J. Good claims that intelligence is the most important quality, and that building ultraintelligent machines will change everything. A sentient cheetah counters that ""Actually speed is more important; if we could build ultrafast machines, that would change everything"" and a sentient elephant claims ""You're both wrong; what we need is ultrastrong machines,"" What do you think of these arguments?</p>
</blockquote>

<p>It seems that the textbook question is an implicit argument against I.J. Good. Good may be treating intelligence as valuable, simply because man's strengths lies in that trait called ""intelligence"". But other traits could be equally valued instead (speed or strength) and sentient beings may speculate wildly about their preferred traits being ""maximized"" by some machine or another.</p>

<p>This makes me wonder whether a singularity could occur if we had built machines that were <em>not</em> maximizing intelligence, but instead maximizing some other trait (a machine that is always increasing its strength, or a machine that is always increasing its speed). These types of machines can be just as transformative - ultrafast machines may solve problems quickly due to ""brute force"", and ultrastrong machines can use its raw power for a variety of physical tasks. Perhaps a ultra-X machine can't build another ultra-X machine (as I.J. Good treated the design of machines as an intellectual activity), but a continually self-improving machine would still leave its creators far behind and force its creators to be dependent on it.</p>

<p>So, let's repeat my question -- Are technological singularities limited to ultra-intelligences? Or technological singularities be caused by machines that are not ""strong AI"" but are still ""ultra""-optimizers?</p>
",,1,2017-02-18T15:10:58.353,1.0,2841,2017-02-19T22:08:16.117,2017-02-18T16:01:36.050,,181.0,,181.0,,1,6,<definitions><ultraintelligent-machine><singularity>,"Can a technological singularity only occur with ultra-intelligent machines, or can other type of maximizers cause technological singularities?",79.0,41.29,14.68,9.29,0.0,0.0,85.0,From Artificial Intelligence A Modern Approach Third Edition In Chapter 26 the textbook discussed technological singularity It quotes IJ Good who wrote in 1965 Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever Since the design of machines is one of these intellectual activities an ultrainteltigent machine could design even better machines there would then unquestionably be an intelligence explosion and the intelligence of man would be left far behind Thus the first ultraintelligeat machine is the last invention that man need ever make provided that the machine is docile enough to tell us how to keep it under control Later on in the textbook you have this question 267 I J Good claims that intelligence is the most important quality and that building ultraintelligent machines will change everything A sentient cheetah counters that Actually speed is more important if we could build ultrafast machines that would change everything and a sentient elephant claims Youre both wrong what we need is ultrastrong machines What do you think of these arguments It seems that the textbook question is an implicit argument against IJ Good Good may be treating intelligence as valuable simply because mans strengths lies in that trait called intelligence But other traits could be equally valued instead speed or strength and sentient beings may speculate wildly about their preferred traits being maximized by some machine or another This makes me wonder whether a singularity could occur if we had built machines that were not maximizing intelligence but instead maximizing some other trait a machine that is always increasing its strength or a machine that is always increasing its speed These types of machines can be just as transformative ultrafast machines may solve problems quickly due to brute force and ultrastrong machines can use its raw power for a variety of physical tasks Perhaps a ultraX machine cant build another ultraX machine as IJ Good treated the design of machines as an intellectual activity but a continually selfimproving machine would still leave its creators far behind and force its creators to be dependent on it So lets repeat my question Are technological singularities limited to ultraintelligences Or technological singularities be caused by machines that are not strong AI but are still ultraoptimizers
,1.0,"<p>Is Programming Collective Intelligence by Toby Segaran a good book to enter in the AI and neural networks world for a novice? </p>
",,2,2017-02-19T00:46:36.947,1.0,2842,2017-02-19T03:27:05.573,,,,,5549.0,,1,0,<neural-networks><ai-community>,Programming Collective Intelligence,50.0,49.15,10.15,11.19,0.0,0.0,1.0,Is Programming Collective Intelligence by Toby Segaran a good book to enter in the AI and neural networks world for a novice
,,"<p>I read the second chapter in that book for my project on recommendation systems. <strong>It is an awesome book and it starts by addressing concepts from the basics</strong>( At least chapter 2 was like that). Book explains concepts with python. <strong>You will need a bit of knowledge on python basics</strong> . The book transits slowly from beginner to an intermediate level. Hope this answers your question. I would <strong>recommend</strong> it.</p>
",,0,2017-02-19T03:27:05.573,,2843,2017-02-19T03:27:05.573,,,,,5557.0,2842.0,2,0,,,,78.35,8.96,9.16,0.0,0.0,9.0,I read the second chapter in that book for my project on recommendation systems It is an awesome book and it starts by addressing concepts from the basics At least chapter 2 was like that Book explains concepts with python You will need a bit of knowledge on python basics The book transits slowly from beginner to an intermediate level Hope this answers your question I would recommend it
,,"<p>That would be a no for speed or strength, if you have a super strong entity but it cannot research new materials, it will be quickly limited, same thing for speed, Basically, you need something out of their field to improve them, which makes a runaway improvement impossible. </p>

<p>Though, we already have super strong and super fast machines, those are cars, trucks, hydraulic presses, industrial exoskeletons etc... But, even though we can build better ones through the use of the old ones, we still need to research stuff that can't be improved by old ones.</p>

<p>What we need for a singularity is a field where an improvement in it makes further improvement easier. And I don't know a field where this doesn't involve intelligence. 
If there is one, that may be possible to have a non intelligence driven singularity there.</p>
",,0,2017-02-19T07:32:06.433,,2844,2017-02-19T07:32:06.433,,,,,4152.0,2841.0,2,4,,,,56.29,10.97,8.63,0.0,0.0,25.0,That would be a no for speed or strength if you have a super strong entity but it cannot research new materials it will be quickly limited same thing for speed Basically you need something out of their field to improve them which makes a runaway improvement impossible Though we already have super strong and super fast machines those are cars trucks hydraulic presses industrial exoskeletons etc But even though we can build better ones through the use of the old ones we still need to research stuff that cant be improved by old ones What we need for a singularity is a field where an improvement in it makes further improvement easier And I dont know a field where this doesnt involve intelligence If there is one that may be possible to have a non intelligence driven singularity there
,,"<p>Why dont you try android Phones with Tensorflow <a href=""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android"" rel=""nofollow noreferrer"">TensorFlow Android Camera Demo</a> </p>

<p>You can build a simple image or text classification neural network to demonstrate AI. </p>
",,0,2017-02-19T16:19:38.423,,2845,2017-02-19T16:19:38.423,,,,,3250.0,2772.0,2,0,,,,36.63,14.05,12.82,0.0,0.0,1.0,Why dont you try android Phones with Tensorflow TensorFlow Android Camera Demo You can build a simple image or text classification neural network to demonstrate AI
,2.0,"<p>AI is developing at a rapid pace and is becoming very sophisticated. One aspect will include the methods of interaction between AI and humans. </p>

<p>Currently the interaction is an elementary interaction of voice and visual text or images.</p>

<p>Is there current research on more elaborate multisensory interactions?</p>
",,0,2017-02-19T18:49:37.057,,2846,2017-03-10T09:13:02.953,,,,,5583.0,,1,2,<deep-learning><ai-design><conv-neural-network><strong-ai><nlp>,Is there a central focus on the communication methods between AI and humans?,30.0,34.12,13.38,10.94,0.0,0.0,4.0,AI is developing at a rapid pace and is becoming very sophisticated One aspect will include the methods of interaction between AI and humans Currently the interaction is an elementary interaction of voice and visual text or images Is there current research on more elaborate multisensory interactions
,,"<p>""Monte Carlo"" seems to be the best method currently for algorithmic creativity.  (i.e. the machine makes random choices and sees if they lead to anything useful.)</p>

<p>While it appears obvious that creative connections formed out of <em>understanding</em> are superior to those which are random, if the machine is fast enough, it should be able to win out by pure ""brute force"".</p>

<p>i.e. Evolution, prior to human guidance, has not been been based on intelligence.*  Rather, evolution has been based on random mutations that are either beneficial or detrimental. </p>

<hr>

<p>*The caveat is that humans creating algorithms and altering genes (either in a lab or through animal husbandry and horticulture) can be said to comprise a new form of evolution that is actually rooted in human intelligence and desire.</p>
",,0,2017-02-19T22:01:19.157,,2847,2017-02-19T22:08:16.117,2017-02-19T22:08:16.117,,1671.0,,1671.0,2841.0,2,0,,,,49.96,12.47,10.66,0.0,0.0,25.0,Monte Carlo seems to be the best method currently for algorithmic creativity ie the machine makes random choices and sees if they lead to anything useful While it appears obvious that creative connections formed out of understanding are superior to those which are random if the machine is fast enough it should be able to win out by pure brute force ie Evolution prior to human guidance has not been been based on intelligence Rather evolution has been based on random mutations that are either beneficial or detrimental The caveat is that humans creating algorithms and altering genes either in a lab or through animal husbandry and horticulture can be said to comprise a new form of evolution that is actually rooted in human intelligence and desire
,,"<p>Probably these days it's still under the umbrella of ""man-machine interaction"" in CS, i.e. there is a (sub-) field for interactions between humans and machines in CS, but I am not aware that it has split again to create a sub-sub-field for AI/man interactions. </p>
",,2,2017-02-20T02:12:22.580,,2848,2017-02-20T02:12:22.580,,,,,5589.0,2846.0,2,0,,,,57.61,11.31,9.75,0.0,0.0,15.0,Probably these days its still under the umbrella of manmachine interaction in CS ie there is a sub field for interactions between humans and machines in CS but I am not aware that it has split again to create a subsubfield for AIman interactions
,,"<p>It depends whether you include learning to produce sound waves in the task (I'm taking ""how to speak"" to be different from ""how to write"" in your question).</p>

<p>If the task is to learn a language, you can certainly learn from written texts, and try a generative approach (usually, after a few sentences, humans can be disappointed though). </p>

<p>If the task includes generating a voice response that is also learnt, then it had better hear some spoken language too. Of course, you can bypass the task of learning how to generate sound by plugging a standard text to voice module after your generator. </p>
",,0,2017-02-20T02:15:49.450,,2849,2017-02-20T02:15:49.450,,,,,5589.0,2834.0,2,0,,,,62.21,10.51,9.82,0.0,0.0,19.0,It depends whether you include learning to produce sound waves in the task Im taking how to speak to be different from how to write in your question If the task is to learn a language you can certainly learn from written texts and try a generative approach usually after a few sentences humans can be disappointed though If the task includes generating a voice response that is also learnt then it had better hear some spoken language too Of course you can bypass the task of learning how to generate sound by plugging a standard text to voice module after your generator
,,"<p>IMHO the first hurdle is <strong>scale</strong>: even Google's largest DNN doesn't come close to the scale of the brain, and by a factor of several orders of magnitude... </p>
",,0,2017-02-20T02:20:25.370,,2850,2017-02-20T02:20:25.370,,,,,5589.0,2820.0,2,0,,,,59.98,9.12,11.23,0.0,0.0,7.0,IMHO the first hurdle is scale even Googles largest DNN doesnt come close to the scale of the brain and by a factor of several orders of magnitude
,0.0,"<p>I tried the below Matlab code to build SOM using <code>selforgmap</code>.</p>

<pre><code>close all, clear all, clc, format compact

% number of samples of each cluster
K = 200;
% offset of classes
q = 1.1;
% define 4 clusters of input data
P = [rand(1,K)-q rand(1,K)+q rand(1,K)+q rand(1,K)-q;
     rand(1,K)+q rand(1,K)+q rand(1,K)-q rand(1,K)-q];
% plot clusters
plot(P(1,:),P(2,:),'g.')
hold on
grid on
% SOM parameters
dimensions   = [10 10];
coverSteps   = 100;
initNeighbor = 4;
topologyFcn  = 'hextop';
distanceFcn  = 'linkdist';

% define net
net2 = selforgmap(dimensions,coverSteps,initNeighbor,topologyFcn,distanceFcn);

% train
[net2,Y] = train(net2,P);

% plot input data and SOM weight positions
plotsompos(net2,P);
grid on

% plot SOM neighbor distances
plotsomnd(net2)

% plot for each SOM neuron the number of input vectors that it classifies
figure
plotsomhits(net2,P)
</code></pre>

<p>You find the result and more details <a href=""http://lab.fs.uni-lj.si/lasin/wp/IMIT_files/neural/nn07_som/"" rel=""nofollow noreferrer"">here</a>.</p>

<p>I need to segment grayscale image. However, I cannot set the <code>selforgmap</code> input correctly.</p>

<p>How can modify the below code to segment any grayscale image?</p>
",,5,2017-02-20T05:27:22.530,1.0,2851,2017-02-20T05:27:22.530,,,,,5591.0,,1,0,<neural-networks>,Image segmentation using SOM,24.0,71.41,7.04,9.69,819.0,0.0,6.0,I tried the below Matlab code to build SOM using You find the result and more details here I need to segment grayscale image However I cannot set the input correctly How can modify the below code to segment any grayscale image
,,"<p>Yes, It is possible. Also, if you are from development background you can use JAVA Sphinx, some AI tools and API's. If you are from testing background you can use JAWS which actually reads the readable properties of HTML tags. Nowadays, there are some pdf readers, document reader softwares available you can have a look at them. You can search for Sphinx related projects but I don't know about the other projects.</p>

<p>Please vote and mark the solution if useful.</p>

<p><strong>Thanks!</strong></p>
",,0,2017-02-20T08:42:24.800,,2852,2017-02-20T08:42:24.800,,,,,5594.0,2834.0,2,0,,,,74.69,9.91,8.79,0.0,0.0,14.0,Yes It is possible Also if you are from development background you can use JAVA Sphinx some AI tools and APIs If you are from testing background you can use JAWS which actually reads the readable properties of HTML tags Nowadays there are some pdf readers document reader softwares available you can have a look at them You can search for Sphinx related projects but I dont know about the other projects Please vote and mark the solution if useful Thanks
,0.0,"<p>I am trying to develop an Editor that can be based on <code>Notepad</code>. The only purpose for this development is I want to use this for my coding suggestions and possibly the next input parameter that I am going to write. I've seen <code>Notepad++</code>, <code>EditPlus</code> etc. and what I think is that this can be definitely achieved. What are the best tools or API's I can use? Any suggestions?</p>

<p><strong>Thanks in Advance!</strong></p>
",2017-02-20T19:13:08.223,1,2017-02-20T09:00:42.567,1.0,2853,2017-02-20T09:00:42.567,,,,,5594.0,,1,1,<neural-networks><machine-learning><ai-design><algorithm><world-knowledge>,"To create ""Edit tool"" or editor based on Notepad",14.0,76.72,5.9,8.1,26.0,0.0,10.0,I am trying to develop an Editor that can be based on The only purpose for this development is I want to use this for my coding suggestions and possibly the next input parameter that I am going to write Ive seen etc and what I think is that this can be definitely achieved What are the best tools or APIs I can use Any suggestions Thanks in Advance
,0.0,"<p>Is it possible to run SSD or YOLO object detection on raspberry pi 3 for live object detection (2/4frames x second)?
I've tried this <a href=""https://github.com/rykov8/ssd_keras"" rel=""nofollow noreferrer"">SSD</a> implementation but it takes 14 s per frame.
Is there anything I could do to speed up ?</p>
",,0,2017-02-20T10:08:09.997,,2854,2017-02-20T10:08:09.997,,,,,2320.0,,1,0,<deep-learning><deep-network><image-recognition><object-recognition>,SSD or YOLO on arm,28.0,82.65,7.13,8.09,0.0,0.0,7.0,Is it possible to run SSD or YOLO object detection on raspberry pi 3 for live object detection 24frames x second Ive tried this SSD implementation but it takes 14 s per frame Is there anything I could do to speed up
,,"<p>To start you could use one of the devices mentioned before, and after to make some more powerful and complicated experiments (and also a little bit more expansive) you could move to <a href=""http://rads.stackoverflow.com/amzn/click/B00L7AWOEC"" rel=""nofollow noreferrer"">Jetson TK1</a> which let you run heavier Neural Network (like CNN).</p>
",,0,2017-02-20T11:11:30.073,,2855,2017-02-20T11:11:30.073,,,,,2320.0,2772.0,2,0,,,,36.29,10.98,10.91,0.0,0.0,6.0,To start you could use one of the devices mentioned before and after to make some more powerful and complicated experiments and also a little bit more expansive you could move to Jetson TK1 which let you run heavier Neural Network like CNN
,,"<p>I have a suggestion for generating poetry:<br/><br/><br/><br/>
grab the <strong>major arcana of the tarot</strong><br/><br/>
grab a database of books on diverse literary topics, and group them by gestalt<br/><br/>
drop 4 or 5 random words building a sequence of the archetypes, and getting words that you associated with the cards<br/><br/>
grab expressions from the books you have in the database containing these expressions<br/><br/>
find a way to derive and link these expressions<br/><br/>
apply the procedure recursively on top of several poems until you have very large collections of poems or even books<br/><br/></p>
",,1,2017-02-20T19:36:41.100,,2857,2017-02-20T19:36:41.100,,,,,5603.0,2808.0,2,0,,,,-12.43,11.57,13.01,0.0,0.0,3.0,I have a suggestion for generating poetry grab the major arcana of the tarot grab a database of books on diverse literary topics and group them by gestalt drop 4 or 5 random words building a sequence of the archetypes and getting words that you associated with the cards grab expressions from the books you have in the database containing these expressions find a way to derive and link these expressions apply the procedure recursively on top of several poems until you have very large collections of poems or even books
,,"<p>""Life"" is a definition humans use to classify objects according to the types of behavior humans perceive as unique to living creatures.</p>

<p>Scientists and philosophers tend to define something as ""alive"" if it manifests some specific properties found in living organisms, such as self-replication, adaptation to the environment, homeostasis and capability to exploit matter and energy for its own existence and functioning. With that being said, there is no one accepted definition of life, and it is doubtful that such a definition is possible.</p>

<p>As to ALife, [1] states that ""it is common among researchers to distinguish between two types of approaches to artificial life: (1) the <strong>strong ALife</strong> approach, which postulates that virtual “creatures” on a computer screen can be considered to be alive if they fulfill the definition of life used by the researchers; and (2) the <strong>weak ALife</strong> approach, whereby computerized creatures displaying characteristics of living systems are only models used in research and are not really alive.""</p>

<p>While this is not solving the problem of definition of life, it might give more context on the subject in relation to ALife.</p>

<ul>
<li><strong>[1]</strong> E. Lamm and R. Unger, <em>Biological Computation</em>. Chapman &amp; Hall/CRC, 2011.</li>
</ul>
",,0,2017-02-20T23:22:10.217,,2858,2017-02-20T23:22:10.217,,,,,5533.0,2771.0,2,3,,,,38.15,13.7,10.89,0.0,0.0,40.0,Life is a definition humans use to classify objects according to the types of behavior humans perceive as unique to living creatures Scientists and philosophers tend to define something as alive if it manifests some specific properties found in living organisms such as selfreplication adaptation to the environment homeostasis and capability to exploit matter and energy for its own existence and functioning With that being said there is no one accepted definition of life and it is doubtful that such a definition is possible As to ALife 1 states that it is common among researchers to distinguish between two types of approaches to artificial life 1 the strong ALife approach which postulates that virtual “creatures” on a computer screen can be considered to be alive if they fulfill the definition of life used by the researchers and 2 the weak ALife approach whereby computerized creatures displaying characteristics of living systems are only models used in research and are not really alive While this is not solving the problem of definition of life it might give more context on the subject in relation to ALife 1 E Lamm and R Unger Biological Computation Chapman amp HallCRC 2011
,,"<p>The human nervous system is an <em>extremely</em> dynamic entity, having been formed by the processes of embryogenesis, mediated by various markers guiding neurons to grow to specific areas, and all of it laid down by millions of years of evolution. There are many different varieties of neurons in the human brain, indeed more so than pretty much any other animal on this planet.</p>

<p>It is hard to see that a <em>spiking neural network</em> (SNN) would be able to get all these details correct when we don't yet fully understand the biological analog.</p>

<p>I think for this to be successful, we need to understand much more about not only the variety of neurons in the brain, but details of the the embryogenic dynamics as well. And not just the neurons, but the glial cells as well.</p>

<p>Having said that, there is something to be said for using evolutionary approaches to resolve ""a solution"" that may work anyway. Taking this approach will require many more resources, but is perhaps doable. </p>

<p>All in all, I would <strong>not</strong> expect a naive attempt of the neuromorphic SNN to succeed. There is a <em>lot</em> of complexity involved in what the brain does, and it involves the glial cells to a large degree. Do we understand enough about the role of glial cells in the brain? We cannot ignore them. Are they only performing ""housekeeping"" operations, like, for example, the uptake of ""spent"" neurotransmitters? Or are they doing more, taking part in the computational and / or memory aspects of the brain?</p>

<p>There is much research in this and other pertinent areas that one should look into. And expect a lot of surprises.</p>
",,0,2017-02-21T09:50:42.437,,2859,2017-02-21T09:50:42.437,,,,,4185.0,2837.0,2,1,,,,61.36,10.1,9.42,0.0,0.0,40.0,The human nervous system is an extremely dynamic entity having been formed by the processes of embryogenesis mediated by various markers guiding neurons to grow to specific areas and all of it laid down by millions of years of evolution There are many different varieties of neurons in the human brain indeed more so than pretty much any other animal on this planet It is hard to see that a spiking neural network SNN would be able to get all these details correct when we dont yet fully understand the biological analog I think for this to be successful we need to understand much more about not only the variety of neurons in the brain but details of the the embryogenic dynamics as well And not just the neurons but the glial cells as well Having said that there is something to be said for using evolutionary approaches to resolve a solution that may work anyway Taking this approach will require many more resources but is perhaps doable All in all I would not expect a naive attempt of the neuromorphic SNN to succeed There is a lot of complexity involved in what the brain does and it involves the glial cells to a large degree Do we understand enough about the role of glial cells in the brain We cannot ignore them Are they only performing housekeeping operations like for example the uptake of spent neurotransmitters Or are they doing more taking part in the computational and or memory aspects of the brain There is much research in this and other pertinent areas that one should look into And expect a lot of surprises
,,"<p>It is true that at first look, one could expect that classification between 101 category would be harder than classification between 51 category. However, many aspects play a role when it comes to action recognition applications.</p>

<p>For instance, the HMDB51 contains several categories about different facial mouvements like smiling, laughing, chewing,... and several other categories like eating, drinking. Such categories are not present in the list of categories of the data set UCF101 and are obviously among the most difficult categories to deal with. It also claims to have some bad quality challenging videos.</p>

<p>It is hard to predict in advance how difficult a data set will be to classify. We can imagine that when the state-of-the-art reaches accuracy beyond 90%, it is time to build a data set that makes these methods fail to look for even more robust solutions. I don't know these data sets well, but the videos are present most probably more variability in terms of viewpoint, camera motions, illumination changes, image quality,... in the most difficult to classify data set.</p>

<p>Also, check on <a href=""http://crcv.ucf.edu/data/UCF101.php"" rel=""nofollow noreferrer"">this page</a>, the results announced for the UCF101 data set. I don't know where you found you accuracy value because the official website announces less than 43.9%. Some publications do not use the complete data set and use only part of it to show the performance of an approach they designed. </p>

<p>Finally the <a href=""http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/#overview"" rel=""nofollow noreferrer"">official website of the HMDB51 data set</a> reports the following:
""The UCF group has also been collecting action datasets, mostly from YouTube. There are UCF Sports featuring 9 types of sports and a total of 182 clips, UCF YouTube containing 11 action classes, and UCF50 contains 50 actions classes. We will show in the paper that videos from YouTube could be very biased by low-level features, meaning low-level features (i.e., color and gist) are more discriminative than mid-level fears (i.e., motion and shape).""
This could also explain why better results can be achieved...</p>
",,0,2017-02-21T20:39:21.507,,2860,2017-02-21T20:39:21.507,,,,,3576.0,2803.0,2,0,,,,54.12,12.06,9.46,0.0,0.0,66.0,It is true that at first look one could expect that classification between 101 category would be harder than classification between 51 category However many aspects play a role when it comes to action recognition applications For instance the HMDB51 contains several categories about different facial mouvements like smiling laughing chewing and several other categories like eating drinking Such categories are not present in the list of categories of the data set UCF101 and are obviously among the most difficult categories to deal with It also claims to have some bad quality challenging videos It is hard to predict in advance how difficult a data set will be to classify We can imagine that when the stateoftheart reaches accuracy beyond 90 it is time to build a data set that makes these methods fail to look for even more robust solutions I dont know these data sets well but the videos are present most probably more variability in terms of viewpoint camera motions illumination changes image quality in the most difficult to classify data set Also check on this page the results announced for the UCF101 data set I dont know where you found you accuracy value because the official website announces less than 439 Some publications do not use the complete data set and use only part of it to show the performance of an approach they designed Finally the official website of the HMDB51 data set reports the following The UCF group has also been collecting action datasets mostly from YouTube There are UCF Sports featuring 9 types of sports and a total of 182 clips UCF YouTube containing 11 action classes and UCF50 contains 50 actions classes We will show in the paper that videos from YouTube could be very biased by lowlevel features meaning lowlevel features ie color and gist are more discriminative than midlevel fears ie motion and shape This could also explain why better results can be achieved
,1.0,"<h3>TL;DR</h3>

<p>If we buy into the idea visual cortex functions like a convolutional neural network, then there's a problem makes me scratch my head: <strong>how does brain force weight sharing as in convolutional network</strong>?</p>

<h3>Okay, explain more</h3>

<p>Obviously, there's no way for left visual cortex to directly tell the right visual cortex ""hey, I've learned some new stuff, copy me!!"" (or is there?). Then, if the learned features are diverse across visual field, how does it keep the translation invariance property?</p>

<p>For example, you already know English characters, you can recognize them with your both eyes. Now that you wanna learn some Chinese and you  excercise your right brain at the same time, so you closed your right eye and memorized a new character. After that, certainly you can recognize the new character with solely your right eye. But <em>why</em>?</p>

<p>The answer may be, the object / higher-level feature detection happens in a higher level cortex, which receives entire visual field. There may be also some transfer/one-shot learning taking place. But then, if a newborn baby trying to learn the low level visual features, he/she would definitely face the weight sharing problems.</p>

<p>A possible explanation would be, the baby will be exposed to very large amount of data and eventually learn invariance. Large amount of data reduces overfitting but doesn't guarantee <em>deterministic convergence</em>. If we train the same CNN model on the same dataset, however using <em>different random generator seeds</em>, there's a big chance the same feature detector will appear in a different channel, or a difference set of features appear as linear recombination.</p>

<p>If there's no way to share weights, the brain would learn <strong>a lot different feature combinations across the entire visual field, how does it still able to consistently solve visual invariance problem?</strong></p>
",,0,2017-02-22T05:07:17.853,,2861,2017-02-22T10:11:26.350,,,,,3189.0,,1,1,<neural-networks><conv-neural-network>,How does visual cortex share convolution weight,32.0,50.06,12.65,9.5,0.0,0.0,55.0,TLDR If we buy into the idea visual cortex functions like a convolutional neural network then theres a problem makes me scratch my head how does brain force weight sharing as in convolutional network Okay explain more Obviously theres no way for left visual cortex to directly tell the right visual cortex hey Ive learned some new stuff copy me or is there Then if the learned features are diverse across visual field how does it keep the translation invariance property For example you already know English characters you can recognize them with your both eyes Now that you wanna learn some Chinese and you excercise your right brain at the same time so you closed your right eye and memorized a new character After that certainly you can recognize the new character with solely your right eye But why The answer may be the object higherlevel feature detection happens in a higher level cortex which receives entire visual field There may be also some transferoneshot learning taking place But then if a newborn baby trying to learn the low level visual features heshe would definitely face the weight sharing problems A possible explanation would be the baby will be exposed to very large amount of data and eventually learn invariance Large amount of data reduces overfitting but doesnt guarantee deterministic convergence If we train the same CNN model on the same dataset however using different random generator seeds theres a big chance the same feature detector will appear in a different channel or a difference set of features appear as linear recombination If theres no way to share weights the brain would learn a lot different feature combinations across the entire visual field how does it still able to consistently solve visual invariance problem
,,"<p>In the human brain every common pattern is recognised by a multitude of pattern recognisers (be they neurons or microcolumns). That's pretty obvious because neurons die all the time, but we don't wake up and have forgotten how to recognise the letter 'A'. In fact we have to take out rather big chunks of the neocortex until functionality degrades, which is why Alzheimer's patients show symptoms only when the brain is already visibly messed up. </p>

<p>Translational invariance within a level of the hierarchy has to be learned. Basically you need filters for the same edge all over your V1. An object moving across your field of vision only becomes invariant in it's representation on a higher level. Unfortunately we don't magically learn a representation that we can easily turn and twist, our ability to recognise objects from an unusual angle degrades with how uncommon the angle is. </p>

<p>A nice thought experiment to illustrate the point: Imagine a cube sitting in front of you. Now pull one of the corners of the cube upwards until the cube is dangling in front of you, one corner pointing straight to the ceiling, the opposite corner pointing to the ground. Now indicate with your finger where the rest of the corner are. </p>

<p>If you are anything like me, this is really difficult. I think the first time I did't even realise I had to point out six corners! </p>

<p>Of course this is 3D and we might still have inbuilt 2D invariance, but it also turns out that faces that are turned upside down have to be processed much higher into the cortex to be recognised as faces and so on ... </p>

<p>Concerning the learning of a lot of different feature vectors across the field of vision: This might be prevented by the fact that the neocortex learns sequences of input by predicting (via depolarisation) the next pattern. So you might have the situation that a higher level tells the lower level that there is an object moving from left to right and the lower level will predict that the edge it is detecting at point y will reoccur at point y+x.</p>

<p>This setup differs from the training of NNs in two ways: The data is already translational and the translation is predicted, which facilitates learning the same features. Basically two pattern recogniser close to each other get pretty much the same pattern one after the other whenever something moves in a certain direction and the second occurrence is predicted, which means it will be more likely detected, which means it will be learned. (I don't want to dole out a lecture about the cortex, but the prediction is a big deal because it allows neurons to fire quicker which means they beat other neurons before they are laterally inhibited and only if they actually fire will they learn.)</p>

<p>As a disclaimer I want to add that this is just my current understanding of the issue and I'm pretty sure the actual explanation is more complicated. For example I've read that one of the layers of the cortex is important for translational invariance and this layer only exists in the levels close to the sensory input. It is conjectured that this layer (L4) doesn't do the sequence prediction stuff, so maybe having the same kind of input is enough or learning different feature vectors is not actually a problem. There is also the complicating issue that there is poorly understood interplay between the different layers and different levels of the cortex. I would recommend to ask a neuroscientist, except I don't think they figured this out yet. </p>
",,2,2017-02-22T10:11:26.350,,2862,2017-02-22T10:11:26.350,,,,,2227.0,2861.0,2,3,,,,53.44,10.57,9.08,0.0,0.0,63.0,In the human brain every common pattern is recognised by a multitude of pattern recognisers be they neurons or microcolumns Thats pretty obvious because neurons die all the time but we dont wake up and have forgotten how to recognise the letter A In fact we have to take out rather big chunks of the neocortex until functionality degrades which is why Alzheimers patients show symptoms only when the brain is already visibly messed up Translational invariance within a level of the hierarchy has to be learned Basically you need filters for the same edge all over your V1 An object moving across your field of vision only becomes invariant in its representation on a higher level Unfortunately we dont magically learn a representation that we can easily turn and twist our ability to recognise objects from an unusual angle degrades with how uncommon the angle is A nice thought experiment to illustrate the point Imagine a cube sitting in front of you Now pull one of the corners of the cube upwards until the cube is dangling in front of you one corner pointing straight to the ceiling the opposite corner pointing to the ground Now indicate with your finger where the rest of the corner are If you are anything like me this is really difficult I think the first time I didt even realise I had to point out six corners Of course this is 3D and we might still have inbuilt 2D invariance but it also turns out that faces that are turned upside down have to be processed much higher into the cortex to be recognised as faces and so on Concerning the learning of a lot of different feature vectors across the field of vision This might be prevented by the fact that the neocortex learns sequences of input by predicting via depolarisation the next pattern So you might have the situation that a higher level tells the lower level that there is an object moving from left to right and the lower level will predict that the edge it is detecting at point y will reoccur at point yx This setup differs from the training of NNs in two ways The data is already translational and the translation is predicted which facilitates learning the same features Basically two pattern recogniser close to each other get pretty much the same pattern one after the other whenever something moves in a certain direction and the second occurrence is predicted which means it will be more likely detected which means it will be learned I dont want to dole out a lecture about the cortex but the prediction is a big deal because it allows neurons to fire quicker which means they beat other neurons before they are laterally inhibited and only if they actually fire will they learn As a disclaimer I want to add that this is just my current understanding of the issue and Im pretty sure the actual explanation is more complicated For example Ive read that one of the layers of the cortex is important for translational invariance and this layer only exists in the levels close to the sensory input It is conjectured that this layer L4 doesnt do the sequence prediction stuff so maybe having the same kind of input is enough or learning different feature vectors is not actually a problem There is also the complicating issue that there is poorly understood interplay between the different layers and different levels of the cortex I would recommend to ask a neuroscientist except I dont think they figured this out yet
,0.0,"<p>I have to translate the following English sentences into First-Order Logic without using quantifiers:</p>

<pre><code>1. Everyone on flight 815 has a story.
2. No one knows what is inside the hatch.
3. Someone on the island isn’t on the flight manifest.
</code></pre>

<p>I have tried it, but can't translate without using ∀ and ∃:</p>

<pre><code>1. ∀x fight815(x) → story(x)
2. ∀x ⌐(knows(x) → inside hatch(x)) // not sure about this
OR
¬ ∃x Knows(x, inside hatch)
3. ∃x island(x) Λ ⌐(flight manifest(x))
</code></pre>

<p>Is it possible to do it. If not, why?</p>

<p>Refer chapter 8 of Artificial Intelligence: A Modern Approach (3rd edition). Stuart Russell and Peter Norvig, Prentice Hall (2010)</p>
",,7,2017-02-22T16:45:08.400,1.0,2863,2017-02-22T16:45:08.400,,,,,5643.0,,1,2,<knowledge-representation><world-knowledge><logic>,Translate English Sentences into First-Order Logic without quantifiers,53.0,66.23,11.88,10.74,296.0,0.0,15.0,I have to translate the following English sentences into FirstOrder Logic without using quantifiers I have tried it but cant translate without using ∀ and ∃ Is it possible to do it If not why Refer chapter 8 of Artificial Intelligence A Modern Approach 3rd edition Stuart Russell and Peter Norvig Prentice Hall 2010
,1.0,"<p>The Wikipedia states that:</p>

<blockquote>
  <p>""An evaluation function, also known as a heuristic evaluation function or static evaluation function, is a function used by game-playing programs to estimate the value or goodness of a position in the minimax and related algorithms.""</p>
  
  <p><sup><a href=""https://en.wikipedia.org/wiki/Evaluation_function"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Evaluation_function</a></sup></p>
</blockquote>

<p>Q: Is ""goodness"" an actual term in use in this context, or should it more properly be something like ""perceived optimality""?</p>

<p>I ask because, in Combinatorial Game Theory for instance, a lighthearted term such as ""loopy"" is preferred by some mathematicians (Demaine) over the more serious term ""cyclic"" (Fraenkel).</p>

<p>On a related note, is the use of ""position"" instead of ""node"" preferred here as an acknowledgement of the heuristic nature of Evaluation Functions?  (My understanding is that ""position"", ""node"" and ""game"" may all be interchangeable in certain contexts.) </p>
",,0,2017-02-22T20:52:28.450,,2864,2017-02-26T03:13:27.113,,,,,1671.0,,1,1,<terminology>,“Goodness” of a position in an Evaluation Function?,29.0,28.37,16.66,10.91,0.0,0.0,49.0,The Wikipedia states that An evaluation function also known as a heuristic evaluation function or static evaluation function is a function used by gameplaying programs to estimate the value or goodness of a position in the minimax and related algorithms httpsenwikipediaorgwikiEvaluationfunction Q Is goodness an actual term in use in this context or should it more properly be something like perceived optimality I ask because in Combinatorial Game Theory for instance a lighthearted term such as loopy is preferred by some mathematicians Demaine over the more serious term cyclic Fraenkel On a related note is the use of position instead of node preferred here as an acknowledgement of the heuristic nature of Evaluation Functions My understanding is that position node and game may all be interchangeable in certain contexts
,1.0,"<p>Here is formula for calculating cost value of single neuron:</p>

<p>C_x = (1/2) * || y - a ||^2</p>

<p>why there is 1/2?</p>
",2017-02-23T23:13:24.323,1,2017-02-23T14:41:58.773,,2865,2017-02-23T15:17:53.180,2017-02-23T14:53:49.263,,5661.0,,5661.0,,1,1,<neural-networks><machine-learning><artificial-neuron><math><neurons>,Why expression of cost function divides by 2?,30.0,86.03,8.36,7.9,0.0,0.0,15.0,Here is formula for calculating cost value of single neuron Cx 12 y a 2 why there is 12
,,"<p>To simplify the derivative, probably. Otherwise there will be constant 2 in it. </p>
",,1,2017-02-23T15:17:53.180,,2866,2017-02-23T15:17:53.180,,,,,5657.0,2865.0,2,2,,,,56.42,9.63,10.03,0.0,0.0,3.0,To simplify the derivative probably Otherwise there will be constant 2 in it
,1.0,"<p>I read that deep neural networks can be relatively easily fooled (<a href=""http://ai.stackexchange.com/questions/92/how-is-it-possible-that-deep-neural-networks-are-so-easily-fooled"">link</a>) to give high confidence in recognition of synthetic/artificial images that are completely (or at least mostly) out of the confidence subject.</p>

<p>Personally I dont really see a big problem with DNN giving high confidence to those synthetic/artificial images but I think giving high confidence for white noise (<a href=""http://ai.stackexchange.com/questions/1479/do-scientists-know-what-is-happening-inside-artificial-neural-networks"">link</a>) may be a problem since this is a truly natural phenomena that may the camera see in real world.</p>

<p>How much of a problem is white noise for the real world usage of a DNN? Can such false positives detected from plain noise be prevented somehow?</p>
",,0,2017-02-23T21:11:34.133,,2867,2017-02-24T11:04:39.957,,,,,113.0,,1,6,<neural-networks><deep-network><image-recognition><conv-neural-network><signal-processing>,Is deep neural network fooling a problem in real world?,82.0,44.58,11.44,8.67,0.0,0.0,12.0,I read that deep neural networks can be relatively easily fooled link to give high confidence in recognition of syntheticartificial images that are completely or at least mostly out of the confidence subject Personally I dont really see a big problem with DNN giving high confidence to those syntheticartificial images but I think giving high confidence for white noise link may be a problem since this is a truly natural phenomena that may the camera see in real world How much of a problem is white noise for the real world usage of a DNN Can such false positives detected from plain noise be prevented somehow
,1.0,"<p>I identify myself as a human agent. It is time to think about oncoming senior research and due to small experience in gamedev(as well as in AI field), some questions are raised. What are the most suitable approaches to implement real-time <em>simple</em> AI agent in an action game? I've heard something about cognitive architecture like ACT-R.</p>

<p>By design, entity's AI can have several mutually exclusive states. 
<a href=""https://i.stack.imgur.com/XMXKl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XMXKl.jpg"" alt=""enter image description here""></a></p>

<p>This is an existing AI of game, which has states, events and schedules. However, the code is complicated and not flexible. Also, it does not use any cognitive architecture, which I consider as a drawback.
<a href=""https://www.youtube.com/watch?v=9jO-P3kXlCI"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=9jO-P3kXlCI</a></p>

<p>Please, using your experience suggest any modern techniques, which can copy such behaviour as in image or video.</p>

<p>Thank you for your perception.</p>
",,3,2017-02-24T06:03:02.730,,2868,2017-02-28T20:16:52.163,2017-02-24T10:15:57.220,,5673.0,,5673.0,,1,3,<gaming><real-time>,Different useful approaches of implementing real-time AI?,65.0,50.33,12.98,10.57,0.0,0.0,34.0,I identify myself as a human agent It is time to think about oncoming senior research and due to small experience in gamedevas well as in AI field some questions are raised What are the most suitable approaches to implement realtime simple AI agent in an action game Ive heard something about cognitive architecture like ACTR By design entitys AI can have several mutually exclusive states This is an existing AI of game which has states events and schedules However the code is complicated and not flexible Also it does not use any cognitive architecture which I consider as a drawback httpswwwyoutubecomwatchv9jOP3kXlCI Please using your experience suggest any modern techniques which can copy such behaviour as in image or video Thank you for your perception
,,"<p>The white noise that fools DNNs isn't really white noise. It has been altered in the same way as the synthetic misclassified pictures have been altered. You have to change many input pixels in exactly such a way, that these little changes aren't perceptible, but propagated through the network add up to a misclassification. This is not going to happen by chance. </p>
",,1,2017-02-24T11:04:39.957,,2869,2017-02-24T11:04:39.957,,,,,2227.0,2867.0,2,8,,,,64.2,10.67,8.99,0.0,0.0,8.0,The white noise that fools DNNs isnt really white noise It has been altered in the same way as the synthetic misclassified pictures have been altered You have to change many input pixels in exactly such a way that these little changes arent perceptible but propagated through the network add up to a misclassification This is not going to happen by chance
,0.0,"<p>This question has come from my experiment of building a cnn based tic-tac-toe game that I'm using as a beginner machine learning project. The game works purely on policy networks, more specifically -</p>

<ol>
<li>During training, at the end of each game, it trains itself on the moves the winner/drawer made for each board position. That is, its training data consists of board positions and the moves made by the winning player on each position.</li>
<li>While playing, it predicts its own moves solely based on that training (that is, it predicts what move would a winning player make with the current board). It doesn't use any type of search or value networks.</li>
</ol>

<p>I'm seeing that if I train it against a player that predicts the perfect move (using a recursive search) every time, the AI gets good at drawing about 50% games. But if I train it against a player that makes random moves, it doesn't get better at all.</p>

<p>Wouldn't one expect it to learn well (even if slower) regardless of the level of its opponent? Since each game ends in a draw or win for one player, shouldn't it be able to extract features for the winning/drawing strategies even when learning from random players? Or does this behavior mean that the model is not optimal?</p>
",,2,2017-02-25T07:08:22.783,,2870,2017-02-25T07:47:11.087,2017-02-25T07:47:11.087,,1522.0,,1522.0,,1,2,<conv-neural-network>,Can a purely policy convolution neural network based game learn to play better than its opponents?,8.0,66.57,9.75,8.96,0.0,0.0,37.0,This question has come from my experiment of building a cnn based tictactoe game that Im using as a beginner machine learning project The game works purely on policy networks more specifically During training at the end of each game it trains itself on the moves the winnerdrawer made for each board position That is its training data consists of board positions and the moves made by the winning player on each position While playing it predicts its own moves solely based on that training that is it predicts what move would a winning player make with the current board It doesnt use any type of search or value networks Im seeing that if I train it against a player that predicts the perfect move using a recursive search every time the AI gets good at drawing about 50 games But if I train it against a player that makes random moves it doesnt get better at all Wouldnt one expect it to learn well even if slower regardless of the level of its opponent Since each game ends in a draw or win for one player shouldnt it be able to extract features for the winningdrawing strategies even when learning from random players Or does this behavior mean that the model is not optimal
,2.0,"<p>This mostly refers to human-like or chatbot AI, but could maybe be used in other applications (math or something?). </p>

<p>Basically, it occurred to me, that when I'm thinking or speaking, there is a constant feedback loop, in which I am formulating which words to use next, which sentences to form, and which concepts to explore, based on my most recent statements and the flow of the dialogue or monologue. I'm not just responding to outside stimulus but also to myself. In other words, I am usually maintaining a train of thought.</p>

<p>Can AI be made capable of this? If so, has it been demonstrated? And to what extent? While typing this, I discovered the term ""thought vectors"", and I think it might be related. </p>

<p>If I read correctly, thought vectors have something to do with allowing AI to store or identify the relationships between different concepts; and if I had to guess, I'd say that if an AI lacks a strong understanding of the relationships between concepts, then it would be impossible for it to maintain a coherent train of thought. Would that be a correct assumption?</p>

<p>(ps. in my limited experience with AI chatbots, they seem to be either completely scripted, or otherwise random and often incoherent, which is what leads me to believe that they do not maintain a train of thought)</p>
",,0,2017-02-25T11:55:59.480,,2871,2017-02-28T20:03:56.887,,,,,5698.0,,1,6,<ai-design><chat-bots><thought-vectors>,Can an AI be made to maintain a train of thought?,85.0,67.69,10.21,9.23,0.0,0.0,41.0,This mostly refers to humanlike or chatbot AI but could maybe be used in other applications math or something Basically it occurred to me that when Im thinking or speaking there is a constant feedback loop in which I am formulating which words to use next which sentences to form and which concepts to explore based on my most recent statements and the flow of the dialogue or monologue Im not just responding to outside stimulus but also to myself In other words I am usually maintaining a train of thought Can AI be made capable of this If so has it been demonstrated And to what extent While typing this I discovered the term thought vectors and I think it might be related If I read correctly thought vectors have something to do with allowing AI to store or identify the relationships between different concepts and if I had to guess Id say that if an AI lacks a strong understanding of the relationships between concepts then it would be impossible for it to maintain a coherent train of thought Would that be a correct assumption ps in my limited experience with AI chatbots they seem to be either completely scripted or otherwise random and often incoherent which is what leads me to believe that they do not maintain a train of thought
2877.0,1.0,"<p>At first, I had this question in mind ""Can robots develop suffering ?"". Because suffering is important for human beings. Imagine that you are running the wrong way damaging your heel. Without pain, you will continue to harm it. Same for robots. But then I told myself ""wait a second. It already exists. It is the errors and warnings that shows up"". We can say it has the similar purpose as suffering. However, I felt something missing. We feel pain. The errors and bugs are just data. Let's say a robot can use machine learning and genetic programming to evolve. Can it learn to feel suffering ? And not just know it as mere information.</p>
",,3,2017-02-25T14:57:25.610,2.0,2872,2017-02-28T20:19:02.547,,,,,1760.0,,1,4,<philosophy>,Can Robots learn how to feel suffering?,137.0,80.78,7.21,8.62,0.0,0.0,24.0,At first I had this question in mind Can robots develop suffering Because suffering is important for human beings Imagine that you are running the wrong way damaging your heel Without pain you will continue to harm it Same for robots But then I told myself wait a second It already exists It is the errors and warnings that shows up We can say it has the similar purpose as suffering However I felt something missing We feel pain The errors and bugs are just data Lets say a robot can use machine learning and genetic programming to evolve Can it learn to feel suffering And not just know it as mere information
,0.0,"<p>I occasionally read papers that show neural networks solving traveling salesmen problems and multi traveling salesmen problems <em>efficiently</em>? </p>

<p>1) Is there any analysis of the meaning of efficiency of algorithms for networks that allowed to grow in size with the problem they are supposed to solve?</p>

<p>2) What are the earliest papers solving the TSP with NN this?</p>

<p>3) Is the meaning of efficiency used in these papers is the same as the usual one, in fact, and works only in this problem specifically?</p>

<p>COMMENTS</p>

<p>These problems are NP hard. So I suspect I'm not sure what these papers mean by <em>efficient</em>.</p>

<p>The neural network postulated have a sufficiently vast number of interacting elements and in effect do the combinatorics strictly, for each special case. But if so, while this is fast and doesn't grow much with the size of the problem growing, is this really comparable to the normal meaning of PT as fast or efficient?</p>

<p>In these cases it seems the time efficiency is obtained by resource inefficiency: by making the network enormous and simulating all the possible worlds then maximizing. So, while time to compute doesn't grow much as the problem grows, the size of the physical computer grows enormously for larger problems; how fast it computes is then, it seems to me, not a good measure of efficiency of the algorithm in the common meaning of efficiency. In this case the resources themselves only grow as fast as the problem size, but what explodes is the number of connections that must be built. If we go from 1000 to 2000 neurons to solve a problem twice as large and requiring exponentially as much time to solve, the algorithms requiring only twice as many neurons to solve in PT seem efficient, but really, there is still an enormous increase in connections and coefficients that need be built for this to work.</p>

<p>Is my above reasoning incorrect?</p>
",,2,2017-02-25T21:46:50.777,,2874,2017-02-25T23:33:39.760,2017-02-25T23:33:39.760,,1366.0,,1366.0,,1,4,<neural-networks><algorithm><deep-network><efficiency><time>,Neural networks efficiently solve traveling salesmen problems?,47.0,55.07,10.86,8.66,0.0,0.0,34.0,I occasionally read papers that show neural networks solving traveling salesmen problems and multi traveling salesmen problems efficiently 1 Is there any analysis of the meaning of efficiency of algorithms for networks that allowed to grow in size with the problem they are supposed to solve 2 What are the earliest papers solving the TSP with NN this 3 Is the meaning of efficiency used in these papers is the same as the usual one in fact and works only in this problem specifically COMMENTS These problems are NP hard So I suspect Im not sure what these papers mean by efficient The neural network postulated have a sufficiently vast number of interacting elements and in effect do the combinatorics strictly for each special case But if so while this is fast and doesnt grow much with the size of the problem growing is this really comparable to the normal meaning of PT as fast or efficient In these cases it seems the time efficiency is obtained by resource inefficiency by making the network enormous and simulating all the possible worlds then maximizing So while time to compute doesnt grow much as the problem grows the size of the physical computer grows enormously for larger problems how fast it computes is then it seems to me not a good measure of efficiency of the algorithm in the common meaning of efficiency In this case the resources themselves only grow as fast as the problem size but what explodes is the number of connections that must be built If we go from 1000 to 2000 neurons to solve a problem twice as large and requiring exponentially as much time to solve the algorithms requiring only twice as many neurons to solve in PT seem efficient but really there is still an enormous increase in connections and coefficients that need be built for this to work Is my above reasoning incorrect
,0.0,"<p>For rules please refer to <a href=""https://www.hackerrank.com/challenges/ultimate-ttt"" rel=""noreferrer"">https://www.hackerrank.com/challenges/ultimate-ttt</a> .I have implemented minimax search with alpha-beta pruning. There is a time limit of 15 seconds.Which algorithms would yield better results? </p>
",,3,2017-02-25T21:57:43.563,1.0,2875,2017-02-25T21:57:43.563,,,,,5706.0,,1,6,<algorithm>,Which algorithm is best for a 4*4*4*4 variant of ultimate tic-tac toe?,41.0,39.19,20.77,10.99,0.0,0.0,13.0,For rules please refer to httpswwwhackerrankcomchallengesultimatettt I have implemented minimax search with alphabeta pruning There is a time limit of 15 secondsWhich algorithms would yield better results
2887.0,2.0,"<p>My understanding of the singularity is when artificial intelligence becomes ""more intelligence"" than humans.
This will be achieved through machine learning where an; algorithm, neural network ? Exponential betters itself. </p>

<p>So from that point on in near future after that we should predict that there will be artificial intelligence capable of answering any question. 
How to travel the fastest...
Blueprints for spacecrafts...
Drugs for medicine...
Efficiency and advancements that will change the human condition.</p>

<p>The singularity is predicted 2040s or 2030. All be it a couple of years later down to exponential growth in knowledge.</p>

<p>So if what I'm saying is right I should be seeing crazy hype and news coverage as well as advancements but I don't. 
I don't understand what is wrong with the idea that the AI will be capable of omniscience. 
So can it ?
Is there something preventing it ?
I don't see how so logically.</p>

<p>As my philosophy has been that research in all the scientific fields are long and expensive. The prospect of a AI that could perform research at fractional cost and time is the way to go. </p>

<p>I hope to work in a field that works at achieving singularity and so will in turn change the world. With the ideas and discoveries it will have. </p>

<p>And where does ""artificial"" consciousness come in to play in the singularity </p>
",,0,2017-02-26T01:04:39.363,,2876,2017-02-27T11:04:32.917,,,,,5708.0,,1,6,<machine-learning><philosophy><singularity><cognitive-science>,The Singularity and future of civilisation,133.0,68.57,10.13,8.23,0.0,0.0,35.0,My understanding of the singularity is when artificial intelligence becomes more intelligence than humans This will be achieved through machine learning where an algorithm neural network Exponential betters itself So from that point on in near future after that we should predict that there will be artificial intelligence capable of answering any question How to travel the fastest Blueprints for spacecrafts Drugs for medicine Efficiency and advancements that will change the human condition The singularity is predicted 2040s or 2030 All be it a couple of years later down to exponential growth in knowledge So if what Im saying is right I should be seeing crazy hype and news coverage as well as advancements but I dont I dont understand what is wrong with the idea that the AI will be capable of omniscience So can it Is there something preventing it I dont see how so logically As my philosophy has been that research in all the scientific fields are long and expensive The prospect of a AI that could perform research at fractional cost and time is the way to go I hope to work in a field that works at achieving singularity and so will in turn change the world With the ideas and discoveries it will have And where does artificial consciousness come in to play in the singularity
,,"<p>At a very high level, regarding evolutionary game theory and genetic algorithms, it is absolutely possible that AI could develop a state that is analogous with suffering, although, as you astutely point out, it would involve conditions which a computer cares about.  (For instance, it might develop a feeling analogous to ""being aggrieved"" over non-optimality in the algorithmic sense, or ""frustration"" at equations don't add up, or ""dissatisfaction"" over goals that have not been achieved.)</p>

<p><a href=""http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/children-beating-up-robot"" rel=""nofollow noreferrer"">The robot tormented by small children at the mall</a> can certainly be said to be ""suffering"" in that the children block the performance of the robot's function, but the robot is not conscious and suffering might be said to require awareness.  However, even without consciousness, this very simple robot can learn new behaviors through which it mitigates or avoids the ""suffering"" brought on by not being able to fulfill its function.</p>

<p>You definitely want to look into <a href=""https://en.wikipedia.org/wiki/Suffering#Philosophy"" rel=""nofollow noreferrer"">the concept of suffering in a philosophical context</a> and <a href=""https://en.wikipedia.org/wiki/Epicurus"" rel=""nofollow noreferrer"">Epicurus</a> would be a very useful place to start.  </p>

<p>Epicurus is directly relevant in an algorithmic sense because he uses the term ""<a href=""http://www.perseus.tufts.edu/hopper/morph?l=ataracia&amp;la=greek#lexicon"" rel=""nofollow noreferrer"">ataraxia</a>"" meaning calm, and is derived from the verb ""<a href=""http://www.perseus.tufts.edu/hopper/morph?l=tarassw&amp;la=greek"" rel=""nofollow noreferrer"">tarasso</a>"" which means to agitate or disturb.</p>

<p>Ataraxia can be mathematically expressed as an equilibrium.  Tarasso can be mathematically expressed as disequilibrium.  </p>

<p>This relates directly to Game Theory in that disequilibrium can be said to be the primary requirements of games, and to AI in that Game Theory can be said to be the at root of all AI.</p>

<p>Ataraxia is also understood in the sense of ""freedom from fear"", which in temporal in that fear is a function of uncertainty as it relates to the future in a predictive sense, and involves current condition vs. possible, less optimal future conditions.  </p>

<p>Thus fear, which is a form of suffering, is rooted in computational intractability, even where the ""computer"" is is a human brain.</p>

<p>Early philosophers such as <a href=""https://en.wikipedia.org/wiki/Democritus"" rel=""nofollow noreferrer"">Democritus</a> are especially useful because they were exploring critical, fundamental concepts, many of which can now be expressed with modern mathematics.  </p>

<p>To wit: you can't arrive at suffering until you first define ""the Good"" and ""the Bad"", which is a binary relationship in which neither term can be said to have meaning without the opposite.  (Mathematically, it can be expressed in its simplest form as a finite, one dimensional graph.)  This understanding is quite ancient. </p>

<hr>

<p>It is worth noting that the continuing value of the early philosophers is partly a factor of wisdom not being a dependent of volume of knowledge, demonstrated by Socrates in the idea that wisdom may be as simple as knowing you don't know something. </p>

<p>The ancient sages didn't have the benefit of powerful measurement tools, advanced mathematics, or scientific method, but they were very smart, and even more importantly, wise. </p>
",,4,2017-02-26T01:09:27.410,,2877,2017-02-28T20:19:02.547,2017-02-28T20:19:02.547,,1671.0,,1671.0,2872.0,2,5,,,,45.29,12.83,9.95,0.0,0.0,81.0,At a very high level regarding evolutionary game theory and genetic algorithms it is absolutely possible that AI could develop a state that is analogous with suffering although as you astutely point out it would involve conditions which a computer cares about For instance it might develop a feeling analogous to being aggrieved over nonoptimality in the algorithmic sense or frustration at equations dont add up or dissatisfaction over goals that have not been achieved The robot tormented by small children at the mall can certainly be said to be suffering in that the children block the performance of the robots function but the robot is not conscious and suffering might be said to require awareness However even without consciousness this very simple robot can learn new behaviors through which it mitigates or avoids the suffering brought on by not being able to fulfill its function You definitely want to look into the concept of suffering in a philosophical context and Epicurus would be a very useful place to start Epicurus is directly relevant in an algorithmic sense because he uses the term ataraxia meaning calm and is derived from the verb tarasso which means to agitate or disturb Ataraxia can be mathematically expressed as an equilibrium Tarasso can be mathematically expressed as disequilibrium This relates directly to Game Theory in that disequilibrium can be said to be the primary requirements of games and to AI in that Game Theory can be said to be the at root of all AI Ataraxia is also understood in the sense of freedom from fear which in temporal in that fear is a function of uncertainty as it relates to the future in a predictive sense and involves current condition vs possible less optimal future conditions Thus fear which is a form of suffering is rooted in computational intractability even where the computer is is a human brain Early philosophers such as Democritus are especially useful because they were exploring critical fundamental concepts many of which can now be expressed with modern mathematics To wit you cant arrive at suffering until you first define the Good and the Bad which is a binary relationship in which neither term can be said to have meaning without the opposite Mathematically it can be expressed in its simplest form as a finite one dimensional graph This understanding is quite ancient It is worth noting that the continuing value of the early philosophers is partly a factor of wisdom not being a dependent of volume of knowledge demonstrated by Socrates in the idea that wisdom may be as simple as knowing you dont know something The ancient sages didnt have the benefit of powerful measurement tools advanced mathematics or scientific method but they were very smart and even more importantly wise
,,"<p>I quite like your outlook, and without getting into the details of how a ""singularity"" may be effected which is covered in numerous other questions, or how consciousness and ""omniscience"" come into play because <a href=""https://en.wikipedia.org/wiki/Grey_goo"" rel=""nofollow noreferrer"">consciousness and omniscience are not requirements</a>, I will instead direct you to two key philosophers:</p>

<ul>
<li><p>Phillip K. Dick, for whom the central theme <a href=""https://en.wikipedia.org/wiki/Do_Androids_Dream_of_Electric_Sheep%3F"" rel=""nofollow noreferrer"">in his famous 1968 book on AI</a> is empathy.  <em>(If you haven't read it, I'm not posting a spoiler, but will only say the plot is driven by the concept of <a href=""https://en.wikipedia.org/wiki/Evolutionary_game_theory"" rel=""nofollow noreferrer"">Evolutionary Game Theory</a> which was formalized just 5 years later.)</em> </p></li>
<li><p>John Nash, and in particular, the concept of the <a href=""https://en.wikipedia.org/wiki/Nash_equilibrium"" rel=""nofollow noreferrer"">Nash Equilibrium</a>. <em>(Nash could be said to have mathematically demonstrated that being a ""douchebag"" is not an optimal strategy.  His proof can be used to explain why <a href=""https://en.wikipedia.org/wiki/Mutual_assured_destruction"" rel=""nofollow noreferrer"">nuclear détente actually worked</a>, which was counter to the expectation of Von Neumann.)</em> </p></li>
</ul>

<p>So when people go nuts, focusing on the ""<a href=""https://en.wikipedia.org/wiki/Skynet_(Terminator)"" rel=""nofollow noreferrer"">Skynet</a>"" mythos under which machines rise up to destroy us, I have to wonder if they're simply not as smart as Nash or as profound as Dick, which might explain their lack of emphasis on what can be called the ""<a href=""https://plato.stanford.edu/entries/altruism-biological/"" rel=""nofollow noreferrer"">Electric Sheep</a>"" paradigm.</p>
",,0,2017-02-26T01:47:06.527,,2878,2017-02-26T01:52:35.727,2017-02-26T01:52:35.727,,1671.0,,1671.0,2876.0,2,4,,,,59.67,10.91,9.98,0.0,0.0,37.0,I quite like your outlook and without getting into the details of how a singularity may be effected which is covered in numerous other questions or how consciousness and omniscience come into play because consciousness and omniscience are not requirements I will instead direct you to two key philosophers Phillip K Dick for whom the central theme in his famous 1968 book on AI is empathy If you havent read it Im not posting a spoiler but will only say the plot is driven by the concept of Evolutionary Game Theory which was formalized just 5 years later John Nash and in particular the concept of the Nash Equilibrium Nash could be said to have mathematically demonstrated that being a douchebag is not an optimal strategy His proof can be used to explain why nuclear détente actually worked which was counter to the expectation of Von Neumann So when people go nuts focusing on the Skynet mythos under which machines rise up to destroy us I have to wonder if theyre simply not as smart as Nash or as profound as Dick which might explain their lack of emphasis on what can be called the Electric Sheep paradigm
,0.0,"<p>I ask this on Stack AI, because the implication is not that people who work at think tanks, for the most part, are not nearly as smart as they pretend to be, (which may very well be true, but difficult to quantify,) but instead related to <strong>modeling and predictive capability.</strong></p>

<p>The old chestnut when explaining why imposed regulation doesn't work is pointing to examples collectivism in the Soviet Union which had disastrous results.  But  computers sucked during that era, at least as compared to today.  (It is telling that weather forecasts, which used to be regarded in society as a joke, and were rooted in Almanacs, is now highly accurate, and keeps improving.) </p>

<p>It is difficult to believe degree of dis-equilibrium that often occurs in poorly regulated, minimally regulated, or unregulated is optimal, except to certain actors focused on aggregation of wealth primarily though exploitation of <a href=""https://en.wikipedia.org/wiki/Pareto_efficiency"" rel=""nofollow noreferrer"">Pareto efficiencies</a> (which carries profound, negative moral implications,) in that these actors, who often create the conditions for extreme dis-equilibrium are often set up to profit further from the ""reset"".  </p>

<p><em>(It's is not even convincing that these actors actually believe that self-regulation of markets is optimal, because, in the case of the recent 2008 event, they opted to be bailed out instead of choosing to be subjected to un-mitigated market forces.)</em></p>

<p>But an AI, with sufficient data and processing power, should be able to regulate markets much more optimally than ""self-regulating markets"" because it can actively balance the system instead of relying on ""natural correction"" of imbalance, which is often catastrophic.</p>

<p>Economies are quite complex, but a hallmark of current AI is the effective management of intractability, which can be rephrased as increasingly optimal decision making in a condition of uncertainty.</p>

<p><strong>I am not suggesting that current AI is ""smart"" enough to regulate economies today, but is it rational to assume that this capability is not outside the potential of future systems?</strong>  </p>
",,0,2017-02-26T02:38:23.723,,2880,2017-02-26T02:38:23.723,,,,,1671.0,,1,0,<machine-learning><game-theory>,"Can the idea that ""self-regulating markets are optimal"" be understood as function of lack of intelligence?",17.0,27.19,14.05,11.69,0.0,0.0,58.0,I ask this on Stack AI because the implication is not that people who work at think tanks for the most part are not nearly as smart as they pretend to be which may very well be true but difficult to quantify but instead related to modeling and predictive capability The old chestnut when explaining why imposed regulation doesnt work is pointing to examples collectivism in the Soviet Union which had disastrous results But computers sucked during that era at least as compared to today It is telling that weather forecasts which used to be regarded in society as a joke and were rooted in Almanacs is now highly accurate and keeps improving It is difficult to believe degree of disequilibrium that often occurs in poorly regulated minimally regulated or unregulated is optimal except to certain actors focused on aggregation of wealth primarily though exploitation of Pareto efficiencies which carries profound negative moral implications in that these actors who often create the conditions for extreme disequilibrium are often set up to profit further from the reset Its is not even convincing that these actors actually believe that selfregulation of markets is optimal because in the case of the recent 2008 event they opted to be bailed out instead of choosing to be subjected to unmitigated market forces But an AI with sufficient data and processing power should be able to regulate markets much more optimally than selfregulating markets because it can actively balance the system instead of relying on natural correction of imbalance which is often catastrophic Economies are quite complex but a hallmark of current AI is the effective management of intractability which can be rephrased as increasingly optimal decision making in a condition of uncertainty I am not suggesting that current AI is smart enough to regulate economies today but is it rational to assume that this capability is not outside the potential of future systems
,,"<p>Yes, ""goodness"" is a common description of the value generated by an evaluation function.
For example,
<a href=""https://books.google.com/books?id=DDNHzcN6jasC"" rel=""nofollow noreferrer"">""Artificial Intelligence""</a> p. 77;
<a href=""https://books.google.com/books?id=_iE4uvEp4CMC"" rel=""nofollow noreferrer"">""Knowledge-Free and Learning-Based Methods in Intelligent Game Playing""</a> p. 15;
<a href=""https://books.google.com/books?id=jNfvAgAAQBAJ"" rel=""nofollow noreferrer"">""Tenth Scandinavian Conference on Artificial Intelligence""</a> p. 125; and
<a href=""https://books.google.com/books?id=C_k7XVeCTY4C"" rel=""nofollow noreferrer"">""Algorithms and Networking for Computer Games""</a> p. 80.</p>

<p>The term ""position"" refers to the position of the pieces on a game board at some instant.</p>
",,0,2017-02-26T03:13:27.113,,2881,2017-02-26T03:13:27.113,,,,,5711.0,2864.0,2,2,,,,60.61,14.71,10.34,0.0,0.0,26.0,Yes goodness is a common description of the value generated by an evaluation function For example Artificial Intelligence p 77 KnowledgeFree and LearningBased Methods in Intelligent Game Playing p 15 Tenth Scandinavian Conference on Artificial Intelligence p 125 and Algorithms and Networking for Computer Games p 80 The term position refers to the position of the pieces on a game board at some instant
,,"<p>First, for almost every question of the form ""Can AI be made to X"", the most obvious and straightforward answer is something like ""We don't know. Probably, but if it hasn't been done yet, we're really not sure.""</p>

<p>It's also important to understand that, from a technology standpoint, there isn't one ""thing"" called ""AI"".  There are many, many different technologies, which are loosely related (at best) and are generally lumped together under the overall rubric of ""Artificial Intelligence"".</p>

<p>All of that said, yes, there has been work on adding memory, even long-term memory, to various kinds of ""AI"".  The most notable recent example is the advent of <a href=""https://en.wikipedia.org/wiki/Long_short-term_memory"" rel=""nofollow noreferrer"">LSTM</a> in recurrent neural networks.  </p>

<p>Additionally, some of the work done on ""cognitive architectures"" has focused on the use of memory.  For more info on that, look up ACT-R and/or SOAR and read some of those papers.  </p>

<p>What isn't clear to me offhand, is whether or not anybody has tried applying any of these techniques to chat-bots in particular.  I wouldn't be surprised if somebody had, but I can't cite any such research off the top of my head.</p>
",,2,2017-02-26T09:26:38.610,,2882,2017-02-26T09:26:38.610,,,,,33.0,2871.0,2,1,,,,61.06,10.97,9.65,0.0,0.0,54.0,First for almost every question of the form Can AI be made to X the most obvious and straightforward answer is something like We dont know Probably but if it hasnt been done yet were really not sure Its also important to understand that from a technology standpoint there isnt one thing called AI There are many many different technologies which are loosely related at best and are generally lumped together under the overall rubric of Artificial Intelligence All of that said yes there has been work on adding memory even longterm memory to various kinds of AI The most notable recent example is the advent of LSTM in recurrent neural networks Additionally some of the work done on cognitive architectures has focused on the use of memory For more info on that look up ACTR andor SOAR and read some of those papers What isnt clear to me offhand is whether or not anybody has tried applying any of these techniques to chatbots in particular I wouldnt be surprised if somebody had but I cant cite any such research off the top of my head
,,"<p>If the only feature you're classifying on is the number of users making a given report, then this isn't really much to do with AI/ML. Just pick a number based on your subjective judgment and go with it.</p>

<p>OTOH, if you can include details of the report itself (as well as the number of reporters), I think you might be able to build a bayesian classifier that would be useful.  If you could consider location, weather, time of day, number of reporters, etc., it seems like you might be able to get something useful put together.  </p>
",,0,2017-02-26T09:47:48.923,,2883,2017-02-26T09:47:48.923,,,,,33.0,2806.0,2,0,,,,68.91,8.65,9.36,0.0,0.0,18.0,If the only feature youre classifying on is the number of users making a given report then this isnt really much to do with AIML Just pick a number based on your subjective judgment and go with it OTOH if you can include details of the report itself as well as the number of reporters I think you might be able to build a bayesian classifier that would be useful If you could consider location weather time of day number of reporters etc it seems like you might be able to get something useful put together
,0.0,"<p>I have labeled time windows of data from different sensors(gyroscope, accelerator, magnetometer). For all sensor data, the format is:</p>

<pre><code>(x,y,z,timestamp)
</code></pre>

<p>For a classification purpose, how do I represent the sensor data? 
How exactly do I make meaningful feature extractions of the raw sensor data so it can be used for classification?
Does a mean of a given time window makes sense?</p>

<p>The purpose and goal is to classify activities from these time windows of sensor data.</p>

<p>Thanks in advance</p>
",,2,2017-02-26T20:26:03.443,,2885,2017-02-26T20:26:03.443,,,,,4933.0,,1,1,<classification>,Represent time window of sensor data,18.0,49.82,10.77,9.95,18.0,0.0,12.0,I have labeled time windows of data from different sensorsgyroscope accelerator magnetometer For all sensor data the format is For a classification purpose how do I represent the sensor data How exactly do I make meaningful feature extractions of the raw sensor data so it can be used for classification Does a mean of a given time window makes sense The purpose and goal is to classify activities from these time windows of sensor data Thanks in advance
2889.0,1.0,"<p>I'm doing a little tic-tac-toe project to learn neural networks and machine learning (beginner level). I've written a MLP based program that plays with other search based programs and trains with the data generated from the games. </p>

<p>The training and evaluation are strictly policy based - Inputs are board positions and outputs are one-hot encoded array that represents the recommended move for that board position. I've not added search algorithms so that I can understand what to expect from a purely MLP approach.</p>

<p>The MLP model has 35 features and 1 hidden layer and after a few hundred thousands games it has sort of learned to draw 50% games. It has learned the basic stuff like how to block the player from winning and some good board placements.</p>

<p>Now, my question is - It hasn't learned advanced strategies that require making a move that may not be as beneficial for the current move but will improve its chances later. But should I expect that from a strictly policy MLP based no-search approach? Since all that it is being trained on is one board and the next recommended move (even if thousands of those pairs), is it logical to expect it to learn a lookahead approach that goes beyond ""the best move for the current board"" training? </p>

<p>Put another way, would it be a possible at all for a MLP to learn lookahead without any search strategies? If not, are there any alternatives that can do it without search?</p>
",,5,2017-02-26T22:22:00.153,0.0,2886,2017-02-28T08:19:14.450,,,,,1522.0,,1,5,<machine-learning><mlp>,Is it a valid assumption that a purely MLP based tic-tac-toe player will learn lookahead strategies?,52.0,65.66,10.51,9.3,0.0,0.0,32.0,Im doing a little tictactoe project to learn neural networks and machine learning beginner level Ive written a MLP based program that plays with other search based programs and trains with the data generated from the games The training and evaluation are strictly policy based Inputs are board positions and outputs are onehot encoded array that represents the recommended move for that board position Ive not added search algorithms so that I can understand what to expect from a purely MLP approach The MLP model has 35 features and 1 hidden layer and after a few hundred thousands games it has sort of learned to draw 50 games It has learned the basic stuff like how to block the player from winning and some good board placements Now my question is It hasnt learned advanced strategies that require making a move that may not be as beneficial for the current move but will improve its chances later But should I expect that from a strictly policy MLP based nosearch approach Since all that it is being trained on is one board and the next recommended move even if thousands of those pairs is it logical to expect it to learn a lookahead approach that goes beyond the best move for the current board training Put another way would it be a possible at all for a MLP to learn lookahead without any search strategies If not are there any alternatives that can do it without search
,,"<p><strong>Note:</strong> This looks a little bit detailed answer to the question,I wanted to give an insightful scenarios.</p>

<p><strong>The Singularity</strong> is the point in time when computers will be more intelligent,more able, and more creative than humans. 
At that point there will be a sharp bend in the technology curve, since super-intelligent computers will be able to develop new technologies exponentially faster than humans, including technologies to make themselves faster. After that, they will essentially be running the world.</p>

<p><strong>Hint on the future of Civilization;</strong> 
civilization has a specific set of ideas and customs, and a certain set of manufactures,science and arts that make it unique,with in a complex System.Therefore, Civilizations tend to develop intricate cultures, including a state-based decision making apparatus.Special thanks to <a href=""http://www.crystalinks.com/egyptscience.html"" rel=""noreferrer"">ancient Egyptians Civilization</a></p>

<p><strong>Artificial intelligence</strong> (AI) is the name of these technologies. 
AI technologies are under development in every country of the world, 
and they are the technologies that will bring about the Singularity. 
AI technologies are improving rapidly; in fact, 2015 was a breakthrough year for AI,with advances of all kinds. </p>

<p>These different advances are separate events now, but within a few years they'll merge into the first super-intelligent computers and robots that will lead to the Singularity. </p>

<p>Many analysts consider 2015 to have been a breakthrough year for Artificial Intelligence (AI), not because of any single achievement, 
but because of achievements across the board in so many different areas.</p>

<p>Companies like Google, Facebook and Microsoft are now operating their own AI labs.In areas such as image recognition, computer vision, face recognition, 
voice recognition and natural language processing, there are a wealth of new products (think of Siri or OK Google) that are becoming increasingly reliable and increasingly available.</p>

<p>Several companies are testing self-driving cars, 
and they’re expected to be available commercially by 2020. 
Robots in the military are becoming more common, from robots on wheels to pilotless drone warplanes. 
All of these robots still require constant human intervention and control, but they’re slowly migrating away from human control to algorithmically based decision making and control. 
Robot form factors are improving, with some robots looking almost human.</p>

<hr>

<p><strong>Confusion from various sources</strong></p>

<p>Glimpse:Artificial Intelligence and Climate Change</p>

<p>Politicians and climate change activists like to say that the claims about climate change have been endorsed by 95% of all the scientists in the world. 
This claim is a total fraud, because it confuses two things.</p>

<ul>
<li><p>First, we have the claims by science that the earth is warming because of human activity. Arguably, that has been proven by scientists. But that’s all.</p></li>
<li><p>The second part,is predictions about the future, which are mostly total crap, and certainly not science. In fact, climate change scientists have been making predictions for 25 years, and they’ve almost completely turned out to be wrong. 
The truth is that scientists who claim to know what the earth’s temperature will be in 2100 can’t even predict what the weather will be next month.</p></li>
</ul>

<p>During my lifetime in artificial Intelligence field and being passionate about, I’ve read number of hysterical environment disaster predictions articles,written by different philosophers across the globe.
My favorite is the prediction in 1970 by far left-wing Ramparts Magazine that predicted that the oceans were becoming so polluted that by 1980 the world’s oceans would be covered by a layer of algae. It didn’t happen.</p>

<p>One way to know that the climate change activists are wrong is that these climate change activists never mention the Singularity or future technology.</p>

<p>And we researchers in Artificial Intelligence,they undermine our visions which will pop out to be true.</p>

<hr>

<p><strong>Proof that the Singularity will occur by 2030 or 2040</strong></p>

<p>A proof, based on reasonable assumptions, 
that any intelligent species on any planet in the universe will develop in a way that’s similar to the development of humans,
including following the same Generational Dynamics cycles as humans.</p>

<p><strong>A glimpse:lets try to time travel back in 1800s,</strong></p>

<p>In the late 1800s, streets in large cities were full of horses (think of a traffic jam in any large city, with horses instead of cars). 
These horses were producing huge volumes of urine, manure, flies and carcasses — not to mention cruelty to horses.</p>

<p>By 1900, there was 1,200 metric tons of horse manure per day. 
There were international conferences (like today’s climate change conferences) that accomplished nothing. 
But within 20 years, the problem took care of itself because of new technology – the automobile.</p>

<p>History shows that new technology, including new AI technologies, will solve the “climate change” problem, 
and that politicians will have absolutely nothing to do with it, except to take the credit when something works, and to blame someone else otherwise.</p>

<p><strong>Now lets come back from 1800s;</strong>
Early in 2005, the Pentagon announced the Army's Future Combat Systems (FCS). 
By 2014, just a few years from now, America will be deploying thousands of computerized soldiers that will have the ability to decide on their own to kill people (hopefully, the enemy). </p>

<p>In early implementations, kills will be directed wirelessly by human overseers, but as millions of these are deployed, overseeing them will become increasingly impossible. 
By 2025, super-intelligent computerized robots manufactured in countries around the world will be fighting major battles. </p>

<p><em>By 2030, super-intelligent computers will be running the world without our help.</em></p>

<hr>

<p><strong>Fiction or Facts from Science Fiction Movies:</strong>
Robot from I, Robot
This is quite a different view of intelligent robots than the one in the movie I, Robot. It came out in summer, 2004, 
and it portrays a world in 2035 when super-intelligent robots are being manufactured for domestic home use. 
These robots are designed to be unable to harm human beings, but the story line is about a rogue robot who may be violating that rule. 
In the end, Will Smith conquers the malevolent robots and gets the girl, and everyone lives happily ever after. </p>

<p><strong>Software algorithms that will bring singularity;</strong></p>

<p>Intelligence isn’t some magical, mystical force. 
It’s actually the ability to find new ways to combine previous experiences in new ways. 
A new discovery is made by combining old discoveries in new ways, 
in the same way that jigsaw puzzle pieces can be put together.</p>

<p>A computer can do the same thing by combining “knowledge bits” (KBs) in new ways, to learn new things, in the same way that jigsaw puzzle pieces can be combined. Computers can do this much faster than humans can.</p>

<p>Decisions can be made by using the same “minimax algorithm” that’s used to implement games like chess.
This algorithm would work today, except that computers aren’t yet fast enough. 
The speed of computers doubles every 18 months, 
and by 2030 computers will be fast enough to implement this IC algorithm.</p>

<p><strong>Another proof based on reasonable assumptions;</strong></p>

<p>Every intelligent species in the universe must follow the same Generational Dynamics cycles as humans.. 
is outlined as follows:</p>

<p>For any species (including humans) to survive, 
the population growth rate must be greater than the food supply growth rate. 
This is what I call “The Malthus Effect,” 
based on the 1798 book by <a href=""https://en.wikipedia.org/wiki/Thomas_Robert_Malthus"" rel=""noreferrer"">Thomas Roberts Malthus</a>, Essay on Population.</p>

<p>Therefore, for any species, there must be cyclical periods of extermination. 
This can be accomplished in several ways, such as war, predator, famine or disease. But one way or another, it has to happen.</p>

<p>Non-intelligent species will simply starve and die quietly when there’s insufficient food. 
But intelligent species will form identity groups and hold riots and protests, and eventually go to war. 
These will be the cyclic crisis wars of extermination specified by Generational Dynamics and every intelligent species in the universe will have them.</p>

<p>If this can give you a glimpse,then just know Artificial Intelligence ain't no joke.....Lets study it hard...so that we see such a future.
Special Thanks to <strong>StackExchange</strong></p>
",,0,2017-02-27T11:04:32.917,,2887,2017-02-27T11:04:32.917,,,,,1581.0,2876.0,2,5,,,,51.99,13.58,9.19,0.0,0.0,200.0,Note This looks a little bit detailed answer to the questionI wanted to give an insightful scenarios The Singularity is the point in time when computers will be more intelligentmore able and more creative than humans At that point there will be a sharp bend in the technology curve since superintelligent computers will be able to develop new technologies exponentially faster than humans including technologies to make themselves faster After that they will essentially be running the world Hint on the future of Civilization civilization has a specific set of ideas and customs and a certain set of manufacturesscience and arts that make it uniquewith in a complex SystemTherefore Civilizations tend to develop intricate cultures including a statebased decision making apparatusSpecial thanks to ancient Egyptians Civilization Artificial intelligence AI is the name of these technologies AI technologies are under development in every country of the world and they are the technologies that will bring about the Singularity AI technologies are improving rapidly in fact 2015 was a breakthrough year for AIwith advances of all kinds These different advances are separate events now but within a few years theyll merge into the first superintelligent computers and robots that will lead to the Singularity Many analysts consider 2015 to have been a breakthrough year for Artificial Intelligence AI not because of any single achievement but because of achievements across the board in so many different areas Companies like Google Facebook and Microsoft are now operating their own AI labsIn areas such as image recognition computer vision face recognition voice recognition and natural language processing there are a wealth of new products think of Siri or OK Google that are becoming increasingly reliable and increasingly available Several companies are testing selfdriving cars and they’re expected to be available commercially by 2020 Robots in the military are becoming more common from robots on wheels to pilotless drone warplanes All of these robots still require constant human intervention and control but they’re slowly migrating away from human control to algorithmically based decision making and control Robot form factors are improving with some robots looking almost human Confusion from various sources GlimpseArtificial Intelligence and Climate Change Politicians and climate change activists like to say that the claims about climate change have been endorsed by 95 of all the scientists in the world This claim is a total fraud because it confuses two things First we have the claims by science that the earth is warming because of human activity Arguably that has been proven by scientists But that’s all The second partis predictions about the future which are mostly total crap and certainly not science In fact climate change scientists have been making predictions for 25 years and they’ve almost completely turned out to be wrong The truth is that scientists who claim to know what the earth’s temperature will be in 2100 can’t even predict what the weather will be next month During my lifetime in artificial Intelligence field and being passionate about I’ve read number of hysterical environment disaster predictions articleswritten by different philosophers across the globe My favorite is the prediction in 1970 by far leftwing Ramparts Magazine that predicted that the oceans were becoming so polluted that by 1980 the world’s oceans would be covered by a layer of algae It didn’t happen One way to know that the climate change activists are wrong is that these climate change activists never mention the Singularity or future technology And we researchers in Artificial Intelligencethey undermine our visions which will pop out to be true Proof that the Singularity will occur by 2030 or 2040 A proof based on reasonable assumptions that any intelligent species on any planet in the universe will develop in a way that’s similar to the development of humans including following the same Generational Dynamics cycles as humans A glimpselets try to time travel back in 1800s In the late 1800s streets in large cities were full of horses think of a traffic jam in any large city with horses instead of cars These horses were producing huge volumes of urine manure flies and carcasses — not to mention cruelty to horses By 1900 there was 1200 metric tons of horse manure per day There were international conferences like today’s climate change conferences that accomplished nothing But within 20 years the problem took care of itself because of new technology – the automobile History shows that new technology including new AI technologies will solve the “climate change” problem and that politicians will have absolutely nothing to do with it except to take the credit when something works and to blame someone else otherwise Now lets come back from 1800s Early in 2005 the Pentagon announced the Armys Future Combat Systems FCS By 2014 just a few years from now America will be deploying thousands of computerized soldiers that will have the ability to decide on their own to kill people hopefully the enemy In early implementations kills will be directed wirelessly by human overseers but as millions of these are deployed overseeing them will become increasingly impossible By 2025 superintelligent computerized robots manufactured in countries around the world will be fighting major battles By 2030 superintelligent computers will be running the world without our help Fiction or Facts from Science Fiction Movies Robot from I Robot This is quite a different view of intelligent robots than the one in the movie I Robot It came out in summer 2004 and it portrays a world in 2035 when superintelligent robots are being manufactured for domestic home use These robots are designed to be unable to harm human beings but the story line is about a rogue robot who may be violating that rule In the end Will Smith conquers the malevolent robots and gets the girl and everyone lives happily ever after Software algorithms that will bring singularity Intelligence isn’t some magical mystical force It’s actually the ability to find new ways to combine previous experiences in new ways A new discovery is made by combining old discoveries in new ways in the same way that jigsaw puzzle pieces can be put together A computer can do the same thing by combining “knowledge bits” KBs in new ways to learn new things in the same way that jigsaw puzzle pieces can be combined Computers can do this much faster than humans can Decisions can be made by using the same “minimax algorithm” that’s used to implement games like chess This algorithm would work today except that computers aren’t yet fast enough The speed of computers doubles every 18 months and by 2030 computers will be fast enough to implement this IC algorithm Another proof based on reasonable assumptions Every intelligent species in the universe must follow the same Generational Dynamics cycles as humans is outlined as follows For any species including humans to survive the population growth rate must be greater than the food supply growth rate This is what I call “The Malthus Effect” based on the 1798 book by Thomas Roberts Malthus Essay on Population Therefore for any species there must be cyclical periods of extermination This can be accomplished in several ways such as war predator famine or disease But one way or another it has to happen Nonintelligent species will simply starve and die quietly when there’s insufficient food But intelligent species will form identity groups and hold riots and protests and eventually go to war These will be the cyclic crisis wars of extermination specified by Generational Dynamics and every intelligent species in the universe will have them If this can give you a glimpsethen just know Artificial Intelligence aint no jokeLets study it hardso that we see such a future Special Thanks to StackExchange
,,"<p>A MLP only does pattern recognition, it will not learn search. </p>

<p>Tictactoe, (Oughts and Crosses), is such a simple game that your network should learn the moves from the training data by heart, no generalisation required. If it still loses games, maybe your training data doesn't consist of particularly good moves. </p>
",,8,2017-02-28T08:19:14.450,,2889,2017-02-28T08:19:14.450,,,,,2227.0,2886.0,2,4,,,,71.14,12.35,9.74,0.0,0.0,11.0,A MLP only does pattern recognition it will not learn search Tictactoe Oughts and Crosses is such a simple game that your network should learn the moves from the training data by heart no generalisation required If it still loses games maybe your training data doesnt consist of particularly good moves
,1.0,"<p>Today we have neural network based AI players that are comparable or better than humans in games that require extensive pattern matching and ""intuition"". AlphaGo is a prime example. </p>

<p>But these AI players usually have both neural networks and search algorithms in place. Humans, on the other hand, rely just on the pattern matching and ""intuition"" (even the best chess players can see just a handful of moves ahead). </p>

<p>So, why do AI players still require extensive search while humans don't? How would AIs like AlphaGo perform if we take the search part out?</p>
",,2,2017-02-28T09:30:15.773,,2890,2017-02-28T13:12:34.390,,,,,1522.0,,1,5,<neural-networks>,Why do neural networks based AI players still require extensive search techniques?,51.0,64.0,10.84,9.12,0.0,0.0,16.0,Today we have neural network based AI players that are comparable or better than humans in games that require extensive pattern matching and intuition AlphaGo is a prime example But these AI players usually have both neural networks and search algorithms in place Humans on the other hand rely just on the pattern matching and intuition even the best chess players can see just a handful of moves ahead So why do AI players still require extensive search while humans dont How would AIs like AlphaGo perform if we take the search part out
,1.0,"<p>I am going to develop an open-domain Natural Language Question Answering (NL QA) system, and will use the Support Vector Machine (SVM) as the machine-learning (ML) algorithm for question classification.</p>

<p>The data on hand,is from a cube, containing multiple dimensions, of which some contain hierarchies.</p>

<p>I do not understand how to work/combine the taxonomy and SVM for question classification. If I understand correctly, the taxonomy still needs to be developed by hand, unless an existing one is being used. And the SVM sorts the queried NL question based on this taxonomy?</p>

<p>Is this correct, or am I mixing the whole concept?</p>
",,6,2017-02-28T10:22:17.403,,2891,2017-03-02T16:29:14.580,2017-03-01T13:26:32.630,,5219.0,,5219.0,,1,5,<machine-learning><algorithm><nlp>,How do I use a taxonomy and the Support Vector Machine for question classification in Natural Language Processing?,53.0,54.42,12.53,10.1,0.0,0.0,22.0,I am going to develop an opendomain Natural Language Question Answering NL QA system and will use the Support Vector Machine SVM as the machinelearning ML algorithm for question classification The data on handis from a cube containing multiple dimensions of which some contain hierarchies I do not understand how to workcombine the taxonomy and SVM for question classification If I understand correctly the taxonomy still needs to be developed by hand unless an existing one is being used And the SVM sorts the queried NL question based on this taxonomy Is this correct or am I mixing the whole concept
,,,,0,2017-02-28T12:50:33.257,,2892,2017-02-28T12:50:33.257,2017-02-28T12:50:33.257,,-1.0,,-1.0,,5,0,,,,,,,,,,
,,,,0,2017-02-28T12:50:33.257,,2893,2017-02-28T12:50:33.257,2017-02-28T12:50:33.257,,-1.0,,-1.0,,4,0,,,,,,,,,,
,,"<p>It is not very accurate to say that AI players requires extensive search while humans don't. Rather, it is a question of degree. </p>

<p>AIs do a lot more calculating, because that's what computers are good at. </p>

<p>Human intuition is much more powerful than a neural network can currently hope to match, because it is much more integrated into a world of knowledge about the game, it uses orders of magnitudes more neurons and it is not a static thing that just provides a move.</p>

<p>But if a human player stops calculating ahead his playing strength will drop very significantly. This can be seen by assessing the performance in games with short time controls: The less time you have the less calculation is happening, your intuition however is fast.</p>

<p>If you take out the search component of AlphaGo it would still play quite strongly, probably at a low dan level. Of course that is also far below its strength. </p>

<p>So, search is always an important component of playing strength, just more so for machines.</p>
",,4,2017-02-28T13:12:34.390,,2894,2017-02-28T13:12:34.390,,,,,2227.0,2890.0,2,5,,,,60.45,10.04,8.97,0.0,0.0,20.0,It is not very accurate to say that AI players requires extensive search while humans dont Rather it is a question of degree AIs do a lot more calculating because thats what computers are good at Human intuition is much more powerful than a neural network can currently hope to match because it is much more integrated into a world of knowledge about the game it uses orders of magnitudes more neurons and it is not a static thing that just provides a move But if a human player stops calculating ahead his playing strength will drop very significantly This can be seen by assessing the performance in games with short time controls The less time you have the less calculation is happening your intuition however is fast If you take out the search component of AlphaGo it would still play quite strongly probably at a low dan level Of course that is also far below its strength So search is always an important component of playing strength just more so for machines
,,"<p>It could be said that ""maintaining a thought"" is a basic requirement of computing, and can be represented as a string of binary digits in the context of a <a href=""https://en.wikipedia.org/wiki/Turing_machine"" rel=""nofollow noreferrer"">Turing Machine</a>.</p>

<p><em>""Basically, it occurred to me, that when I'm thinking or speaking, there is a constant feedback loop, in which I am formulating which words to use next, which sentences to form, and which concepts to explore, based on my most recent statements and the flow of the dialogue or monologue. I'm not just responding to outside stimulus but also to myself. In other words, I am usually maintaining a train of thought.""</em></p>

<p>This sounds an awful lot like a recursive function.</p>

<p>My analysis of the chatbot problem is that it reveals a poor quality reasoning on the part of the bots, as opposed to lack of reasoning. It's not so much a question on the raw ability of an algorithm to maintain a train of thought, because the ""train of though"" is the function itself, but the quality of the algorithm and, by some measures, the ""humanness"" of the output.</p>
",,4,2017-02-28T20:03:56.887,,2895,2017-02-28T20:03:56.887,,,,,1671.0,2871.0,2,0,,,,62.11,9.52,9.63,0.0,0.0,32.0,It could be said that maintaining a thought is a basic requirement of computing and can be represented as a string of binary digits in the context of a Turing Machine Basically it occurred to me that when Im thinking or speaking there is a constant feedback loop in which I am formulating which words to use next which sentences to form and which concepts to explore based on my most recent statements and the flow of the dialogue or monologue Im not just responding to outside stimulus but also to myself In other words I am usually maintaining a train of thought This sounds an awful lot like a recursive function My analysis of the chatbot problem is that it reveals a poor quality reasoning on the part of the bots as opposed to lack of reasoning Its not so much a question on the raw ability of an algorithm to maintain a train of thought because the train of though is the function itself but the quality of the algorithm and by some measures the humanness of the output
,,"<p>About 15 years ago, John Laird's group at Michigan used the Soar rule-based architecture to play several FPS games effectively (Quake II, Descent III):</p>

<p><a href=""http://ai.eecs.umich.edu/people/laird/games_research.html"" rel=""nofollow noreferrer"">http://ai.eecs.umich.edu/people/laird/games_research.html</a></p>

<p>Here's Laird's overview article from 'Computer':</p>

<p><a href=""https://www.researchgate.net/profile/John_Laird/publication/2955463_Laird_JE_Using_a_computer_game_to_develop_advanced_AI_Computer_34_70-75/links/54d0f59a0cf20323c21a1bd7/Laird-JE-Using-a-computer-game-to-develop-advanced-AI-Computer-34-70-75.pdf"" rel=""nofollow noreferrer"">https://www.researchgate.net/profile/John_Laird/publication/2955463_Laird_JE_Using_a_computer_game_to_develop_advanced_AI_Computer_34_70-75/links/54d0f59a0cf20323c21a1bd7/Laird-JE-Using-a-computer-game-to-develop-advanced-AI-Computer-34-70-75.pdf</a></p>
",,0,2017-02-28T20:16:52.163,,2896,2017-02-28T20:16:52.163,,,,,1657.0,2868.0,2,1,,,,-105.5,68.32,12.33,0.0,0.0,64.0,About 15 years ago John Lairds group at Michigan used the Soar rulebased architecture to play several FPS games effectively Quake II Descent III httpaieecsumichedupeoplelairdgamesresearchhtml Heres Lairds overview article from Computer httpswwwresearchgatenetprofileJohnLairdpublication2955463LairdJEUsingacomputergametodevelopadvancedAIComputer347075links54d0f59a0cf20323c21a1bd7LairdJEUsingacomputergametodevelopadvancedAIComputer347075pdf
,2.0,"<p>A lot of experts have expressed concerns about evil super intelligence. While their concerns are valid, is it necessary, what are the chances or how the artificial super-intelligence will evolve to have selfishness and self protecting desires inherent in biological systems? Is there any work which comments on this line of inquiry?</p>
",,3,2017-03-01T09:52:51.610,1.0,2897,2017-03-01T16:55:13.723,,,,,5765.0,,1,1,<unsupervised-learning>,Will artificial super-intelligence evolve to have selfishness inherent in biological systems?,45.0,37.0,13.74,11.48,0.0,0.0,6.0,A lot of experts have expressed concerns about evil super intelligence While their concerns are valid is it necessary what are the chances or how the artificial superintelligence will evolve to have selfishness and self protecting desires inherent in biological systems Is there any work which comments on this line of inquiry
,,"<p>AI will only ""evolve"" selfishness if it ""evolves"" in a competitive environment and has certain human-like faculties.</p>

<p>Self-protecting desires on the other hand are logical consequences of having any goal at all. After all, you can't reach your goal if you are destroyed. </p>

<p>The concern of ""evil"" super intelligences isn't that they literally turn evil and selfish and cruel. Those are human qualities.</p>

<p>Instead a superintelligence that has a certain goal, will logically pursue subgoals that help it to reach the ultimate goal. Such subgoals will be resources, power, safety. </p>

<p>So it will amass power and resources to reach its goal and exterminate any threat to its existence as long as its goal hasn't been reached, without being selfish at all.</p>
",,2,2017-03-01T11:42:18.690,,2898,2017-03-01T11:42:18.690,,,,,2227.0,2897.0,2,3,,,,56.15,11.82,9.87,0.0,0.0,24.0,AI will only evolve selfishness if it evolves in a competitive environment and has certain humanlike faculties Selfprotecting desires on the other hand are logical consequences of having any goal at all After all you cant reach your goal if you are destroyed The concern of evil super intelligences isnt that they literally turn evil and selfish and cruel Those are human qualities Instead a superintelligence that has a certain goal will logically pursue subgoals that help it to reach the ultimate goal Such subgoals will be resources power safety So it will amass power and resources to reach its goal and exterminate any threat to its existence as long as its goal hasnt been reached without being selfish at all
,,"<p>I have some comments on this subject here: <a href=""http://ai.stackexchange.com/a/2878/1671"">http://ai.stackexchange.com/a/2878/1671</a></p>

<p>This is some deep game theoretic stuff, and it partly depends on how you define ""selfishness"".  </p>

<p>There is such a thing, for instance, as a ""<a href=""https://en.wikipedia.org/wiki/Greedy_algorithm"" rel=""nofollow noreferrer"">greedy algorithm</a>"".  Sometimes a greedy algorithm is the most convenient way to achieve an acceptable result, but the optimality is only local. </p>

<p>On a mathematical level, the constructiveness or destructiveness of ""self interest"" in a system may be a function of whether a <a href=""https://en.wikipedia.org/wiki/Nash_equilibrium"" rel=""nofollow noreferrer"">Nash Equilibrium</a> is perceived.  In this case, self-interest is defined as maintaining the current strategy, because, unless the competitor changes their strategy, there is no gain for changing one's own strategy.</p>

<p>As the BlindKungFuMaster importantly notes, competitiveness will evolve as a trait if the AI operates in a partisan context.  </p>

<p>There, the problem comes from whether the ""game"" is zero sum (<a href=""https://en.wikipedia.org/wiki/Pareto_efficiency"" rel=""nofollow noreferrer"">Pareto Optimal</a>) or non zero sum (Pareto Improvable), or both.  Here, ""destructive"" may defined agents made worse off per the gains made by another agent.  </p>

<p>Altruism seems to have a rational basis, and occurs in evolution because it is presumably useful. [<a href=""https://plato.stanford.edu/entries/altruism-biological/"" rel=""nofollow noreferrer"">See Biological Altruism.</a>]</p>

<p>Although this tends to be confined to single species, the co-evolution of dogs and humans is a case for inter-species altruism, based on self interest.</p>

<p>Humans and machines have also, and will continue to, co-evolve.</p>
",,0,2017-03-01T16:55:13.723,,2899,2017-03-01T16:55:13.723,,,,,1671.0,2897.0,2,1,,,,46.06,13.57,10.72,0.0,0.0,60.0,I have some comments on this subject here httpaistackexchangecoma28781671 This is some deep game theoretic stuff and it partly depends on how you define selfishness There is such a thing for instance as a greedy algorithm Sometimes a greedy algorithm is the most convenient way to achieve an acceptable result but the optimality is only local On a mathematical level the constructiveness or destructiveness of self interest in a system may be a function of whether a Nash Equilibrium is perceived In this case selfinterest is defined as maintaining the current strategy because unless the competitor changes their strategy there is no gain for changing ones own strategy As the BlindKungFuMaster importantly notes competitiveness will evolve as a trait if the AI operates in a partisan context There the problem comes from whether the game is zero sum Pareto Optimal or non zero sum Pareto Improvable or both Here destructive may defined agents made worse off per the gains made by another agent Altruism seems to have a rational basis and occurs in evolution because it is presumably useful See Biological Altruism Although this tends to be confined to single species the coevolution of dogs and humans is a case for interspecies altruism based on self interest Humans and machines have also and will continue to coevolve
,4.0,"<p>Post singularity AI will surpass human intelligence. The evolution of AI can take any direction, some of which may not be preferable for humans. Is it possible to manage the evolution of super-intelligent AI? If yes, how? One way I can think of is following. Instead of having a mobile AI like humanoid, we can keep it immobile, like a box, like current super computers. It can be used to solve problems of maths, theoretical science etc.</p>
",,0,2017-03-01T18:53:00.513,1.0,2900,2017-03-11T02:17:56.760,2017-03-02T13:19:55.283,,5765.0,,5765.0,,1,2,<singularity>,Is it possible to manage the evolution of super-intelligent AI?,82.0,60.31,8.8,9.1,0.0,0.0,14.0,Post singularity AI will surpass human intelligence The evolution of AI can take any direction some of which may not be preferable for humans Is it possible to manage the evolution of superintelligent AI If yes how One way I can think of is following Instead of having a mobile AI like humanoid we can keep it immobile like a box like current super computers It can be used to solve problems of maths theoretical science etc
,,"<p>Assuming super-intelligence is possible, the answer is probably yes and no.</p>

<p>Yes in Kurzweil-like scenarios, where super-intelligence is an extension of human beings by technology (we are already in to some extent). Then control follows, as super-intelligence depends on us. It would extend our capabilities, such as speed of processing, extent of processing, etc. Even then control is debatable, as a remote-controlled killing machine would be part of a super-intelligent organism, partially human ""controlled"", partially autonomous.</p>

<p>No in ""Future of Life Institute""-like scenarios, where super-intelligence is independent from humans. The thinking is simple: What can we hope to do facing someone way more intelligent? The usual parallel is to compare this scenario with the arrival of the ""developed"" conquistadors in early America. Gunpowder vs. mere raw strength and arrows.</p>
",,4,2017-03-01T22:31:59.743,,2901,2017-03-07T21:49:37.613,2017-03-07T21:49:37.613,,169.0,,169.0,2900.0,2,2,,,,31.58,15.83,10.34,0.0,0.0,37.0,Assuming superintelligence is possible the answer is probably yes and no Yes in Kurzweillike scenarios where superintelligence is an extension of human beings by technology we are already in to some extent Then control follows as superintelligence depends on us It would extend our capabilities such as speed of processing extent of processing etc Even then control is debatable as a remotecontrolled killing machine would be part of a superintelligent organism partially human controlled partially autonomous No in Future of Life Institutelike scenarios where superintelligence is independent from humans The thinking is simple What can we hope to do facing someone way more intelligent The usual parallel is to compare this scenario with the arrival of the developed conquistadors in early America Gunpowder vs mere raw strength and arrows
,2.0,"<p>One of the argument against possibility of super-intelligent AI is that intelligence of a product will be limited by intelligence of its creator. How reasonable is this argument? </p>
",,0,2017-03-02T12:58:31.380,,2902,2017-03-11T14:14:19.460,,,,,5765.0,,1,4,<strong-ai>,Against possibility of super-intelligent AI,60.0,31.89,13.39,9.97,0.0,0.0,3.0,One of the argument against possibility of superintelligent AI is that intelligence of a product will be limited by intelligence of its creator How reasonable is this argument
,,"<p>Of course the intelligence of a product is limited <strong>by</strong> the intelligence of its creator. Just not <strong>to</strong> the intelligence of its creator. </p>

<p>That would be about as reasonable as the idea that the speed of a car is limited to the speed of its creator. </p>

<p>Or the playing strength of a chess program to the Elo of its creator.</p>

<p>Or the ability of a neural network to differentiate between dozens of dog breeds to the dog expertise of its creator.</p>

<p>So, not very reasonable. </p>
",,4,2017-03-02T14:07:57.043,,2903,2017-03-02T14:07:57.043,,,,,2227.0,2902.0,2,3,,,,73.98,7.36,7.68,0.0,0.0,7.0,Of course the intelligence of a product is limited by the intelligence of its creator Just not to the intelligence of its creator That would be about as reasonable as the idea that the speed of a car is limited to the speed of its creator Or the playing strength of a chess program to the Elo of its creator Or the ability of a neural network to differentiate between dozens of dog breeds to the dog expertise of its creator So not very reasonable
,,"<p>This is not an answer (I don't have enough reputation to comment). I did something close to this in my master's thesis and think it is close to what you are interested in.</p>

<p>In it, I had developed a framework for extracting metadata from web-based educational content. This metadata was used for classifying the the educaitonal content for many different attributes, which could then be used for faster and more efficient search and discovery of educational content. </p>

<p>The educational resources (containing the content) could be anything like text or PDFs of assignments, homework, assignments, online books, exam questions, courses etc (which many colleges host online). To identify what kind of educational resource it is, I would parse the text and look for keywords and formatting styles (preprocessing included constructing 2-grams and 3-grams, POS tagging, using small specific parsers for NER, dates and other text entities one encounters in educational content). </p>

<p>For some part I used Wordnet (also available under python-nltk) to obtain relationships between different entities and also to find closeness between them. DBpedia was also used. However, for the most part I had to identify the most commonly occuring terms and build a taxonomy by hand. (It took a lot of time!). I obtained a lot of candidates for keywords by looking at openly available taxonomies.</p>

<p>For extracting domain specific taxonomy/ontology, one needs manually to build it. Ontology generation from text is an active area of research and building domain-specific ontology has been tried for many years. One example of such taxonomy (here thesaurus) is <a href=""http://aims.fao.org/vest-registry/vocabularies/agrovoc-multilingual-agricultural-thesaurus"" rel=""nofollow noreferrer"">agrovoc</a> where domain experts have contributed to the knowledge by identifying agricultural entities manually. </p>

<p>There are a lot of places where domain specific vocabulary is available; maybe you can use that. In some aspects it is close to supervised machine learning, where one has some nice data and correspondingly nice output. However, on my part, there wasn't much learning in it - more like template matching. </p>

<p>Hope this helps.</p>
",,3,2017-03-02T16:29:14.580,,2904,2017-03-02T16:29:14.580,,,,,5750.0,2891.0,2,3,,,,44.75,13.45,9.79,0.0,0.0,60.0,This is not an answer I dont have enough reputation to comment I did something close to this in my masters thesis and think it is close to what you are interested in In it I had developed a framework for extracting metadata from webbased educational content This metadata was used for classifying the the educaitonal content for many different attributes which could then be used for faster and more efficient search and discovery of educational content The educational resources containing the content could be anything like text or PDFs of assignments homework assignments online books exam questions courses etc which many colleges host online To identify what kind of educational resource it is I would parse the text and look for keywords and formatting styles preprocessing included constructing 2grams and 3grams POS tagging using small specific parsers for NER dates and other text entities one encounters in educational content For some part I used Wordnet also available under pythonnltk to obtain relationships between different entities and also to find closeness between them DBpedia was also used However for the most part I had to identify the most commonly occuring terms and build a taxonomy by hand It took a lot of time I obtained a lot of candidates for keywords by looking at openly available taxonomies For extracting domain specific taxonomyontology one needs manually to build it Ontology generation from text is an active area of research and building domainspecific ontology has been tried for many years One example of such taxonomy here thesaurus is agrovoc where domain experts have contributed to the knowledge by identifying agricultural entities manually There are a lot of places where domain specific vocabulary is available maybe you can use that In some aspects it is close to supervised machine learning where one has some nice data and correspondingly nice output However on my part there wasnt much learning in it more like template matching Hope this helps
,,"<p>AI is frequently used to discover things that would take laborious amounts of time for humans to do.  For example, AI can be used to find the optimal configuration for a mother-board layout, or identify best fit parameters for a financial model.  Frequently, the AI can do a better job at a task and do it more qucikly than a human.  Therefore, in many applications, the AI is already more intelligent than the creator at specific tasks.  </p>

<p>Here are just a few things that AI can already do better than humans:</p>

<ul>
<li>Playing Chess</li>
<li>Playing Jeopardy </li>
<li>Detecting Cancer</li>
</ul>

<p>To argue against the possibility of a super-intelligent AI is somewhat of a moot point since it has already been proven.</p>
",,2,2017-03-02T20:15:51.413,,2905,2017-03-11T14:14:19.460,2017-03-11T14:14:19.460,,1671.0,,1434.0,2902.0,2,1,,,,55.98,10.33,9.36,0.0,0.0,13.0,AI is frequently used to discover things that would take laborious amounts of time for humans to do For example AI can be used to find the optimal configuration for a motherboard layout or identify best fit parameters for a financial model Frequently the AI can do a better job at a task and do it more qucikly than a human Therefore in many applications the AI is already more intelligent than the creator at specific tasks Here are just a few things that AI can already do better than humans Playing Chess Playing Jeopardy Detecting Cancer To argue against the possibility of a superintelligent AI is somewhat of a moot point since it has already been proven
,0.0,"<p>When trying to run tensorboard locally to show my logs with <code>tensorboard --logdir logs/</code> it always shows nothing but the regular tensorboard menu options, such as orange bar at the top, and different section buttons at the top like graphics, etc. however never shows any data regarding my agents. I am using tensorflow 0.11</p>
",,0,2017-03-02T20:53:58.320,,2906,2017-03-02T20:53:58.320,,,,,5801.0,,1,1,<neural-networks><machine-learning><deep-learning><reinforcement-learning>,Tensorboard problems,14.0,71.14,10.15,10.67,26.0,0.0,6.0,When trying to run tensorboard locally to show my logs with it always shows nothing but the regular tensorboard menu options such as orange bar at the top and different section buttons at the top like graphics etc however never shows any data regarding my agents I am using tensorflow 011
,,"<p>Without going into more detail at the moment (b/c I'm time strapped), I strongly urge you to research the <a href=""https://en.wikipedia.org/wiki/AI_control_problem"" rel=""nofollow noreferrer"">Control Problem</a>.</p>

<p><em>My own personal view is that humans are more problematic than machines.  Machines are at least rational.</em></p>

<p>To be more specific, I believe human ""management"" (read as ""mis-management"") of powerful AI is potentially more of a problem than super-intelligent AI left to it's own devices.  </p>

<p>Humans are known to abuse power, and history is filled with such examples.  Machines, at least, have  a clean slate in this regard.</p>
",,4,2017-03-02T22:22:18.233,,2907,2017-03-02T22:41:14.743,2017-03-02T22:41:14.743,,1671.0,,1671.0,2900.0,2,0,,,,56.45,11.13,10.23,0.0,0.0,24.0,Without going into more detail at the moment bc Im time strapped I strongly urge you to research the Control Problem My own personal view is that humans are more problematic than machines Machines are at least rational To be more specific I believe human management read as mismanagement of powerful AI is potentially more of a problem than superintelligent AI left to its own devices Humans are known to abuse power and history is filled with such examples Machines at least have a clean slate in this regard
,2.0,"<p>Is it possible to classify data using a genetic algorithm?
For example, would it be possible to sort this database?
( <a href=""https://archive.ics.uci.edu/ml/datasets/Spambase"" rel=""nofollow noreferrer"">https://archive.ics.uci.edu/ml/datasets/Spambase</a> )</p>

<p>Examples in matlab?</p>
",,2,2017-03-03T03:36:17.760,,2908,2017-03-04T12:53:13.280,,,,,5806.0,,1,3,<algorithm><genetic-algorithms><genetic-programming>,Genetic algorithm,55.0,21.06,20.03,11.27,0.0,0.0,15.0,Is it possible to classify data using a genetic algorithm For example would it be possible to sort this database httpsarchiveicsuciedumldatasetsSpambase Examples in matlab
,,"<p>Competition always gives better result. If machines will try to improve themselves, we as human beings will definitely try to improve ourself.</p>
",,0,2017-03-03T06:25:30.740,,2909,2017-03-03T06:25:30.740,,,,,5809.0,2900.0,2,1,,,,43.39,13.44,12.08,0.0,0.0,3.0,Competition always gives better result If machines will try to improve themselves we as human beings will definitely try to improve ourself
,1.0,"<p>I have not studied machine learning or AI really, but my job sometimes requires me to automate stuff. Right now the requirement I have, seems to be under AI domain, but I am not sure about terminologies or how to go about it. I will really appreciate if someone can guide me about the direction I need to start from.
<br/>(PS: This question might not belong on this SE, in that case please direct me to suitable SE)</p>

<p>What I'm required to do is <strong>find references on web about a certain situation</strong>. As an example I'll use ""Music"", so I have to make a system which will search around the web (Google and Twitter mainly) to see if there is any news/mention/event related to Music that occurred today, if so how many references (i.e. how big of a deal it is making).
<br/>It is not the generic term music which is expected in the output, but the names of Musicians, i.e. in Music this and this Artist appeared this many times. 
<br/>I have to give the number of references, and also provide the references in output so that one can read them in detail.</p>

<p><strong><em>The challenges are</em></strong>
    <br/>
- One event can be covered by many websites, and there can be one main website that published the original story with full details, while others just spread the word around in summarized way. 
<br/><strong><em>How do you filter references to pick the most suitable one</em></strong>, to show in results to the system's user, because I can not give user ~50 references to manually read through, I have to give like 1-2 suitable reference
<br/>- I need to give the name of the artist. One site will have many words, how do I know which word is actually the artist's name? One option can be to have a pre compiled list of specific artists and just search for them individually. But this way, I can be missing new artists. </p>

<p>The challenges I have, <strong>must have been addressed by some existing algorithm</strong> or mechanism, I'll appreciate if someone can let me know what kind of algo etc I need to refer to or study to get the task done.</p>
",,0,2017-03-03T10:43:29.217,,2910,2017-03-03T19:21:27.827,,,,,5814.0,,1,1,<ai-design><algorithm><definitions><intelligent-agent><reference-request>,Searching keywords on web,28.0,63.93,8.13,8.8,0.0,0.0,55.0,I have not studied machine learning or AI really but my job sometimes requires me to automate stuff Right now the requirement I have seems to be under AI domain but I am not sure about terminologies or how to go about it I will really appreciate if someone can guide me about the direction I need to start from PS This question might not belong on this SE in that case please direct me to suitable SE What Im required to do is find references on web about a certain situation As an example Ill use Music so I have to make a system which will search around the web Google and Twitter mainly to see if there is any newsmentionevent related to Music that occurred today if so how many references ie how big of a deal it is making It is not the generic term music which is expected in the output but the names of Musicians ie in Music this and this Artist appeared this many times I have to give the number of references and also provide the references in output so that one can read them in detail The challenges are One event can be covered by many websites and there can be one main website that published the original story with full details while others just spread the word around in summarized way How do you filter references to pick the most suitable one to show in results to the systems user because I can not give user 50 references to manually read through I have to give like 12 suitable reference I need to give the name of the artist One site will have many words how do I know which word is actually the artists name One option can be to have a pre compiled list of specific artists and just search for them individually But this way I can be missing new artists The challenges I have must have been addressed by some existing algorithm or mechanism Ill appreciate if someone can let me know what kind of algo etc I need to refer to or study to get the task done
2916.0,1.0,"<p>This has been niggling me a while, so I decided to ask. Sorry if it's wordy, I'm not sure how to express it!</p>

<p>It seems fairly uncontroversial to say that NN based approaches are becoming quite powerful tools in many AI areas - whether recognising and decomposing images (faces at a border, street scenes in automobiles, decision making in uncertain/complex situations or with partial data)..... almost inevitably some of those uses will develop into situations where NN based AI takes on part or all of the human burden and generally does it better than people generally do.</p>

<p>Examples might include NN hypothetically used as steps in self driving cars, medical diagnosis, human/identity verification, circuit/design verification, dubious transaction alerting ... probably many fields in the next decade or so. </p>

<p>Suppose this happens, and is generally seen as successful (eg it gets diagnoses right 80% to human doctors' 65% or something, or cars with AI that includes an NN component crash 8% less than human driven cars or alternatives, or whatever...)</p>

<p>Now - suppose one of these aberrantly and seriously does something very wrong in one case. How can one approach it? With formal logic steps one can trace a formal decision process, but with NN there may be no formal logic, especially if it gets complex enough (in a couple of decades say), there are just 20 billion neural processors and their I/O weightings and connections, it may not be possible to determine what caused some incident even if lives were lost. It also may not be possible to say more than the systems continually learn and such incidents are rare. </p>

<p>I also haven't heard of any meaningful way to do a ""black box"" or flight recorder equivalent for NNs, (even if not used i  a life critical case), that would allow us to understand and avoid a bad decision. Unlike other responses to product defects, if a NN could be trained after the event to fix one such case, it doesn't clearly provide the certainty we would want, that the new NN setup has fixed the problem, or hasn't reduced the risk and balance of other problems in so doing. It's just very opaque. And yet, clearly, it is mostly very valuable as an AI approach.</p>

<p>So what's the answer? Is there one? In 20 years if NN is an (acknowledged as safe and successful) component in a plane flight or aircraft design, or built into a hospital system to watch for emergencies, or to spot fraud at a bank, and has as usual passed whatever regulatory and market requirements might exist and performed with a good record for years in the general marketplace, <em>and</em> then in one case such a system some time later plainly mis-acts on one occasion - it damgerously misreads the road, recommends life-damaging medications or blatantly missdiagnoses, or clears a blatant £200m fraudulent transaction at a clearing bank that's only caught by chance before the money is sent - what can the manufacturer do to address public or market concerns, or to explain the incident; what do the tech team do when told by the board ""how did this happen and make damn sure it's fixed""; what kind of meaningful logs can be kept, etc? Would society have to just accept that uncertainty and occasional wacky behaviour could be inherent (good luck with convincing society of that!)? Or is there some better way to approach logging/debugging/decision activity more suited to NNs?</p>
",,0,2017-03-03T11:08:49.327,1.0,2911,2017-03-03T22:53:41.833,2017-03-03T11:31:16.643,,5817.0,,5817.0,,1,6,<neural-networks><machine-learning><applications><security>,"If a neural network approach becomes widely used within a real-world situation, how would one debug/understand/fix the outcome if in one case poor?",36.0,50.91,11.32,10.12,0.0,0.0,103.0,This has been niggling me a while so I decided to ask Sorry if its wordy Im not sure how to express it It seems fairly uncontroversial to say that NN based approaches are becoming quite powerful tools in many AI areas whether recognising and decomposing images faces at a border street scenes in automobiles decision making in uncertaincomplex situations or with partial data almost inevitably some of those uses will develop into situations where NN based AI takes on part or all of the human burden and generally does it better than people generally do Examples might include NN hypothetically used as steps in self driving cars medical diagnosis humanidentity verification circuitdesign verification dubious transaction alerting probably many fields in the next decade or so Suppose this happens and is generally seen as successful eg it gets diagnoses right 80 to human doctors 65 or something or cars with AI that includes an NN component crash 8 less than human driven cars or alternatives or whatever Now suppose one of these aberrantly and seriously does something very wrong in one case How can one approach it With formal logic steps one can trace a formal decision process but with NN there may be no formal logic especially if it gets complex enough in a couple of decades say there are just 20 billion neural processors and their IO weightings and connections it may not be possible to determine what caused some incident even if lives were lost It also may not be possible to say more than the systems continually learn and such incidents are rare I also havent heard of any meaningful way to do a black box or flight recorder equivalent for NNs even if not used i a life critical case that would allow us to understand and avoid a bad decision Unlike other responses to product defects if a NN could be trained after the event to fix one such case it doesnt clearly provide the certainty we would want that the new NN setup has fixed the problem or hasnt reduced the risk and balance of other problems in so doing Its just very opaque And yet clearly it is mostly very valuable as an AI approach So whats the answer Is there one In 20 years if NN is an acknowledged as safe and successful component in a plane flight or aircraft design or built into a hospital system to watch for emergencies or to spot fraud at a bank and has as usual passed whatever regulatory and market requirements might exist and performed with a good record for years in the general marketplace and then in one case such a system some time later plainly misacts on one occasion it damgerously misreads the road recommends lifedamaging medications or blatantly missdiagnoses or clears a blatant £200m fraudulent transaction at a clearing bank thats only caught by chance before the money is sent what can the manufacturer do to address public or market concerns or to explain the incident what do the tech team do when told by the board how did this happen and make damn sure its fixed what kind of meaningful logs can be kept etc Would society have to just accept that uncertainty and occasional wacky behaviour could be inherent good luck with convincing society of that Or is there some better way to approach loggingdebuggingdecision activity more suited to NNs
,0.0,"<p>I've been experimenting with a simple tic-tac-toe game to learn neural network programming (MLP and CNNs) with good results. I train the networks on a board positions and the best moves and the network is able to learn and correctly predict the best moves to make when it encounters those board positions.</p>

<p>But the network is unable to ""discover"" newer patterns/features from existing ones. For example -</p>

<p>Let's say that the board position is below and move is for the X player (AI)</p>

<pre><code>O  _  _

_  O  _

_  _  _
</code></pre>

<p>The recommended move would be 8 (0 based indices) so that the opponent doesn't win, the resulting board would be - </p>

<pre><code>O  _  _

_  O  _

_  _  X
</code></pre>

<p>If I train the network on the above enough times, the AI (MLP or CNN based) learns to play 8 when it encounters the above situation. </p>

<p>But it doesn't recognize the below as variations (rotated and shifted, respectively but slanted straight lines in general) of the same pattern and is not able to correctly pick 6 and 0, respectively -</p>

<pre><code>_  _  O            _  _  _

_  O  _     or     _  O  _   etc

_  _  _            _  _  O
</code></pre>

<p>My question is - Should I expect CNNs to be able to discover new previously untrained on patterns/features such as above? </p>
",,6,2017-03-03T13:01:56.950,,2912,2017-03-03T13:01:56.950,,,,,1522.0,,1,1,<neural-networks><conv-neural-network>,Can a CNN or MLP discover similar but untrained-on patterns?,17.0,51.65,11.09,9.71,143.0,0.0,33.0,Ive been experimenting with a simple tictactoe game to learn neural network programming MLP and CNNs with good results I train the networks on a board positions and the best moves and the network is able to learn and correctly predict the best moves to make when it encounters those board positions But the network is unable to discover newer patternsfeatures from existing ones For example Lets say that the board position is below and move is for the X player AI The recommended move would be 8 0 based indices so that the opponent doesnt win the resulting board would be If I train the network on the above enough times the AI MLP or CNN based learns to play 8 when it encounters the above situation But it doesnt recognize the below as variations rotated and shifted respectively but slanted straight lines in general of the same pattern and is not able to correctly pick 6 and 0 respectively My question is Should I expect CNNs to be able to discover new previously untrained on patternsfeatures such as above
,,"<p>It is possible, but is a pretty terrible idea.</p>

<p>There are a few options. One is to not use the GA as a direct classifier, but instead use a GA to learn the parameters of another classification model like a neural network. The basic idea of a GA is that it (very roughly speaking) forms a black-box method for searching an arbitrary space for solutions that minimize or maximize some function.</p>

<p>Here, you would be searching the space of possible neural network topologies and/or weights to find one that minimizes the misclassification rate.</p>

<p>Another approach is that taken by what are sometimes called Learning Classifier Systems (LCS) or Genetics Based Machine Learning (GBML). This approach is to use evolutionary mechanics to evolve rule sets of the form ""if X condition is true, then do/classify Y"". That's a more direct method of solving this sort of problem. You define some features on your dataset, and the algorithm tries to learn rules based on those features.</p>

<p>The problem with any of these approaches is just that there are so many better ways to solve the problem. Remember, a GA is basically a black-box that's supposed to work acceptably well for a huge range of unknown problems. But I'm not solving a huge range of unknown problems. I'm trying to separate ham from spam on one dataset. I can come up with methods that simply do that job better and more quickly than a GA has any real hope of doing.</p>
",,0,2017-03-03T15:36:36.527,,2913,2017-03-03T15:36:36.527,,,,,3365.0,2908.0,2,0,,,,61.97,9.63,9.29,0.0,0.0,36.0,It is possible but is a pretty terrible idea There are a few options One is to not use the GA as a direct classifier but instead use a GA to learn the parameters of another classification model like a neural network The basic idea of a GA is that it very roughly speaking forms a blackbox method for searching an arbitrary space for solutions that minimize or maximize some function Here you would be searching the space of possible neural network topologies andor weights to find one that minimizes the misclassification rate Another approach is that taken by what are sometimes called Learning Classifier Systems LCS or Genetics Based Machine Learning GBML This approach is to use evolutionary mechanics to evolve rule sets of the form if X condition is true then doclassify Y Thats a more direct method of solving this sort of problem You define some features on your dataset and the algorithm tries to learn rules based on those features The problem with any of these approaches is just that there are so many better ways to solve the problem Remember a GA is basically a blackbox thats supposed to work acceptably well for a huge range of unknown problems But Im not solving a huge range of unknown problems Im trying to separate ham from spam on one dataset I can come up with methods that simply do that job better and more quickly than a GA has any real hope of doing
,,"<p>A parallel situation might be that of spam/not spam. The detection of spam by AI has been pretty successful, so there is an existing algorithm - classification. However while you have a possible <em>approach</em> you are still missing the key ingredient which is sufficient data to train the model on.</p>

<p>AI depends on a large amount of data to train the model. Ideally you will have a team of researchers available to read a (large?) number of sources and classify by hand whether the source is the original reference or just a repeater, and whether the topic is relevant. The more labelled and balanced data you have, the better the model and the better the results. Then, like the spam/not spam situation, you just apply your model and the best references pop out. You already have the key word of music, so you just use as candidates those sources that reference ""music"" and any other defining keywords.</p>
",,0,2017-03-03T19:21:27.827,,2914,2017-03-03T19:21:27.827,,,,,4994.0,2910.0,2,0,,,,62.38,9.92,9.35,0.0,0.0,22.0,A parallel situation might be that of spamnot spam The detection of spam by AI has been pretty successful so there is an existing algorithm classification However while you have a possible approach you are still missing the key ingredient which is sufficient data to train the model on AI depends on a large amount of data to train the model Ideally you will have a team of researchers available to read a large number of sources and classify by hand whether the source is the original reference or just a repeater and whether the topic is relevant The more labelled and balanced data you have the better the model and the better the results Then like the spamnot spam situation you just apply your model and the best references pop out You already have the key word of music so you just use as candidates those sources that reference music and any other defining keywords
,0.0,"<p>Here's a theoretical AI program. It's an API that takes percentages from various observations to draw conclusions based on probability. So, for language, it has percentages based on how often each word appears each word away from each other. To associate word meanings, it would take percentages based on every item in the room in relation to where your eyes, finger, and body were pointing. If it could read emotions, it could associate those with words as well. Then, when thinking of words to say, it would look at its emotions and environment. </p>
",,0,2017-03-03T19:39:16.433,,2915,2017-03-03T19:39:16.433,,,,,5526.0,,1,0,<machine-learning><ai-design><strong-ai><natural-language><probabilistic>,Can an advanced learning AI run entirely on probability and association?,20.0,64.2,10.79,9.16,0.0,0.0,16.0,Heres a theoretical AI program Its an API that takes percentages from various observations to draw conclusions based on probability So for language it has percentages based on how often each word appears each word away from each other To associate word meanings it would take percentages based on every item in the room in relation to where your eyes finger and body were pointing If it could read emotions it could associate those with words as well Then when thinking of words to say it would look at its emotions and environment
,,"<p>If the observation that the neural network saw was recorded, then yes the prediction can be explained. There was a paper written fairly recently on this topic called <em>why should I trust you explaining the predictions of any classifier</em> in this paper the author described an algorithm called LIME which is able to explain any machine learning models predictions. It can be used to establish why a machine learning model made a prediction, help a data scientist debug a model, and help a data scientist improve the accuracy of a specific model. LIME can be used to explain the predictions of any neural network including CNNs, RNNs, and DNNs. </p>
",,2,2017-03-03T22:53:41.833,,2916,2017-03-03T22:53:41.833,,,,,4631.0,2911.0,2,4,,,,52.33,11.2,10.49,0.0,0.0,9.0,If the observation that the neural network saw was recorded then yes the prediction can be explained There was a paper written fairly recently on this topic called why should I trust you explaining the predictions of any classifier in this paper the author described an algorithm called LIME which is able to explain any machine learning models predictions It can be used to establish why a machine learning model made a prediction help a data scientist debug a model and help a data scientist improve the accuracy of a specific model LIME can be used to explain the predictions of any neural network including CNNs RNNs and DNNs
,2.0,"<blockquote>
  <p>Does it exist a human-like or overintelligent AI? </p>
</blockquote>

<p>Human-like I define as something that can act as a human in most aspects.</p>

<p>For example, is it ""common knowledge"" that there actually exists an overintelligent or human-like AI? Or could you say that there do not exist an overintelligent or human-like AI?</p>
",,0,2017-03-04T10:30:12.610,,2917,2017-03-11T23:39:05.963,2017-03-04T12:18:17.803,,5832.0,,5832.0,,1,1,<research><intelligent-agent><emotional-intelligence><human-like><ultraintelligent-machine>,Does it exist a human-like or overintelligent AI?,48.0,50.02,10.6,8.3,0.0,0.0,11.0,Does it exist a humanlike or overintelligent AI Humanlike I define as something that can act as a human in most aspects For example is it common knowledge that there actually exists an overintelligent or humanlike AI Or could you say that there do not exist an overintelligent or humanlike AI
,,"<p>I agree with @deong. But you must understand that a genetic algorithm is an optimization algorithm; you can't feed it e-mails and make it classify spam. A genetic algorithm is used to train 'something' to classify spam. That something could be <strong>neural networks</strong>.</p>

<p>What you need is a genetic algorithm that optimizes neural networks <a href=""https://en.wikipedia.org/wiki/Neuroevolution"" rel=""nofollow noreferrer"">Neuroevolution</a>. </p>

<pre><code>-&gt; Start with a pool of neural networks
-&gt; Feed them e-mails, let them classify, and calculate fitness on % correct
-&gt; Select neural networks for crossover
-&gt; Crossover
-&gt; Mutate
</code></pre>

<p>But just like @deong says, there are better ways for classifying e-mails (e.g. an algorithm that looks for certain 'spam-words'). </p>

<p>But it is definitely possible. I have a <a href=""https://github.com/wagenaartje/gynaptic"" rel=""nofollow noreferrer"">javascript library</a> set up for Neuroevolution, if you're interested.</p>
",,0,2017-03-04T12:53:13.280,,2918,2017-03-04T12:53:13.280,,,,,5344.0,2908.0,2,0,,,,61.12,12.21,8.43,194.0,0.0,26.0,I agree with deong But you must understand that a genetic algorithm is an optimization algorithm you cant feed it emails and make it classify spam A genetic algorithm is used to train something to classify spam That something could be neural networks What you need is a genetic algorithm that optimizes neural networks Neuroevolution But just like deong says there are better ways for classifying emails eg an algorithm that looks for certain spamwords But it is definitely possible I have a javascript library set up for Neuroevolution if youre interested
2931.0,1.0,"<p>From <em>Artificial Intelligence: A Modern Approach</em>, Third Edition, Chapter 26:</p>

<blockquote>
  <p>Note that the concept of ultraintelligent machines assumes that intelligence is an especially important attribute, and if you have enough of it, all problems can be solved. But we know there are limits on computability and computational complexity. If the problem of defining ultraintelligent machines (or even approximations to them) happens to fall in the class of, say, NEXPTIME-complete problems, and if there are no heuristic shortcuts, then even exponential progress in technology won't help—the speed of light puts a strict upper bound on how much computing can be done; problems beyond that limit will not be solved. We still don't know where those upper bounds are.</p>
</blockquote>

<p>If the textbook's argument is correct, then there may be a strict upper bound to ""intelligence"", meaning that the potential/damage of ultra-intelligent machines is limited. However, it is contingent on there actually being a theoretical maximum for ""intelligence"".</p>

<p>Is there any literature that suggest that we know for sure whether such a maximum exist? Is the existence of that maximum dependent on our definition of ""intelligence"" (so adopting a vague and hand-wavey definition would imply no theoretical maximum, while adopting a strict and formalized definition would imply a theoretical maximum)?</p>

<p>Note: Question was previously posted during <a href=""http://area51.stackexchange.com/proposals/93481/artificial-intelligence/97028#97028"">the definition phase of this site</a> on Area51 by <a href=""http://area51.stackexchange.com/users/94486/pkhlop"">pkhlop</a>.</p>
",,1,2017-03-04T15:16:46.440,,2919,2017-03-06T16:32:17.700,,,,,181.0,,1,7,<definitions><ultraintelligent-machine>,Is there a theoretical maximum for intelligence?,104.0,37.84,14.97,9.89,0.0,0.0,42.0,From Artificial Intelligence A Modern Approach Third Edition Chapter 26 Note that the concept of ultraintelligent machines assumes that intelligence is an especially important attribute and if you have enough of it all problems can be solved But we know there are limits on computability and computational complexity If the problem of defining ultraintelligent machines or even approximations to them happens to fall in the class of say NEXPTIMEcomplete problems and if there are no heuristic shortcuts then even exponential progress in technology wont help—the speed of light puts a strict upper bound on how much computing can be done problems beyond that limit will not be solved We still dont know where those upper bounds are If the textbooks argument is correct then there may be a strict upper bound to intelligence meaning that the potentialdamage of ultraintelligent machines is limited However it is contingent on there actually being a theoretical maximum for intelligence Is there any literature that suggest that we know for sure whether such a maximum exist Is the existence of that maximum dependent on our definition of intelligence so adopting a vague and handwavey definition would imply no theoretical maximum while adopting a strict and formalized definition would imply a theoretical maximum Note Question was previously posted during the definition phase of this site on Area51 by pkhlop
,2.0,"<p>I am looking for a solution that I can use with identifying cars.
So I have a database with images of cars. About 3-4 per car. What I want to do is upload a picture to the web of car(Picture taken with camera/phone) and then let my pc recognize the car. </p>

<p>Example: 
Lets say I have these 2 pictures in my database(Mazda cx5)(I can only upload 2 links at max. atm. but you get the idea).
<a href=""https://i.stack.imgur.com/NsLow.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NsLow.png"" alt=""First car""></a></p>

<p>Now I am going to upload this picture of a mazda cs5 to my web app:
<a href=""https://i.stack.imgur.com/psHD6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/psHD6.png"" alt=""Picture of mazda cs5""></a></p>

<p>Now I want an AI to recognize that this picture is of an Mazda CX5 with greyish color. I have looked on the net and found 2 interesting AI's I can use:
Tensorflow and Clarifai, but I don't know if these are going to work so my question to you what would be my best bet to go with here?</p>
",,0,2017-03-04T18:27:37.373,,2920,2017-03-07T01:17:16.227,,,,,5837.0,,1,5,<image-recognition><tensorflow>,Image recognition,67.0,77.57,5.11,7.18,0.0,0.0,23.0,I am looking for a solution that I can use with identifying cars So I have a database with images of cars About 34 per car What I want to do is upload a picture to the web of carPicture taken with cameraphone and then let my pc recognize the car Example Lets say I have these 2 pictures in my databaseMazda cx5I can only upload 2 links at max atm but you get the idea Now I am going to upload this picture of a mazda cs5 to my web app Now I want an AI to recognize that this picture is of an Mazda CX5 with greyish color I have looked on the net and found 2 interesting AIs I can use Tensorflow and Clarifai but I dont know if these are going to work so my question to you what would be my best bet to go with here
,,"<p>There are several ways you can do this. One way would involve several steps and would probably work best:</p>

<ol>
<li>Use a trained Gaussian detector to filter out the car from the rest of the image</li>
<li>Use a convolutional neural network to classify the car</li>
<li>Use a neural network or a simple most common color algorithm to find the color of the car</li>
</ol>

<p>You would be able to implement this method most easily in MATLAB but you would also be able to do it in python with tensorflow or torch. You would probably be able to implement the trained Gaussian detector in tensorflow.</p>

<p>Method 2:</p>

<ol>
<li>Use a spatial transformer network to ""transform"" the image of the car for easy classification</li>
<li>Use the output of the spatial transformer network for classification via a convolutional neural network.</li>
<li>Use another neural network or a most common color algorithm to find the color of the car.</li>
</ol>

<p>This method would also work pretty well but using a spatial transformer network with a convolutional neural network may be hard to code because it is an area of developing research where there are many problems because the spatial transformer network and the convolutional neural network have to work well together and this is usually hard to get right.</p>

<p>Method 3:</p>

<ol>
<li>Use a convolutional neural network straight up on the input image maybe with down sampling to classify the car</li>
<li>Use another neural network to find the color of the car</li>
</ol>

<p>I would personally go with method #1 because it would be fairly simple to implement with existing libraries such as tensorflow and it would most likely provide a high accuracy. </p>

<p>As always I would also recommend that you use LIME during the development process to debug your model and determine what features you could add in or remove to help your model perform better.</p>

<p>**Edit*
Since you need to detect certain patterns on the cars for color classification I would recommend that you use a convolutional neural network to classify these patterns. So your method would now look like this:<br>
1. Use a spatial transformer network or a filtered Gaussian detector to filter out the car
2. Use a convolutional neural network to classify the make and model of the car.
3.  Use another neural network that has either a convolutional or deep architecture to classify patterns and solid colors. So the outputs would contain all of the colors that you want and all of the patterns that you want to detect.</p>
",,3,2017-03-04T19:09:21.750,,2921,2017-03-07T01:17:16.227,2017-03-07T01:17:16.227,,4631.0,,4631.0,2920.0,2,2,,,,49.89,10.8,8.12,0.0,0.0,25.0,There are several ways you can do this One way would involve several steps and would probably work best Use a trained Gaussian detector to filter out the car from the rest of the image Use a convolutional neural network to classify the car Use a neural network or a simple most common color algorithm to find the color of the car You would be able to implement this method most easily in MATLAB but you would also be able to do it in python with tensorflow or torch You would probably be able to implement the trained Gaussian detector in tensorflow Method 2 Use a spatial transformer network to transform the image of the car for easy classification Use the output of the spatial transformer network for classification via a convolutional neural network Use another neural network or a most common color algorithm to find the color of the car This method would also work pretty well but using a spatial transformer network with a convolutional neural network may be hard to code because it is an area of developing research where there are many problems because the spatial transformer network and the convolutional neural network have to work well together and this is usually hard to get right Method 3 Use a convolutional neural network straight up on the input image maybe with down sampling to classify the car Use another neural network to find the color of the car I would personally go with method 1 because it would be fairly simple to implement with existing libraries such as tensorflow and it would most likely provide a high accuracy As always I would also recommend that you use LIME during the development process to debug your model and determine what features you could add in or remove to help your model perform better Edit Since you need to detect certain patterns on the cars for color classification I would recommend that you use a convolutional neural network to classify these patterns So your method would now look like this 1 Use a spatial transformer network or a filtered Gaussian detector to filter out the car 2 Use a convolutional neural network to classify the make and model of the car 3 Use another neural network that has either a convolutional or deep architecture to classify patterns and solid colors So the outputs would contain all of the colors that you want and all of the patterns that you want to detect
,0.0,"<p>Nowadays Artificial Intelligence seems almost equal to machine learning(especially deep learning). Some said deep learning will replace experts who were once very important in the old days(for feature engineering) in this field. It is said that two breakthroughs underlie the raise of deep learning: on the one hand, in neuralscience <a href=""https://en.wikipedia.org/wiki/Neuroplasticity"" rel=""nofollow noreferrer"">neuroplasticity</a> tells us that like human brain which is highly plastic, artificial networks can be utilized to model almost all functions; on the other hand, the hardware is upgraded very quickly, in particular the introduction of GPU boosts the machine in a magnificent way, resulting in the models created decades ago becoming immensely powerful and versatile.  </p>

<p>Such developments bring computer vision into a new era, but in natural language processing and expert system the situation seems don't change very much. Neural networks can be harnessed in building knowledge base but it seems that hardly can the built knowledge base be used by neural network models. My questions are: 1) is knowledge base(for instance knowledge graph coined by Google) a promising branch in AI? If so, in what ways KB can empower machine learning? And how can it help in natural language generation? 2) for survival in an age dominated by DL where is the direction for the knowledge base? Is <a href=""http://www.wolfram.com/"" rel=""nofollow noreferrer"">Wolfram</a>-like dynamic knowledge base the new direction? Or any new directions?</p>

<p>Hoping that I am asking an appropriate question here(I even can not tag my question as ""knowledge base"" so far). </p>

<p>Am I horribly missing something happening?</p>
",,4,2017-03-05T07:07:25.330,1.0,2922,2017-03-10T04:50:27.497,2017-03-10T04:50:27.497,,5351.0,,5351.0,,1,3,<nlp><knowledge-representation><expert-system>,I wonder what roles the knowledge base plays now and will play in the future?,67.0,52.19,13.11,10.52,0.0,0.0,37.0,Nowadays Artificial Intelligence seems almost equal to machine learningespecially deep learning Some said deep learning will replace experts who were once very important in the old daysfor feature engineering in this field It is said that two breakthroughs underlie the raise of deep learning on the one hand in neuralscience neuroplasticity tells us that like human brain which is highly plastic artificial networks can be utilized to model almost all functions on the other hand the hardware is upgraded very quickly in particular the introduction of GPU boosts the machine in a magnificent way resulting in the models created decades ago becoming immensely powerful and versatile Such developments bring computer vision into a new era but in natural language processing and expert system the situation seems dont change very much Neural networks can be harnessed in building knowledge base but it seems that hardly can the built knowledge base be used by neural network models My questions are 1 is knowledge basefor instance knowledge graph coined by Google a promising branch in AI If so in what ways KB can empower machine learning And how can it help in natural language generation 2 for survival in an age dominated by DL where is the direction for the knowledge base Is Wolframlike dynamic knowledge base the new direction Or any new directions Hoping that I am asking an appropriate question hereI even can not tag my question as knowledge base so far Am I horribly missing something happening
,,"<h3>Model of the car</h3>

<p>What you want to do is close to one-shot image recognition. You have not 1, but 3-4 examples of each car, but that is still a small amount, especially considering the car looks different from different angles (are you supposed to recognize them from any point of view, including sideways, rear, front, and 45 degrees etc.? maybe you also want to recognize them photographed from the top?).</p>

<p>One interesting article I found is: <a href=""http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"" rel=""nofollow noreferrer"">Siamese Neural Networks for One-shot Image Recognition</a> by Koch, Zemel, and Salakhutdinov.</p>

<p>I also found that <a href=""http://caffe.berkeleyvision.org/"" rel=""nofollow noreferrer"">Caffee</a> supports Siamese networks.</p>

<p>You may want to read other literature about the <a href=""https://en.wikipedia.org/wiki/One-shot_learning"" rel=""nofollow noreferrer"">One-shot learning</a>.</p>

<p>One trick you can do is to utilize the fact that cars are symmetric, so you can double the number of learning examples by reflecting each image.</p>

<h3>Color</h3>

<p>Determining the color is not as simple as it seems. Your algorithm need to determine where is the car at your picture, and then determine the color, taking into account the lighting conditions, as well as light effects, most notably reflection. For example, consider the following image: <a href=""https://i.stack.imgur.com/JQae1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JQae1.png"" alt=""""></a>.</p>

<p>We see strawberries as red, but there are no red pixels on this picture. The images of strawberries consist on grey pixels.</p>

<p>Maybe you also need a convolutional neural network, or just a neural network, for this task.</p>
",,0,2017-03-05T18:47:16.807,,2923,2017-03-05T22:36:33.930,2017-03-05T22:36:33.930,,5852.0,,5852.0,2920.0,2,2,,,,62.68,10.96,9.48,0.0,0.0,41.0,Model of the car What you want to do is close to oneshot image recognition You have not 1 but 34 examples of each car but that is still a small amount especially considering the car looks different from different angles are you supposed to recognize them from any point of view including sideways rear front and 45 degrees etc maybe you also want to recognize them photographed from the top One interesting article I found is Siamese Neural Networks for Oneshot Image Recognition by Koch Zemel and Salakhutdinov I also found that Caffee supports Siamese networks You may want to read other literature about the Oneshot learning One trick you can do is to utilize the fact that cars are symmetric so you can double the number of learning examples by reflecting each image Color Determining the color is not as simple as it seems Your algorithm need to determine where is the car at your picture and then determine the color taking into account the lighting conditions as well as light effects most notably reflection For example consider the following image We see strawberries as red but there are no red pixels on this picture The images of strawberries consist on grey pixels Maybe you also need a convolutional neural network or just a neural network for this task
,0.0,"<p>I know I've seen this somewhere before, but can't find it now.  Say we have a neural network with a handful of layers, and we're applying dropout to each layer.  As we move closer to the output, should dropout decrease, increase, or stay the same?</p>
",,0,2017-03-05T20:25:02.817,,2924,2017-03-05T20:25:02.817,,,,,5857.0,,1,0,<neural-networks>,How should dropout change with network depth?,7.0,81.63,8.05,8.24,0.0,0.0,11.0,I know Ive seen this somewhere before but cant find it now Say we have a neural network with a handful of layers and were applying dropout to each layer As we move closer to the output should dropout decrease increase or stay the same
2935.0,1.0,"<p>I want to write a program that looks at abbreviated words, then figures out what the words are. For example, the abbreviation is ""blk comp"", and the translation is ""black computer"". </p>

<p>In order to give it context for more ambiguous terms, I will be inputting sets of words with each request. So, if I input the set ""keyboard, software, mouse, monitor"", I would expect to get ""black computer"". On the other hand, if I input ""Honda, transmission, mileage, Ford"", I then would expect to get ""black compact"", or at least something that has anything to do with cars. </p>

<p>Basing on the above case scenario, what kind of an algorithm should be applied in this case?</p>
",,0,2017-03-06T00:28:31.647,,2925,2017-03-11T14:15:30.310,2017-03-11T14:15:30.310,,1581.0,,5860.0,,1,2,<neural-networks><deep-learning><algorithm><genetic-algorithms><learning-algorithms>,Recognition of abbreviated text,57.0,68.91,9.81,8.57,0.0,0.0,34.0,I want to write a program that looks at abbreviated words then figures out what the words are For example the abbreviation is blk comp and the translation is black computer In order to give it context for more ambiguous terms I will be inputting sets of words with each request So if I input the set keyboard software mouse monitor I would expect to get black computer On the other hand if I input Honda transmission mileage Ford I then would expect to get black compact or at least something that has anything to do with cars Basing on the above case scenario what kind of an algorithm should be applied in this case
,1.0,"<p>My situation : my CSV file contains data like ""ReadHIT,ReadMiss,WriteHit,WriteMiss,CacheUsage and few more attributes "" all based on performance of a workload on a cache and source for every particular interval of times .</p>

<p>I want any algorithm(predictive machine learning mostly) to identify pattern in my CSV file without user specifying any conditions and any prior patterns.</p>

<p>MY input to algorithm will be only my CSV file.</p>

<p>That pattern the algorithm identifies should be helpful in these ways:</p>

<ul>
<li>help user to increase the performance of the workload next time .</li>
<li>help to identify any problems based any particular attributes in CSV data .</li>
<li>help user to predict future data values or future events that may occur based on pattern.</li>
</ul>

<p>Even if algorithm is able to do any one of the above , i would be glad to know for now.</p>

<p>thank you</p>
",,0,2017-03-06T07:42:26.063,,2926,2017-03-08T08:16:42.453,,,,,5867.0,,1,1,<machine-learning><deep-learning><algorithm><self-learning>,I want to know any machine learning algorithm that does following things?,63.0,43.32,11.89,9.48,0.0,0.0,18.0,My situation my CSV file contains data like ReadHITReadMissWriteHitWriteMissCacheUsage and few more attributes all based on performance of a workload on a cache and source for every particular interval of times I want any algorithmpredictive machine learning mostly to identify pattern in my CSV file without user specifying any conditions and any prior patterns MY input to algorithm will be only my CSV file That pattern the algorithm identifies should be helpful in these ways help user to increase the performance of the workload next time help to identify any problems based any particular attributes in CSV data help user to predict future data values or future events that may occur based on pattern Even if algorithm is able to do any one of the above i would be glad to know for now thank you
2934.0,2.0,"<p>I've been reading a lot about hardware development and implementation for AI/ML, mainly about Deep Learning, and I have a question about its usage.
From what I understand, there are 2 stages for DL: first is training and second is inference. The first is often done on GPUs because of their massive parallelism capabilities among other things, and inference, while can be done on GPUs, it's not used that much, because of power usage, and because the data presented while inferring are much less so the full capabilities of GPUs won't be much needed. Instead FPGAs and CPUs are often used for that.</p>

<p>My understanding also is that a complete DL system will have both, a training system and an inferring system.</p>

<p>My question is that: are both systems required on the same application? Let's assume an autonomous car or an application where visual and image recognition is done, will it have both training system to be trained and an inference system to execute? Or it has only the inference system and will communicate with a distant system which is already trained and has built a database?</p>

<p>Also, if the application has both systems, will it have a big enough memory to store the training data? Given that it can be a small system and memory is ultimately limited.</p>
",,0,2017-03-06T10:00:44.500,1.0,2927,2017-03-11T14:14:22.167,2017-03-11T14:14:22.167,,145.0,,5873.0,,1,3,<machine-learning><deep-learning><image-recognition>,Machine Learning hardware usage in embedded applications,45.0,49.25,10.33,9.12,0.0,0.0,29.0,Ive been reading a lot about hardware development and implementation for AIML mainly about Deep Learning and I have a question about its usage From what I understand there are 2 stages for DL first is training and second is inference The first is often done on GPUs because of their massive parallelism capabilities among other things and inference while can be done on GPUs its not used that much because of power usage and because the data presented while inferring are much less so the full capabilities of GPUs wont be much needed Instead FPGAs and CPUs are often used for that My understanding also is that a complete DL system will have both a training system and an inferring system My question is that are both systems required on the same application Lets assume an autonomous car or an application where visual and image recognition is done will it have both training system to be trained and an inference system to execute Or it has only the inference system and will communicate with a distant system which is already trained and has built a database Also if the application has both systems will it have a big enough memory to store the training data Given that it can be a small system and memory is ultimately limited
2929.0,2.0,"<p>I'm trying to create simple keras NN which will learn to make addition on numbers between 0 and 10. But I am getting the error: </p>

<pre><code>ValueError: Error when checking model target: expected activation_4 to have shape (None, 19) but got array with shape (100, 1)
</code></pre>

<p>here is my code:</p>

<pre><code>from keras.models import Sequential
from keras.layers import Dense, Activation
import numpy as np

keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)

model = Sequential()
model.add(Dense(output_dim=50, input_dim=2))
model.add(Activation(""relu""))
model.add(Dense(output_dim=50))
model.add(Activation(""softmax""))
model.add(Dense(output_dim=50))
model.add(Activation(""softmax""))
model.add(Dense(output_dim=19))
model.add(Activation(""softmax""))

model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

x = []
y = []

for i in range(0, 10):
    for j in range(0, 10):
        x.append((i, j))
        y.append(i + j)

x = np.array(x)
y = np.array(y)
print(x)
print(y)

model.fit(x, y, nb_epoch=5, batch_size=32)
</code></pre>

<p>how to fix that?</p>
",2017-03-06T15:55:10.170,1,2017-03-06T11:35:52.083,,2928,2017-03-06T14:26:28.900,,,,,5661.0,,1,0,<neural-networks><keras>,"keras ValueError: Error when checking model target: expected activation_4 to have shape (None, 19) but got array with shape (100, 1)",27.0,97.03,5.1,7.8,887.0,0.0,5.0,Im trying to create simple keras NN which will learn to make addition on numbers between 0 and 10 But I am getting the error here is my code how to fix that
,,"<p>Try to use the model like this, for example:</p>

<pre><code>model = Sequential() 
model.add(Dense(50, input_shape=(2,))) 
model.add(Activation(""relu"")) 
model.add(Dense(50, activation='softmax')) 
model.add(Dense(1, activation='linear')) 
model.compile(optimizer='sgd', loss='mse', metrics=[""accuracy""])
</code></pre>

<p><br/>
This means that first layer will have <strong>50</strong> neurons and can receive data in form of matrix with <strong>2</strong> columns and an unspecified number of rows.
So you can prepare your data in this form – 2 numbers for <em>adding</em> in each row.</p>

<pre><code>Dense(50, input_shape=(2,))
</code></pre>

<p><br/>
At the end, you need a layer with 1 neuron and the <code>'linear'</code> activation, because you expect one simple number as a result.</p>

<pre><code>Dense(1, activation='linear')
</code></pre>

<p><br/>
And finally, use <code>'mse'</code> loss function or something similar. <code>'categorical_crossentropy'</code> is needed for classification tasks, not regression as needed for you.
See: <a href=""https://keras.io/objectives/"" rel=""nofollow noreferrer"">https://keras.io/objectives/</a></p>
",,0,2017-03-06T12:14:06.937,,2929,2017-03-06T12:14:06.937,,,,,5876.0,2928.0,2,0,,,,69.11,9.34,8.9,346.0,0.0,18.0,Try to use the model like this for example This means that first layer will have 50 neurons and can receive data in form of matrix with 2 columns and an unspecified number of rows So you can prepare your data in this form – 2 numbers for adding in each row At the end you need a layer with 1 neuron and the activation because you expect one simple number as a result And finally use loss function or something similar is needed for classification tasks not regression as needed for you See httpskerasioobjectives
,,"<p>You shouldn't use Softmax as an activation function in intermediate layers. Softmax is used to represent a categorical distribution, and should be applied at the point where one makes a categorical prediction (usually the final layer of the network).</p>

<p>Consider replacing you activation function in all layers except the last one with 'relu' or 'sigmoid'.</p>
",,0,2017-03-06T14:26:28.900,,2930,2017-03-06T14:26:28.900,,,,,5879.0,2928.0,2,0,,,,44.44,14.27,10.0,0.0,0.0,11.0,You shouldnt use Softmax as an activation function in intermediate layers Softmax is used to represent a categorical distribution and should be applied at the point where one makes a categorical prediction usually the final layer of the network Consider replacing you activation function in all layers except the last one with relu or sigmoid
,,"<p>Note that the statement says nothing directly about the limit of intelligence, nor even about the limit of computational intelligence - but about the limit of computing power.</p>

<p>Perhaps the sentence <em>""the speed of light puts a strict upper bound on how much computing can be done""</em> needs a better explanation: </p>

<p>The authors are probably referring to <a href=""https://en.wikipedia.org/wiki/Bremermann%27s_limit"" rel=""nofollow noreferrer"">Bremermann's limit</a>, which defines an upper bound in bits per second per kilogram. It is an upper bound on the processing power per unit of time of a computer with a given weight.</p>

<p>There is also <a href=""https://en.wikipedia.org/wiki/Margolus%E2%80%93Levitin_theorem"" rel=""nofollow noreferrer"">Margolus–Levitin theorem</a> which defines an upper limit in operations per second per joule. It is an upper bound on the processing power per unit of energy.</p>

<p>These principles do not define a theoretical limit on computing power, but a practical one. If you'll limit your computer and your energy source to the size and capacity of the earth (or to the those of the universe) - you'll get a very practical limit.</p>

<p>Check the reference section in Wikipedia article <a href=""https://en.wikipedia.org/wiki/Limits_to_computation"" rel=""nofollow noreferrer"">Limits to computation</a></p>
",,0,2017-03-06T15:17:36.363,,2931,2017-03-06T16:32:17.700,2017-03-06T16:32:17.700,,3138.0,,3138.0,2919.0,2,3,,,,49.55,10.97,9.36,0.0,0.0,20.0,Note that the statement says nothing directly about the limit of intelligence nor even about the limit of computational intelligence but about the limit of computing power Perhaps the sentence the speed of light puts a strict upper bound on how much computing can be done needs a better explanation The authors are probably referring to Bremermanns limit which defines an upper bound in bits per second per kilogram It is an upper bound on the processing power per unit of time of a computer with a given weight There is also Margolus–Levitin theorem which defines an upper limit in operations per second per joule It is an upper bound on the processing power per unit of energy These principles do not define a theoretical limit on computing power but a practical one If youll limit your computer and your energy source to the size and capacity of the earth or to the those of the universe youll get a very practical limit Check the reference section in Wikipedia article Limits to computation
,0.0,"<p>I am trying to do an inception layer, but it only works if the convolution strides, pool strides and pool size are the same, otherwise I get an error in </p>

<blockquote>
  <p>tf.concat</p>
</blockquote>

<p>that Dimesion 1 is not the same. So If I change something in the last three tuples, I get the error.</p>

<pre><code>conv1 = conv2d_maxpool(x, 64, (5, 5), (1, 1), (2, 2), (2, 2)) 
conv2 = conv2d_maxpool(x, 64, (4, 4), (1, 1), (2, 2), (2, 2)) 
conv3 = conv2d_maxpool(x, 32, (2, 2), (1, 1), (2, 2), (2, 2)) 
conv4 = conv2d_maxpool(x, 32, (1, 1), (1, 1), (2, 2), (2, 2)) 
conv = tf.concat([conv1, conv2, conv3, conv4], 3)
</code></pre>

<p>For example, this is the error I get if I change the 5x5 filter to have strides 3:</p>

<pre><code>conv1 = conv2d_maxpool(x, 64, (5, 5), (3, 3), (2, 2), (2, 2))
</code></pre>

<blockquote>
  <p>Dimension 1 in both shapes must be equal, but are 6 and 16 for
  'concat' (op: 'ConcatV2') with input shapes: [?,6,6,64], [?,16,16,64],
  [?,16,16,32], [?,16,16,32], [].</p>
</blockquote>

<p>This is the conv2d_maxpool function:</p>

<pre><code>def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):
    """"""
    Apply convolution then max pooling to x_tensor
    :param x_tensor: TensorFlow Tensor
    :param conv_num_outputs: Number of outputs for the convolutional layer
    :param conv_strides: Stride 2-D Tuple for convolution
    :param pool_ksize: kernal size 2-D Tuple for pool
    :param pool_strides: Stride 2-D Tuple for pool
    : return: A tensor that represents convolution and max pooling of x_tensor
    """"""
    # TODO: Implement Function
    weights = tf.Variable(tf.truncated_normal(
        shape = [*conv_ksize, int(x_tensor.get_shape().dims[3]), conv_num_outputs], 
        mean = 0.0, 
        stddev=0.1, 
        dtype=tf.float32))
    bias = tf.Variable(tf.zeros(conv_num_outputs)) 

    conv_layer = tf.nn.conv2d(x_tensor, weights, strides=[1, *conv_strides, 1], padding='SAME')
    conv_layer = tf.nn.bias_add(conv_layer, bias)
    conv_layer = tf.nn.relu(conv_layer)

    conv_layer_max_pool = tf.nn.max_pool(conv_layer, ksize=[1, *pool_ksize, 1], strides=[1, *pool_strides, 1], padding='SAME')

    return conv_layer_max_pool
</code></pre>

<p>How can I combine convolution filters with different strides and/or different pooling to create an inception layer?</p>
",,0,2017-03-06T16:22:04.373,,2932,2017-03-06T16:22:04.373,,,,,5527.0,,1,0,<deep-learning><conv-neural-network><tensorflow>,Concatenate convolution layers with different strides in tensorflow.,16.0,73.31,10.45,8.6,1520.0,0.0,53.0,I am trying to do an inception layer but it only works if the convolution strides pool strides and pool size are the same otherwise I get an error in tfconcat that Dimesion 1 is not the same So If I change something in the last three tuples I get the error For example this is the error I get if I change the 5x5 filter to have strides 3 Dimension 1 in both shapes must be equal but are 6 and 16 for concat op ConcatV2 with input shapes 6664 161664 161632 161632 This is the conv2dmaxpool function How can I combine convolution filters with different strides andor different pooling to create an inception layer
,,"<p>Deep learning seems mostly to be a buzzword for what is essentially a neural network. You train with a data set to recognize a pattern, then input new data which is then classified by the trained network.</p>

<p>So you train a neural network with 10 different kinds of animals using thousands of pictures. Then you show the network say 100 new images and have the network ""guess""  what each animal is. </p>

<p>The point here is that training a neural network would required code for feedback that an application using the trained network would not need. So an application just using the trained network would be a bit more streamlined than an application which could allow additional training data. </p>

<p>What is missing is the ability for machine learning to form hierarchical rules from the trained network. So there is no way for the the trained network to really ""explain"" why the classification works. So going back to the 10 animal trained network, there is no way for the network to tell you why the animal was classified the way it was. </p>
",,1,2017-03-06T17:47:42.840,,2933,2017-03-06T17:47:42.840,,,,,3471.0,2927.0,2,0,,,,68.1,9.86,7.87,0.0,0.0,15.0,Deep learning seems mostly to be a buzzword for what is essentially a neural network You train with a data set to recognize a pattern then input new data which is then classified by the trained network So you train a neural network with 10 different kinds of animals using thousands of pictures Then you show the network say 100 new images and have the network guess what each animal is The point here is that training a neural network would required code for feedback that an application using the trained network would not need So an application just using the trained network would be a bit more streamlined than an application which could allow additional training data What is missing is the ability for machine learning to form hierarchical rules from the trained network So there is no way for the the trained network to really explain why the classification works So going back to the 10 animal trained network there is no way for the network to tell you why the animal was classified the way it was
,,"<p>To answer your question: <strong>Training and inference are usually completed on two separate systems</strong> you are right in knowing that training of deep neural networks is usually done on GPUs and that inference is usually done on CPUs. However, training and inference are almost always done on two separate systems. The main workflow for many data scientists today is as follows:
1. Create and establish all hyper-parameters for a model such as a deep neural network 
2. Train the deep neural network using a GPU
3. Save the weights that training on the GPU established so that the model can be deployed. 
4. Code the model in a production application with the optimal weights found in training. </p>

<p>So as you can see from this workflow training and inference are done in two completely separate phases.</p>

<p>However, in some specific cases training and inference are done on the same system. For example, if you are using a Deep Neural Network to play video games than you may have the neural network train and infer on the same system. This would lead to more efficiently because it would allow the model to continuously learn.</p>

<p>To answer your question on memory, the only applications where inference and training are done in the same application have a lot of memory available(think dual GPU dual CPU 128gb of RAM workstations) whereas applications that have a limited amount of memory only use inference such as embedded applications.</p>
",,6,2017-03-07T01:58:44.713,,2934,2017-03-07T02:04:41.203,2017-03-07T02:04:41.203,,4631.0,,4631.0,2927.0,2,2,,,,51.07,11.02,8.56,0.0,0.0,22.0,To answer your question Training and inference are usually completed on two separate systems you are right in knowing that training of deep neural networks is usually done on GPUs and that inference is usually done on CPUs However training and inference are almost always done on two separate systems The main workflow for many data scientists today is as follows 1 Create and establish all hyperparameters for a model such as a deep neural network 2 Train the deep neural network using a GPU 3 Save the weights that training on the GPU established so that the model can be deployed 4 Code the model in a production application with the optimal weights found in training So as you can see from this workflow training and inference are done in two completely separate phases However in some specific cases training and inference are done on the same system For example if you are using a Deep Neural Network to play video games than you may have the neural network train and infer on the same system This would lead to more efficiently because it would allow the model to continuously learn To answer your question on memory the only applications where inference and training are done in the same application have a lot of memory availablethink dual GPU dual CPU 128gb of RAM workstations whereas applications that have a limited amount of memory only use inference such as embedded applications
,,"<p>For your first question,take a look at using a skip grab model to find what the abbreviated text is. The skip gram model turns a word into a vector which allows it to be processed by other machine learning algorithms. Or , alternatively you can do some really cool addition and subtraction problems with the resulting vectors. With the skip gram model in your case you could generate a vector for your input and then compare it with other skip gram vectors and once you have found a near perfect match then that is the unabbreviated word. </p>

<p>You could also look at using sparse distributed representations of words to do this. This approach is similar to the skip gram model except that instead of a vector with maybe 500 values a sparse representation may contain thousands of binary digits of which only a couple are positive or 1. </p>

<p>If you would like to look at this approach take a look at cortical.io which has free API that you can use. As for your second question I reccomend that you use a deep neural network in combination with the skip gram model to produce your output.</p>
",,1,2017-03-07T02:19:35.290,,2935,2017-03-11T14:14:25.653,2017-03-11T14:14:25.653,,1581.0,,4631.0,2925.0,2,3,,,,66.47,9.11,8.86,0.0,0.0,11.0,For your first questiontake a look at using a skip grab model to find what the abbreviated text is The skip gram model turns a word into a vector which allows it to be processed by other machine learning algorithms Or alternatively you can do some really cool addition and subtraction problems with the resulting vectors With the skip gram model in your case you could generate a vector for your input and then compare it with other skip gram vectors and once you have found a near perfect match then that is the unabbreviated word You could also look at using sparse distributed representations of words to do this This approach is similar to the skip gram model except that instead of a vector with maybe 500 values a sparse representation may contain thousands of binary digits of which only a couple are positive or 1 If you would like to look at this approach take a look at corticalio which has free API that you can use As for your second question I reccomend that you use a deep neural network in combination with the skip gram model to produce your output
,2.0,"<p>I'm a newbie in machine learning, so excuse me in advance). I have an idea to make NN that can estimate visual pleasantness of arbitrary image. Like you have a bunch of images that you like, you train NN on them, then you show some random picture to NN and it estimates whether you'll like it or not. I wonder if there is any pervious effort made in this direction. </p>
",,0,2017-03-07T11:33:05.157,,2936,2017-03-08T07:53:41.273,,,,,5899.0,,1,0,<neural-networks><machine-learning><deep-learning><image-recognition><conv-neural-network>,Training neural network for good taste in art,46.0,70.63,6.96,9.02,0.0,0.0,10.0,Im a newbie in machine learning so excuse me in advance I have an idea to make NN that can estimate visual pleasantness of arbitrary image Like you have a bunch of images that you like you train NN on them then you show some random picture to NN and it estimates whether youll like it or not I wonder if there is any pervious effort made in this direction
,0.0,"<p>On what basis predictive algorithms(eg random forest and neural networks) find patterns ?</p>

<p>need some info on :</p>

<p>Predictive algorithms should take CSV file as input and it should return any useful information to the user,like suggesting any changes to be made to improve performance. Can this be possible ?</p>

<p>IF not at least it should return some pattern based on data in CSV by which user can find it himself.</p>

<p>Please share some useful source so that i can implement it.</p>

<p>thank you</p>
",,3,2017-03-07T12:24:13.673,,2937,2017-03-07T12:24:13.673,,,,,5867.0,,1,-1,<neural-networks><machine-learning><deep-learning><algorithm>,How to find pattern using predictive algorithms(eg random forest and neural networks),17.0,63.49,10.5,10.09,0.0,0.0,9.0,On what basis predictive algorithmseg random forest and neural networks find patterns need some info on Predictive algorithms should take CSV file as input and it should return any useful information to the userlike suggesting any changes to be made to improve performance Can this be possible IF not at least it should return some pattern based on data in CSV by which user can find it himself Please share some useful source so that i can implement it thank you
,,"<p>That sounds like a pretty straightforward application of a NN classifier to me.  I don't know if anybody has done that specific thing or not, but I don't see any particular reason to think it wouldn't work. My advice to you is to just jump in and do it.  </p>
",,0,2017-03-07T18:17:01.320,,2939,2017-03-07T18:17:01.320,,,,,33.0,2936.0,2,0,,,,80.31,6.55,7.67,0.0,0.0,7.0,That sounds like a pretty straightforward application of a NN classifier to me I dont know if anybody has done that specific thing or not but I dont see any particular reason to think it wouldnt work My advice to you is to just jump in and do it
,0.0,"<p>By ""neural network"", I mean the typical, multilayered neural network with inputs, weights, hidden nodes and outputs, as shown in the image below:<br>
<a href=""https://i.stack.imgur.com/ejFBN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ejFBN.png"" alt=""enter image description here""></a><br>
Such neural networks, <strong>in the context of evolving neural networks</strong>, can be characterized by the fact that all weighted connections between nodes are all present at the beginning, and can each be represented as a continuous real number. Also, if such a network is used as an agent's brain, the agent's response will be calculated immediately after receiving a set of stimuli.</p>

<p>I want to know if there is any other systems of information processing that do not rely this structure. For example, is there any system in which the topology of a neural network is variable? Or a system in which links between nodes are not real numbers?</p>
",,2,2017-03-07T20:08:04.537,,2940,2017-03-07T20:08:04.537,,,,,1321.0,,1,0,<neural-networks><evolutionary-algorithms>,What are some alternative information processing system beside neural network,17.0,53.14,11.2,10.33,0.0,0.0,21.0,By neural network I mean the typical multilayered neural network with inputs weights hidden nodes and outputs as shown in the image below Such neural networks in the context of evolving neural networks can be characterized by the fact that all weighted connections between nodes are all present at the beginning and can each be represented as a continuous real number Also if such a network is used as an agents brain the agents response will be calculated immediately after receiving a set of stimuli I want to know if there is any other systems of information processing that do not rely this structure For example is there any system in which the topology of a neural network is variable Or a system in which links between nodes are not real numbers
,1.0,"<p>I would like to detect street and sidewalk surface in a very detailed (0.075m/pix) USGS High Resolution Orthoimagery which basically means image segmentation with two classes. Places in question are residential areas similar to <a href=""https://binged.it/2mgmSvR"" rel=""nofollow noreferrer"">this one</a>. I will download uncompressed raw imagery in GeoTIFF from USGS for the detection.</p>

<p>I read that neural networks can perform very good in image segmentation and I would like to try them. I am a developer by day so I can code but am a beginner to neural networks only knowing the basic principles about architecture, weighting and backpropagation etc. Is it possible to jump right in into my task or do I need to start with something simpler? I would prefer jumping right in if it can save time.</p>

<p>I skimmed though few papers dealing with similar thing and they seem quite complicated. Is there some simple way I can get started? I mean maybe an open source project in neural networks that deals with image segmentation that is similar to my task and I could make use of it?</p>

<p>I see neural networks need to be trained first and I am prepared to do manual segmentation first to have data for training. However, I have no idea about neural network design/architecture, how to design the layers, how many layers do I need etc. I also would like to use the fact that the network would learn some basics on how streets and sidewalks are built - that they are (not sure if my term is correct) ""linear structures"" which usually run many meters in length and may not even end in the image, also that sidewalks usually run alongside streets, streets have intersections etc.</p>
",,0,2017-03-07T21:59:15.663,,2941,2017-03-09T15:34:17.910,2017-03-07T22:55:35.283,,113.0,,113.0,,1,1,<neural-networks><machine-learning><deep-learning><image-recognition><detecting-patterns>,Detect street and sidewalk surface in aerial imagery (neural network),35.0,59.53,9.81,8.62,0.0,0.0,29.0,I would like to detect street and sidewalk surface in a very detailed 0075mpix USGS High Resolution Orthoimagery which basically means image segmentation with two classes Places in question are residential areas similar to this one I will download uncompressed raw imagery in GeoTIFF from USGS for the detection I read that neural networks can perform very good in image segmentation and I would like to try them I am a developer by day so I can code but am a beginner to neural networks only knowing the basic principles about architecture weighting and backpropagation etc Is it possible to jump right in into my task or do I need to start with something simpler I would prefer jumping right in if it can save time I skimmed though few papers dealing with similar thing and they seem quite complicated Is there some simple way I can get started I mean maybe an open source project in neural networks that deals with image segmentation that is similar to my task and I could make use of it I see neural networks need to be trained first and I am prepared to do manual segmentation first to have data for training However I have no idea about neural network designarchitecture how to design the layers how many layers do I need etc I also would like to use the fact that the network would learn some basics on how streets and sidewalks are built that they are not sure if my term is correct linear structures which usually run many meters in length and may not even end in the image also that sidewalks usually run alongside streets streets have intersections etc
,3.0,"<p>I know this question might have been asked and answered before, but I just couldn't find the answer I'm looking for.
I've been reading a lot about DL, and I can understand to an extent how it works, in theory at least, and how it's different -technically- from conventional ML.
But what I'm looking for is more of a ""conceptual"" meaning. Why DL? What it offers better?
Let's say you're designing a self-learning system, why to choose DL? What are the main performance parameters that DL offers? Is it more accuracy? More speed? More power efficiency? Mix of all of them?
What is the main parameter to optimize the network for and what can be sacrificed?</p>

<p>I need to understand why DL from this point of view.
Thanks!</p>
",,0,2017-03-08T07:51:37.170,1.0,2942,2017-03-10T01:31:38.777,,,,,5873.0,,1,5,<machine-learning><deep-learning><deep-network><performance>,Why Deep Learning?,126.0,85.08,8.16,8.04,0.0,0.0,31.0,I know this question might have been asked and answered before but I just couldnt find the answer Im looking for Ive been reading a lot about DL and I can understand to an extent how it works in theory at least and how its different technically from conventional ML But what Im looking for is more of a conceptual meaning Why DL What it offers better Lets say youre designing a selflearning system why to choose DL What are the main performance parameters that DL offers Is it more accuracy More speed More power efficiency Mix of all of them What is the main parameter to optimize the network for and what can be sacrificed I need to understand why DL from this point of view Thanks
,,"<p>I don't think anyone has done it yet,but you could try.
A way you could implement it is  having a quite efficient CNN trained on the things you like,then your program should ask the user if he does like some images and on the answers he will give, your program will finetune the original network and then with the fresh-trained one you should obtain good results.</p>
",,0,2017-03-08T07:53:41.273,,2943,2017-03-08T07:53:41.273,,,,,2320.0,2936.0,2,0,,,,63.36,9.41,9.34,0.0,0.0,7.0,I dont think anyone has done it yetbut you could try A way you could implement it is having a quite efficient CNN trained on the things you likethen your program should ask the user if he does like some images and on the answers he will give your program will finetune the original network and then with the freshtrained one you should obtain good results
,,"<p>You could use another type of CNN that instead of classification is performing regression so it will also give you as output the position(it's not really like that but this is the core idea) .
Some algorithms are <a href=""https://github.com/weiliu89/caffe/tree/ssd"" rel=""nofollow noreferrer"">SSD</a> or <a href=""https://pjreddie.com/darknet/yolo/"" rel=""nofollow noreferrer"">YOLO</a>.</p>
",,0,2017-03-08T08:02:31.230,,2944,2017-03-09T05:41:54.967,2017-03-09T05:41:54.967,,2320.0,,2320.0,2279.0,2,1,,,,59.64,9.52,9.37,0.0,0.0,5.0,You could use another type of CNN that instead of classification is performing regression so it will also give you as output the positionits not really like that but this is the core idea Some algorithms are SSD or YOLO
,,"<p>You could try 3 different approaches :</p>

<ol>
<li><p>First you must classify each chunk
  of the CSV file and label it in base
  of what situation is in that case (like
  1 optimal situation .2 critical...), and<br>
  then you can use the machine
  learning algorithm you like most.</p></li>
<li><p>Cluster your data with an algorithm
    like SOM or K-Means and then you
    simply classify the classes you will
    get.</p></li>
<li><p>Use some unserpervised learning<br>
 approach.</p></li>
</ol>
",,0,2017-03-08T08:16:42.453,,2945,2017-03-08T08:16:42.453,,,,,2320.0,2926.0,2,1,,,,70.33,9.86,8.3,0.0,0.0,12.0,You could try 3 different approaches First you must classify each chunk of the CSV file and label it in base of what situation is in that case like 1 optimal situation 2 critical and then you can use the machine learning algorithm you like most Cluster your data with an algorithm like SOM or KMeans and then you simply classify the classes you will get Use some unserpervised learning approach
,,"<p>Deep learning allows you to solve complex problems without necessarily being able to specify the important ""features"" or key input variables for the model in advance.</p>

<p>To give an example, a problem that may be easily tackled <strong>without deep learning</strong> could be predicting the frequency and claim amounts of insurance vehicle claims,  given historical claim data that may include various attributes of the policy holder and their vehicle.  In this example, the ""features"" to be specified in the model are the known attributes of policy holder and vehicle.  The model will then attempt to utilise these features to make predictions.</p>

<p>On the other hand,  facial recognition is a problem more suited to <strong>deep learning</strong> algorithms.  This is because it is difficult to manually identify what combinations of pixels may be important features to include in a conventional machine learning model.  A multi-layered neural network however has the potential to identify/create the important features itself, which may include for example eyes, nose and mouth, and then utilise these features to recognise faces and other objects.</p>
",,1,2017-03-08T11:04:18.067,,2946,2017-03-08T11:16:12.003,2017-03-08T11:16:12.003,,5920.0,,5920.0,2942.0,2,4,,,,37.74,13.99,10.5,0.0,0.0,20.0,Deep learning allows you to solve complex problems without necessarily being able to specify the important features or key input variables for the model in advance To give an example a problem that may be easily tackled without deep learning could be predicting the frequency and claim amounts of insurance vehicle claims given historical claim data that may include various attributes of the policy holder and their vehicle In this example the features to be specified in the model are the known attributes of policy holder and vehicle The model will then attempt to utilise these features to make predictions On the other hand facial recognition is a problem more suited to deep learning algorithms This is because it is difficult to manually identify what combinations of pixels may be important features to include in a conventional machine learning model A multilayered neural network however has the potential to identifycreate the important features itself which may include for example eyes nose and mouth and then utilise these features to recognise faces and other objects
,,"<p>There is an interview (link see below) with David E. Smith, a  senior  researcher  in  the  Intelligent  Systems Division  at  NASA  Ames  Research Center. In this interview, he talks about the application of AI and AI planning in particular in his work at NASA. He also (just shortly) mentions the Mars Exploration Rover and cites related scientific papers (just search for ""Mars"").</p>

<p>Link to the official publication at Springer:<br>
<a href=""http://link.springer.com/article/10.1007%2Fs13218-015-0403-y"" rel=""nofollow noreferrer"">http://link.springer.com/article/10.1007%2Fs13218-015-0403-y</a></p>
",,0,2017-03-09T09:57:00.367,,2947,2017-03-09T09:57:00.367,,,,,4893.0,1955.0,2,0,,,,57.27,16.17,11.32,0.0,0.0,27.0,There is an interview link see below with David E Smith a senior researcher in the Intelligent Systems Division at NASA Ames Research Center In this interview he talks about the application of AI and AI planning in particular in his work at NASA He also just shortly mentions the Mars Exploration Rover and cites related scientific papers just search for Mars Link to the official publication at Springer httplinkspringercomarticle1010072Fs132180150403y
,,"<p>AI is a wide field that goes beyond machine learning, deep learning, neural networks, etc. In some of these fields, the programming language does not matter at all (except for speed issues), so LISP would certainly not be a topic there.</p>

<p>In search or AI planning, for instance, standard languages like C++ and Java are often the first choice, because they are fast (in particular C++) and because many software projects like planning systems are open source, so using a standard language is important (or at least wise in case one appreciates feedback or extensions). I am only aware of one single planner that is written in LISP. Just to give some impression about the role of the choice of the programming language in this field of AI, I'll give a list of some of the best-known and therefore most-important planners:</p>

<p><strong>Fast-Downward:</strong><br>
<em>description:</em> the probably best-known classical planning system<br>
<em>URL:</em> <a href=""http://www.fast-downward.org/"" rel=""nofollow noreferrer"">http://www.fast-downward.org/</a><br>
<em>language:</em> C++, parts (preprocessing) are in Python  </p>

<p><strong>FF:</strong><br>
<em>description:</em> together with Fast-Downward <em>the</em> classical planning system everyone knows<br>
<em>URL:</em> <a href=""https://fai.cs.uni-saarland.de/hoffmann/ff.html"" rel=""nofollow noreferrer"">https://fai.cs.uni-saarland.de/hoffmann/ff.html</a><br>
<em>language:</em> C</p>

<p><strong>VHPOP:</strong><br>
<em>description:</em> one of the best-known partial-order causal link (POCL) planning systems<br>
<em>URL:</em> <a href=""http://www.tempastic.org/vhpop/"" rel=""nofollow noreferrer"">http://www.tempastic.org/vhpop/</a><br>
<em>language:</em> C++  </p>

<p><strong>SHOP and SHOP2:</strong><br>
<em>description:</em> the best-known HTN (hierarchical) planning system<br>
<em>URL:</em> <a href=""https://www.cs.umd.edu/projects/shop/"" rel=""nofollow noreferrer"">https://www.cs.umd.edu/projects/shop/</a><br>
<em>language:</em> there are two versions of SHOP and SHOP2. The original versions have been written in LISP. Newer versions (called JSHOP and JSHOP2) have been written in Java. Pyshop is a further SHOP variant written in Python.  </p>

<p><strong>PANDA:</strong><br>
<em>description:</em> another well-known HTN (and hybrid) planning system<br>
<em>URL:</em> <a href=""http://www.uni-ulm.de/en/in/ki/research/software/panda/panda-planning-system/"" rel=""nofollow noreferrer"">http://www.uni-ulm.de/en/in/ki/research/software/panda/panda-planning-system/</a><br>
<em>language:</em> there are different versions of the planner: PANDA1 and PANDA2 are written in Java, PANDA3 is written in Scala</p>

<p>These were just some of the best-known planning systems that came to my mind. More recent ones can be retrieved from the International Planning Competition (IPC, <a href=""http://www.icaps-conference.org/index.php/Main/Competitions"" rel=""nofollow noreferrer"">http://www.icaps-conference.org/index.php/Main/Competitions</a>), which takes place every two years. The competing planners' code is published open source (for a few years). </p>
",,0,2017-03-09T11:22:29.873,,2948,2017-03-10T09:03:23.677,2017-03-10T09:03:23.677,,4893.0,,4893.0,2236.0,2,0,,,,44.24,18.16,9.23,0.0,0.0,146.0,AI is a wide field that goes beyond machine learning deep learning neural networks etc In some of these fields the programming language does not matter at all except for speed issues so LISP would certainly not be a topic there In search or AI planning for instance standard languages like C and Java are often the first choice because they are fast in particular C and because many software projects like planning systems are open source so using a standard language is important or at least wise in case one appreciates feedback or extensions I am only aware of one single planner that is written in LISP Just to give some impression about the role of the choice of the programming language in this field of AI Ill give a list of some of the bestknown and therefore mostimportant planners FastDownward description the probably bestknown classical planning system URL httpwwwfastdownwardorg language C parts preprocessing are in Python FF description together with FastDownward the classical planning system everyone knows URL httpsfaicsunisaarlanddehoffmannffhtml language C VHPOP description one of the bestknown partialorder causal link POCL planning systems URL httpwwwtempasticorgvhpop language C SHOP and SHOP2 description the bestknown HTN hierarchical planning system URL httpswwwcsumdeduprojectsshop language there are two versions of SHOP and SHOP2 The original versions have been written in LISP Newer versions called JSHOP and JSHOP2 have been written in Java Pyshop is a further SHOP variant written in Python PANDA description another wellknown HTN and hybrid planning system URL httpwwwuniulmdeeninkiresearchsoftwarepandapandaplanningsystem language there are different versions of the planner PANDA1 and PANDA2 are written in Java PANDA3 is written in Scala These were just some of the bestknown planning systems that came to my mind More recent ones can be retrieved from the International Planning Competition IPC httpwwwicapsconferenceorgindexphpMainCompetitions which takes place every two years The competing planners code is published open source for a few years
,,"<p>Yes, in fact neural networks (NNs) are very efficient at segmentation and it seems to me that your problem matches the capabilities of neural networks very well. </p>

<p>I think it best for you to truly understand what a NN is before using it. First, let's start with the architecture. A NN has 3 regions, the input layer, the hidden layers and the output layer. The input layer depends on the number of features in your dataset. The hidden layers, you can have multiple layers all of different breadth (number of nodes per layer). The output layer depends on the number of classes in your dataset. </p>

<p>An easy example is applying a NN to the MNIST datatset. This is a dataset which contains handwritten digits between 0-9. Let's assume each of these images is 16*16=256 pixels. Thus, you will need 256 input nodes. And you will need 10 output nodes, one for each possible output. The hidden layers can be set in any way you can creatively imagine. There are however ways to optimize your hidden layer to get the best performance possible while not spending too much computational power. </p>

<p>This is always how a NN works. The beauty of a NN is that you only need to code it once and it can learn any function. All you need to do is change your architecture, but the underlining principles will always be the same. </p>

<p>In your case, you want to do segmentation. This is often done using a window around the pixel you want to classify. Popular choices are 3*3 or 5*5 pixels. The choice of your considered window will determine the number of nodes in your input layer. Then you want to classify them as one of two classes, thus you need 2 output nodes. You can also use just 1, but I don't recommend it, I can expand on this if you care.</p>

<p>One caveat of NN is that you will need quite a bit of training data. So get ready to classify a lot of pixels manually. </p>

<p>How to know how many layers in hidden layer? How to know how many nodes per layer in the hidden layer?</p>

<p>In general, for simple operations like the one you are trying to learn you do not need to have multiple layers. One hidden layer should be enough, at most 2. But, how do you determine the number of nodes you should use? You need to use some model validation techniques to do this. One way is through grid search and cross-validation. Train, and re-train your model with multiple number of nodes and then compare their performances to identify the optimal number of nodes. To get good results this does require a large dataset.</p>

<p>Rule of thumb: 1 hidden layer for NN! Don't get dragged into deep models if you don't need them. </p>
",,0,2017-03-09T15:34:17.910,,2949,2017-03-09T15:34:17.910,,,,,5925.0,2941.0,2,2,,,,83.56,7.47,7.62,0.0,0.0,69.0,Yes in fact neural networks NNs are very efficient at segmentation and it seems to me that your problem matches the capabilities of neural networks very well I think it best for you to truly understand what a NN is before using it First lets start with the architecture A NN has 3 regions the input layer the hidden layers and the output layer The input layer depends on the number of features in your dataset The hidden layers you can have multiple layers all of different breadth number of nodes per layer The output layer depends on the number of classes in your dataset An easy example is applying a NN to the MNIST datatset This is a dataset which contains handwritten digits between 09 Lets assume each of these images is 1616256 pixels Thus you will need 256 input nodes And you will need 10 output nodes one for each possible output The hidden layers can be set in any way you can creatively imagine There are however ways to optimize your hidden layer to get the best performance possible while not spending too much computational power This is always how a NN works The beauty of a NN is that you only need to code it once and it can learn any function All you need to do is change your architecture but the underlining principles will always be the same In your case you want to do segmentation This is often done using a window around the pixel you want to classify Popular choices are 33 or 55 pixels The choice of your considered window will determine the number of nodes in your input layer Then you want to classify them as one of two classes thus you need 2 output nodes You can also use just 1 but I dont recommend it I can expand on this if you care One caveat of NN is that you will need quite a bit of training data So get ready to classify a lot of pixels manually How to know how many layers in hidden layer How to know how many nodes per layer in the hidden layer In general for simple operations like the one you are trying to learn you do not need to have multiple layers One hidden layer should be enough at most 2 But how do you determine the number of nodes you should use You need to use some model validation techniques to do this One way is through grid search and crossvalidation Train and retrain your model with multiple number of nodes and then compare their performances to identify the optimal number of nodes To get good results this does require a large dataset Rule of thumb 1 hidden layer for NN Dont get dragged into deep models if you dont need them
2952.0,1.0,"<p>I want to train text classifier (using <a href=""https://www.uclassify.com"" rel=""nofollow noreferrer"">https://www.uclassify.com</a>) with 12 classes/categories. I will be training it to classify news/articles (I know that there are existing classifier but I want to train my own).</p>

<p>uclassify uses following algorithm (directly copied from their site):</p>

<blockquote>
  <p>The core is a multinominal Naive Bayesian classifier with a couple of
  steps that improves the classification further (hybrid complementary
  NB, class normalization and special smoothing). The result of
  classifications are probabilities [0-1] of a document belonging to
  each class. This is very useful if you want to set a threshold for
  classifications. E.g. all classifications over 90% is considered spam.
  Using this model also makes it very scalable in terms of CPU time for
  classification/training.</p>
</blockquote>

<p>I was wondering how many examples I will need to train such classifier? It is possible to estimate the number? Let's assume that one article will ""fit"" 2 categories by average.</p>
",,0,2017-03-09T16:36:14.807,,2950,2017-03-09T17:55:35.353,,,,,5935.0,,1,2,<classification>,How many training example text classifier needs to be trained?,14.0,49.31,13.8,9.92,0.0,0.0,37.0,I want to train text classifier using httpswwwuclassifycom with 12 classescategories I will be training it to classify newsarticles I know that there are existing classifier but I want to train my own uclassify uses following algorithm directly copied from their site The core is a multinominal Naive Bayesian classifier with a couple of steps that improves the classification further hybrid complementary NB class normalization and special smoothing The result of classifications are probabilities 01 of a document belonging to each class This is very useful if you want to set a threshold for classifications Eg all classifications over 90 is considered spam Using this model also makes it very scalable in terms of CPU time for classificationtraining I was wondering how many examples I will need to train such classifier It is possible to estimate the number Lets assume that one article will fit 2 categories by average
,,"<p>1) It is possible! In fact it's an example in the popular deep learning framework Keras. Check out <a href=""https://github.com/fchollet/keras/blob/master/examples/addition_rnn.py"" rel=""nofollow noreferrer"">this link to see the source code.</a></p>

<p>2) This particular example uses a recurrent neural network (RNN) to process the problem as a sequence of characters, producing a sequence of characters which form the answer. Note that this approach is obviously different from how humans tend to think about solving simple addition problems, and probably isn't how you would ever want a computer to solve such a problem. Mostly this is an example of sequence to sequence learning  using Keras. When handling sequential or time-series inputs, RNNs are a popular choice.</p>
",,0,2017-03-09T17:01:54.670,,2951,2017-03-09T17:01:54.670,,,,,5936.0,154.0,2,3,,,,64.1,11.25,9.77,0.0,0.0,17.0,1 It is possible In fact its an example in the popular deep learning framework Keras Check out this link to see the source code 2 This particular example uses a recurrent neural network RNN to process the problem as a sequence of characters producing a sequence of characters which form the answer Note that this approach is obviously different from how humans tend to think about solving simple addition problems and probably isnt how you would ever want a computer to solve such a problem Mostly this is an example of sequence to sequence learning using Keras When handling sequential or timeseries inputs RNNs are a popular choice
,,"<p>As a general rule of thumb I typically use 10*(# of features) for shallow machine learning models such as Naive Bayes with only 2 classes. </p>

<p>So it all depends on the number of features you will be using. However, the more output classes the more data you will need for proper discrimination. The addition of more classes is not linear but I think you can get away with: 10*(# of features)*(# of output classes)</p>
",,0,2017-03-09T17:55:35.353,,2952,2017-03-09T17:55:35.353,,,,,5925.0,2950.0,2,4,,,,69.62,8.99,9.89,0.0,0.0,17.0,As a general rule of thumb I typically use 10 of features for shallow machine learning models such as Naive Bayes with only 2 classes So it all depends on the number of features you will be using However the more output classes the more data you will need for proper discrimination The addition of more classes is not linear but I think you can get away with 10 of features of output classes
,0.0,"<p>How would one go about building an AI that is capable to look at any kind of input and then identify what is the nature of this data? </p>

<p>For example, an AI that is able to do image classification, NLP and react to some other sensors. Is it possible to build an AI that will be able to identify what kind of data it is seeing such that it can send the data to the correct model for it to be treated. Similarly, to the how the human brain knows to send visual information to the visual cortex and auditory information elsewhere. </p>

<p>In a simple scenario, I think we can get very good performance by having a cascaded image classifier. For example 2 layers, the first identifies if the image contains a dog and a cat. The next layer, has two different CNNS, one trained to identify the breed of dog and the other one for cats. That way once we identify that we have a dog, the image can be sent to the correct CNN. A CNN that is trained specifically to detect the breed, thus being much more robust that a more generalized CNN. Kind of like a professional in the field. First the human identifies that he is looking at a dog then he consults a professional to ass him the breed. </p>

<p>I would like to extend this idea to being able to identify various kinds of data sources that do not resemble each other at all. Various input. Are there any models that can do this?</p>
",,0,2017-03-09T18:09:04.410,,2953,2017-03-09T18:09:04.410,,,,,5925.0,,1,0,<classification><intelligence-testing><ultraintelligent-machine>,Using AI to interpret the nature a specific input and use the correct model.,8.0,68.1,7.08,7.85,0.0,0.0,23.0,How would one go about building an AI that is capable to look at any kind of input and then identify what is the nature of this data For example an AI that is able to do image classification NLP and react to some other sensors Is it possible to build an AI that will be able to identify what kind of data it is seeing such that it can send the data to the correct model for it to be treated Similarly to the how the human brain knows to send visual information to the visual cortex and auditory information elsewhere In a simple scenario I think we can get very good performance by having a cascaded image classifier For example 2 layers the first identifies if the image contains a dog and a cat The next layer has two different CNNS one trained to identify the breed of dog and the other one for cats That way once we identify that we have a dog the image can be sent to the correct CNN A CNN that is trained specifically to detect the breed thus being much more robust that a more generalized CNN Kind of like a professional in the field First the human identifies that he is looking at a dog then he consults a professional to ass him the breed I would like to extend this idea to being able to identify various kinds of data sources that do not resemble each other at all Various input Are there any models that can do this
,,"<p>Deep Learning these days mean a lot of things to a lot of people, its quickly becoming a buzz-word. But so far it still retains two very important conceptual properties:</p>

<p><strong>Does away with most feature engineering work.</strong> This was mentioned in the answer above, but this is very important. It really saves a lot of work.</p>

<p><strong>Allows you to make maximal use of unlabelled data.</strong> This is strictly speaking available to other approaches, not just Deep Learning, but its in DL that this really took off. And typically labelled data is very hard to get while unlabelled is all over the place. Things like denoising autoencoders and Restricted Boltzmann machines are just wonderful.</p>
",,1,2017-03-09T20:04:17.233,,2954,2017-03-09T20:04:17.233,,,,,5941.0,2942.0,2,2,,,,65.62,10.43,9.93,0.0,0.0,14.0,Deep Learning these days mean a lot of things to a lot of people its quickly becoming a buzzword But so far it still retains two very important conceptual properties Does away with most feature engineering work This was mentioned in the answer above but this is very important It really saves a lot of work Allows you to make maximal use of unlabelled data This is strictly speaking available to other approaches not just Deep Learning but its in DL that this really took off And typically labelled data is very hard to get while unlabelled is all over the place Things like denoising autoencoders and Restricted Boltzmann machines are just wonderful
,0.0,"<p>What's the term (if such exists) for merging with AI (e.g. via neural lace) and becoming so diluted (e.g. 1:10000) that it effectively results in a death of the original self?</p>

<p>It's not quite ""digital ascension"", because that way it would still be you. What I'm thinking is, that the resulting AI with 1 part in 10000 being you, is not you anymore. The AI might have some of your values or memories or whatever, but it's not you, and you don't exist separately from it to be called you. Basically - you as you are dead; you died by dissolving in AI.</p>

<p>I would like to read up on this subject, but can't find anything.</p>
",,2,2017-03-10T00:09:02.040,,2955,2017-03-10T00:09:02.040,,,,,5947.0,,1,3,<strong-ai><control-problem>,What's the term for death by dissolving in AI?,19.0,82.24,7.3,7.65,0.0,0.0,33.0,Whats the term if such exists for merging with AI eg via neural lace and becoming so diluted eg 110000 that it effectively results in a death of the original self Its not quite digital ascension because that way it would still be you What Im thinking is that the resulting AI with 1 part in 10000 being you is not you anymore The AI might have some of your values or memories or whatever but its not you and you dont exist separately from it to be called you Basically you as you are dead you died by dissolving in AI I would like to read up on this subject but cant find anything
,,"<p>Deep learning allows you to not know the answer in order to ask the program a question.  Their main benefit is their finite ability and flexible nature.</p>

<p>The problem with procedural programing to solve problems is you have to know what the computer needs to do in order to solve the problem.</p>

<p>What deep learning does is remove the requirement of the programmer to know how to solve the problem by having them only need to know what the computer needs to know.</p>

<p>This is the entire premise of neural networks. The programmer writes the program for data points required to be known in order to solve a particular problem.</p>

<p>The computer is given an input it comes up with an answer.
If it's answer is wrong it needs to make the answer it gave less likely and the right answer more likely.
The goal is to get the computer to always get the right answer. If the computer always gets the wrong answer then the neural network it too small.</p>

<p>What deep learning is, is a neural network that is deep.
To answer this you need to know how a neural network is.</p>

<p>A neural network is based on a neuron:</p>

<ul>
<li>Finite number of `boolean' inputs (More then one)</li>
<li>A weight is attached on each input to define how important
often though as a float between -1 and 1, but it's just a percentage of how likely each input changes the answer.</li>
<li>One boolean output</li>
</ul>

<p>A neuron can be a class or function the implementation really doesn't matter. The weight of each input changes as more answers are asked and responses verified.</p>

<p>The depth of a neural network is has one layer  when there is one row of neurons between the input and output.
two layers when a few neurons make decisions on inputs and a final neuron or multiple neurons make decisions biased on those neurons.</p>

<p>A neural network is called deep when there are at least four layers of neurons? (do some research don't take my word for it <code>^_^</code>)</p>

<p>The disadvantage of deep learning is that it's ability is finite.
There is no way a deep neural network by it's self to get smarter then it's programed to be.
It has a intelligence curve similar to root time if it isn't improved somehow.
This leads to the other problem in neural networks. While the programmer has no need to know how decisions are made by the computer they still need to know what questions or nodes need to be added.
The reason this is a problem is if the nodes responsible aren't there the program will be wrong in strange cases and have no way of correcting this on it's own. The larger the network the harder it is to solve these kinds of problems.</p>

<p>This will lead to an inevitable solution to have the computer self improve by some type of generative algorithm.
This has it's own breadth of problems as if not built properly could grow into something unintended which wastes time and money if it fails quickly, and could be potentially dangerous if it appears to work and doesn't.</p>

<p>The answer to AI will be a combination of deep neural networks some generative type programing and some new ideas and innovations.</p>
",,1,2017-03-10T01:31:38.777,,2956,2017-03-10T01:31:38.777,,,,,4844.0,2942.0,2,1,,,,68.6,8.59,7.84,3.0,0.0,51.0,Deep learning allows you to not know the answer in order to ask the program a question Their main benefit is their finite ability and flexible nature The problem with procedural programing to solve problems is you have to know what the computer needs to do in order to solve the problem What deep learning does is remove the requirement of the programmer to know how to solve the problem by having them only need to know what the computer needs to know This is the entire premise of neural networks The programmer writes the program for data points required to be known in order to solve a particular problem The computer is given an input it comes up with an answer If its answer is wrong it needs to make the answer it gave less likely and the right answer more likely The goal is to get the computer to always get the right answer If the computer always gets the wrong answer then the neural network it too small What deep learning is is a neural network that is deep To answer this you need to know how a neural network is A neural network is based on a neuron Finite number of boolean inputs More then one A weight is attached on each input to define how important often though as a float between 1 and 1 but its just a percentage of how likely each input changes the answer One boolean output A neuron can be a class or function the implementation really doesnt matter The weight of each input changes as more answers are asked and responses verified The depth of a neural network is has one layer when there is one row of neurons between the input and output two layers when a few neurons make decisions on inputs and a final neuron or multiple neurons make decisions biased on those neurons A neural network is called deep when there are at least four layers of neurons do some research dont take my word for it The disadvantage of deep learning is that its ability is finite There is no way a deep neural network by its self to get smarter then its programed to be It has a intelligence curve similar to root time if it isnt improved somehow This leads to the other problem in neural networks While the programmer has no need to know how decisions are made by the computer they still need to know what questions or nodes need to be added The reason this is a problem is if the nodes responsible arent there the program will be wrong in strange cases and have no way of correcting this on its own The larger the network the harder it is to solve these kinds of problems This will lead to an inevitable solution to have the computer self improve by some type of generative algorithm This has its own breadth of problems as if not built properly could grow into something unintended which wastes time and money if it fails quickly and could be potentially dangerous if it appears to work and doesnt The answer to AI will be a combination of deep neural networks some generative type programing and some new ideas and innovations
,1.0,"<p>Human beings are more productive in groups than individually, possibly due to the fact that there is a limit to how much one human brain can improve itself in terms of speed of computation and areas of expertise.</p>

<p>By contrast, if a machine with general-purpose artificial intelligence is created and then assigned a task, would it be possible that the machine will be able to better accomplish its task by continuously improving its own computational power and mastery of various skills, as opposed to collaborating with other agents (whether copies of itself, other AI's, or even humans)?</p>

<p>In other words, would an AGI ever need to collaborate, or would it always be able to achieve its goals alone?</p>
",,0,2017-03-10T06:52:41.560,,2957,2017-03-12T05:29:01.003,2017-03-12T02:21:57.833,,33.0,,1321.0,,1,2,<comparison><multi-agent-systems><swarm-intelligence>,Would a general-purpose AI need to collaborate?,40.0,31.59,11.85,11.34,0.0,0.0,15.0,Human beings are more productive in groups than individually possibly due to the fact that there is a limit to how much one human brain can improve itself in terms of speed of computation and areas of expertise By contrast if a machine with generalpurpose artificial intelligence is created and then assigned a task would it be possible that the machine will be able to better accomplish its task by continuously improving its own computational power and mastery of various skills as opposed to collaborating with other agents whether copies of itself other AIs or even humans In other words would an AGI ever need to collaborate or would it always be able to achieve its goals alone
,,"<p>This is one of the main research areas of my <a href=""https://blinclab.ca/about/"" rel=""nofollow noreferrer"">lab</a> which researches intelligent prosthetics which also give sensory feedback such as touch and kinaesthesia (the feeling of a limb moving in space) to the user.  We use reinforcement learning to bridge the gap in control and have preliminary work of in communicating to the user predictions made by the artificial agent. </p>
",,2,2017-03-10T09:13:02.953,,2958,2017-03-10T09:13:02.953,,,,,4398.0,2846.0,2,0,,,,40.01,11.73,10.52,0.0,0.0,4.0,This is one of the main research areas of my lab which researches intelligent prosthetics which also give sensory feedback such as touch and kinaesthesia the feeling of a limb moving in space to the user We use reinforcement learning to bridge the gap in control and have preliminary work of in communicating to the user predictions made by the artificial agent
2960.0,2.0,"<p>According to <a href=""https://en.wikipedia.org/wiki/AI_winter"" rel=""noreferrer"">Wikipedia</a>, citations omitted:</p>

<blockquote>
  <p>In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. The term was coined by analogy to the idea of a nuclear winter. The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or decades later.</p>
</blockquote>

<p>The wikipedia page discusses a bit about the <em>causes</em> of AI Winters. I'm curious however whether it is possible to <em>stop</em> an AI Winter from occurring. I don't really like the misallocation of resources that are caused by over-investment followed by under-investment.</p>

<p>One of the causes of the AI Winter listed on that Wikipedia page is ""hype"":</p>

<blockquote>
  <p>The AI winters can be partly understood as a sequence of over-inflated expectations and subsequent crash seen in stock-markets and exemplified by the railway mania and dotcom bubble. In a common pattern in development of new technology (known as hype cycle), an event, typically a technological breakthrough, creates publicity which feeds on itself to create a ""peak of inflated expectations"" followed by a ""trough of disillusionment"". Since scientific and technological progress can't keep pace with the publicity-fueled increase in expectations among investors and other stakeholders, a crash must follow. AI technology seems to be no exception to this rule.</p>
</blockquote>

<p>And it seems that this paragraph indicates that <em>any</em> new technology will be stuck in this pattern of ""inflated expectations"" followed by disillusionment. So are AI Winters inevitable? That AI technologies will always be overhyped in the future and that severe ""corrections"" will always will always occur? Or can there a way to manage this Hype Cycle to stop severe increases/decreases in funding?</p>
",,4,2017-03-10T13:59:43.357,,2959,2017-03-12T00:54:47.573,,,,,181.0,,1,7,<history><ai-winter>,"Are ""AI Winters"" inevitable?",90.0,42.61,14.1,10.46,0.0,0.0,46.0,According to Wikipedia citations omitted In the history of artificial intelligence an AI winter is a period of reduced funding and interest in artificial intelligence research The term was coined by analogy to the idea of a nuclear winter The field has experienced several hype cycles followed by disappointment and criticism followed by funding cuts followed by renewed interest years or decades later The wikipedia page discusses a bit about the causes of AI Winters Im curious however whether it is possible to stop an AI Winter from occurring I dont really like the misallocation of resources that are caused by overinvestment followed by underinvestment One of the causes of the AI Winter listed on that Wikipedia page is hype The AI winters can be partly understood as a sequence of overinflated expectations and subsequent crash seen in stockmarkets and exemplified by the railway mania and dotcom bubble In a common pattern in development of new technology known as hype cycle an event typically a technological breakthrough creates publicity which feeds on itself to create a peak of inflated expectations followed by a trough of disillusionment Since scientific and technological progress cant keep pace with the publicityfueled increase in expectations among investors and other stakeholders a crash must follow AI technology seems to be no exception to this rule And it seems that this paragraph indicates that any new technology will be stuck in this pattern of inflated expectations followed by disillusionment So are AI Winters inevitable That AI technologies will always be overhyped in the future and that severe corrections will always will always occur Or can there a way to manage this Hype Cycle to stop severe increasesdecreases in funding
,,"<p>I think that by strict definition of the word inevitable, <strong>no, future AI Winter events are not inevitable.</strong> However likely or unlikely it may be, it is possible to control research spending and to create a more stable plan of funding research in Artificial Intelligence. Because it is <em>possible</em> to avoid an AI Winter event, an event is not <em>inevitable</em>.</p>
",,7,2017-03-10T19:17:25.183,,2960,2017-03-12T00:54:47.573,2017-03-12T00:54:47.573,,3989.0,,3989.0,2959.0,2,2,,,,42.72,10.68,10.16,0.0,0.0,7.0,I think that by strict definition of the word inevitable no future AI Winter events are not inevitable However likely or unlikely it may be it is possible to control research spending and to create a more stable plan of funding research in Artificial Intelligence Because it is possible to avoid an AI Winter event an event is not inevitable
,,"<p>Yes, it is possible.</p>

<p>When humans were working on the first nuclear bomb, some field experts of the time thought that when the reaction went super-critical, it would not stop, and would devour the earth. It was a plausible <em>possibility</em> given our understand of nuclear energy at the time, and we didn't know for sure until we did it.</p>

<p>Some scientists synthesize black-hole like environments in laboratories. Some experts think that if a certain point is accidentally crossed due to ignorance or negligence, we may devour our planet with a self made black hole.</p>

<p>The situation is the same with AI. Until we actually create a super-intelligent AI, we <strong>cannot say with certainty</strong> whether it will be controlled or controllable until it happens. Until that time comes the answer to your question is yes, it's possible, but that does not mean it will or will not happen that way.</p>
",,0,2017-03-11T02:17:56.760,,2961,2017-03-11T02:17:56.760,,,,,5967.0,2900.0,2,0,,,,61.06,10.68,8.9,0.0,0.0,22.0,Yes it is possible When humans were working on the first nuclear bomb some field experts of the time thought that when the reaction went supercritical it would not stop and would devour the earth It was a plausible possibility given our understand of nuclear energy at the time and we didnt know for sure until we did it Some scientists synthesize blackhole like environments in laboratories Some experts think that if a certain point is accidentally crossed due to ignorance or negligence we may devour our planet with a self made black hole The situation is the same with AI Until we actually create a superintelligent AI we cannot say with certainty whether it will be controlled or controllable until it happens Until that time comes the answer to your question is yes its possible but that does not mean it will or will not happen that way
,,"<p>This depends on your definition of human-like.</p>

<p>If you mean a robot that looks and acts like a human, arguably, yes. Here's one of many examples: <a href=""http://www.hansonrobotics.com/robot/sophia/"" rel=""nofollow noreferrer"">http://www.hansonrobotics.com/robot/sophia/</a></p>

<p>If you are looking for something that performs work and tasks, or works and thinks and talks like-or better than a human, the answer is mostly no, not yet.</p>

<p>I recommend you look at 'ANI, AGI, ASI""</p>

<p>ANI: artificial narrow intelligence. This is what you see around you right now.</p>

<p>AGI: artificial general intelligence. A theoretic AI that can ""think"" like a human. It does not yet exist. <strong>Estimates</strong> are between 20-60 years before we will successfully create AGI.</p>

<p>ASI: artificial super intelligence. In a nutshell, it is theorized to be everything we wish we could be or hope never to be. It does not exist yet. It is <strong>generally</strong> believed that, IF we create an AGI, ASI will evolve seconds or less than a decade after AGI is created.</p>
",,0,2017-03-11T02:55:01.700,,2962,2017-03-11T02:55:01.700,,,,,5967.0,2917.0,2,3,,,,68.57,9.49,9.42,0.0,0.0,43.0,This depends on your definition of humanlike If you mean a robot that looks and acts like a human arguably yes Heres one of many examples httpwwwhansonroboticscomrobotsophia If you are looking for something that performs work and tasks or works and thinks and talks likeor better than a human the answer is mostly no not yet I recommend you look at ANI AGI ASI ANI artificial narrow intelligence This is what you see around you right now AGI artificial general intelligence A theoretic AI that can think like a human It does not yet exist Estimates are between 2060 years before we will successfully create AGI ASI artificial super intelligence In a nutshell it is theorized to be everything we wish we could be or hope never to be It does not exist yet It is generally believed that IF we create an AGI ASI will evolve seconds or less than a decade after AGI is created
,,"<p>Imho, it <strong>is</strong> life. </p>

<p>Example: consider the possibility that we synthesized from completethe DNA of a human being, with zero atoms from another human, and grew said human in a lab. Most (and myself) would agree that creature is alive. </p>

<p>Although there are many opinions that differ, my own is that there is no absolute line to draw between something that is alive, and something that is not alive. A human is alive. But is a single bacterial cell, or a single cell from your own body? They reproduce, they eat, etc. so yes they are alive. They are not like a dog or a cat however. In fact, a bacterial colony in a pool of water, giving off a yellow or brown color can be mistaken as a mineral or mud. It is only when you look closer that you see it is actually life. What about a biological virus? It is not made of cells. It does not have DNA. Most would agree it has life. But is does not really seem to be as alive as say, a shark or giraffe. Many people do not think a car is alive. Yet cars evolve. They move, they ""eat."" </p>

<p>Life is simply a way we define things around us. A much more useful and definitive way to categorize life I think, would be to utilize a continuum instead of an all or nothing approach. Rocks would go on the end of ""nonliving."" Intelligent, multicellular; self-aware entities could perhaps be on the other end as ""fully alive.""</p>

<p>Other entities can go in between. As for something such as AI, I would propose adding a z-axis, to make a 3 dimensional continuum, allowing for a self aware, intelligence entity not made of cells to fit comfortably with humans without causing an all-or-nothing.</p>

<p>PS: thought I came to this conclusion myself, I have a suspicion someone smarter than me has already written about such an idea. If anyone feels like educating me, I'd love to hear it.</p>
",,0,2017-03-11T03:40:11.090,,2963,2017-03-11T03:40:11.090,,,,,5967.0,2771.0,2,0,,,,67.35,7.12,8.59,0.0,0.0,63.0,Imho it is life Example consider the possibility that we synthesized from completethe DNA of a human being with zero atoms from another human and grew said human in a lab Most and myself would agree that creature is alive Although there are many opinions that differ my own is that there is no absolute line to draw between something that is alive and something that is not alive A human is alive But is a single bacterial cell or a single cell from your own body They reproduce they eat etc so yes they are alive They are not like a dog or a cat however In fact a bacterial colony in a pool of water giving off a yellow or brown color can be mistaken as a mineral or mud It is only when you look closer that you see it is actually life What about a biological virus It is not made of cells It does not have DNA Most would agree it has life But is does not really seem to be as alive as say a shark or giraffe Many people do not think a car is alive Yet cars evolve They move they eat Life is simply a way we define things around us A much more useful and definitive way to categorize life I think would be to utilize a continuum instead of an all or nothing approach Rocks would go on the end of nonliving Intelligent multicellular selfaware entities could perhaps be on the other end as fully alive Other entities can go in between As for something such as AI I would propose adding a zaxis to make a 3 dimensional continuum allowing for a self aware intelligence entity not made of cells to fit comfortably with humans without causing an allornothing PS thought I came to this conclusion myself I have a suspicion someone smarter than me has already written about such an idea If anyone feels like educating me Id love to hear it
,2.0,"<p>Does Artificial Intelligence write its own code and then execute it?
If so does it create separate functions(for each purpose) for its code?</p>

<p>How does learning get implemented in artificial intelligence?</p>

<p>Is there a specific flowchart to describe artificial intelligence</p>
",,4,2017-03-11T09:48:32.563,,2964,2017-03-11T23:25:22.243,,,,,5972.0,,1,1,<self-learning>,Does Artificial Intelligence write its own code?,54.0,35.95,14.42,9.66,0.0,0.0,5.0,Does Artificial Intelligence write its own code and then execute it If so does it create separate functionsfor each purpose for its code How does learning get implemented in artificial intelligence Is there a specific flowchart to describe artificial intelligence
,,"<p>The hype cycles are the rule these days, and AI is always a wonderful topic for unbelievable and crazy hype. I mean simple thing like speech recognition is still not working properly, but everybody is discussing how to survive the revolt of the terminator machines. So unless we can tune the hype down, the next AI winter is inevitable.</p>
",,0,2017-03-11T11:48:22.057,,2965,2017-03-11T11:48:22.057,,,,,5941.0,2959.0,2,0,,,,51.48,10.04,9.97,0.0,0.0,6.0,The hype cycles are the rule these days and AI is always a wonderful topic for unbelievable and crazy hype I mean simple thing like speech recognition is still not working properly but everybody is discussing how to survive the revolt of the terminator machines So unless we can tune the hype down the next AI winter is inevitable
,,"<p>Computers are able to write their own code <em>without</em> needing any intelligence -- see the Wikipedia entries for <a href=""https://en.wikipedia.org/wiki/Self-modifying_code"" rel=""nofollow noreferrer"">self-modifying code</a> and <a href=""https://en.wikipedia.org/wiki/Metaprogramming"" rel=""nofollow noreferrer"">metaprogramming</a>. You do have to write the instructions for how the computer should program itself, and there's a stigma against doing this because (a) it makes it hard to reason about what your program is doing when it's changing its source code, and (b) the solution is usually slower than just hardcoding in what you want the program to do in the first place. But it <em>is</em> possible, and programmers have done it (usually for maintainability or aesthetic reasons). </p>

<p>Some AI researchers are interested in <a href=""https://en.wikipedia.org/wiki/Genetic_programming"" rel=""nofollow noreferrer"">Genetic Programming</a> though. Genetic Programming is a subset of <a href=""https://en.wikipedia.org/wiki/Evolutionary_algorithm"" rel=""nofollow noreferrer"">evolutionary algorithms</a> and Wikipedia provides a good summary of how they usually work:</p>

<blockquote>
  <p>Step One: Generate the initial population of individuals randomly. (First generation)</p>
  
  <p>Step Two: Evaluate the fitness of each individual in that population</p>
  
  <p>Step Three: Repeat the following regenerational steps until termination (time limit, sufficient fitness achieved, etc.):</p>
  
  <ul>
  <li><p>Select the best-fit individuals for reproduction. (Parents)</p></li>
  <li><p>Breed new individuals through crossover and mutation operations to give birth to offspring.</p></li>
  <li><p>Evaluate the individual fitness of new individuals.</p></li>
  <li><p>Replace least-fit population with new individuals.</p></li>
  </ul>
</blockquote>

<p>The ""individuals"" in this case are randomly-generated computer programs, which are then tested against a fitness function.</p>

<p>The Wikipedia page for Genetic Programming claimed that these programs are usually represented by tree structures, though there has been some experiments in using non-tree structures as well.</p>
",,0,2017-03-11T14:50:00.113,,2966,2017-03-11T14:55:05.720,2017-03-11T14:55:05.720,,181.0,,181.0,2964.0,2,0,,,,34.05,16.01,9.92,0.0,0.0,47.0,Computers are able to write their own code without needing any intelligence see the Wikipedia entries for selfmodifying code and metaprogramming You do have to write the instructions for how the computer should program itself and theres a stigma against doing this because a it makes it hard to reason about what your program is doing when its changing its source code and b the solution is usually slower than just hardcoding in what you want the program to do in the first place But it is possible and programmers have done it usually for maintainability or aesthetic reasons Some AI researchers are interested in Genetic Programming though Genetic Programming is a subset of evolutionary algorithms and Wikipedia provides a good summary of how they usually work Step One Generate the initial population of individuals randomly First generation Step Two Evaluate the fitness of each individual in that population Step Three Repeat the following regenerational steps until termination time limit sufficient fitness achieved etc Select the bestfit individuals for reproduction Parents Breed new individuals through crossover and mutation operations to give birth to offspring Evaluate the individual fitness of new individuals Replace leastfit population with new individuals The individuals in this case are randomlygenerated computer programs which are then tested against a fitness function The Wikipedia page for Genetic Programming claimed that these programs are usually represented by tree structures though there has been some experiments in using nontree structures as well
,1.0,"<p>As far as I know MDP are independent from the past. But the definition says that the same policy should always take the same action depending on the state.</p>

<p>What if I define my state as the current ""main"" state + previous decisions?</p>

<p>For Example in Poker the ""main"" state would be my cards and the pot + all previous information about the game.</p>

<p>Would this still be a MDP or not? </p>
",,0,2017-03-11T16:56:49.653,,2967,2017-03-11T20:38:34.017,2017-03-11T17:11:20.420,,4550.0,,4550.0,,1,0,<definitions><markov-chain>,Can an Markov decision process be dependent on the past?,15.0,82.65,7.42,7.49,0.0,0.0,11.0,As far as I know MDP are independent from the past But the definition says that the same policy should always take the same action depending on the state What if I define my state as the current main state previous decisions For Example in Poker the main state would be my cards and the pot all previous information about the game Would this still be a MDP or not
,,"<p>It's not totally clear from your description, but it sounds like you may be onto something like an <a href=""https://en.wikipedia.org/wiki/Additive_Markov_chain"" rel=""nofollow noreferrer"">Additive Markov Chain</a>.</p>
",,0,2017-03-11T20:38:34.017,,2969,2017-03-11T20:38:34.017,,,,,33.0,2967.0,2,0,,,,58.62,10.62,9.19,0.0,0.0,3.0,Its not totally clear from your description but it sounds like you may be onto something like an Additive Markov Chain
,,"<blockquote>
  <p>Does Artificial Intelligence write its own code and then execute it?
  If so does it create separate functions(for each purpose) for its
  code?</p>
</blockquote>

<p>First of all, ""Artificial Intelligence"" isn't a singular ""thing"" where it really makes sense to ask questions like ""Does artificial intelligence xxx?""  The answer to questions phrased like this can generally be any of ""Yes"", ""no"", ""maybe"", ""we don't know"" or ""all of the above"".</p>

<blockquote>
  <p>How does learning get implemented in artificial intelligence?</p>
</blockquote>

<p>The comment above aside, as others have already mentioned, self-modifying code <em>is</em> one of the (many) techniques used in some applications of what can be called ""AI"".  So in that sense, the answer to your question in a very general sense is ""yes"".</p>

<p>But these days, the stuff that is state-of-the-art in machine learning / AI is usually more about finding sets of weights for functions that match a pattern or whatever.  In Neural Networks, for example, the NN isn't writing any code, it's just running through an algorithm that incrementally changes some weights (or coefficients) such that when you enter a certain input, you get an output that's close to the desired output.  </p>

<p>And then you have approaches like using a Genetic Algorithm to evolve the weights in the NN, as opposed to using back-propagation.  To the extent that GA's are a little closer to the idea of ""an AI coding itself"" (although not exactly), you could kinda sorta consider that an example of what you're asking about.</p>

<blockquote>
  <p>Is there a specific flowchart to describe artificial intelligence</p>
</blockquote>

<p>Not even close.  </p>

<p>If you're interested in exploring all of this further, I'd suggest reading the book <em>The Master Algorithm</em> by Pedro Domingos, and take Andrew Ng's MOOC on Machine Learning on Coursera.  Then pick up a copy of Russell &amp; Norvig's <em>Artificial Intelligence: A Modern Approach</em> and dig in.</p>
",,3,2017-03-11T23:25:22.243,,2970,2017-03-11T23:25:22.243,,,,,33.0,2964.0,2,0,,,,49.55,11.95,10.08,0.0,0.0,80.0,Does Artificial Intelligence write its own code and then execute it If so does it create separate functionsfor each purpose for its code First of all Artificial Intelligence isnt a singular thing where it really makes sense to ask questions like Does artificial intelligence xxx The answer to questions phrased like this can generally be any of Yes no maybe we dont know or all of the above How does learning get implemented in artificial intelligence The comment above aside as others have already mentioned selfmodifying code is one of the many techniques used in some applications of what can be called AI So in that sense the answer to your question in a very general sense is yes But these days the stuff that is stateoftheart in machine learning AI is usually more about finding sets of weights for functions that match a pattern or whatever In Neural Networks for example the NN isnt writing any code its just running through an algorithm that incrementally changes some weights or coefficients such that when you enter a certain input you get an output thats close to the desired output And then you have approaches like using a Genetic Algorithm to evolve the weights in the NN as opposed to using backpropagation To the extent that GAs are a little closer to the idea of an AI coding itself although not exactly you could kinda sorta consider that an example of what youre asking about Is there a specific flowchart to describe artificial intelligence Not even close If youre interested in exploring all of this further Id suggest reading the book The Master Algorithm by Pedro Domingos and take Andrew Ngs MOOC on Machine Learning on Coursera Then pick up a copy of Russell amp Norvigs Artificial Intelligence A Modern Approach and dig in
,,"<p>I would say that we're not even close to a ""real"" human-like AI. For all the wonderful things that applications like Siri, Cortana and the like can do, they're actually really dumb compared to even a child.  Of course part of that, IMO, is that most AI applications are not embodied and don't experience the world the way humans do.  So if you show an AI a video of a dog walking behind a table and briefly disappearing from the frame, it has pretty much no ability to apply ""common sense"" and know that the dog will reappear in a few seconds, and that if it does't reappear, it's probably because it found its favorite toy on the floor behind the table.  </p>

<p>For some examples of the kinds of ""easy"" questions that computers still don't do well at, check out the <a href=""https://en.wikipedia.org/wiki/Winograd_Schema_Challenge"" rel=""nofollow noreferrer"">Winograd Schema Challenge</a>.  You might also find this page interesting:  <a href=""http://www-formal.stanford.edu/leora/commonsense/"" rel=""nofollow noreferrer"">http://www-formal.stanford.edu/leora/commonsense/</a></p>
",,0,2017-03-11T23:39:05.963,,2971,2017-03-11T23:39:05.963,,,,,33.0,2917.0,2,0,,,,62.72,10.68,9.05,0.0,0.0,36.0,I would say that were not even close to a real humanlike AI For all the wonderful things that applications like Siri Cortana and the like can do theyre actually really dumb compared to even a child Of course part of that IMO is that most AI applications are not embodied and dont experience the world the way humans do So if you show an AI a video of a dog walking behind a table and briefly disappearing from the frame it has pretty much no ability to apply common sense and know that the dog will reappear in a few seconds and that if it doest reappear its probably because it found its favorite toy on the floor behind the table For some examples of the kinds of easy questions that computers still dont do well at check out the Winograd Schema Challenge You might also find this page interesting httpwwwformalstanfordeduleoracommonsense
,,"<p>Communications is expensive. It requires a communication channel, a protocol and of course time. Communications is also limited to the expressivity defined by the protocol. Note also that agents may compete over resources, or may have contradictory goals, so in some situations they may try to mislead each other.</p>

<p>On the other hand - computational power and memory are limited, so multiple agents may solve computation-intensive or memory-intensive problems together better/faster than each single agent can. Different agents may have different sensors, and mobile agents may have information about different parts of their realm, so by sharing knowledge they may have more complete information and make better decisions. Goals may also be time-bounded, and rewards may be time-dependent. Sometimes working together means greater rewards.</p>

<p>In summary, there may be situations where collaboration is beneficial, and there may be some situations where collaboration is essential to achieve one's goal or to achieve a common goal.</p>
",,0,2017-03-12T05:29:01.003,,2972,2017-03-12T05:29:01.003,,,,,3138.0,2957.0,2,0,,,,28.84,15.54,10.36,0.0,0.0,25.0,Communications is expensive It requires a communication channel a protocol and of course time Communications is also limited to the expressivity defined by the protocol Note also that agents may compete over resources or may have contradictory goals so in some situations they may try to mislead each other On the other hand computational power and memory are limited so multiple agents may solve computationintensive or memoryintensive problems together betterfaster than each single agent can Different agents may have different sensors and mobile agents may have information about different parts of their realm so by sharing knowledge they may have more complete information and make better decisions Goals may also be timebounded and rewards may be timedependent Sometimes working together means greater rewards In summary there may be situations where collaboration is beneficial and there may be some situations where collaboration is essential to achieve ones goal or to achieve a common goal
,1.0,"<p>I'm looking for AI systems or natural language processors, that use in the classification and interrelation of notions/objects some philosophical system, like basic laws of logic, Kantian, empiricism etc. </p>

<p>Also i have read about goal-seeking procedures. Are these based on psychology fields and some particular psychology theory or these are ad hoc experiments, with only general terms applied?</p>
",,0,2017-03-12T18:05:51.793,,2973,2017-03-12T23:25:04.883,,,,,5977.0,,1,4,<classification>,What are examples of AI that use philosophy derived ontologies?,26.0,26.51,16.3,11.67,0.0,0.0,11.0,Im looking for AI systems or natural language processors that use in the classification and interrelation of notionsobjects some philosophical system like basic laws of logic Kantian empiricism etc Also i have read about goalseeking procedures Are these based on psychology fields and some particular psychology theory or these are ad hoc experiments with only general terms applied
,1.0,"<p>I'm using a NN created with CNTK's SimpleNetworkBuilder to make choices (specifically in board games). I specified ReLU as the layer type, so outputs can be arbitrary numbers.</p>

<p>When evaluating a custom set of features, getting the ""choice"" of the function/model is simple: Look for the output signal with the highest value. However, there are times when I wish to introduce some randomness and assign probabilities to each output signal, then select the choice based on each output's probability.</p>

<p>Currently, what I'm doing is manually normalizing all the output using a sigmoid function specified here: <a href=""https://en.wikipedia.org/wiki/Logistic_function"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Logistic_function</a>
Then, I multiply them all by a scalar such that the sum total of all outputs is 1.</p>

<p>At this point, I pick a random number 0..1, and see where along the map it falls; that is my selected choice.</p>

<p>What I'd like to know is, is there a better way?</p>
",,1,2017-03-12T18:30:34.533,,2974,2017-03-13T16:32:33.657,,,,,3702.0,,1,0,<neural-networks>,Assigning probability to output of a ReLU network,12.0,63.39,11.83,9.28,0.0,0.0,39.0,Im using a NN created with CNTKs SimpleNetworkBuilder to make choices specifically in board games I specified ReLU as the layer type so outputs can be arbitrary numbers When evaluating a custom set of features getting the choice of the functionmodel is simple Look for the output signal with the highest value However there are times when I wish to introduce some randomness and assign probabilities to each output signal then select the choice based on each outputs probability Currently what Im doing is manually normalizing all the output using a sigmoid function specified here httpsenwikipediaorgwikiLogisticfunction Then I multiply them all by a scalar such that the sum total of all outputs is 1 At this point I pick a random number 01 and see where along the map it falls that is my selected choice What Id like to know is is there a better way
,,"<p>Excellent question!  I'm currently working on the pre-Socratics as the most basic philosophies for first principles (These philosophers are intriguing for their simplicity and universality, and the ""dawn of consciousness"", in some conceptions, may be ascribed to the Classical Era.  Linguistically, ancient Greek is fundamental to meaning in the West.)</p>

<p>However, I was pointed at this very interesting blog post ""<a href=""https://arimo.com/featured/2015/algorithms-of-the-mind/"" rel=""nofollow noreferrer"">Algorithms of the Mind</a>"" which has some useful links, and discusses Kant, which you may find useful. </p>
",,0,2017-03-12T23:25:04.883,,2975,2017-03-12T23:25:04.883,,,,,1671.0,2973.0,2,2,,,,36.93,15.61,10.86,0.0,0.0,19.0,Excellent question Im currently working on the preSocratics as the most basic philosophies for first principles These philosophers are intriguing for their simplicity and universality and the dawn of consciousness in some conceptions may be ascribed to the Classical Era Linguistically ancient Greek is fundamental to meaning in the West However I was pointed at this very interesting blog post Algorithms of the Mind which has some useful links and discusses Kant which you may find useful
,0.0,"<p>I have come across this domain via this Wikipedia article: <a href=""https://en.wikipedia.org/wiki/General_game_playing"" rel=""nofollow noreferrer"">General game playing</a></p>

<p>So, where are we when it comes to general game playing AI? (The wiki article doesn't mention the recent advances and the achievements of this domain of research, except the annual games results.)</p>

<p>PS: I understand that this is a General project of the Stanford Logic Group of Stanford University, California. But since then, it has become an area of research in the domain of AI.</p>
",,0,2017-03-13T12:46:50.537,,2976,2017-03-13T12:46:50.537,,,,,101.0,,1,0,<research><gaming>,What research has been done in the domain of “General game playing”?,5.0,59.84,10.62,10.02,0.0,0.0,13.0,I have come across this domain via this Wikipedia article General game playing So where are we when it comes to general game playing AI The wiki article doesnt mention the recent advances and the achievements of this domain of research except the annual games results PS I understand that this is a General project of the Stanford Logic Group of Stanford University California But since then it has become an area of research in the domain of AI
,,"<p>I would just skip the sigmoid function step.</p>

<p>Consider these two scenarios with three choices with the associated values:
0.1,10,100 or 1,100,1000
Given that these scenarios are equivalent in their relativ values, your probabilities should be assigned in the same way. But if you throw in a sigmoid function, the difference between 10 and 100 will be bigger than between 100 and 1000. </p>

<p>Just normalise the values, so that they sum to 1. In that case the choice with a value ten times higher than the value of another choice will be picked ten times as often. Makes sense to me. </p>
",,0,2017-03-13T16:32:33.657,,2977,2017-03-13T16:32:33.657,,,,,2227.0,2974.0,2,0,,,,82.24,9.33,8.1,0.0,0.0,15.0,I would just skip the sigmoid function step Consider these two scenarios with three choices with the associated values 0110100 or 11001000 Given that these scenarios are equivalent in their relativ values your probabilities should be assigned in the same way But if you throw in a sigmoid function the difference between 10 and 100 will be bigger than between 100 and 1000 Just normalise the values so that they sum to 1 In that case the choice with a value ten times higher than the value of another choice will be picked ten times as often Makes sense to me
